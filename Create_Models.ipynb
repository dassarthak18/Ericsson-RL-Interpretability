{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dd7c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/usr/local/lib/python3.9/dist-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/usr/local/lib/python3.9/dist-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/usr/local/lib/python3.9/dist-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/usr/local/lib/python3.9/dist-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/usr/local/lib/python3.9/dist-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from src import algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191863ca",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 DQN (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6468a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   47/10000 [..............................] - ETA: 10s - reward: -1.9574  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 105s 11ms/step - reward: 1.6893\n",
      "1131 episodes - episode_reward: 14.915 [-966.000, 30.000] - loss: 0.452 - mse: 80.894 - mean_q: 6.795\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 2.4517\n",
      "1161 episodes - episode_reward: 21.124 [-118.000, 30.000] - loss: 0.304 - mse: 109.816 - mean_q: 11.817\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 1.8409\n",
      "864 episodes - episode_reward: 21.325 [-330.000, 30.000] - loss: 0.477 - mse: 125.841 - mean_q: 12.358\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 2.4800\n",
      "986 episodes - episode_reward: 25.143 [-299.000, 30.000] - loss: 0.504 - mse: 137.964 - mean_q: 12.948\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: 2.3690\n",
      "done, took 537.786 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v0_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660c616",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 DQN (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4daa609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 * \n",
      "0 * 0 0 \n",
      "G 0 * 0 \n",
      "0 X 0 * \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 106s 11ms/step - reward: -1.3004\n",
      "297 episodes - episode_reward: -43.680 [-1019.000, 52.000] - loss: 8.719 - mse: 654.463 - mean_q: -24.026\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.2156\n",
      "364 episodes - episode_reward: -29.657 [-2438.000, 54.000] - loss: 17.239 - mse: 954.275 - mean_q: -29.808\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -0.7184\n",
      "521 episodes - episode_reward: -16.445 [-1987.000, 55.000] - loss: 17.983 - mse: 823.085 - mean_q: -27.236\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: -0.5394\n",
      "460 episodes - episode_reward: -11.713 [-504.000, 53.000] - loss: 18.218 - mse: 656.603 - mean_q: -23.623\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 120s 12ms/step - reward: -0.5070\n",
      "done, took 570.374 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v1_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f91bfe",
   "metadata": {},
   "source": [
    "## CartPole-v1 DQN (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a50c64fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   41/10000 [..............................] - ETA: 12s - reward: 1.0000  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 104s 10ms/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.514 [9.000, 235.000] - loss: 1.258 - mse: 798.311 - mean_q: 35.024\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 1.0000\n",
      "54 episodes - episode_reward: 183.667 [15.000, 401.000] - loss: 2.547 - mse: 2514.995 - mean_q: 67.145\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 105s 11ms/step - reward: 1.0000\n",
      "35 episodes - episode_reward: 288.286 [162.000, 500.000] - loss: 4.587 - mse: 3272.027 - mean_q: 78.168\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 1.0000\n",
      "31 episodes - episode_reward: 319.613 [162.000, 500.000] - loss: 4.117 - mse: 4256.138 - mean_q: 88.931\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 1.0000\n",
      "done, took 552.835 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:12:52.275044: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/CartPole_v1_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/CartPole_v1_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-green",
   "metadata": {},
   "source": [
    "## FrozenLake-v1 DQN (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incoming-melissa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/lib/python3/dist-packages/pygame/_numpysurfarray.py:59: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if hasattr(numpy, type_name):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   79/10000 [..............................] - ETA: 12s - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 98s 10ms/step - reward: 3.0000e-04\n",
      "373 episodes - episode_reward: 0.008 [0.000, 1.000] - loss: 0.005 - mse: 0.019 - mean_q: 0.146 - prob: 0.333\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0022\n",
      "600 episodes - episode_reward: 0.037 [0.000, 1.000] - loss: 0.000 - mse: 0.001 - mean_q: 0.048 - prob: 0.333\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0072\n",
      "633 episodes - episode_reward: 0.114 [0.000, 1.000] - loss: 0.001 - mse: 0.011 - mean_q: 0.128 - prob: 0.333\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: 0.0096\n",
      "589 episodes - episode_reward: 0.163 [0.000, 1.000] - loss: 0.002 - mse: 0.016 - mean_q: 0.156 - prob: 0.333\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0103\n",
      "done, took 519.396 seconds\n",
      "INFO:tensorflow:Assets written to: models/FrozenLake_v1_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/FrozenLake_v1_DQN_Eps_50000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
