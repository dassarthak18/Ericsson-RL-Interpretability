{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dd7c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 22:14:59.215179: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:14:59.215221: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from src import algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af40dc",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747f179d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 4:01 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 22:15:01.965801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-03 22:15:01.967042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.967232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.967430: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.967618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.967805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.967988: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.968172: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.968356: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-03 22:15:01.968387: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-03 22:15:01.970116: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.2687\n",
      "400 episodes - episode_reward: -31.692 [-453.000, 13.000] - mean_best_reward: 9.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1978\n",
      "487 episodes - episode_reward: -24.462 [-480.000, 15.000] - mean_best_reward: 8.800\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1401\n",
      "595 episodes - episode_reward: -19.267 [-444.000, 22.000] - mean_best_reward: 10.417\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1534\n",
      "579 episodes - episode_reward: -19.910 [-347.000, 19.000] - mean_best_reward: 9.625\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1744\n",
      "527 episodes - episode_reward: -22.288 [-479.000, 15.000] - mean_best_reward: 8.250\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1501\n",
      "573 episodes - episode_reward: -20.105 [-490.000, 22.000] - mean_best_reward: 8.625\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0494\n",
      "721 episodes - episode_reward: -14.556 [-397.000, 25.000] - mean_best_reward: 10.286\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0780\n",
      "669 episodes - episode_reward: -16.045 [-447.000, 21.000] - mean_best_reward: 9.107\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1593\n",
      "592 episodes - episode_reward: -19.620 [-499.000, 15.000] - mean_best_reward: 9.500\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1205\n",
      "done, took 132.587 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 22:17:15.357888: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_CEM_100000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.cem(env)\n",
    "model.save(\"models/SimpleMaze_v0_CEM_100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910681a2",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0_new CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a79ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   95/10000 [..............................] - ETA: 10s - reward: -1.6842 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.2759\n",
      "417 episodes - episode_reward: -30.561 [-839.000, 15.000] - mean_best_reward: 9.214\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1769\n",
      "537 episodes - episode_reward: -21.831 [-625.000, 15.000] - mean_best_reward: 9.273\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1514\n",
      "553 episodes - episode_reward: -20.931 [-380.000, 16.000] - mean_best_reward: 9.591\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.1015\n",
      "597 episodes - episode_reward: -18.446 [-446.000, 27.000] - mean_best_reward: 9.917\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1420\n",
      "556 episodes - episode_reward: -20.538 [-486.000, 20.000] - mean_best_reward: 11.045\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1962\n",
      "502 episodes - episode_reward: -23.779 [-490.000, 18.000] - mean_best_reward: 8.500\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0923\n",
      "625 episodes - episode_reward: -17.523 [-500.000, 16.000] - mean_best_reward: 9.167\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.0201\n",
      "621 episodes - episode_reward: -16.398 [-486.000, 17.000] - mean_best_reward: 8.923\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.2884\n",
      "429 episodes - episode_reward: -29.930 [-969.000, 21.000] - mean_best_reward: 11.438\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.2293\n",
      "done, took 130.985 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_new_CEM_100000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0_new import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.cem(env)\n",
    "model.save(\"models/SimpleMaze_v0_new_CEM_100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c1d3d",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce97284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* 0 0 0 \n",
      "0 G * 0 \n",
      "* 0 * X \n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  146/10000 [..............................] - ETA: 10s - reward: -1.6301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5306\n",
      "173 episodes - episode_reward: -87.329 [-1239.000, 22.000] - mean_best_reward: 10.167\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.5499\n",
      "166 episodes - episode_reward: -94.392 [-911.000, 14.000] - mean_best_reward: 8.667\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5294\n",
      "173 episodes - episode_reward: -88.601 [-1112.000, 27.000] - mean_best_reward: 9.625\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5141\n",
      "182 episodes - episode_reward: -82.203 [-1309.000, 21.000] - mean_best_reward: 10.667\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6324\n",
      "130 episodes - episode_reward: -126.054 [-1472.000, 13.000] - mean_best_reward: 6.667\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6428\n",
      "127 episodes - episode_reward: -130.228 [-1977.000, 18.000] - mean_best_reward: 12.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5088\n",
      "171 episodes - episode_reward: -87.474 [-951.000, 20.000] - mean_best_reward: 10.833\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.5570\n",
      "166 episodes - episode_reward: -87.608 [-901.000, 15.000] - mean_best_reward: 11.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.6184\n",
      "153 episodes - episode_reward: -113.333 [-1868.000, 18.000] - mean_best_reward: 11.333\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5764\n",
      "done, took 115.490 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_CEM_100000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.cem(env)\n",
    "model.save(\"models/SimpleMaze_v1_CEM_100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a53d36",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1_new CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58df6e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "0 0 * * \n",
      "* G * 0 \n",
      "X 0 0 0 \n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   78/10000 [..............................] - ETA: 18s - reward: -0.8846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5476\n",
      "149 episodes - episode_reward: -97.718 [-1213.000, 19.000] - mean_best_reward: 11.250\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.7071\n",
      "95 episodes - episode_reward: -188.516 [-3119.000, 31.000] - mean_best_reward: 14.750\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.5733\n",
      "150 episodes - episode_reward: -105.080 [-1586.000, 15.000] - mean_best_reward: 15.167\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6607\n",
      "125 episodes - episode_reward: -133.248 [-1549.000, 10.000] - mean_best_reward: 8.667\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6391\n",
      "128 episodes - episode_reward: -126.484 [-1571.000, 17.000] - mean_best_reward: 12.500\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6099\n",
      "133 episodes - episode_reward: -122.008 [-1157.000, 19.000] - mean_best_reward: 8.833\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6266\n",
      "134 episodes - episode_reward: -121.970 [-1350.000, 16.000] - mean_best_reward: 11.333\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6267\n",
      "126 episodes - episode_reward: -127.167 [-2471.000, 18.000] - mean_best_reward: 7.750\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6640\n",
      "122 episodes - episode_reward: -137.844 [-1665.000, 24.000] - mean_best_reward: 10.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6037\n",
      "done, took 114.554 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_new_CEM_100000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1_new import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.cem(env)\n",
    "model.save(\"models/SimpleMaze_v1_new_CEM_100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191863ca",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6468a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   41/10000 [..............................] - ETA: 12s - reward: -1.0244  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 100s 10ms/step - reward: -0.5264\n",
      "657 episodes - episode_reward: -8.021 [-326.000, 24.000] - loss: 4.554 - mse: 5.415 - mean_q: 1.136\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: -0.2747\n",
      "606 episodes - episode_reward: -4.530 [-121.000, 23.000] - loss: 5.061 - mse: 4.229 - mean_q: 0.851\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: -0.5068\n",
      "501 episodes - episode_reward: -10.044 [-247.000, 25.000] - loss: 5.187 - mse: 4.256 - mean_q: 0.920\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: -0.0974\n",
      "792 episodes - episode_reward: -1.250 [-179.000, 23.000] - loss: 5.246 - mse: 4.896 - mean_q: 0.322\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -0.2941\n",
      "done, took 524.035 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_DQN_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v0_DQN_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc3541",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0_new DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85b2588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 109s 11ms/step - reward: -0.4136\n",
      "687 episodes - episode_reward: -5.924 [-328.000, 27.000] - loss: 4.691 - mse: 6.440 - mean_q: 1.598\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -0.3752\n",
      "559 episodes - episode_reward: -6.828 [-222.000, 26.000] - loss: 5.015 - mse: 4.440 - mean_q: 0.989\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: -0.3654\n",
      "621 episodes - episode_reward: -5.886 [-411.000, 26.000] - loss: 5.134 - mse: 4.777 - mean_q: 0.784\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 130s 13ms/step - reward: -0.4421\n",
      "606 episodes - episode_reward: -7.305 [-309.000, 26.000] - loss: 5.105 - mse: 4.942 - mean_q: 0.925\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 135s 14ms/step - reward: -0.4658\n",
      "done, took 594.932 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_new_DQN_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0_new import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v0_new_DQN_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660c616",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4daa609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "* 0 0 * \n",
      "0 G 0 0 \n",
      "0 * 0 0 \n",
      "0 * 0 X \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 122s 12ms/step - reward: -1.1144\n",
      "309 episodes - episode_reward: -36.061 [-527.000, 23.000] - loss: 7.080 - mse: 191.571 - mean_q: -14.562\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -0.9397\n",
      "370 episodes - episode_reward: -25.432 [-275.000, 29.000] - loss: 8.233 - mse: 209.775 - mean_q: -15.789\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -0.6880\n",
      "477 episodes - episode_reward: -14.390 [-175.000, 32.000] - loss: 6.512 - mse: 97.912 - mean_q: -10.175\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -0.7871\n",
      "431 episodes - episode_reward: -18.304 [-565.000, 23.000] - loss: 5.927 - mse: 62.561 - mean_q: -7.961\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 118s 12ms/step - reward: -0.7189\n",
      "done, took 580.132 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_DQN_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v1_DQN_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e667aa5",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1_new DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4d45a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 * * \n",
      "0 * 0 0 \n",
      "0 0 X * \n",
      "0 G 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 108s 11ms/step - reward: -1.1454\n",
      "312 episodes - episode_reward: -36.593 [-619.000, 21.000] - loss: 7.127 - mse: 208.477 - mean_q: -14.903\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: -0.9145\n",
      "387 episodes - episode_reward: -23.760 [-473.000, 21.000] - loss: 7.616 - mse: 180.696 - mean_q: -14.446\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -0.7424\n",
      "457 episodes - episode_reward: -16.230 [-313.000, 27.000] - loss: 6.597 - mse: 105.224 - mean_q: -10.690\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -0.7732\n",
      "419 episodes - episode_reward: -18.370 [-271.000, 30.000] - loss: 6.150 - mse: 72.678 - mean_q: -8.680\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: -0.6761\n",
      "done, took 551.138 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_new_DQN_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1_new import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v1_new_DQN_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4245b",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d6fb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 93s 9ms/step - reward: -1.8679\n",
      "69 episodes - episode_reward: -21.870 [-515.000, 9.000] - loss: 1.085 - mse: 26443.063 - mean_q: -168.262\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -2.0000\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -2.0000\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -2.0000\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -2.0000\n",
      "done, took 461.073 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_SARSA_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.sarsa(env)\n",
    "model.save(\"models/SimpleMaze_v0_SARSA_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2324d7",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0_new SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cb98946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 95s 10ms/step - reward: -1.8490\n",
      "85 episodes - episode_reward: -32.588 [-1318.000, 13.000] - loss: 3.249 - mse: 27653.667 - mean_q: -166.032\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 94s 9ms/step - reward: -2.0000\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 94s 9ms/step - reward: -1.9997\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 97s 10ms/step - reward: -2.0000\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -2.0000\n",
      "done, took 479.755 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_new_SARSA_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0_new import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.sarsa(env)\n",
    "model.save(\"models/SimpleMaze_v0_new_SARSA_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980eade1",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "617acce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 * * 0 \n",
      "0 0 * 0 \n",
      "0 X * 0 \n",
      "G 0 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 103s 10ms/step - reward: -1.9405\n",
      "25 episodes - episode_reward: -152.600 [-1988.000, 7.000] - loss: 3.165 - mse: 27479.473 - mean_q: -171.351\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: -1.9977\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 105s 11ms/step - reward: -2.0000\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: -2.0000\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: -2.0000\n",
      "done, took 512.253 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_SARSA_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.sarsa(env)\n",
    "model.save(\"models/SimpleMaze_v1_SARSA_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62737bad",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1_new SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1273c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 * 0 \n",
      "* 0 * G \n",
      "0 * 0 0 \n",
      "0 X 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 103s 10ms/step - reward: -1.9335\n",
      "25 episodes - episode_reward: -90.440 [-1400.000, 14.000] - loss: 2.399 - mse: 28587.946 - mean_q: -180.569\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: -1.9997\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: -2.0000\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 102s 10ms/step - reward: -2.0000\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 105s 11ms/step - reward: -2.0000\n",
      "done, took 512.664 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_new_SARSA_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1_new import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.sarsa(env)\n",
    "model.save(\"models/SimpleMaze_v1_new_SARSA_50000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
