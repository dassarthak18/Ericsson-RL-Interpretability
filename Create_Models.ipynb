{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7dd7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from src import algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af40dc",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 CEM (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747f179d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 15:45:55.115842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 15:45:55.117096: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.117444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.117716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.117975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.118230: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.118485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.118743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.119064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-24 15:45:55.119107: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-24 15:45:55.120222: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 592us/step - reward: -1.1353\n",
      "488 episodes - episode_reward: -23.273 [-313.000, 27.000] - mean_best_reward: 11.438\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 10s 968us/step - reward: -1.0666\n",
      "606 episodes - episode_reward: -17.594 [-374.000, 20.000] - mean_best_reward: 10.917\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1343\n",
      "586 episodes - episode_reward: -19.177 [-315.000, 24.000] - mean_best_reward: 10.708\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.1223\n",
      "637 episodes - episode_reward: -17.786 [-469.000, 17.000] - mean_best_reward: 10.038\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -0.9797\n",
      "686 episodes - episode_reward: -14.281 [-192.000, 22.000] - mean_best_reward: 9.964\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0755\n",
      "654 episodes - episode_reward: -16.414 [-295.000, 19.000] - mean_best_reward: 10.731\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0065\n",
      "696 episodes - episode_reward: -14.487 [-313.000, 19.000] - mean_best_reward: 11.357\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0134\n",
      "705 episodes - episode_reward: -14.210 [-469.000, 25.000] - mean_best_reward: 10.071\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 14s 1ms/step - reward: -1.0172\n",
      "648 episodes - episode_reward: -15.816 [-230.000, 21.000] - mean_best_reward: 12.115\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 13s 1ms/step - reward: -1.0882\n",
      "done, took 125.389 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 15:48:04.728762: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_CEM_Eps_100000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.cem(env)\n",
    "model.save(\"models/SimpleMaze_v0_CEM_Eps_100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c1d3d",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 CEM (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce97284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "X * 0 0 \n",
      "* 0 0 0 \n",
      "G * 0 0 \n",
      "0 0 0 * \n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   99/10000 [..............................] - ETA: 15s - reward: -1.5051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.6247\n",
      "143 episodes - episode_reward: -104.601 [-1727.000, 27.000] - mean_best_reward: 1.750\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6024\n",
      "126 episodes - episode_reward: -134.563 [-1553.000, 15.000] - mean_best_reward: 6.500\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.4877\n",
      "193 episodes - episode_reward: -78.938 [-1877.000, 43.000] - mean_best_reward: 10.250\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5110\n",
      "176 episodes - episode_reward: -85.602 [-1229.000, 32.000] - mean_best_reward: 10.833\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5779\n",
      "147 episodes - episode_reward: -107.633 [-1063.000, 12.000] - mean_best_reward: 10.500\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.5792\n",
      "128 episodes - episode_reward: -121.930 [-951.000, 11.000] - mean_best_reward: 10.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6421\n",
      "135 episodes - episode_reward: -121.511 [-2085.000, 10.000] - mean_best_reward: 20.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.5291\n",
      "162 episodes - episode_reward: -95.099 [-1065.000, 11.000] - mean_best_reward: 12.875\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 11s 1ms/step - reward: -1.6277\n",
      "135 episodes - episode_reward: -119.526 [-1581.000, 37.000] - mean_best_reward: 12.250\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 12s 1ms/step - reward: -1.6633\n",
      "done, took 115.663 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_CEM_Eps_100000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.cem(env)\n",
    "model.save(\"models/SimpleMaze_v1_CEM_Eps_100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191863ca",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 DQN (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6468a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   47/10000 [..............................] - ETA: 10s - reward: -1.9574  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 105s 11ms/step - reward: 1.6893\n",
      "1131 episodes - episode_reward: 14.915 [-966.000, 30.000] - loss: 0.452 - mse: 80.894 - mean_q: 6.795\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 2.4517\n",
      "1161 episodes - episode_reward: 21.124 [-118.000, 30.000] - loss: 0.304 - mse: 109.816 - mean_q: 11.817\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 1.8409\n",
      "864 episodes - episode_reward: 21.325 [-330.000, 30.000] - loss: 0.477 - mse: 125.841 - mean_q: 12.358\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 2.4800\n",
      "986 episodes - episode_reward: 25.143 [-299.000, 30.000] - loss: 0.504 - mse: 137.964 - mean_q: 12.948\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: 2.3690\n",
      "done, took 537.786 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v0_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d8991",
   "metadata": {},
   "source": [
    "## SimpleMaze_v0 DQN (Boltzmann Q Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6150616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 0 \n",
      "* X 0 * \n",
      "0 G * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 28:04 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 108s 11ms/step - reward: 2.1001\n",
      "980 episodes - episode_reward: 21.420 [-209.000, 30.000] - loss: 0.996 - mse: 74.903 - mean_q: 8.307\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: 2.8597\n",
      "1015 episodes - episode_reward: 28.174 [-84.000, 30.000] - loss: 0.432 - mse: 112.106 - mean_q: 13.771\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: 2.7883\n",
      "1000 episodes - episode_reward: 27.883 [-66.000, 30.000] - loss: 0.261 - mse: 119.216 - mean_q: 14.463\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 2.5140\n",
      "977 episodes - episode_reward: 25.709 [-10.000, 30.000] - loss: 0.305 - mse: 122.367 - mean_q: 14.560\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 116s 12ms/step - reward: 1.5643\n",
      "done, took 567.721 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v0_DQN_BQ_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v0 import SimpleMaze\n",
    "env = SimpleMaze(True)\n",
    "model, agent, train_rewards = algo.dqn(env,50000,3)\n",
    "model.save(\"models/SimpleMaze_v0_DQN_BQ_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660c616",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 DQN (Epsilon-Greedy Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4daa609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 * \n",
      "0 * 0 0 \n",
      "G 0 * 0 \n",
      "0 X 0 * \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 106s 11ms/step - reward: -1.3004\n",
      "297 episodes - episode_reward: -43.680 [-1019.000, 52.000] - loss: 8.719 - mse: 654.463 - mean_q: -24.026\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.2156\n",
      "364 episodes - episode_reward: -29.657 [-2438.000, 54.000] - loss: 17.239 - mse: 954.275 - mean_q: -29.808\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -0.7184\n",
      "521 episodes - episode_reward: -16.445 [-1987.000, 55.000] - loss: 17.983 - mse: 823.085 - mean_q: -27.236\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: -0.5394\n",
      "460 episodes - episode_reward: -11.713 [-504.000, 53.000] - loss: 18.218 - mse: 656.603 - mean_q: -23.623\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 120s 12ms/step - reward: -0.5070\n",
      "done, took 570.374 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/SimpleMaze_v1_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d9dcd",
   "metadata": {},
   "source": [
    "## SimpleMaze_v1 DQN (Boltzmann Q Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5aaad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "* X 0 0 \n",
      "0 0 0 G \n",
      "* 0 * 0 \n",
      "0 * 0 0 \n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 108s 11ms/step - reward: -0.3359\n",
      "336 episodes - episode_reward: -10.024 [-1356.000, 55.000] - loss: 8.700 - mse: 40.697 - mean_q: -0.306\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: 0.0759\n",
      "321 episodes - episode_reward: 2.371 [-1348.000, 53.000] - loss: 9.234 - mse: 37.307 - mean_q: 2.882\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: 0.1244\n",
      "321 episodes - episode_reward: 3.854 [-392.000, 54.000] - loss: 10.088 - mse: 41.714 - mean_q: 3.542\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: 0.1962\n",
      "338 episodes - episode_reward: 5.799 [-348.000, 55.000] - loss: 10.585 - mse: 42.778 - mean_q: 2.943\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: 0.3414\n",
      "done, took 559.953 seconds\n",
      "INFO:tensorflow:Assets written to: models/SimpleMaze_v1_DQN_BQ_50000/assets\n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)\n",
    "model, agent, train_rewards = algo.dqn(env,50000,3)\n",
    "model.save(\"models/SimpleMaze_v1_DQN_BQ_50000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f91bfe",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e73eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:\n",
      "0 0 0 G \n",
      "X 0 0 0 \n",
      "* 0 * 0 \n",
      "* * 0 0 \n"
     ]
    }
   ],
   "source": [
    "from env.SimpleMaze_v1 import SimpleMaze\n",
    "env = SimpleMaze(4,4,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3479ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"models/SimpleMaze_v1_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a50c64fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   41/10000 [..............................] - ETA: 12s - reward: 1.0000  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 104s 10ms/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.514 [9.000, 235.000] - loss: 1.258 - mse: 798.311 - mean_q: 35.024\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 1.0000\n",
      "54 episodes - episode_reward: 183.667 [15.000, 401.000] - loss: 2.547 - mse: 2514.995 - mean_q: 67.145\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 105s 11ms/step - reward: 1.0000\n",
      "35 episodes - episode_reward: 288.286 [162.000, 500.000] - loss: 4.587 - mse: 3272.027 - mean_q: 78.168\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 1.0000\n",
      "31 episodes - episode_reward: 319.613 [162.000, 500.000] - loss: 4.117 - mse: 4256.138 - mean_q: 88.931\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: 1.0000\n",
      "done, took 552.835 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:12:52.275044: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/CartPole_v1_DQN_Eps_50000/assets\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "model, agent, train_rewards = algo.dqn(env)\n",
    "model.save(\"models/CartPole_v1_DQN_Eps_50000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c179aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 500 timesteps with reward 500.0.\n",
      "Episode 2 finished after 500 timesteps with reward 500.0.\n",
      "Episode 3 finished after 500 timesteps with reward 500.0.\n",
      "Episode 4 finished after 500 timesteps with reward 500.0.\n",
      "Episode 5 finished after 500 timesteps with reward 500.0.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(5):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    net_reward = 0\n",
    "    t = 0\n",
    "    while not done:\n",
    "        observation_arr = np.array([list(observation)]).reshape(1,1,len(observation))\n",
    "        action = np.argmax(model.predict(observation_arr))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        net_reward += reward\n",
    "        t = t + 1\n",
    "        env.render()\n",
    "    print(f\"Episode {i+1} finished after {t} timesteps with reward {net_reward}.\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
