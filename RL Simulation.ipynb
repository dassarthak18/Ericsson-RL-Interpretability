{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1013c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c4f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a3fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    65/50000: episode: 1, duration: 0.289s, episode steps:  65, steps per second: 225, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   101/50000: episode: 2, duration: 1.426s, episode steps:  36, steps per second:  25, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
      "   133/50000: episode: 3, duration: 0.299s, episode steps:  32, steps per second: 107, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 0.420629, mae: 0.549729, mean_q: 0.201939\n",
      "   147/50000: episode: 4, duration: 0.131s, episode steps:  14, steps per second: 107, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.245270, mae: 0.555674, mean_q: 0.469608\n",
      "   166/50000: episode: 5, duration: 0.184s, episode steps:  19, steps per second: 103, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.122260, mae: 0.595050, mean_q: 0.810366\n",
      "   199/50000: episode: 6, duration: 0.300s, episode steps:  33, steps per second: 110, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.059762, mae: 0.678069, mean_q: 1.151805\n",
      "   244/50000: episode: 7, duration: 0.409s, episode steps:  45, steps per second: 110, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.032194, mae: 0.787827, mean_q: 1.491976\n",
      "   265/50000: episode: 8, duration: 0.190s, episode steps:  21, steps per second: 111, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.021258, mae: 0.891484, mean_q: 1.718031\n",
      "   302/50000: episode: 9, duration: 0.333s, episode steps:  37, steps per second: 111, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.030247, mae: 1.001584, mean_q: 1.924562\n",
      "   332/50000: episode: 10, duration: 0.275s, episode steps:  30, steps per second: 109, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.032201, mae: 1.123844, mean_q: 2.192243\n",
      "   349/50000: episode: 11, duration: 0.155s, episode steps:  17, steps per second: 109, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.038842, mae: 1.208245, mean_q: 2.334135\n",
      "   367/50000: episode: 12, duration: 0.168s, episode steps:  18, steps per second: 107, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.033546, mae: 1.279745, mean_q: 2.487941\n",
      "   386/50000: episode: 13, duration: 0.172s, episode steps:  19, steps per second: 110, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.045497, mae: 1.349154, mean_q: 2.640229\n",
      "   418/50000: episode: 14, duration: 0.293s, episode steps:  32, steps per second: 109, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.055106, mae: 1.458594, mean_q: 2.820551\n",
      "   429/50000: episode: 15, duration: 0.103s, episode steps:  11, steps per second: 107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.103381, mae: 1.566651, mean_q: 2.963979\n",
      "   468/50000: episode: 16, duration: 0.351s, episode steps:  39, steps per second: 111, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.069096, mae: 1.643837, mean_q: 3.165344\n",
      "   488/50000: episode: 17, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.072259, mae: 1.747119, mean_q: 3.372279\n",
      "   503/50000: episode: 18, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.144474, mae: 1.840634, mean_q: 3.470466\n",
      "   518/50000: episode: 19, duration: 0.143s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.077240, mae: 1.895998, mean_q: 3.668099\n",
      "   530/50000: episode: 20, duration: 0.112s, episode steps:  12, steps per second: 107, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.124952, mae: 1.935057, mean_q: 3.704502\n",
      "   561/50000: episode: 21, duration: 0.283s, episode steps:  31, steps per second: 110, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.141239, mae: 2.043000, mean_q: 3.927797\n",
      "   586/50000: episode: 22, duration: 0.229s, episode steps:  25, steps per second: 109, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.185028, mae: 2.153123, mean_q: 4.092111\n",
      "   602/50000: episode: 23, duration: 0.150s, episode steps:  16, steps per second: 107, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.131675, mae: 2.230942, mean_q: 4.303723\n",
      "   684/50000: episode: 24, duration: 0.825s, episode steps:  82, steps per second:  99, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.201203, mae: 2.439683, mean_q: 4.674005\n",
      "   697/50000: episode: 25, duration: 0.121s, episode steps:  13, steps per second: 108, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.172781, mae: 2.629594, mean_q: 5.113725\n",
      "   723/50000: episode: 26, duration: 0.300s, episode steps:  26, steps per second:  87, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.214300, mae: 2.727813, mean_q: 5.240158\n",
      "   759/50000: episode: 27, duration: 0.334s, episode steps:  36, steps per second: 108, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.243135, mae: 2.829386, mean_q: 5.421890\n",
      "   778/50000: episode: 28, duration: 0.172s, episode steps:  19, steps per second: 110, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.273041, mae: 2.969240, mean_q: 5.685184\n",
      "   802/50000: episode: 29, duration: 0.225s, episode steps:  24, steps per second: 107, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.298717, mae: 3.048644, mean_q: 5.850941\n",
      "   841/50000: episode: 30, duration: 0.368s, episode steps:  39, steps per second: 106, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 0.333589, mae: 3.175349, mean_q: 6.109115\n",
      "   871/50000: episode: 31, duration: 0.296s, episode steps:  30, steps per second: 101, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.272037, mae: 3.284992, mean_q: 6.389121\n",
      "   912/50000: episode: 32, duration: 0.486s, episode steps:  41, steps per second:  84, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 0.332552, mae: 3.461807, mean_q: 6.681513\n",
      "   942/50000: episode: 33, duration: 0.283s, episode steps:  30, steps per second: 106, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.284542, mae: 3.604031, mean_q: 7.026228\n",
      "   957/50000: episode: 34, duration: 0.139s, episode steps:  15, steps per second: 108, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.430877, mae: 3.686870, mean_q: 7.083826\n",
      "   971/50000: episode: 35, duration: 0.138s, episode steps:  14, steps per second: 102, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.349397, mae: 3.733696, mean_q: 7.231479\n",
      "  1004/50000: episode: 36, duration: 0.305s, episode steps:  33, steps per second: 108, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 0.522702, mae: 3.846023, mean_q: 7.334564\n",
      "  1071/50000: episode: 37, duration: 0.745s, episode steps:  67, steps per second:  90, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.373 [0.000, 1.000],  loss: 0.404493, mae: 4.022769, mean_q: 7.754383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1108/50000: episode: 38, duration: 0.333s, episode steps:  37, steps per second: 111, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.470347, mae: 4.212126, mean_q: 8.100561\n",
      "  1147/50000: episode: 39, duration: 0.352s, episode steps:  39, steps per second: 111, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 0.468725, mae: 4.376017, mean_q: 8.456325\n",
      "  1224/50000: episode: 40, duration: 0.847s, episode steps:  77, steps per second:  91, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.403932, mae: 4.626245, mean_q: 9.024426\n",
      "  1292/50000: episode: 41, duration: 0.639s, episode steps:  68, steps per second: 106, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.410763, mae: 4.895397, mean_q: 9.595551\n",
      "  1341/50000: episode: 42, duration: 0.446s, episode steps:  49, steps per second: 110, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.409402, mae: 5.135674, mean_q: 10.061970\n",
      "  1376/50000: episode: 43, duration: 0.319s, episode steps:  35, steps per second: 110, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.499466, mae: 5.325100, mean_q: 10.452804\n",
      "  1464/50000: episode: 44, duration: 0.790s, episode steps:  88, steps per second: 111, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 0.482318, mae: 5.532280, mean_q: 10.887160\n",
      "  1488/50000: episode: 45, duration: 0.219s, episode steps:  24, steps per second: 109, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.704843, mae: 5.741801, mean_q: 11.291915\n",
      "  1550/50000: episode: 46, duration: 0.564s, episode steps:  62, steps per second: 110, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.540004, mae: 5.936170, mean_q: 11.722019\n",
      "  1574/50000: episode: 47, duration: 0.228s, episode steps:  24, steps per second: 105, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.693969, mae: 6.015150, mean_q: 11.864311\n",
      "  1647/50000: episode: 48, duration: 0.791s, episode steps:  73, steps per second:  92, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.685509, mae: 6.262472, mean_q: 12.371427\n",
      "  1737/50000: episode: 49, duration: 0.822s, episode steps:  90, steps per second: 109, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.581839, mae: 6.567356, mean_q: 13.085741\n",
      "  1878/50000: episode: 50, duration: 1.294s, episode steps: 141, steps per second: 109, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.651242, mae: 6.966928, mean_q: 14.004004\n",
      "  2025/50000: episode: 51, duration: 1.325s, episode steps: 147, steps per second: 111, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.700218, mae: 7.584954, mean_q: 15.353456\n",
      "  2228/50000: episode: 52, duration: 1.793s, episode steps: 203, steps per second: 113, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.833299, mae: 8.416648, mean_q: 17.115557\n",
      "  2390/50000: episode: 53, duration: 1.446s, episode steps: 162, steps per second: 112, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.964514, mae: 9.315603, mean_q: 18.900673\n",
      "done, took 24.620 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe67036ae20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f69ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 159.000, steps: 159\n",
      "Episode 2: reward: 131.000, steps: 131\n",
      "Episode 3: reward: 183.000, steps: 183\n",
      "Episode 4: reward: 143.000, steps: 143\n",
      "Episode 5: reward: 185.000, steps: 185\n",
      "Episode 6: reward: 188.000, steps: 188\n",
      "Episode 7: reward: 153.000, steps: 153\n",
      "Episode 8: reward: 129.000, steps: 129\n",
      "Episode 9: reward: 147.000, steps: 147\n",
      "Episode 10: reward: 135.000, steps: 135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe67036a910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
