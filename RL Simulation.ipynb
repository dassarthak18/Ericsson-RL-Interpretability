{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1013c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53d623",
   "metadata": {},
   "source": [
    "# Classic Control\n",
    "\n",
    "## Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c4f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a3fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    47/50000: episode: 1, duration: 0.234s, episode steps:  47, steps per second: 201, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
      "    63/50000: episode: 2, duration: 0.012s, episode steps:  16, steps per second: 1335, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   103/50000: episode: 3, duration: 0.882s, episode steps:  40, steps per second:  45, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.522412, mae: 0.534500, mean_q: -0.013017\n",
      "   125/50000: episode: 4, duration: 0.093s, episode steps:  22, steps per second: 237, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.407394, mae: 0.498958, mean_q: 0.108280\n",
      "   161/50000: episode: 5, duration: 0.153s, episode steps:  36, steps per second: 235, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.185644, mae: 0.529316, mean_q: 0.538482\n",
      "   180/50000: episode: 6, duration: 0.080s, episode steps:  19, steps per second: 236, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.058134, mae: 0.624777, mean_q: 1.083646\n",
      "   198/50000: episode: 7, duration: 0.077s, episode steps:  18, steps per second: 233, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.035998, mae: 0.693222, mean_q: 1.269525\n",
      "   217/50000: episode: 8, duration: 0.093s, episode steps:  19, steps per second: 205, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.015767, mae: 0.725312, mean_q: 1.372138\n",
      "   229/50000: episode: 9, duration: 0.065s, episode steps:  12, steps per second: 186, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.014131, mae: 0.792195, mean_q: 1.535923\n",
      "   270/50000: episode: 10, duration: 0.212s, episode steps:  41, steps per second: 193, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.341 [0.000, 1.000],  loss: 0.020046, mae: 0.882000, mean_q: 1.694762\n",
      "   288/50000: episode: 11, duration: 0.094s, episode steps:  18, steps per second: 192, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.019671, mae: 0.985778, mean_q: 1.926078\n",
      "   325/50000: episode: 12, duration: 0.190s, episode steps:  37, steps per second: 195, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.045298, mae: 1.106709, mean_q: 2.113786\n",
      "   354/50000: episode: 13, duration: 0.155s, episode steps:  29, steps per second: 187, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 0.037340, mae: 1.217659, mean_q: 2.357788\n",
      "   368/50000: episode: 14, duration: 0.084s, episode steps:  14, steps per second: 167, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.029590, mae: 1.294452, mean_q: 2.526975\n",
      "   396/50000: episode: 15, duration: 0.135s, episode steps:  28, steps per second: 207, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.053017, mae: 1.392653, mean_q: 2.691065\n",
      "   422/50000: episode: 16, duration: 0.119s, episode steps:  26, steps per second: 219, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.071426, mae: 1.509820, mean_q: 2.905990\n",
      "   446/50000: episode: 17, duration: 0.113s, episode steps:  24, steps per second: 213, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.077797, mae: 1.607983, mean_q: 3.079446\n",
      "   477/50000: episode: 18, duration: 0.147s, episode steps:  31, steps per second: 211, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.102526, mae: 1.728006, mean_q: 3.328366\n",
      "   495/50000: episode: 19, duration: 0.086s, episode steps:  18, steps per second: 210, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.115203, mae: 1.830294, mean_q: 3.489998\n",
      "   510/50000: episode: 20, duration: 0.071s, episode steps:  15, steps per second: 210, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.070858, mae: 1.885917, mean_q: 3.664003\n",
      "   565/50000: episode: 21, duration: 0.254s, episode steps:  55, steps per second: 216, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.101750, mae: 2.037474, mean_q: 3.962507\n",
      "   581/50000: episode: 22, duration: 0.076s, episode steps:  16, steps per second: 211, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.152032, mae: 2.192084, mean_q: 4.218211\n",
      "   597/50000: episode: 23, duration: 0.074s, episode steps:  16, steps per second: 217, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.163823, mae: 2.271502, mean_q: 4.339593\n",
      "   608/50000: episode: 24, duration: 0.049s, episode steps:  11, steps per second: 223, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.188069, mae: 2.319247, mean_q: 4.405931\n",
      "   648/50000: episode: 25, duration: 0.172s, episode steps:  40, steps per second: 233, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.233755, mae: 2.436255, mean_q: 4.644877\n",
      "   659/50000: episode: 26, duration: 0.048s, episode steps:  11, steps per second: 228, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.211741, mae: 2.514825, mean_q: 4.758497\n",
      "   701/50000: episode: 27, duration: 0.179s, episode steps:  42, steps per second: 234, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.204148, mae: 2.647642, mean_q: 5.068279\n",
      "   728/50000: episode: 28, duration: 0.115s, episode steps:  27, steps per second: 236, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.230496, mae: 2.785223, mean_q: 5.325216\n",
      "   743/50000: episode: 29, duration: 0.066s, episode steps:  15, steps per second: 227, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.232663, mae: 2.898212, mean_q: 5.561623\n",
      "   773/50000: episode: 30, duration: 0.153s, episode steps:  30, steps per second: 196, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.276209, mae: 2.971704, mean_q: 5.662959\n",
      "   785/50000: episode: 31, duration: 0.064s, episode steps:  12, steps per second: 188, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.271190, mae: 3.034296, mean_q: 5.795158\n",
      "   798/50000: episode: 32, duration: 0.069s, episode steps:  13, steps per second: 189, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.427191, mae: 3.098980, mean_q: 5.829432\n",
      "   859/50000: episode: 33, duration: 0.279s, episode steps:  61, steps per second: 219, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.298735, mae: 3.233727, mean_q: 6.193176\n",
      "   869/50000: episode: 34, duration: 0.045s, episode steps:  10, steps per second: 222, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.285166, mae: 3.383873, mean_q: 6.573519\n",
      "   887/50000: episode: 35, duration: 0.080s, episode steps:  18, steps per second: 224, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.406923, mae: 3.412384, mean_q: 6.497027\n",
      "   903/50000: episode: 36, duration: 0.077s, episode steps:  16, steps per second: 208, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.268978, mae: 3.473972, mean_q: 6.739861\n",
      "  1002/50000: episode: 37, duration: 0.458s, episode steps:  99, steps per second: 216, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.305124, mae: 3.709686, mean_q: 7.242987\n",
      "  1022/50000: episode: 38, duration: 0.102s, episode steps:  20, steps per second: 195, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.321143, mae: 3.971680, mean_q: 7.805130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1044/50000: episode: 39, duration: 0.116s, episode steps:  22, steps per second: 189, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.538945, mae: 4.070412, mean_q: 7.919444\n",
      "  1063/50000: episode: 40, duration: 0.099s, episode steps:  19, steps per second: 191, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.385834, mae: 4.145517, mean_q: 8.117569\n",
      "  1073/50000: episode: 41, duration: 0.054s, episode steps:  10, steps per second: 185, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.354219, mae: 4.213254, mean_q: 8.284635\n",
      "  1106/50000: episode: 42, duration: 0.170s, episode steps:  33, steps per second: 194, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 0.540375, mae: 4.288280, mean_q: 8.331957\n",
      "  1147/50000: episode: 43, duration: 0.207s, episode steps:  41, steps per second: 198, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.358577, mae: 4.428304, mean_q: 8.776426\n",
      "  1162/50000: episode: 44, duration: 0.080s, episode steps:  15, steps per second: 187, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.323525, mae: 4.550148, mean_q: 9.095727\n",
      "  1191/50000: episode: 45, duration: 0.149s, episode steps:  29, steps per second: 195, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.746272, mae: 4.648358, mean_q: 9.033178\n",
      "  1212/50000: episode: 46, duration: 0.108s, episode steps:  21, steps per second: 194, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.365266, mae: 4.790902, mean_q: 9.503935\n",
      "  1305/50000: episode: 47, duration: 0.468s, episode steps:  93, steps per second: 199, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.529143, mae: 4.993279, mean_q: 9.913876\n",
      "  1363/50000: episode: 48, duration: 0.334s, episode steps:  58, steps per second: 174, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.653448, mae: 5.312976, mean_q: 10.579027\n",
      "  1537/50000: episode: 49, duration: 0.723s, episode steps: 174, steps per second: 241, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.604365, mae: 5.881185, mean_q: 11.813942\n",
      "  1639/50000: episode: 50, duration: 0.491s, episode steps: 102, steps per second: 208, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.699525, mae: 6.491898, mean_q: 13.063993\n",
      "  1739/50000: episode: 51, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.634936, mae: 7.020959, mean_q: 14.214684\n",
      "  1842/50000: episode: 52, duration: 0.482s, episode steps: 103, steps per second: 214, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 1.123254, mae: 7.501791, mean_q: 15.101938\n",
      "  1986/50000: episode: 53, duration: 0.665s, episode steps: 144, steps per second: 216, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.018212, mae: 8.045521, mean_q: 16.212132\n",
      "  2099/50000: episode: 54, duration: 0.498s, episode steps: 113, steps per second: 227, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.868148, mae: 8.637578, mean_q: 17.531044\n",
      "  2246/50000: episode: 55, duration: 0.699s, episode steps: 147, steps per second: 210, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.979683, mae: 9.230507, mean_q: 18.698826\n",
      "  2387/50000: episode: 56, duration: 0.637s, episode steps: 141, steps per second: 221, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.954495, mae: 9.917373, mean_q: 20.167723\n",
      "  2544/50000: episode: 57, duration: 0.689s, episode steps: 157, steps per second: 228, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.028452, mae: 10.663611, mean_q: 21.639948\n",
      "  2725/50000: episode: 58, duration: 0.784s, episode steps: 181, steps per second: 231, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.224829, mae: 11.412344, mean_q: 23.172716\n",
      "  2886/50000: episode: 59, duration: 0.740s, episode steps: 161, steps per second: 217, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.390838, mae: 12.120979, mean_q: 24.626129\n",
      "  3055/50000: episode: 60, duration: 0.782s, episode steps: 169, steps per second: 216, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.357176, mae: 12.821326, mean_q: 26.090372\n",
      "  3218/50000: episode: 61, duration: 0.828s, episode steps: 163, steps per second: 197, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.374070, mae: 13.464762, mean_q: 27.498730\n",
      "  3370/50000: episode: 62, duration: 0.845s, episode steps: 152, steps per second: 180, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.800875, mae: 14.231495, mean_q: 29.023453\n",
      "  3554/50000: episode: 63, duration: 0.837s, episode steps: 184, steps per second: 220, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.861230, mae: 14.949402, mean_q: 30.382658\n",
      "  3751/50000: episode: 64, duration: 0.945s, episode steps: 197, steps per second: 209, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.671876, mae: 15.772047, mean_q: 32.054932\n",
      "  3920/50000: episode: 65, duration: 0.925s, episode steps: 169, steps per second: 183, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 1.651976, mae: 16.435049, mean_q: 33.377365\n",
      "  4120/50000: episode: 66, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.462778, mae: 17.231487, mean_q: 35.001575\n",
      "  4305/50000: episode: 67, duration: 0.837s, episode steps: 185, steps per second: 221, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.149543, mae: 17.904306, mean_q: 36.462837\n",
      "  4473/50000: episode: 68, duration: 0.760s, episode steps: 168, steps per second: 221, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.125124, mae: 18.726423, mean_q: 37.988449\n",
      "  4753/50000: episode: 69, duration: 1.341s, episode steps: 280, steps per second: 209, episode reward: 280.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.357468, mae: 19.431812, mean_q: 39.447231\n",
      "  4933/50000: episode: 70, duration: 0.751s, episode steps: 180, steps per second: 240, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.899031, mae: 20.347301, mean_q: 41.523415\n",
      "  5111/50000: episode: 71, duration: 0.797s, episode steps: 178, steps per second: 223, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.243504, mae: 21.032728, mean_q: 42.818619\n",
      "  5280/50000: episode: 72, duration: 0.832s, episode steps: 169, steps per second: 203, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.775509, mae: 21.654469, mean_q: 44.029938\n",
      "  5439/50000: episode: 73, duration: 0.796s, episode steps: 159, steps per second: 200, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 1.979055, mae: 22.140039, mean_q: 45.046501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5603/50000: episode: 74, duration: 0.817s, episode steps: 164, steps per second: 201, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.352743, mae: 22.864025, mean_q: 46.518192\n",
      "  5848/50000: episode: 75, duration: 1.216s, episode steps: 245, steps per second: 201, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.471309, mae: 23.540297, mean_q: 47.829960\n",
      "  6007/50000: episode: 76, duration: 0.863s, episode steps: 159, steps per second: 184, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.842201, mae: 24.256903, mean_q: 49.294983\n",
      "  6199/50000: episode: 77, duration: 1.115s, episode steps: 192, steps per second: 172, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.525446, mae: 24.856138, mean_q: 50.548573\n",
      "  6357/50000: episode: 78, duration: 1.116s, episode steps: 158, steps per second: 142, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.722080, mae: 25.342306, mean_q: 51.531437\n",
      "  6613/50000: episode: 79, duration: 1.352s, episode steps: 256, steps per second: 189, episode reward: 256.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.798944, mae: 25.907162, mean_q: 52.666679\n",
      "  6811/50000: episode: 80, duration: 1.092s, episode steps: 198, steps per second: 181, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.616266, mae: 26.790640, mean_q: 54.355877\n",
      "  7002/50000: episode: 81, duration: 1.054s, episode steps: 191, steps per second: 181, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.586215, mae: 27.518877, mean_q: 55.878078\n",
      "  7197/50000: episode: 82, duration: 1.081s, episode steps: 195, steps per second: 180, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.642287, mae: 28.009436, mean_q: 56.915306\n",
      "  7420/50000: episode: 83, duration: 1.237s, episode steps: 223, steps per second: 180, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.888333, mae: 28.799866, mean_q: 58.446133\n",
      "  7610/50000: episode: 84, duration: 1.095s, episode steps: 190, steps per second: 173, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.801014, mae: 29.036020, mean_q: 58.915867\n",
      "  7774/50000: episode: 85, duration: 0.729s, episode steps: 164, steps per second: 225, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.893275, mae: 29.514345, mean_q: 59.907608\n",
      "  7999/50000: episode: 86, duration: 1.020s, episode steps: 225, steps per second: 221, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.258944, mae: 29.903025, mean_q: 60.656658\n",
      "  8258/50000: episode: 87, duration: 1.391s, episode steps: 259, steps per second: 186, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.610387, mae: 30.547014, mean_q: 61.974186\n",
      "  8534/50000: episode: 88, duration: 1.562s, episode steps: 276, steps per second: 177, episode reward: 276.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2.297246, mae: 31.472130, mean_q: 63.970123\n",
      "  8703/50000: episode: 89, duration: 0.796s, episode steps: 169, steps per second: 212, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.375218, mae: 31.885057, mean_q: 64.630638\n",
      "  8921/50000: episode: 90, duration: 1.078s, episode steps: 218, steps per second: 202, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.758313, mae: 32.119972, mean_q: 65.275078\n",
      "  9128/50000: episode: 91, duration: 0.931s, episode steps: 207, steps per second: 222, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.090090, mae: 32.880310, mean_q: 66.693565\n",
      "  9302/50000: episode: 92, duration: 0.896s, episode steps: 174, steps per second: 194, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.627794, mae: 33.067284, mean_q: 67.084740\n",
      "  9523/50000: episode: 93, duration: 0.978s, episode steps: 221, steps per second: 226, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 3.179571, mae: 33.791283, mean_q: 68.509529\n",
      "  9708/50000: episode: 94, duration: 0.890s, episode steps: 185, steps per second: 208, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.372113, mae: 34.289383, mean_q: 69.504906\n",
      "  9859/50000: episode: 95, duration: 0.845s, episode steps: 151, steps per second: 179, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.001340, mae: 34.776196, mean_q: 70.499458\n",
      " 10064/50000: episode: 96, duration: 1.147s, episode steps: 205, steps per second: 179, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.563016, mae: 35.023300, mean_q: 70.989914\n",
      " 10226/50000: episode: 97, duration: 0.736s, episode steps: 162, steps per second: 220, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.762471, mae: 34.960365, mean_q: 70.946083\n",
      " 10394/50000: episode: 98, duration: 0.736s, episode steps: 168, steps per second: 228, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.801748, mae: 35.524151, mean_q: 72.070534\n",
      " 10581/50000: episode: 99, duration: 0.971s, episode steps: 187, steps per second: 193, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.582573, mae: 36.010376, mean_q: 72.996620\n",
      " 10730/50000: episode: 100, duration: 0.837s, episode steps: 149, steps per second: 178, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 1.964562, mae: 35.965664, mean_q: 73.007530\n",
      " 10896/50000: episode: 101, duration: 0.929s, episode steps: 166, steps per second: 179, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 2.652413, mae: 36.501659, mean_q: 73.856712\n",
      " 11115/50000: episode: 102, duration: 1.225s, episode steps: 219, steps per second: 179, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.194816, mae: 36.970150, mean_q: 74.841743\n",
      " 11301/50000: episode: 103, duration: 0.935s, episode steps: 186, steps per second: 199, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.748101, mae: 36.762096, mean_q: 74.432747\n",
      " 11466/50000: episode: 104, duration: 0.839s, episode steps: 165, steps per second: 197, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.765613, mae: 37.788448, mean_q: 76.547836\n",
      " 11707/50000: episode: 105, duration: 1.135s, episode steps: 241, steps per second: 212, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.835040, mae: 37.372894, mean_q: 75.624870\n",
      " 11929/50000: episode: 106, duration: 1.269s, episode steps: 222, steps per second: 175, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.214388, mae: 37.994820, mean_q: 76.826050\n",
      " 12091/50000: episode: 107, duration: 0.901s, episode steps: 162, steps per second: 180, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.559720, mae: 37.873959, mean_q: 76.765488\n",
      " 12251/50000: episode: 108, duration: 0.849s, episode steps: 160, steps per second: 188, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.396632, mae: 38.124161, mean_q: 77.127899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12419/50000: episode: 109, duration: 0.889s, episode steps: 168, steps per second: 189, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.300701, mae: 38.254597, mean_q: 77.292595\n",
      " 12631/50000: episode: 110, duration: 0.961s, episode steps: 212, steps per second: 221, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.399673, mae: 38.655155, mean_q: 78.234779\n",
      " 12800/50000: episode: 111, duration: 0.826s, episode steps: 169, steps per second: 205, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 3.505029, mae: 38.581188, mean_q: 77.977440\n",
      " 12982/50000: episode: 112, duration: 1.013s, episode steps: 182, steps per second: 180, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.804895, mae: 39.190376, mean_q: 79.293739\n",
      " 13138/50000: episode: 113, duration: 0.880s, episode steps: 156, steps per second: 177, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.780391, mae: 39.163956, mean_q: 79.187019\n",
      " 13382/50000: episode: 114, duration: 1.458s, episode steps: 244, steps per second: 167, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.682288, mae: 39.452564, mean_q: 79.763435\n",
      " 13577/50000: episode: 115, duration: 1.241s, episode steps: 195, steps per second: 157, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.977212, mae: 39.465305, mean_q: 79.815292\n",
      " 13746/50000: episode: 116, duration: 0.957s, episode steps: 169, steps per second: 177, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.912312, mae: 39.646450, mean_q: 80.225311\n",
      " 13973/50000: episode: 117, duration: 1.110s, episode steps: 227, steps per second: 204, episode reward: 227.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.441174, mae: 39.924011, mean_q: 80.682472\n",
      " 14191/50000: episode: 118, duration: 1.388s, episode steps: 218, steps per second: 157, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.871348, mae: 39.992153, mean_q: 80.893028\n",
      " 14397/50000: episode: 119, duration: 1.541s, episode steps: 206, steps per second: 134, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 4.302176, mae: 39.897072, mean_q: 80.589165\n",
      " 14555/50000: episode: 120, duration: 0.919s, episode steps: 158, steps per second: 172, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 3.174602, mae: 40.341839, mean_q: 81.569809\n",
      " 14734/50000: episode: 121, duration: 1.057s, episode steps: 179, steps per second: 169, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.160138, mae: 40.767246, mean_q: 82.451485\n",
      " 14884/50000: episode: 122, duration: 0.717s, episode steps: 150, steps per second: 209, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.144296, mae: 40.626541, mean_q: 82.230873\n",
      " 15040/50000: episode: 123, duration: 0.796s, episode steps: 156, steps per second: 196, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 3.818862, mae: 40.767204, mean_q: 82.329590\n",
      " 15209/50000: episode: 124, duration: 1.050s, episode steps: 169, steps per second: 161, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.001098, mae: 40.429752, mean_q: 81.803154\n",
      " 15386/50000: episode: 125, duration: 0.884s, episode steps: 177, steps per second: 200, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.464910, mae: 40.715836, mean_q: 82.419968\n",
      " 15552/50000: episode: 126, duration: 0.975s, episode steps: 166, steps per second: 170, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2.446001, mae: 40.612518, mean_q: 82.154022\n",
      " 15729/50000: episode: 127, duration: 1.031s, episode steps: 177, steps per second: 172, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.593256, mae: 40.703457, mean_q: 82.313438\n",
      " 15886/50000: episode: 128, duration: 0.770s, episode steps: 157, steps per second: 204, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.141017, mae: 40.648357, mean_q: 82.036789\n",
      " 16062/50000: episode: 129, duration: 0.959s, episode steps: 176, steps per second: 184, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.286366, mae: 41.030159, mean_q: 82.780243\n",
      " 16217/50000: episode: 130, duration: 0.950s, episode steps: 155, steps per second: 163, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.598437, mae: 41.024845, mean_q: 82.797989\n",
      " 16402/50000: episode: 131, duration: 0.881s, episode steps: 185, steps per second: 210, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 2.403353, mae: 41.123013, mean_q: 83.131485\n",
      " 16551/50000: episode: 132, duration: 0.842s, episode steps: 149, steps per second: 177, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.439031, mae: 40.999523, mean_q: 83.004074\n",
      " 16724/50000: episode: 133, duration: 0.995s, episode steps: 173, steps per second: 174, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.411535, mae: 41.263447, mean_q: 83.496323\n",
      " 17009/50000: episode: 134, duration: 1.750s, episode steps: 285, steps per second: 163, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.008471, mae: 40.688019, mean_q: 82.265068\n",
      " 17198/50000: episode: 135, duration: 0.968s, episode steps: 189, steps per second: 195, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.616805, mae: 40.651310, mean_q: 82.160736\n",
      " 17391/50000: episode: 136, duration: 1.103s, episode steps: 193, steps per second: 175, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.007231, mae: 40.840870, mean_q: 82.492783\n",
      " 17585/50000: episode: 137, duration: 1.090s, episode steps: 194, steps per second: 178, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.726768, mae: 41.179840, mean_q: 83.091911\n",
      " 17782/50000: episode: 138, duration: 1.067s, episode steps: 197, steps per second: 185, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.375331, mae: 40.504414, mean_q: 81.713036\n",
      " 17950/50000: episode: 139, duration: 0.989s, episode steps: 168, steps per second: 170, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.900087, mae: 41.273937, mean_q: 83.212242\n",
      " 18193/50000: episode: 140, duration: 1.183s, episode steps: 243, steps per second: 205, episode reward: 243.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.362239, mae: 40.583569, mean_q: 81.897942\n",
      " 18368/50000: episode: 141, duration: 0.865s, episode steps: 175, steps per second: 202, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.647298, mae: 40.749695, mean_q: 82.263153\n",
      " 18637/50000: episode: 142, duration: 1.354s, episode steps: 269, steps per second: 199, episode reward: 269.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.720423, mae: 40.964218, mean_q: 82.690437\n",
      " 18775/50000: episode: 143, duration: 0.675s, episode steps: 138, steps per second: 204, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.366208, mae: 41.044777, mean_q: 82.663719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18997/50000: episode: 144, duration: 1.071s, episode steps: 222, steps per second: 207, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.113759, mae: 40.261192, mean_q: 81.262848\n",
      " 19192/50000: episode: 145, duration: 0.892s, episode steps: 195, steps per second: 219, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.819534, mae: 40.560352, mean_q: 81.952698\n",
      " 19356/50000: episode: 146, duration: 0.801s, episode steps: 164, steps per second: 205, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 2.608602, mae: 40.545334, mean_q: 81.919838\n",
      " 19531/50000: episode: 147, duration: 0.917s, episode steps: 175, steps per second: 191, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 1.975867, mae: 40.884697, mean_q: 82.527863\n",
      " 19713/50000: episode: 148, duration: 0.814s, episode steps: 182, steps per second: 224, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.995111, mae: 40.903233, mean_q: 82.637489\n",
      " 19924/50000: episode: 149, duration: 0.986s, episode steps: 211, steps per second: 214, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.881994, mae: 40.676067, mean_q: 82.156990\n",
      " 20084/50000: episode: 150, duration: 0.849s, episode steps: 160, steps per second: 188, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.077846, mae: 41.528522, mean_q: 83.725693\n",
      " 20287/50000: episode: 151, duration: 1.037s, episode steps: 203, steps per second: 196, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.795848, mae: 40.658520, mean_q: 82.134956\n",
      " 20499/50000: episode: 152, duration: 1.227s, episode steps: 212, steps per second: 173, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 1.727776, mae: 40.870552, mean_q: 82.410927\n",
      " 20721/50000: episode: 153, duration: 1.370s, episode steps: 222, steps per second: 162, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.018362, mae: 40.672840, mean_q: 81.984283\n",
      " 20962/50000: episode: 154, duration: 1.290s, episode steps: 241, steps per second: 187, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.529094, mae: 41.179058, mean_q: 83.015709\n",
      " 21148/50000: episode: 155, duration: 0.896s, episode steps: 186, steps per second: 208, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.449800, mae: 40.885044, mean_q: 82.374718\n",
      " 21543/50000: episode: 156, duration: 2.156s, episode steps: 395, steps per second: 183, episode reward: 395.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.551163, mae: 40.999603, mean_q: 82.490898\n",
      " 21802/50000: episode: 157, duration: 1.608s, episode steps: 259, steps per second: 161, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.258408, mae: 40.826126, mean_q: 82.310913\n",
      " 22044/50000: episode: 158, duration: 1.474s, episode steps: 242, steps per second: 164, episode reward: 242.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 2.089603, mae: 41.075100, mean_q: 82.756073\n",
      " 22266/50000: episode: 159, duration: 1.130s, episode steps: 222, steps per second: 196, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.914980, mae: 41.293850, mean_q: 83.188515\n",
      " 22568/50000: episode: 160, duration: 1.920s, episode steps: 302, steps per second: 157, episode reward: 302.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.206699, mae: 41.200169, mean_q: 82.925774\n",
      " 22774/50000: episode: 161, duration: 1.193s, episode steps: 206, steps per second: 173, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.194581, mae: 41.306896, mean_q: 83.060776\n",
      " 22980/50000: episode: 162, duration: 1.045s, episode steps: 206, steps per second: 197, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.567388, mae: 40.962208, mean_q: 82.337074\n",
      " 23253/50000: episode: 163, duration: 1.498s, episode steps: 273, steps per second: 182, episode reward: 273.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.563375, mae: 41.176514, mean_q: 82.660141\n",
      " 23467/50000: episode: 164, duration: 1.929s, episode steps: 214, steps per second: 111, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.912269, mae: 41.265900, mean_q: 82.858276\n",
      " 23670/50000: episode: 165, duration: 2.052s, episode steps: 203, steps per second:  99, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 2.532671, mae: 41.004826, mean_q: 82.467323\n",
      " 23929/50000: episode: 166, duration: 2.346s, episode steps: 259, steps per second: 110, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.333417, mae: 41.237907, mean_q: 82.925056\n",
      " 24149/50000: episode: 167, duration: 2.017s, episode steps: 220, steps per second: 109, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.355230, mae: 41.131561, mean_q: 82.743553\n",
      " 24379/50000: episode: 168, duration: 2.114s, episode steps: 230, steps per second: 109, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.585833, mae: 41.544655, mean_q: 83.637291\n",
      " 24578/50000: episode: 169, duration: 1.821s, episode steps: 199, steps per second: 109, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1.700450, mae: 41.019680, mean_q: 82.535645\n",
      " 24814/50000: episode: 170, duration: 2.416s, episode steps: 236, steps per second:  98, episode reward: 236.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.593601, mae: 41.600842, mean_q: 83.721031\n",
      " 25057/50000: episode: 171, duration: 2.464s, episode steps: 243, steps per second:  99, episode reward: 243.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.138923, mae: 41.713867, mean_q: 83.985535\n",
      " 25274/50000: episode: 172, duration: 1.957s, episode steps: 217, steps per second: 111, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.290749, mae: 42.003464, mean_q: 84.627838\n",
      " 25491/50000: episode: 173, duration: 1.914s, episode steps: 217, steps per second: 113, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.332242, mae: 41.930191, mean_q: 84.393898\n",
      " 25736/50000: episode: 174, duration: 2.169s, episode steps: 245, steps per second: 113, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 3.361725, mae: 42.090187, mean_q: 84.750061\n",
      " 25969/50000: episode: 175, duration: 2.063s, episode steps: 233, steps per second: 113, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 1.929812, mae: 42.338657, mean_q: 85.400352\n",
      " 26146/50000: episode: 176, duration: 1.570s, episode steps: 177, steps per second: 113, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.002609, mae: 42.739021, mean_q: 86.226562\n",
      " 26419/50000: episode: 177, duration: 2.413s, episode steps: 273, steps per second: 113, episode reward: 273.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.471818, mae: 43.465752, mean_q: 87.776115\n",
      " 26783/50000: episode: 178, duration: 3.266s, episode steps: 364, steps per second: 111, episode reward: 364.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.745213, mae: 43.699177, mean_q: 88.275002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27283/50000: episode: 179, duration: 4.427s, episode steps: 500, steps per second: 113, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.008116, mae: 44.793072, mean_q: 90.405579\n",
      " 27783/50000: episode: 180, duration: 4.422s, episode steps: 500, steps per second: 113, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.662230, mae: 45.768658, mean_q: 92.222610\n",
      " 28283/50000: episode: 181, duration: 4.499s, episode steps: 500, steps per second: 111, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.007236, mae: 47.151665, mean_q: 95.022461\n",
      " 28783/50000: episode: 182, duration: 4.446s, episode steps: 500, steps per second: 112, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.857229, mae: 47.944393, mean_q: 96.545860\n",
      " 29283/50000: episode: 183, duration: 4.440s, episode steps: 500, steps per second: 113, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.797626, mae: 49.389484, mean_q: 99.483521\n",
      " 29783/50000: episode: 184, duration: 4.437s, episode steps: 500, steps per second: 113, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.470437, mae: 50.371670, mean_q: 101.599953\n",
      " 30283/50000: episode: 185, duration: 4.440s, episode steps: 500, steps per second: 113, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.893397, mae: 51.405422, mean_q: 103.491249\n",
      " 30783/50000: episode: 186, duration: 4.453s, episode steps: 500, steps per second: 112, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.863686, mae: 52.102821, mean_q: 105.031281\n",
      " 31283/50000: episode: 187, duration: 4.458s, episode steps: 500, steps per second: 112, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.733658, mae: 53.483624, mean_q: 107.793251\n",
      " 31783/50000: episode: 188, duration: 4.469s, episode steps: 500, steps per second: 112, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.532908, mae: 54.541084, mean_q: 109.873390\n",
      " 32283/50000: episode: 189, duration: 4.460s, episode steps: 500, steps per second: 112, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.515354, mae: 55.697014, mean_q: 112.395798\n",
      " 32783/50000: episode: 190, duration: 4.587s, episode steps: 500, steps per second: 109, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.959751, mae: 56.843632, mean_q: 114.419418\n",
      " 33283/50000: episode: 191, duration: 4.767s, episode steps: 500, steps per second: 105, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.174216, mae: 58.176041, mean_q: 117.285683\n",
      " 33783/50000: episode: 192, duration: 4.826s, episode steps: 500, steps per second: 104, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.710656, mae: 59.272564, mean_q: 119.780174\n",
      " 34283/50000: episode: 193, duration: 4.543s, episode steps: 500, steps per second: 110, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.628035, mae: 60.408775, mean_q: 121.708992\n",
      " 34783/50000: episode: 194, duration: 4.495s, episode steps: 500, steps per second: 111, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.129137, mae: 61.119537, mean_q: 123.086617\n",
      " 35151/50000: episode: 195, duration: 3.320s, episode steps: 368, steps per second: 111, episode reward: 368.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 6.483275, mae: 61.679394, mean_q: 124.256111\n",
      " 35491/50000: episode: 196, duration: 3.058s, episode steps: 340, steps per second: 111, episode reward: 340.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 10.170304, mae: 61.469990, mean_q: 123.539818\n",
      " 35773/50000: episode: 197, duration: 2.554s, episode steps: 282, steps per second: 110, episode reward: 282.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 6.561542, mae: 61.443718, mean_q: 123.696808\n",
      " 35995/50000: episode: 198, duration: 2.013s, episode steps: 222, steps per second: 110, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 7.597779, mae: 61.127296, mean_q: 123.054970\n",
      " 36221/50000: episode: 199, duration: 2.050s, episode steps: 226, steps per second: 110, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 5.344752, mae: 61.177216, mean_q: 123.178650\n",
      " 36508/50000: episode: 200, duration: 2.585s, episode steps: 287, steps per second: 111, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.536016, mae: 61.343273, mean_q: 123.593285\n",
      " 36775/50000: episode: 201, duration: 2.413s, episode steps: 267, steps per second: 111, episode reward: 267.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 7.506628, mae: 61.061367, mean_q: 123.086815\n",
      " 37009/50000: episode: 202, duration: 2.125s, episode steps: 234, steps per second: 110, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 8.413591, mae: 60.911415, mean_q: 122.772766\n",
      " 37172/50000: episode: 203, duration: 1.478s, episode steps: 163, steps per second: 110, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.800711, mae: 61.002602, mean_q: 122.904549\n",
      " 37327/50000: episode: 204, duration: 1.406s, episode steps: 155, steps per second: 110, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 7.836126, mae: 61.317703, mean_q: 123.625023\n",
      " 37479/50000: episode: 205, duration: 1.379s, episode steps: 152, steps per second: 110, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.874178, mae: 61.118336, mean_q: 123.159813\n",
      " 37625/50000: episode: 206, duration: 1.327s, episode steps: 146, steps per second: 110, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.951581, mae: 61.199806, mean_q: 123.453712\n",
      " 37796/50000: episode: 207, duration: 1.551s, episode steps: 171, steps per second: 110, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.804661, mae: 60.999184, mean_q: 122.641083\n",
      " 37976/50000: episode: 208, duration: 1.635s, episode steps: 180, steps per second: 110, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 7.119388, mae: 61.473827, mean_q: 123.599174\n",
      " 38123/50000: episode: 209, duration: 1.339s, episode steps: 147, steps per second: 110, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 9.980386, mae: 61.346313, mean_q: 123.274208\n",
      " 38234/50000: episode: 210, duration: 1.012s, episode steps: 111, steps per second: 110, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 10.759981, mae: 60.686932, mean_q: 121.838997\n",
      " 38362/50000: episode: 211, duration: 1.167s, episode steps: 128, steps per second: 110, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 8.412066, mae: 61.420921, mean_q: 123.449326\n",
      " 38485/50000: episode: 212, duration: 1.122s, episode steps: 123, steps per second: 110, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 9.153670, mae: 61.611721, mean_q: 124.034851\n",
      " 38613/50000: episode: 213, duration: 1.165s, episode steps: 128, steps per second: 110, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.522075, mae: 61.598396, mean_q: 124.139221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38750/50000: episode: 214, duration: 1.243s, episode steps: 137, steps per second: 110, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 9.790737, mae: 61.172958, mean_q: 122.864136\n",
      " 38882/50000: episode: 215, duration: 1.207s, episode steps: 132, steps per second: 109, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 6.772850, mae: 60.766201, mean_q: 122.355309\n",
      " 39007/50000: episode: 216, duration: 1.137s, episode steps: 125, steps per second: 110, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.293504, mae: 60.666222, mean_q: 122.322533\n",
      " 39148/50000: episode: 217, duration: 1.291s, episode steps: 141, steps per second: 109, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 10.803409, mae: 60.965466, mean_q: 122.534370\n",
      " 39283/50000: episode: 218, duration: 1.230s, episode steps: 135, steps per second: 110, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 9.044390, mae: 61.752468, mean_q: 124.216133\n",
      " 39399/50000: episode: 219, duration: 1.065s, episode steps: 116, steps per second: 109, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 6.828286, mae: 61.520676, mean_q: 124.095245\n",
      " 39523/50000: episode: 220, duration: 1.163s, episode steps: 124, steps per second: 107, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 9.401425, mae: 61.623058, mean_q: 124.127045\n",
      " 39635/50000: episode: 221, duration: 1.022s, episode steps: 112, steps per second: 110, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 9.565455, mae: 61.443756, mean_q: 123.708130\n",
      " 39751/50000: episode: 222, duration: 1.068s, episode steps: 116, steps per second: 109, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.329188, mae: 61.771130, mean_q: 124.504494\n",
      " 39860/50000: episode: 223, duration: 1.001s, episode steps: 109, steps per second: 109, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.331130, mae: 61.958546, mean_q: 124.984161\n",
      " 39970/50000: episode: 224, duration: 1.009s, episode steps: 110, steps per second: 109, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 12.504925, mae: 61.698849, mean_q: 124.040405\n",
      " 40085/50000: episode: 225, duration: 1.054s, episode steps: 115, steps per second: 109, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.341591, mae: 61.995796, mean_q: 124.915436\n",
      " 40186/50000: episode: 226, duration: 0.927s, episode steps: 101, steps per second: 109, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.740185, mae: 62.839622, mean_q: 126.801010\n",
      " 40292/50000: episode: 227, duration: 0.981s, episode steps: 106, steps per second: 108, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 4.485692, mae: 61.919033, mean_q: 124.521469\n",
      " 40401/50000: episode: 228, duration: 1.001s, episode steps: 109, steps per second: 109, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 9.212438, mae: 62.054310, mean_q: 124.698273\n",
      " 40518/50000: episode: 229, duration: 1.080s, episode steps: 117, steps per second: 108, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 8.254652, mae: 62.460678, mean_q: 125.792465\n",
      " 40645/50000: episode: 230, duration: 1.165s, episode steps: 127, steps per second: 109, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 5.286265, mae: 62.976212, mean_q: 126.790771\n",
      " 40764/50000: episode: 231, duration: 1.090s, episode steps: 119, steps per second: 109, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.350005, mae: 62.398438, mean_q: 125.680687\n",
      " 40999/50000: episode: 232, duration: 2.241s, episode steps: 235, steps per second: 105, episode reward: 235.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.769285, mae: 61.980946, mean_q: 124.723770\n",
      " 41131/50000: episode: 233, duration: 1.260s, episode steps: 132, steps per second: 105, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 15.280128, mae: 62.440613, mean_q: 125.429985\n",
      " 41260/50000: episode: 234, duration: 1.284s, episode steps: 129, steps per second: 100, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 11.194257, mae: 62.421898, mean_q: 125.494278\n",
      " 41380/50000: episode: 235, duration: 1.139s, episode steps: 120, steps per second: 105, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.962098, mae: 62.089558, mean_q: 124.860306\n",
      " 41512/50000: episode: 236, duration: 1.271s, episode steps: 132, steps per second: 104, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 13.001878, mae: 62.171513, mean_q: 125.082565\n",
      " 41659/50000: episode: 237, duration: 1.624s, episode steps: 147, steps per second:  91, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.221333, mae: 62.067085, mean_q: 125.086044\n",
      " 41774/50000: episode: 238, duration: 1.215s, episode steps: 115, steps per second:  95, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 8.812633, mae: 62.295311, mean_q: 125.394859\n",
      " 41891/50000: episode: 239, duration: 1.249s, episode steps: 117, steps per second:  94, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 6.073875, mae: 62.205387, mean_q: 125.353821\n",
      " 42027/50000: episode: 240, duration: 1.291s, episode steps: 136, steps per second: 105, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 9.137740, mae: 61.907635, mean_q: 124.646500\n",
      " 42154/50000: episode: 241, duration: 1.239s, episode steps: 127, steps per second: 102, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.723005, mae: 62.309944, mean_q: 125.581741\n",
      " 42274/50000: episode: 242, duration: 1.259s, episode steps: 120, steps per second:  95, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 8.750696, mae: 61.982006, mean_q: 124.926750\n",
      " 42397/50000: episode: 243, duration: 1.407s, episode steps: 123, steps per second:  87, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 7.076950, mae: 62.261829, mean_q: 125.202385\n",
      " 42516/50000: episode: 244, duration: 1.233s, episode steps: 119, steps per second:  96, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.325181, mae: 61.791309, mean_q: 124.551712\n",
      " 42635/50000: episode: 245, duration: 1.101s, episode steps: 119, steps per second: 108, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.413620, mae: 61.677460, mean_q: 124.415016\n",
      " 42748/50000: episode: 246, duration: 1.044s, episode steps: 113, steps per second: 108, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 7.411520, mae: 62.240337, mean_q: 125.304108\n",
      " 42863/50000: episode: 247, duration: 1.060s, episode steps: 115, steps per second: 108, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 8.296066, mae: 62.004879, mean_q: 124.798050\n",
      " 42979/50000: episode: 248, duration: 1.073s, episode steps: 116, steps per second: 108, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 10.877148, mae: 62.141670, mean_q: 125.073715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43104/50000: episode: 249, duration: 1.163s, episode steps: 125, steps per second: 107, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 4.278934, mae: 61.930954, mean_q: 124.768044\n",
      " 43218/50000: episode: 250, duration: 1.073s, episode steps: 114, steps per second: 106, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 12.499614, mae: 61.815792, mean_q: 124.446983\n",
      " 43327/50000: episode: 251, duration: 1.013s, episode steps: 109, steps per second: 108, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 8.637574, mae: 62.122372, mean_q: 124.853523\n",
      " 43453/50000: episode: 252, duration: 1.171s, episode steps: 126, steps per second: 108, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.916406, mae: 61.352242, mean_q: 123.727837\n",
      " 43566/50000: episode: 253, duration: 1.043s, episode steps: 113, steps per second: 108, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 7.128732, mae: 61.474064, mean_q: 123.807808\n",
      " 43683/50000: episode: 254, duration: 1.085s, episode steps: 117, steps per second: 108, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 15.200925, mae: 62.064022, mean_q: 124.589912\n",
      " 43818/50000: episode: 255, duration: 1.247s, episode steps: 135, steps per second: 108, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 12.285456, mae: 61.124523, mean_q: 122.876274\n",
      " 43969/50000: episode: 256, duration: 1.400s, episode steps: 151, steps per second: 108, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 10.083354, mae: 60.982227, mean_q: 122.406097\n",
      " 44089/50000: episode: 257, duration: 1.109s, episode steps: 120, steps per second: 108, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 4.256154, mae: 61.000473, mean_q: 122.708252\n",
      " 44208/50000: episode: 258, duration: 1.102s, episode steps: 119, steps per second: 108, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.179332, mae: 60.736633, mean_q: 122.143753\n",
      " 44384/50000: episode: 259, duration: 1.629s, episode steps: 176, steps per second: 108, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.525640, mae: 60.071987, mean_q: 120.866463\n",
      " 44608/50000: episode: 260, duration: 2.071s, episode steps: 224, steps per second: 108, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 8.429158, mae: 60.311558, mean_q: 121.200241\n",
      " 44765/50000: episode: 261, duration: 1.453s, episode steps: 157, steps per second: 108, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 7.896159, mae: 59.694305, mean_q: 119.878410\n",
      " 44954/50000: episode: 262, duration: 1.750s, episode steps: 189, steps per second: 108, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 10.882939, mae: 60.194981, mean_q: 120.781197\n",
      " 45179/50000: episode: 263, duration: 2.077s, episode steps: 225, steps per second: 108, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.864658, mae: 59.148151, mean_q: 118.850815\n",
      " 45460/50000: episode: 264, duration: 2.593s, episode steps: 281, steps per second: 108, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 6.191106, mae: 58.684246, mean_q: 117.808624\n",
      " 45684/50000: episode: 265, duration: 2.073s, episode steps: 224, steps per second: 108, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.411766, mae: 58.737705, mean_q: 117.839706\n",
      " 45973/50000: episode: 266, duration: 2.670s, episode steps: 289, steps per second: 108, episode reward: 289.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.886125, mae: 58.246498, mean_q: 117.069122\n",
      " 46473/50000: episode: 267, duration: 4.616s, episode steps: 500, steps per second: 108, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 5.217097, mae: 57.968014, mean_q: 116.519142\n",
      " 46973/50000: episode: 268, duration: 4.641s, episode steps: 500, steps per second: 108, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.836636, mae: 57.966187, mean_q: 116.445610\n",
      " 47473/50000: episode: 269, duration: 4.628s, episode steps: 500, steps per second: 108, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.239272, mae: 58.161709, mean_q: 116.955887\n",
      " 47973/50000: episode: 270, duration: 4.641s, episode steps: 500, steps per second: 108, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.668664, mae: 58.691509, mean_q: 117.917534\n",
      " 48473/50000: episode: 271, duration: 4.657s, episode steps: 500, steps per second: 107, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.955860, mae: 58.241859, mean_q: 117.138748\n",
      " 48973/50000: episode: 272, duration: 4.659s, episode steps: 500, steps per second: 107, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.438273, mae: 59.035961, mean_q: 118.826576\n",
      " 49473/50000: episode: 273, duration: 4.640s, episode steps: 500, steps per second: 108, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.076163, mae: 59.603104, mean_q: 119.989159\n",
      " 49973/50000: episode: 274, duration: 4.672s, episode steps: 500, steps per second: 107, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.199275, mae: 59.988701, mean_q: 120.690155\n",
      "done, took 369.347 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe72ad2f550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f69ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n",
      "Episode 6: reward: 500.000, steps: 500\n",
      "Episode 7: reward: 500.000, steps: 500\n",
      "Episode 8: reward: 500.000, steps: 500\n",
      "Episode 9: reward: 500.000, steps: 500\n",
      "Episode 10: reward: 500.000, steps: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe72ae72c70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8174f",
   "metadata": {},
   "source": [
    "## Mountain Car (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274d85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   200/50000: episode: 1, duration: 0.997s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.232180, mae: 0.502601, mean_q: -0.382186\n",
      "   400/50000: episode: 2, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.002554, mae: 1.240121, mean_q: -1.828955\n",
      "   600/50000: episode: 3, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.013305, mae: 2.286235, mean_q: -3.385422\n",
      "   800/50000: episode: 4, duration: 1.059s, episode steps: 200, steps per second: 189, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.024968, mae: 3.398019, mean_q: -5.041681\n",
      "  1000/50000: episode: 5, duration: 1.001s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.098529, mae: 4.531662, mean_q: -6.685009\n",
      "  1200/50000: episode: 6, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.126104, mae: 5.658993, mean_q: -8.345527\n",
      "  1400/50000: episode: 7, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.161234, mae: 6.793865, mean_q: -10.057255\n",
      "  1600/50000: episode: 8, duration: 1.048s, episode steps: 200, steps per second: 191, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.317720, mae: 7.836492, mean_q: -11.567700\n",
      "  1800/50000: episode: 9, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.413486, mae: 8.829528, mean_q: -13.071622\n",
      "  2000/50000: episode: 10, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.464452, mae: 9.839207, mean_q: -14.526527\n",
      "  2200/50000: episode: 11, duration: 1.019s, episode steps: 200, steps per second: 196, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.514886, mae: 10.793745, mean_q: -15.978088\n",
      "  2400/50000: episode: 12, duration: 1.036s, episode steps: 200, steps per second: 193, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.630508, mae: 11.715725, mean_q: -17.336573\n",
      "  2600/50000: episode: 13, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.824631, mae: 12.583589, mean_q: -18.631460\n",
      "  2800/50000: episode: 14, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 1.011773, mae: 13.460798, mean_q: -19.900908\n",
      "  3000/50000: episode: 15, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.847183, mae: 14.295047, mean_q: -21.178520\n",
      "  3200/50000: episode: 16, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.429626, mae: 15.077954, mean_q: -22.277100\n",
      "  3400/50000: episode: 17, duration: 1.054s, episode steps: 200, steps per second: 190, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 1.154058, mae: 15.785544, mean_q: -23.388540\n",
      "  3600/50000: episode: 18, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.372830, mae: 16.568615, mean_q: -24.545813\n",
      "  3800/50000: episode: 19, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.559329, mae: 17.329266, mean_q: -25.666821\n",
      "  4000/50000: episode: 20, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.373126, mae: 17.972853, mean_q: -26.596031\n",
      "  4200/50000: episode: 21, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.805520, mae: 18.679338, mean_q: -27.697508\n",
      "  4400/50000: episode: 22, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.785371, mae: 19.316950, mean_q: -28.615097\n",
      "  4600/50000: episode: 23, duration: 1.058s, episode steps: 200, steps per second: 189, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.249405, mae: 19.875637, mean_q: -29.346167\n",
      "  4800/50000: episode: 24, duration: 1.154s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.119860, mae: 20.376480, mean_q: -30.185453\n",
      "  5000/50000: episode: 25, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 2.157899, mae: 20.962778, mean_q: -31.065014\n",
      "  5200/50000: episode: 26, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.445693, mae: 21.444551, mean_q: -31.751762\n",
      "  5400/50000: episode: 27, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 2.052275, mae: 21.981785, mean_q: -32.620235\n",
      "  5600/50000: episode: 28, duration: 0.916s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.940556, mae: 22.557066, mean_q: -33.504879\n",
      "  5800/50000: episode: 29, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 3.362926, mae: 23.127285, mean_q: -34.242142\n",
      "  6000/50000: episode: 30, duration: 1.011s, episode steps: 200, steps per second: 198, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.033964, mae: 23.539122, mean_q: -34.845715\n",
      "  6200/50000: episode: 31, duration: 0.987s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 2.689851, mae: 23.980991, mean_q: -35.542740\n",
      "  6400/50000: episode: 32, duration: 1.168s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 3.549601, mae: 24.292370, mean_q: -35.899361\n",
      "  6600/50000: episode: 33, duration: 1.183s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 5.109139, mae: 24.637800, mean_q: -36.357178\n",
      "  6800/50000: episode: 34, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.163703, mae: 24.997612, mean_q: -36.927025\n",
      "  7000/50000: episode: 35, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 2.921032, mae: 25.357861, mean_q: -37.609001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7200/50000: episode: 36, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 3.310073, mae: 25.745474, mean_q: -38.204140\n",
      "  7400/50000: episode: 37, duration: 1.021s, episode steps: 200, steps per second: 196, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 3.292670, mae: 26.148848, mean_q: -38.738998\n",
      "  7600/50000: episode: 38, duration: 1.066s, episode steps: 200, steps per second: 188, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 4.061584, mae: 26.513691, mean_q: -39.252823\n",
      "  7800/50000: episode: 39, duration: 0.935s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 4.419078, mae: 26.842304, mean_q: -39.741764\n",
      "  8000/50000: episode: 40, duration: 1.140s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 4.447155, mae: 27.134283, mean_q: -40.199154\n",
      "  8200/50000: episode: 41, duration: 1.079s, episode steps: 200, steps per second: 185, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 4.196093, mae: 27.414824, mean_q: -40.625954\n",
      "  8400/50000: episode: 42, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.578476, mae: 27.753569, mean_q: -41.216377\n",
      "  8600/50000: episode: 43, duration: 1.083s, episode steps: 200, steps per second: 185, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.651278, mae: 28.121607, mean_q: -41.579571\n",
      "  8800/50000: episode: 44, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 5.139534, mae: 28.325727, mean_q: -41.930737\n",
      "  9000/50000: episode: 45, duration: 1.139s, episode steps: 200, steps per second: 176, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.577489, mae: 28.564487, mean_q: -42.306934\n",
      "  9200/50000: episode: 46, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 4.164646, mae: 28.901821, mean_q: -42.864578\n",
      "  9400/50000: episode: 47, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.127271, mae: 29.084770, mean_q: -43.005928\n",
      "  9600/50000: episode: 48, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 3.867012, mae: 29.346138, mean_q: -43.562637\n",
      "  9800/50000: episode: 49, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 4.990279, mae: 29.551674, mean_q: -43.697929\n",
      " 10000/50000: episode: 50, duration: 2.254s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 3.814234, mae: 29.809942, mean_q: -44.235626\n",
      " 10200/50000: episode: 51, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 4.129663, mae: 30.099363, mean_q: -44.687778\n",
      " 10400/50000: episode: 52, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 5.726099, mae: 30.289600, mean_q: -44.839333\n",
      " 10600/50000: episode: 53, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 5.184398, mae: 30.515221, mean_q: -45.242245\n",
      " 10800/50000: episode: 54, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 4.723963, mae: 30.646700, mean_q: -45.366493\n",
      " 11000/50000: episode: 55, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 3.870847, mae: 30.952826, mean_q: -45.938442\n",
      " 11200/50000: episode: 56, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 5.154847, mae: 31.162104, mean_q: -46.230545\n",
      " 11400/50000: episode: 57, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.852026, mae: 31.427040, mean_q: -46.534115\n",
      " 11600/50000: episode: 58, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 5.930788, mae: 31.528505, mean_q: -46.764824\n",
      " 11800/50000: episode: 59, duration: 2.168s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.525216, mae: 31.719864, mean_q: -47.059738\n",
      " 12000/50000: episode: 60, duration: 2.144s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 6.030387, mae: 31.953066, mean_q: -47.387432\n",
      " 12200/50000: episode: 61, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.504922, mae: 32.065395, mean_q: -47.540321\n",
      " 12400/50000: episode: 62, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.007244, mae: 32.292061, mean_q: -47.916943\n",
      " 12600/50000: episode: 63, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 4.851155, mae: 32.561424, mean_q: -48.359386\n",
      " 12800/50000: episode: 64, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 4.460292, mae: 32.786610, mean_q: -48.610340\n",
      " 13000/50000: episode: 65, duration: 1.991s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.597787, mae: 32.983147, mean_q: -48.992676\n",
      " 13200/50000: episode: 66, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 4.145270, mae: 33.247437, mean_q: -49.347321\n",
      " 13400/50000: episode: 67, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 5.956425, mae: 33.273811, mean_q: -49.335117\n",
      " 13600/50000: episode: 68, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 4.663964, mae: 33.455162, mean_q: -49.681152\n",
      " 13800/50000: episode: 69, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 4.829434, mae: 33.637581, mean_q: -49.884975\n",
      " 14000/50000: episode: 70, duration: 1.981s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 6.897422, mae: 33.765099, mean_q: -49.879440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14200/50000: episode: 71, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.096649, mae: 33.881943, mean_q: -50.306210\n",
      " 14400/50000: episode: 72, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 5.003302, mae: 34.160900, mean_q: -50.746593\n",
      " 14600/50000: episode: 73, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 6.219534, mae: 34.300228, mean_q: -50.822369\n",
      " 14800/50000: episode: 74, duration: 1.983s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.981527, mae: 34.264828, mean_q: -50.762550\n",
      " 15000/50000: episode: 75, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 5.791919, mae: 34.301189, mean_q: -50.782665\n",
      " 15200/50000: episode: 76, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 8.797423, mae: 34.158470, mean_q: -50.485146\n",
      " 15400/50000: episode: 77, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 7.695879, mae: 34.077030, mean_q: -50.462540\n",
      " 15600/50000: episode: 78, duration: 1.958s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 5.445336, mae: 34.201256, mean_q: -50.861427\n",
      " 15800/50000: episode: 79, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 6.293309, mae: 34.357506, mean_q: -50.983257\n",
      " 16000/50000: episode: 80, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 4.852835, mae: 34.401039, mean_q: -51.079937\n",
      " 16200/50000: episode: 81, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 5.720233, mae: 34.626148, mean_q: -51.458485\n",
      " 16400/50000: episode: 82, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 5.384392, mae: 34.762486, mean_q: -51.673359\n",
      " 16600/50000: episode: 83, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.284724, mae: 34.790592, mean_q: -51.590733\n",
      " 16800/50000: episode: 84, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 7.557188, mae: 34.798786, mean_q: -51.469372\n",
      " 17000/50000: episode: 85, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 6.563522, mae: 34.743057, mean_q: -51.435429\n",
      " 17200/50000: episode: 86, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 4.495392, mae: 34.731644, mean_q: -51.632938\n",
      " 17400/50000: episode: 87, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 7.594485, mae: 34.828968, mean_q: -51.457626\n",
      " 17600/50000: episode: 88, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.128181, mae: 34.825199, mean_q: -51.674824\n",
      " 17800/50000: episode: 89, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 7.427139, mae: 34.896839, mean_q: -51.704185\n",
      " 18000/50000: episode: 90, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 6.510604, mae: 34.999454, mean_q: -51.911064\n",
      " 18200/50000: episode: 91, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.264250, mae: 35.036163, mean_q: -51.921356\n",
      " 18400/50000: episode: 92, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 7.378738, mae: 35.041046, mean_q: -51.814465\n",
      " 18600/50000: episode: 93, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 7.986006, mae: 35.052269, mean_q: -51.913456\n",
      " 18800/50000: episode: 94, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.709260, mae: 35.058731, mean_q: -51.860279\n",
      " 19000/50000: episode: 95, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 7.083813, mae: 35.035187, mean_q: -51.951916\n",
      " 19200/50000: episode: 96, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.053105, mae: 35.093063, mean_q: -51.984287\n",
      " 19400/50000: episode: 97, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 7.731295, mae: 35.113102, mean_q: -52.008457\n",
      " 19600/50000: episode: 98, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 8.944699, mae: 35.046844, mean_q: -51.870251\n",
      " 19800/50000: episode: 99, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.035892, mae: 34.990353, mean_q: -51.976524\n",
      " 20000/50000: episode: 100, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 6.320077, mae: 35.065674, mean_q: -51.966812\n",
      " 20200/50000: episode: 101, duration: 1.933s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 5.419811, mae: 35.008251, mean_q: -51.885761\n",
      " 20400/50000: episode: 102, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 7.964887, mae: 35.069569, mean_q: -52.040096\n",
      " 20600/50000: episode: 103, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 5.376060, mae: 35.144535, mean_q: -52.115253\n",
      " 20800/50000: episode: 104, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.457909, mae: 35.200756, mean_q: -52.241196\n",
      " 21000/50000: episode: 105, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 7.904465, mae: 35.189442, mean_q: -52.241112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21200/50000: episode: 106, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 9.674154, mae: 35.215267, mean_q: -52.163406\n",
      " 21400/50000: episode: 107, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.470133, mae: 35.185429, mean_q: -52.191994\n",
      " 21600/50000: episode: 108, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 5.426492, mae: 35.206001, mean_q: -52.215843\n",
      " 21800/50000: episode: 109, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 5.525233, mae: 35.367081, mean_q: -52.536747\n",
      " 22000/50000: episode: 110, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.886405, mae: 35.513779, mean_q: -52.737255\n",
      " 22200/50000: episode: 111, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.245318, mae: 35.652016, mean_q: -52.950947\n",
      " 22400/50000: episode: 112, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 6.136022, mae: 35.716862, mean_q: -52.953735\n",
      " 22600/50000: episode: 113, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 5.330854, mae: 35.811043, mean_q: -53.176231\n",
      " 22800/50000: episode: 114, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.152398, mae: 35.927521, mean_q: -53.397842\n",
      " 23000/50000: episode: 115, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 6.924302, mae: 36.008438, mean_q: -53.330002\n",
      " 23200/50000: episode: 116, duration: 1.919s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 6.625166, mae: 36.011757, mean_q: -53.433578\n",
      " 23400/50000: episode: 117, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.151166, mae: 36.024036, mean_q: -53.457920\n",
      " 23600/50000: episode: 118, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 6.702738, mae: 36.140350, mean_q: -53.596867\n",
      " 23800/50000: episode: 119, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 8.618159, mae: 36.045631, mean_q: -53.409023\n",
      " 24000/50000: episode: 120, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 6.492860, mae: 36.051723, mean_q: -53.449451\n",
      " 24200/50000: episode: 121, duration: 1.914s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.832114, mae: 35.939159, mean_q: -53.248173\n",
      " 24400/50000: episode: 122, duration: 1.952s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 7.288182, mae: 35.953014, mean_q: -53.320133\n",
      " 24600/50000: episode: 123, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.468937, mae: 36.014015, mean_q: -53.542759\n",
      " 24800/50000: episode: 124, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.207530, mae: 36.188091, mean_q: -53.732716\n",
      " 25000/50000: episode: 125, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 5.970210, mae: 36.192726, mean_q: -53.835262\n",
      " 25200/50000: episode: 126, duration: 1.965s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 6.776489, mae: 36.370037, mean_q: -54.028603\n",
      " 25400/50000: episode: 127, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.682224, mae: 36.400131, mean_q: -54.029034\n",
      " 25600/50000: episode: 128, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.905565, mae: 36.656414, mean_q: -54.501102\n",
      " 25800/50000: episode: 129, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.828860, mae: 36.879547, mean_q: -54.813702\n",
      " 26000/50000: episode: 130, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.308938, mae: 36.944080, mean_q: -54.800819\n",
      " 26200/50000: episode: 131, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 6.709368, mae: 36.937958, mean_q: -54.882824\n",
      " 26400/50000: episode: 132, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.508290, mae: 37.009281, mean_q: -54.929665\n",
      " 26600/50000: episode: 133, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.795245, mae: 37.043457, mean_q: -54.878555\n",
      " 26800/50000: episode: 134, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 6.900417, mae: 36.996883, mean_q: -54.870605\n",
      " 27000/50000: episode: 135, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.515083, mae: 37.058784, mean_q: -54.977924\n",
      " 27200/50000: episode: 136, duration: 1.896s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 4.465055, mae: 37.111389, mean_q: -55.238293\n",
      " 27400/50000: episode: 137, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 6.913485, mae: 37.264462, mean_q: -55.317551\n",
      " 27600/50000: episode: 138, duration: 1.995s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.024808, mae: 37.400578, mean_q: -55.612381\n",
      " 27800/50000: episode: 139, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 8.740259, mae: 37.504715, mean_q: -55.655922\n",
      " 28000/50000: episode: 140, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.554328, mae: 37.639034, mean_q: -55.961742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28200/50000: episode: 141, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.879042, mae: 37.806087, mean_q: -56.215790\n",
      " 28400/50000: episode: 142, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 9.094000, mae: 37.924915, mean_q: -56.188515\n",
      " 28600/50000: episode: 143, duration: 1.914s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 7.319055, mae: 37.800850, mean_q: -56.164639\n",
      " 28800/50000: episode: 144, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.656014, mae: 37.848076, mean_q: -56.157051\n",
      " 29000/50000: episode: 145, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.408804, mae: 37.918468, mean_q: -56.317089\n",
      " 29200/50000: episode: 146, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 6.333021, mae: 37.992783, mean_q: -56.507999\n",
      " 29400/50000: episode: 147, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.050543, mae: 38.155899, mean_q: -56.609375\n",
      " 29600/50000: episode: 148, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.771170, mae: 38.135773, mean_q: -56.698971\n",
      " 29800/50000: episode: 149, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.643209, mae: 38.188271, mean_q: -56.604374\n",
      " 30000/50000: episode: 150, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.017368, mae: 38.335102, mean_q: -56.965542\n",
      " 30200/50000: episode: 151, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 10.077570, mae: 38.141781, mean_q: -56.455135\n",
      " 30400/50000: episode: 152, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 7.497553, mae: 38.259884, mean_q: -56.839127\n",
      " 30600/50000: episode: 153, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.277493, mae: 38.304752, mean_q: -56.845715\n",
      " 30800/50000: episode: 154, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.582137, mae: 38.199547, mean_q: -56.547989\n",
      " 31000/50000: episode: 155, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 8.855474, mae: 38.212231, mean_q: -56.697498\n",
      " 31200/50000: episode: 156, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.845241, mae: 38.281689, mean_q: -56.858273\n",
      " 31400/50000: episode: 157, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 9.534972, mae: 38.150143, mean_q: -56.620293\n",
      " 31600/50000: episode: 158, duration: 1.914s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 7.278694, mae: 38.268360, mean_q: -56.822910\n",
      " 31800/50000: episode: 159, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.384090, mae: 38.316151, mean_q: -56.863144\n",
      " 32000/50000: episode: 160, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.325297, mae: 38.344933, mean_q: -57.007851\n",
      " 32200/50000: episode: 161, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 9.565139, mae: 38.366867, mean_q: -56.831982\n",
      " 32400/50000: episode: 162, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 4.952649, mae: 38.420094, mean_q: -57.152920\n",
      " 32600/50000: episode: 163, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.857490, mae: 38.591717, mean_q: -57.315697\n",
      " 32800/50000: episode: 164, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 11.723682, mae: 38.504684, mean_q: -57.052479\n",
      " 33000/50000: episode: 165, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 6.276217, mae: 38.431671, mean_q: -57.088280\n",
      " 33200/50000: episode: 166, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 11.392630, mae: 38.362598, mean_q: -56.771374\n",
      " 33400/50000: episode: 167, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.755685, mae: 38.422722, mean_q: -57.093365\n",
      " 33600/50000: episode: 168, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.137253, mae: 38.630661, mean_q: -57.536854\n",
      " 33800/50000: episode: 169, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.894515, mae: 38.745640, mean_q: -57.441963\n",
      " 34000/50000: episode: 170, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.605556, mae: 38.720844, mean_q: -57.422691\n",
      " 34200/50000: episode: 171, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.189868, mae: 38.422222, mean_q: -57.008877\n",
      " 34400/50000: episode: 172, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.323701, mae: 38.614326, mean_q: -57.250389\n",
      " 34600/50000: episode: 173, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 6.754981, mae: 38.658100, mean_q: -57.426144\n",
      " 34800/50000: episode: 174, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.833506, mae: 38.660934, mean_q: -57.443592\n",
      " 35000/50000: episode: 175, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.651031, mae: 38.686661, mean_q: -57.420303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35200/50000: episode: 176, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 10.073669, mae: 38.692265, mean_q: -57.439507\n",
      " 35400/50000: episode: 177, duration: 1.914s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.551554, mae: 38.802181, mean_q: -57.562332\n",
      " 35600/50000: episode: 178, duration: 1.995s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.556763, mae: 38.851189, mean_q: -57.698624\n",
      " 35800/50000: episode: 179, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.063097, mae: 38.887787, mean_q: -57.771786\n",
      " 36000/50000: episode: 180, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.741241, mae: 38.945599, mean_q: -57.789494\n",
      " 36200/50000: episode: 181, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 10.115276, mae: 38.996857, mean_q: -57.915062\n",
      " 36400/50000: episode: 182, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.282869, mae: 38.968395, mean_q: -57.930763\n",
      " 36600/50000: episode: 183, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.457626, mae: 39.079029, mean_q: -58.139004\n",
      " 36800/50000: episode: 184, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 6.368551, mae: 39.264351, mean_q: -58.410267\n",
      " 37000/50000: episode: 185, duration: 1.932s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 10.058080, mae: 39.297863, mean_q: -58.253605\n",
      " 37200/50000: episode: 186, duration: 2.080s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.741979, mae: 39.400620, mean_q: -58.548683\n",
      " 37400/50000: episode: 187, duration: 2.028s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.165721, mae: 39.462856, mean_q: -58.556190\n",
      " 37600/50000: episode: 188, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.200432, mae: 39.358601, mean_q: -58.442482\n",
      " 37800/50000: episode: 189, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.714143, mae: 39.418457, mean_q: -58.640938\n",
      " 38000/50000: episode: 190, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 10.885813, mae: 39.523083, mean_q: -58.568405\n",
      " 38200/50000: episode: 191, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 5.796196, mae: 39.552021, mean_q: -58.878075\n",
      " 38400/50000: episode: 192, duration: 1.996s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.014843, mae: 39.551586, mean_q: -58.673828\n",
      " 38600/50000: episode: 193, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 11.587371, mae: 39.556393, mean_q: -58.626602\n",
      " 38800/50000: episode: 194, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.524384, mae: 39.467499, mean_q: -58.558231\n",
      " 39000/50000: episode: 195, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.214292, mae: 39.453251, mean_q: -58.436935\n",
      " 39200/50000: episode: 196, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.902097, mae: 39.462696, mean_q: -58.661671\n",
      " 39400/50000: episode: 197, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.318241, mae: 39.479652, mean_q: -58.739548\n",
      " 39600/50000: episode: 198, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.156776, mae: 39.551239, mean_q: -58.763885\n",
      " 39800/50000: episode: 199, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.345480, mae: 39.547718, mean_q: -58.598114\n",
      " 40000/50000: episode: 200, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 11.385084, mae: 39.386444, mean_q: -58.316124\n",
      " 40200/50000: episode: 201, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.263362, mae: 39.367897, mean_q: -58.463112\n",
      " 40400/50000: episode: 202, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.769723, mae: 39.427246, mean_q: -58.573334\n",
      " 40600/50000: episode: 203, duration: 1.938s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.901589, mae: 39.305363, mean_q: -58.318478\n",
      " 40800/50000: episode: 204, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.833768, mae: 39.278343, mean_q: -58.246964\n",
      " 41000/50000: episode: 205, duration: 1.965s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 9.379104, mae: 39.135513, mean_q: -58.093403\n",
      " 41200/50000: episode: 206, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.780801, mae: 39.060917, mean_q: -58.008877\n",
      " 41400/50000: episode: 207, duration: 2.013s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 6.909439, mae: 39.120846, mean_q: -58.198952\n",
      " 41600/50000: episode: 208, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.510448, mae: 39.073898, mean_q: -57.984177\n",
      " 41800/50000: episode: 209, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 10.117373, mae: 39.171165, mean_q: -58.086983\n",
      " 42000/50000: episode: 210, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 6.434872, mae: 39.250832, mean_q: -58.355225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42200/50000: episode: 211, duration: 1.954s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.441375, mae: 39.293556, mean_q: -58.286102\n",
      " 42400/50000: episode: 212, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.939999, mae: 39.237450, mean_q: -58.338303\n",
      " 42600/50000: episode: 213, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.844736, mae: 39.180714, mean_q: -58.172676\n",
      " 42800/50000: episode: 214, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.431265, mae: 39.334480, mean_q: -58.584892\n",
      " 43000/50000: episode: 215, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.720726, mae: 39.437641, mean_q: -58.594925\n",
      " 43200/50000: episode: 216, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.285361, mae: 39.290810, mean_q: -58.226818\n",
      " 43400/50000: episode: 217, duration: 1.963s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 8.173650, mae: 39.331448, mean_q: -58.419491\n",
      " 43600/50000: episode: 218, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 10.502483, mae: 39.211945, mean_q: -58.105469\n",
      " 43800/50000: episode: 219, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.532795, mae: 39.132339, mean_q: -58.157001\n",
      " 44000/50000: episode: 220, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.861984, mae: 39.143497, mean_q: -58.098221\n",
      " 44200/50000: episode: 221, duration: 1.967s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.484337, mae: 39.132301, mean_q: -58.046173\n",
      " 44400/50000: episode: 222, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.829386, mae: 39.150440, mean_q: -58.182362\n",
      " 44600/50000: episode: 223, duration: 1.967s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.881133, mae: 39.203098, mean_q: -58.165207\n",
      " 44800/50000: episode: 224, duration: 2.030s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.934265, mae: 39.099579, mean_q: -58.106693\n",
      " 45000/50000: episode: 225, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 6.374751, mae: 39.180634, mean_q: -58.312500\n",
      " 45200/50000: episode: 226, duration: 1.958s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 6.260943, mae: 39.250881, mean_q: -58.340088\n",
      " 45400/50000: episode: 227, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.163165, mae: 39.285030, mean_q: -58.389038\n",
      " 45600/50000: episode: 228, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 11.988037, mae: 39.255604, mean_q: -58.099121\n",
      " 45800/50000: episode: 229, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 5.516778, mae: 39.142860, mean_q: -58.180187\n",
      " 46000/50000: episode: 230, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 6.321579, mae: 39.380993, mean_q: -58.648174\n",
      " 46200/50000: episode: 231, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 9.597957, mae: 39.364403, mean_q: -58.446915\n",
      " 46400/50000: episode: 232, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 9.739574, mae: 39.374130, mean_q: -58.295528\n",
      " 46600/50000: episode: 233, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.253317, mae: 39.183605, mean_q: -58.306206\n",
      " 46800/50000: episode: 234, duration: 1.970s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.270395, mae: 39.187096, mean_q: -58.246300\n",
      " 47000/50000: episode: 235, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 6.387305, mae: 39.296513, mean_q: -58.400108\n",
      " 47200/50000: episode: 236, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.091799, mae: 39.527607, mean_q: -58.817051\n",
      " 47400/50000: episode: 237, duration: 2.040s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.649281, mae: 39.443687, mean_q: -58.459560\n",
      " 47600/50000: episode: 238, duration: 1.983s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 10.828552, mae: 39.276558, mean_q: -58.261856\n",
      " 47800/50000: episode: 239, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 6.861240, mae: 39.188251, mean_q: -58.258072\n",
      " 48000/50000: episode: 240, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.303449, mae: 39.275375, mean_q: -58.376034\n",
      " 48200/50000: episode: 241, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.561721, mae: 39.288860, mean_q: -58.255638\n",
      " 48400/50000: episode: 242, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.212340, mae: 39.122993, mean_q: -57.941708\n",
      " 48600/50000: episode: 243, duration: 2.000s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.555266, mae: 39.087101, mean_q: -58.095669\n",
      " 48800/50000: episode: 244, duration: 1.998s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 8.948218, mae: 39.034103, mean_q: -57.831730\n",
      " 49000/50000: episode: 245, duration: 2.015s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 9.601199, mae: 38.729954, mean_q: -57.386478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49200/50000: episode: 246, duration: 2.079s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.736361, mae: 38.624165, mean_q: -57.365799\n",
      " 49400/50000: episode: 247, duration: 2.077s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.970424, mae: 38.616264, mean_q: -57.341972\n",
      " 49600/50000: episode: 248, duration: 2.001s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.675717, mae: 38.467510, mean_q: -57.051014\n",
      " 49800/50000: episode: 249, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 7.064775, mae: 38.244556, mean_q: -56.784630\n",
      " 50000/50000: episode: 250, duration: 2.083s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 7.180045, mae: 38.143845, mean_q: -56.663273\n",
      "done, took 447.084 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7d4e216d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f826a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: -200.000, steps: 200\n",
      "Episode 3: reward: -200.000, steps: 200\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "Episode 5: reward: -200.000, steps: 200\n",
      "Episode 6: reward: -200.000, steps: 200\n",
      "Episode 7: reward: -200.000, steps: 200\n",
      "Episode 8: reward: -200.000, steps: 200\n",
      "Episode 9: reward: -200.000, steps: 200\n",
      "Episode 10: reward: -200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7d4e20eb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a9958",
   "metadata": {},
   "source": [
    "## Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "828ed29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f512b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500/50000: episode: 1, duration: 5.216s, episode steps: 500, steps per second:  96, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.039595, mae: 1.345360, mean_q: -1.863840\n",
      "  1000/50000: episode: 2, duration: 4.665s, episode steps: 500, steps per second: 107, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.025499, mae: 3.668244, mean_q: -5.367797\n",
      "  1447/50000: episode: 3, duration: 4.228s, episode steps: 447, steps per second: 106, episode reward: -446.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.077571, mae: 6.075470, mean_q: -8.900464\n",
      "  1947/50000: episode: 4, duration: 4.649s, episode steps: 500, steps per second: 108, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.116497, mae: 8.383890, mean_q: -12.331873\n",
      "  2279/50000: episode: 5, duration: 3.282s, episode steps: 332, steps per second: 101, episode reward: -331.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.167442, mae: 10.277800, mean_q: -15.141746\n",
      "  2750/50000: episode: 6, duration: 4.381s, episode steps: 471, steps per second: 108, episode reward: -470.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.275859, mae: 12.034747, mean_q: -17.725975\n",
      "  3124/50000: episode: 7, duration: 3.494s, episode steps: 374, steps per second: 107, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.379303, mae: 13.783840, mean_q: -20.306030\n",
      "  3614/50000: episode: 8, duration: 4.663s, episode steps: 490, steps per second: 105, episode reward: -489.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.507950, mae: 15.383195, mean_q: -22.618124\n",
      "  3929/50000: episode: 9, duration: 2.941s, episode steps: 315, steps per second: 107, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.499342, mae: 16.675413, mean_q: -24.525906\n",
      "  4276/50000: episode: 10, duration: 3.255s, episode steps: 347, steps per second: 107, episode reward: -346.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.487698, mae: 17.800049, mean_q: -26.228470\n",
      "  4776/50000: episode: 11, duration: 4.656s, episode steps: 500, steps per second: 107, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.575617, mae: 19.075491, mean_q: -28.093027\n",
      "  5168/50000: episode: 12, duration: 3.751s, episode steps: 392, steps per second: 104, episode reward: -391.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.712422, mae: 20.313435, mean_q: -29.895632\n",
      "  5594/50000: episode: 13, duration: 3.980s, episode steps: 426, steps per second: 107, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.714568, mae: 21.285019, mean_q: -31.331078\n",
      "  5876/50000: episode: 14, duration: 2.648s, episode steps: 282, steps per second: 107, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.938682, mae: 21.922316, mean_q: -32.236008\n",
      "  6161/50000: episode: 15, duration: 2.681s, episode steps: 285, steps per second: 106, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.906605, mae: 22.541252, mean_q: -33.150105\n",
      "  6498/50000: episode: 16, duration: 3.161s, episode steps: 337, steps per second: 107, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.906367, mae: 23.179375, mean_q: -34.096443\n",
      "  6785/50000: episode: 17, duration: 2.727s, episode steps: 287, steps per second: 105, episode reward: -286.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.811418, mae: 23.740971, mean_q: -34.919048\n",
      "  7089/50000: episode: 18, duration: 2.854s, episode steps: 304, steps per second: 107, episode reward: -303.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 1.119756, mae: 24.294798, mean_q: -35.723953\n",
      "  7480/50000: episode: 19, duration: 3.674s, episode steps: 391, steps per second: 106, episode reward: -390.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.910066, mae: 24.925072, mean_q: -36.694988\n",
      "  7761/50000: episode: 20, duration: 2.649s, episode steps: 281, steps per second: 106, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 1.069058, mae: 25.442970, mean_q: -37.440704\n",
      "  8060/50000: episode: 21, duration: 2.822s, episode steps: 299, steps per second: 106, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.263571, mae: 25.857449, mean_q: -38.028355\n",
      "  8284/50000: episode: 22, duration: 2.114s, episode steps: 224, steps per second: 106, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.552758, mae: 26.165257, mean_q: -38.443256\n",
      "  8550/50000: episode: 23, duration: 2.525s, episode steps: 266, steps per second: 105, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.969618, mae: 26.650225, mean_q: -39.219093\n",
      "  8871/50000: episode: 24, duration: 3.063s, episode steps: 321, steps per second: 105, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 1.068403, mae: 27.035131, mean_q: -39.779621\n",
      "  9114/50000: episode: 25, duration: 2.613s, episode steps: 243, steps per second:  93, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 1.489109, mae: 27.388933, mean_q: -40.286449\n",
      "  9400/50000: episode: 26, duration: 3.012s, episode steps: 286, steps per second:  95, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 1.728430, mae: 27.648848, mean_q: -40.662807\n",
      "  9576/50000: episode: 27, duration: 1.718s, episode steps: 176, steps per second: 102, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.388860, mae: 27.824461, mean_q: -40.902908\n",
      "  9934/50000: episode: 28, duration: 3.634s, episode steps: 358, steps per second:  99, episode reward: -357.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.324431, mae: 28.212320, mean_q: -41.473999\n",
      " 10243/50000: episode: 29, duration: 2.936s, episode steps: 309, steps per second: 105, episode reward: -308.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 1.401278, mae: 28.356171, mean_q: -41.648678\n",
      " 10497/50000: episode: 30, duration: 2.413s, episode steps: 254, steps per second: 105, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 1.583822, mae: 28.575500, mean_q: -41.989658\n",
      " 10729/50000: episode: 31, duration: 2.206s, episode steps: 232, steps per second: 105, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.310920, mae: 28.932116, mean_q: -42.547604\n",
      " 10952/50000: episode: 32, duration: 2.121s, episode steps: 223, steps per second: 105, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 1.579758, mae: 29.297895, mean_q: -43.087303\n",
      " 11208/50000: episode: 33, duration: 2.427s, episode steps: 256, steps per second: 105, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.701032, mae: 29.367224, mean_q: -43.155846\n",
      " 11451/50000: episode: 34, duration: 2.321s, episode steps: 243, steps per second: 105, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.589709, mae: 29.438011, mean_q: -43.227455\n",
      " 11657/50000: episode: 35, duration: 1.969s, episode steps: 206, steps per second: 105, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 1.210388, mae: 29.637894, mean_q: -43.542301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12025/50000: episode: 36, duration: 3.488s, episode steps: 368, steps per second: 106, episode reward: -367.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.766647, mae: 29.760349, mean_q: -43.690399\n",
      " 12238/50000: episode: 37, duration: 2.015s, episode steps: 213, steps per second: 106, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 1.520958, mae: 29.821804, mean_q: -43.768421\n",
      " 12496/50000: episode: 38, duration: 2.446s, episode steps: 258, steps per second: 105, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.534717, mae: 29.835117, mean_q: -43.746193\n",
      " 12635/50000: episode: 39, duration: 1.329s, episode steps: 139, steps per second: 105, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.465238, mae: 29.937311, mean_q: -43.941143\n",
      " 12910/50000: episode: 40, duration: 2.606s, episode steps: 275, steps per second: 106, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 1.615297, mae: 30.016663, mean_q: -44.048084\n",
      " 13092/50000: episode: 41, duration: 1.740s, episode steps: 182, steps per second: 105, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 1.527516, mae: 30.077250, mean_q: -44.146099\n",
      " 13306/50000: episode: 42, duration: 2.030s, episode steps: 214, steps per second: 105, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 1.971252, mae: 30.264626, mean_q: -44.436649\n",
      " 13507/50000: episode: 43, duration: 1.984s, episode steps: 201, steps per second: 101, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.725018, mae: 30.153063, mean_q: -44.238853\n",
      " 13727/50000: episode: 44, duration: 2.188s, episode steps: 220, steps per second: 101, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.917891, mae: 30.186562, mean_q: -44.265327\n",
      " 13864/50000: episode: 45, duration: 1.310s, episode steps: 137, steps per second: 105, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 1.410809, mae: 30.262728, mean_q: -44.399986\n",
      " 14088/50000: episode: 46, duration: 2.263s, episode steps: 224, steps per second:  99, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.528075, mae: 30.395376, mean_q: -44.601093\n",
      " 14312/50000: episode: 47, duration: 2.298s, episode steps: 224, steps per second:  97, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 1.649927, mae: 30.317972, mean_q: -44.411709\n",
      " 14550/50000: episode: 48, duration: 2.300s, episode steps: 238, steps per second: 103, episode reward: -237.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.587063, mae: 30.448866, mean_q: -44.635338\n",
      " 14712/50000: episode: 49, duration: 1.559s, episode steps: 162, steps per second: 104, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.642685, mae: 30.160561, mean_q: -44.160831\n",
      " 14914/50000: episode: 50, duration: 2.003s, episode steps: 202, steps per second: 101, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 1.542832, mae: 30.206427, mean_q: -44.247925\n",
      " 15068/50000: episode: 51, duration: 1.497s, episode steps: 154, steps per second: 103, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.869509, mae: 30.427423, mean_q: -44.567028\n",
      " 15220/50000: episode: 52, duration: 1.487s, episode steps: 152, steps per second: 102, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 1.725291, mae: 30.080853, mean_q: -44.010406\n",
      " 15395/50000: episode: 53, duration: 1.706s, episode steps: 175, steps per second: 103, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.222680, mae: 30.217585, mean_q: -44.259617\n",
      " 15582/50000: episode: 54, duration: 1.834s, episode steps: 187, steps per second: 102, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 1.629422, mae: 30.222269, mean_q: -44.248997\n",
      " 15832/50000: episode: 55, duration: 2.491s, episode steps: 250, steps per second: 100, episode reward: -249.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 1.502175, mae: 30.248173, mean_q: -44.301258\n",
      " 16024/50000: episode: 56, duration: 1.884s, episode steps: 192, steps per second: 102, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.573835, mae: 30.053556, mean_q: -44.001942\n",
      " 16210/50000: episode: 57, duration: 1.802s, episode steps: 186, steps per second: 103, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.242662, mae: 30.194204, mean_q: -44.218685\n",
      " 16372/50000: episode: 58, duration: 1.558s, episode steps: 162, steps per second: 104, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 1.627696, mae: 30.152210, mean_q: -44.114681\n",
      " 16614/50000: episode: 59, duration: 2.315s, episode steps: 242, steps per second: 105, episode reward: -241.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 1.396128, mae: 30.239439, mean_q: -44.305889\n",
      " 16743/50000: episode: 60, duration: 1.251s, episode steps: 129, steps per second: 103, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.572708, mae: 29.948456, mean_q: -43.816502\n",
      " 17011/50000: episode: 61, duration: 2.584s, episode steps: 268, steps per second: 104, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 1.245760, mae: 30.144281, mean_q: -44.150520\n",
      " 17200/50000: episode: 62, duration: 1.877s, episode steps: 189, steps per second: 101, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 1.675843, mae: 29.890139, mean_q: -43.690105\n",
      " 17377/50000: episode: 63, duration: 1.710s, episode steps: 177, steps per second: 103, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 1.570922, mae: 29.967710, mean_q: -43.807026\n",
      " 17575/50000: episode: 64, duration: 1.908s, episode steps: 198, steps per second: 104, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 1.231100, mae: 29.872086, mean_q: -43.699402\n",
      " 17728/50000: episode: 65, duration: 1.472s, episode steps: 153, steps per second: 104, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.756266, mae: 29.628990, mean_q: -43.336128\n",
      " 17984/50000: episode: 66, duration: 2.462s, episode steps: 256, steps per second: 104, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 1.325804, mae: 29.844997, mean_q: -43.679195\n",
      " 18175/50000: episode: 67, duration: 1.842s, episode steps: 191, steps per second: 104, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.625154, mae: 29.635118, mean_q: -43.320663\n",
      " 18331/50000: episode: 68, duration: 1.499s, episode steps: 156, steps per second: 104, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.482115, mae: 29.697397, mean_q: -43.418072\n",
      " 18508/50000: episode: 69, duration: 1.708s, episode steps: 177, steps per second: 104, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 1.323425, mae: 29.844580, mean_q: -43.646458\n",
      " 19008/50000: episode: 70, duration: 4.787s, episode steps: 500, steps per second: 104, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000],  loss: 1.399928, mae: 29.533009, mean_q: -43.144726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19169/50000: episode: 71, duration: 1.551s, episode steps: 161, steps per second: 104, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.941131, mae: 29.144310, mean_q: -42.484146\n",
      " 19368/50000: episode: 72, duration: 1.918s, episode steps: 199, steps per second: 104, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 1.800464, mae: 29.227705, mean_q: -42.648849\n",
      " 19571/50000: episode: 73, duration: 1.996s, episode steps: 203, steps per second: 102, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 1.604700, mae: 29.011965, mean_q: -42.347225\n",
      " 19718/50000: episode: 74, duration: 1.747s, episode steps: 147, steps per second:  84, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.347512, mae: 28.857824, mean_q: -42.113514\n",
      " 19895/50000: episode: 75, duration: 2.064s, episode steps: 177, steps per second:  86, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.373302, mae: 28.960117, mean_q: -42.260067\n",
      " 20054/50000: episode: 76, duration: 2.020s, episode steps: 159, steps per second:  79, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 1.380874, mae: 28.898376, mean_q: -42.173538\n",
      " 20220/50000: episode: 77, duration: 1.915s, episode steps: 166, steps per second:  87, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 1.518384, mae: 28.701254, mean_q: -41.872707\n",
      " 20430/50000: episode: 78, duration: 2.134s, episode steps: 210, steps per second:  98, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 1.665157, mae: 28.774698, mean_q: -41.980106\n",
      " 20573/50000: episode: 79, duration: 1.463s, episode steps: 143, steps per second:  98, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 1.349154, mae: 28.988014, mean_q: -42.316589\n",
      " 20714/50000: episode: 80, duration: 1.431s, episode steps: 141, steps per second:  99, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.801 [0.000, 2.000],  loss: 1.713769, mae: 28.753767, mean_q: -41.875904\n",
      " 20916/50000: episode: 81, duration: 2.067s, episode steps: 202, steps per second:  98, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 1.418000, mae: 28.524178, mean_q: -41.575890\n",
      " 21045/50000: episode: 82, duration: 1.312s, episode steps: 129, steps per second:  98, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 1.345369, mae: 28.641001, mean_q: -41.769489\n",
      " 21218/50000: episode: 83, duration: 1.753s, episode steps: 173, steps per second:  99, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 1.544003, mae: 28.543488, mean_q: -41.604908\n",
      " 21400/50000: episode: 84, duration: 1.843s, episode steps: 182, steps per second:  99, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 1.255926, mae: 28.595810, mean_q: -41.721569\n",
      " 21530/50000: episode: 85, duration: 1.317s, episode steps: 130, steps per second:  99, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.780162, mae: 28.603939, mean_q: -41.721321\n",
      " 21673/50000: episode: 86, duration: 1.467s, episode steps: 143, steps per second:  97, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 1.511720, mae: 28.197308, mean_q: -41.061756\n",
      " 21878/50000: episode: 87, duration: 2.089s, episode steps: 205, steps per second:  98, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.408481, mae: 28.254192, mean_q: -41.166878\n",
      " 22080/50000: episode: 88, duration: 2.055s, episode steps: 202, steps per second:  98, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 1.445303, mae: 28.327999, mean_q: -41.271370\n",
      " 22216/50000: episode: 89, duration: 1.401s, episode steps: 136, steps per second:  97, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 1.638410, mae: 28.093422, mean_q: -40.899990\n",
      " 22380/50000: episode: 90, duration: 1.746s, episode steps: 164, steps per second:  94, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.406239, mae: 27.863285, mean_q: -40.561081\n",
      " 22504/50000: episode: 91, duration: 1.321s, episode steps: 124, steps per second:  94, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.502342, mae: 27.996172, mean_q: -40.770908\n",
      " 22634/50000: episode: 92, duration: 1.310s, episode steps: 130, steps per second:  99, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 1.388373, mae: 27.756338, mean_q: -40.378254\n",
      " 22836/50000: episode: 93, duration: 1.955s, episode steps: 202, steps per second: 103, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 1.574423, mae: 27.709591, mean_q: -40.297897\n",
      " 22994/50000: episode: 94, duration: 1.538s, episode steps: 158, steps per second: 103, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 1.228848, mae: 27.650486, mean_q: -40.279026\n",
      " 23165/50000: episode: 95, duration: 1.657s, episode steps: 171, steps per second: 103, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.332645, mae: 27.625374, mean_q: -40.217392\n",
      " 23336/50000: episode: 96, duration: 1.661s, episode steps: 171, steps per second: 103, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.072578, mae: 27.745066, mean_q: -40.470013\n",
      " 23482/50000: episode: 97, duration: 1.418s, episode steps: 146, steps per second: 103, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.192879, mae: 27.674496, mean_q: -40.345997\n",
      " 23635/50000: episode: 98, duration: 1.490s, episode steps: 153, steps per second: 103, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 1.323751, mae: 27.634434, mean_q: -40.251957\n",
      " 23830/50000: episode: 99, duration: 1.889s, episode steps: 195, steps per second: 103, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 1.263041, mae: 27.356195, mean_q: -39.849880\n",
      " 23976/50000: episode: 100, duration: 1.420s, episode steps: 146, steps per second: 103, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 1.287441, mae: 27.524584, mean_q: -40.089306\n",
      " 24158/50000: episode: 101, duration: 1.768s, episode steps: 182, steps per second: 103, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 1.130649, mae: 27.337315, mean_q: -39.836582\n",
      " 24340/50000: episode: 102, duration: 1.771s, episode steps: 182, steps per second: 103, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 1.170190, mae: 27.503380, mean_q: -40.089211\n",
      " 24550/50000: episode: 103, duration: 2.048s, episode steps: 210, steps per second: 103, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.123387, mae: 27.543068, mean_q: -40.156223\n",
      " 24775/50000: episode: 104, duration: 2.184s, episode steps: 225, steps per second: 103, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 1.189009, mae: 27.392876, mean_q: -39.931553\n",
      " 24947/50000: episode: 105, duration: 1.679s, episode steps: 172, steps per second: 102, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 1.215370, mae: 27.225267, mean_q: -39.670807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25076/50000: episode: 106, duration: 1.268s, episode steps: 129, steps per second: 102, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 1.389147, mae: 27.163595, mean_q: -39.547760\n",
      " 25230/50000: episode: 107, duration: 1.510s, episode steps: 154, steps per second: 102, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 1.258999, mae: 27.207317, mean_q: -39.618813\n",
      " 25403/50000: episode: 108, duration: 1.689s, episode steps: 173, steps per second: 102, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 1.510840, mae: 27.176559, mean_q: -39.543266\n",
      " 25539/50000: episode: 109, duration: 1.328s, episode steps: 136, steps per second: 102, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.097504, mae: 26.912144, mean_q: -39.147583\n",
      " 25651/50000: episode: 110, duration: 1.101s, episode steps: 112, steps per second: 102, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 1.400666, mae: 26.745268, mean_q: -38.899353\n",
      " 25865/50000: episode: 111, duration: 2.077s, episode steps: 214, steps per second: 103, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.135047, mae: 26.911972, mean_q: -39.179562\n",
      " 26021/50000: episode: 112, duration: 1.524s, episode steps: 156, steps per second: 102, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 1.376819, mae: 26.789062, mean_q: -38.983379\n",
      " 26166/50000: episode: 113, duration: 1.418s, episode steps: 145, steps per second: 102, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 1.089495, mae: 26.712164, mean_q: -38.885784\n",
      " 26298/50000: episode: 114, duration: 1.291s, episode steps: 132, steps per second: 102, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 1.173445, mae: 26.723825, mean_q: -38.901047\n",
      " 26519/50000: episode: 115, duration: 2.151s, episode steps: 221, steps per second: 103, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 1.229662, mae: 26.704638, mean_q: -38.871731\n",
      " 26698/50000: episode: 116, duration: 1.750s, episode steps: 179, steps per second: 102, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 1.208315, mae: 26.875059, mean_q: -39.127350\n",
      " 26950/50000: episode: 117, duration: 2.463s, episode steps: 252, steps per second: 102, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 1.106642, mae: 26.562534, mean_q: -38.690231\n",
      " 27105/50000: episode: 118, duration: 1.523s, episode steps: 155, steps per second: 102, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 1.272705, mae: 26.527529, mean_q: -38.638664\n",
      " 27293/50000: episode: 119, duration: 1.842s, episode steps: 188, steps per second: 102, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 1.141971, mae: 26.263735, mean_q: -38.225685\n",
      " 27415/50000: episode: 120, duration: 1.278s, episode steps: 122, steps per second:  95, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.104062, mae: 26.670616, mean_q: -38.869461\n",
      " 27578/50000: episode: 121, duration: 1.606s, episode steps: 163, steps per second: 101, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 1.116218, mae: 26.409599, mean_q: -38.442791\n",
      " 27824/50000: episode: 122, duration: 2.446s, episode steps: 246, steps per second: 101, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 1.300835, mae: 26.544422, mean_q: -38.649292\n",
      " 27982/50000: episode: 123, duration: 1.641s, episode steps: 158, steps per second:  96, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 1.187118, mae: 26.366693, mean_q: -38.407307\n",
      " 28133/50000: episode: 124, duration: 1.509s, episode steps: 151, steps per second: 100, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.461684, mae: 26.304493, mean_q: -38.281620\n",
      " 28320/50000: episode: 125, duration: 1.830s, episode steps: 187, steps per second: 102, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.350144, mae: 26.242514, mean_q: -38.215633\n",
      " 28524/50000: episode: 126, duration: 2.007s, episode steps: 204, steps per second: 102, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 1.326850, mae: 26.072500, mean_q: -37.957500\n",
      " 28713/50000: episode: 127, duration: 1.868s, episode steps: 189, steps per second: 101, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 1.156118, mae: 26.301191, mean_q: -38.302929\n",
      " 28904/50000: episode: 128, duration: 1.876s, episode steps: 191, steps per second: 102, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.869 [0.000, 2.000],  loss: 1.141696, mae: 26.164606, mean_q: -38.118374\n",
      " 29081/50000: episode: 129, duration: 1.748s, episode steps: 177, steps per second: 101, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.023273, mae: 26.205425, mean_q: -38.168182\n",
      " 29233/50000: episode: 130, duration: 1.565s, episode steps: 152, steps per second:  97, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 1.155583, mae: 26.277561, mean_q: -38.282936\n",
      " 29441/50000: episode: 131, duration: 2.078s, episode steps: 208, steps per second: 100, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.071021, mae: 26.098106, mean_q: -38.039288\n",
      " 29634/50000: episode: 132, duration: 1.902s, episode steps: 193, steps per second: 101, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 1.322325, mae: 25.979662, mean_q: -37.844707\n",
      " 29798/50000: episode: 133, duration: 1.614s, episode steps: 164, steps per second: 102, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 1.120086, mae: 26.136538, mean_q: -38.113972\n",
      " 29942/50000: episode: 134, duration: 1.423s, episode steps: 144, steps per second: 101, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 1.075539, mae: 25.939653, mean_q: -37.794479\n",
      " 30178/50000: episode: 135, duration: 2.323s, episode steps: 236, steps per second: 102, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.061746, mae: 26.080055, mean_q: -38.010960\n",
      " 30356/50000: episode: 136, duration: 1.754s, episode steps: 178, steps per second: 101, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.001889, mae: 26.033110, mean_q: -37.956764\n",
      " 30538/50000: episode: 137, duration: 1.813s, episode steps: 182, steps per second: 100, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 1.119158, mae: 26.166172, mean_q: -38.138386\n",
      " 30702/50000: episode: 138, duration: 1.668s, episode steps: 164, steps per second:  98, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.330124, mae: 26.402611, mean_q: -38.473495\n",
      " 31202/50000: episode: 139, duration: 4.891s, episode steps: 500, steps per second: 102, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 1.160749, mae: 26.179754, mean_q: -38.147961\n",
      " 31351/50000: episode: 140, duration: 1.466s, episode steps: 149, steps per second: 102, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 1.175444, mae: 26.128788, mean_q: -38.054394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31492/50000: episode: 141, duration: 1.398s, episode steps: 141, steps per second: 101, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 1.196766, mae: 26.396847, mean_q: -38.456200\n",
      " 31673/50000: episode: 142, duration: 1.792s, episode steps: 181, steps per second: 101, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.181003, mae: 26.056076, mean_q: -37.966812\n",
      " 31861/50000: episode: 143, duration: 1.856s, episode steps: 188, steps per second: 101, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 1.404072, mae: 26.019905, mean_q: -37.872993\n",
      " 32001/50000: episode: 144, duration: 1.384s, episode steps: 140, steps per second: 101, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 1.158157, mae: 26.208202, mean_q: -38.198048\n",
      " 32188/50000: episode: 145, duration: 1.852s, episode steps: 187, steps per second: 101, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 1.253794, mae: 26.120478, mean_q: -38.043949\n",
      " 32383/50000: episode: 146, duration: 1.930s, episode steps: 195, steps per second: 101, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.057814, mae: 26.043444, mean_q: -37.955109\n",
      " 32587/50000: episode: 147, duration: 2.015s, episode steps: 204, steps per second: 101, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 1.302200, mae: 26.254396, mean_q: -38.266548\n",
      " 32765/50000: episode: 148, duration: 1.798s, episode steps: 178, steps per second:  99, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 1.197815, mae: 26.216702, mean_q: -38.250904\n",
      " 32975/50000: episode: 149, duration: 2.095s, episode steps: 210, steps per second: 100, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.210024, mae: 26.124832, mean_q: -38.076057\n",
      " 33195/50000: episode: 150, duration: 2.177s, episode steps: 220, steps per second: 101, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 1.139851, mae: 26.139675, mean_q: -38.097321\n",
      " 33369/50000: episode: 151, duration: 1.875s, episode steps: 174, steps per second:  93, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 1.098797, mae: 26.221529, mean_q: -38.228119\n",
      " 33540/50000: episode: 152, duration: 1.696s, episode steps: 171, steps per second: 101, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 1.330905, mae: 26.199007, mean_q: -38.156811\n",
      " 33715/50000: episode: 153, duration: 1.739s, episode steps: 175, steps per second: 101, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.297936, mae: 26.225363, mean_q: -38.178509\n",
      " 33929/50000: episode: 154, duration: 2.122s, episode steps: 214, steps per second: 101, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.139564, mae: 26.323425, mean_q: -38.400902\n",
      " 34081/50000: episode: 155, duration: 1.569s, episode steps: 152, steps per second:  97, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 1.116182, mae: 26.228205, mean_q: -38.236481\n",
      " 34242/50000: episode: 156, duration: 1.687s, episode steps: 161, steps per second:  95, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.051975, mae: 26.570349, mean_q: -38.770546\n",
      " 34421/50000: episode: 157, duration: 1.780s, episode steps: 179, steps per second: 101, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 1.142283, mae: 26.541737, mean_q: -38.701561\n",
      " 34603/50000: episode: 158, duration: 1.809s, episode steps: 182, steps per second: 101, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 1.293978, mae: 26.252119, mean_q: -38.256359\n",
      " 34810/50000: episode: 159, duration: 2.061s, episode steps: 207, steps per second: 100, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.324609, mae: 26.324280, mean_q: -38.367245\n",
      " 34984/50000: episode: 160, duration: 1.738s, episode steps: 174, steps per second: 100, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 1.141432, mae: 26.220156, mean_q: -38.257488\n",
      " 35143/50000: episode: 161, duration: 1.589s, episode steps: 159, steps per second: 100, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 1.045326, mae: 26.435204, mean_q: -38.558926\n",
      " 35388/50000: episode: 162, duration: 2.440s, episode steps: 245, steps per second: 100, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 1.185951, mae: 26.409323, mean_q: -38.508507\n",
      " 35563/50000: episode: 163, duration: 1.877s, episode steps: 175, steps per second:  93, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 1.201184, mae: 26.617590, mean_q: -38.853584\n",
      " 35740/50000: episode: 164, duration: 1.795s, episode steps: 177, steps per second:  99, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.067560, mae: 26.444744, mean_q: -38.583477\n",
      " 35938/50000: episode: 165, duration: 1.989s, episode steps: 198, steps per second: 100, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.160069, mae: 26.654425, mean_q: -38.886932\n",
      " 36163/50000: episode: 166, duration: 2.250s, episode steps: 225, steps per second: 100, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 1.204627, mae: 26.548502, mean_q: -38.698097\n",
      " 36364/50000: episode: 167, duration: 2.010s, episode steps: 201, steps per second: 100, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 1.250584, mae: 26.584455, mean_q: -38.790028\n",
      " 36596/50000: episode: 168, duration: 2.316s, episode steps: 232, steps per second: 100, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 1.121839, mae: 26.707169, mean_q: -38.988068\n",
      " 36770/50000: episode: 169, duration: 1.741s, episode steps: 174, steps per second: 100, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.158679, mae: 26.992653, mean_q: -39.407413\n",
      " 36960/50000: episode: 170, duration: 1.905s, episode steps: 190, steps per second: 100, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.191068, mae: 26.769606, mean_q: -39.090897\n",
      " 37201/50000: episode: 171, duration: 2.410s, episode steps: 241, steps per second: 100, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.268437, mae: 26.814302, mean_q: -39.118267\n",
      " 37423/50000: episode: 172, duration: 2.231s, episode steps: 222, steps per second: 100, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 1.320786, mae: 26.785646, mean_q: -39.116970\n",
      " 37627/50000: episode: 173, duration: 2.043s, episode steps: 204, steps per second: 100, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.154154, mae: 27.133736, mean_q: -39.612633\n",
      " 37820/50000: episode: 174, duration: 1.935s, episode steps: 193, steps per second: 100, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.329295, mae: 27.046051, mean_q: -39.477295\n",
      " 38041/50000: episode: 175, duration: 2.216s, episode steps: 221, steps per second: 100, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 1.220602, mae: 26.980942, mean_q: -39.394497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38250/50000: episode: 176, duration: 2.090s, episode steps: 209, steps per second: 100, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.856 [0.000, 2.000],  loss: 1.226825, mae: 26.879259, mean_q: -39.215065\n",
      " 38516/50000: episode: 177, duration: 2.658s, episode steps: 266, steps per second: 100, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 1.296716, mae: 26.839060, mean_q: -39.165936\n",
      " 38700/50000: episode: 178, duration: 1.847s, episode steps: 184, steps per second: 100, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 1.173377, mae: 27.020279, mean_q: -39.467403\n",
      " 38866/50000: episode: 179, duration: 1.667s, episode steps: 166, steps per second: 100, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 1.089788, mae: 27.030245, mean_q: -39.470814\n",
      " 38991/50000: episode: 180, duration: 1.265s, episode steps: 125, steps per second:  99, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 1.156874, mae: 27.138653, mean_q: -39.617943\n",
      " 39171/50000: episode: 181, duration: 1.802s, episode steps: 180, steps per second: 100, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.268107, mae: 27.152594, mean_q: -39.619678\n",
      " 39358/50000: episode: 182, duration: 1.874s, episode steps: 187, steps per second: 100, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.159753, mae: 27.309147, mean_q: -39.889328\n",
      " 39568/50000: episode: 183, duration: 2.168s, episode steps: 210, steps per second:  97, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 1.340158, mae: 27.127295, mean_q: -39.601460\n",
      " 39748/50000: episode: 184, duration: 1.822s, episode steps: 180, steps per second:  99, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 1.313349, mae: 27.195766, mean_q: -39.733864\n",
      " 39912/50000: episode: 185, duration: 1.777s, episode steps: 164, steps per second:  92, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 1.223383, mae: 27.356678, mean_q: -39.916416\n",
      " 40059/50000: episode: 186, duration: 1.505s, episode steps: 147, steps per second:  98, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 1.200051, mae: 27.246666, mean_q: -39.775467\n",
      " 40222/50000: episode: 187, duration: 1.681s, episode steps: 163, steps per second:  97, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 1.202364, mae: 27.336994, mean_q: -39.878105\n",
      " 40411/50000: episode: 188, duration: 1.927s, episode steps: 189, steps per second:  98, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 1.279613, mae: 27.178194, mean_q: -39.641750\n",
      " 40598/50000: episode: 189, duration: 1.920s, episode steps: 187, steps per second:  97, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 1.356211, mae: 27.251486, mean_q: -39.738750\n",
      " 40777/50000: episode: 190, duration: 1.808s, episode steps: 179, steps per second:  99, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.334897, mae: 27.175833, mean_q: -39.648651\n",
      " 40987/50000: episode: 191, duration: 2.120s, episode steps: 210, steps per second:  99, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 1.273535, mae: 27.086212, mean_q: -39.524391\n",
      " 41160/50000: episode: 192, duration: 1.746s, episode steps: 173, steps per second:  99, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 1.304829, mae: 27.326771, mean_q: -39.885094\n",
      " 41361/50000: episode: 193, duration: 2.018s, episode steps: 201, steps per second: 100, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 1.253702, mae: 27.195080, mean_q: -39.664413\n",
      " 41503/50000: episode: 194, duration: 1.436s, episode steps: 142, steps per second:  99, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.218521, mae: 27.239267, mean_q: -39.766483\n",
      " 41700/50000: episode: 195, duration: 2.033s, episode steps: 197, steps per second:  97, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 1.430892, mae: 27.086390, mean_q: -39.503300\n",
      " 41935/50000: episode: 196, duration: 2.374s, episode steps: 235, steps per second:  99, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 1.216924, mae: 27.322790, mean_q: -39.895821\n",
      " 42071/50000: episode: 197, duration: 1.381s, episode steps: 136, steps per second:  98, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 1.387480, mae: 26.959469, mean_q: -39.323406\n",
      " 42288/50000: episode: 198, duration: 2.192s, episode steps: 217, steps per second:  99, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 1.223923, mae: 27.235027, mean_q: -39.738834\n",
      " 42521/50000: episode: 199, duration: 2.353s, episode steps: 233, steps per second:  99, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 1.297766, mae: 27.277491, mean_q: -39.788479\n",
      " 42661/50000: episode: 200, duration: 1.424s, episode steps: 140, steps per second:  98, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 1.482628, mae: 27.178713, mean_q: -39.623638\n",
      " 42852/50000: episode: 201, duration: 1.929s, episode steps: 191, steps per second:  99, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.157 [0.000, 2.000],  loss: 1.326415, mae: 27.215733, mean_q: -39.685627\n",
      " 43076/50000: episode: 202, duration: 2.279s, episode steps: 224, steps per second:  98, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 1.439817, mae: 27.127865, mean_q: -39.544262\n",
      " 43223/50000: episode: 203, duration: 1.485s, episode steps: 147, steps per second:  99, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 1.170222, mae: 27.338692, mean_q: -39.874641\n",
      " 43430/50000: episode: 204, duration: 2.092s, episode steps: 207, steps per second:  99, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 1.299511, mae: 27.221132, mean_q: -39.693115\n",
      " 43580/50000: episode: 205, duration: 1.526s, episode steps: 150, steps per second:  98, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 1.226860, mae: 27.147585, mean_q: -39.617886\n",
      " 43758/50000: episode: 206, duration: 1.811s, episode steps: 178, steps per second:  98, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 1.169407, mae: 27.181103, mean_q: -39.669788\n",
      " 43947/50000: episode: 207, duration: 1.919s, episode steps: 189, steps per second:  98, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 1.174029, mae: 27.137732, mean_q: -39.599499\n",
      " 44113/50000: episode: 208, duration: 1.683s, episode steps: 166, steps per second:  99, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 1.614730, mae: 26.983072, mean_q: -39.347084\n",
      " 44290/50000: episode: 209, duration: 1.877s, episode steps: 177, steps per second:  94, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 1.249973, mae: 26.884407, mean_q: -39.207497\n",
      " 44449/50000: episode: 210, duration: 1.615s, episode steps: 159, steps per second:  98, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 1.523064, mae: 27.038765, mean_q: -39.420204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44610/50000: episode: 211, duration: 1.641s, episode steps: 161, steps per second:  98, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 1.197696, mae: 27.264977, mean_q: -39.765633\n",
      " 44800/50000: episode: 212, duration: 1.952s, episode steps: 190, steps per second:  97, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 1.181147, mae: 27.177534, mean_q: -39.642944\n",
      " 45077/50000: episode: 213, duration: 2.808s, episode steps: 277, steps per second:  99, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 1.125964, mae: 27.165583, mean_q: -39.627708\n",
      " 45247/50000: episode: 214, duration: 1.767s, episode steps: 170, steps per second:  96, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 1.206549, mae: 27.178841, mean_q: -39.638271\n",
      " 45514/50000: episode: 215, duration: 2.757s, episode steps: 267, steps per second:  97, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 1.248848, mae: 27.097141, mean_q: -39.507515\n",
      " 45723/50000: episode: 216, duration: 2.126s, episode steps: 209, steps per second:  98, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 1.302160, mae: 27.106922, mean_q: -39.539982\n",
      " 45908/50000: episode: 217, duration: 1.888s, episode steps: 185, steps per second:  98, episode reward: -184.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 1.141911, mae: 27.053717, mean_q: -39.467049\n",
      " 46153/50000: episode: 218, duration: 2.493s, episode steps: 245, steps per second:  98, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 1.307808, mae: 27.056381, mean_q: -39.485821\n",
      " 46311/50000: episode: 219, duration: 1.616s, episode steps: 158, steps per second:  98, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.128359, mae: 27.066109, mean_q: -39.505428\n",
      " 46576/50000: episode: 220, duration: 2.705s, episode steps: 265, steps per second:  98, episode reward: -264.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 1.364438, mae: 27.070665, mean_q: -39.484592\n",
      " 46728/50000: episode: 221, duration: 1.563s, episode steps: 152, steps per second:  97, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 1.456678, mae: 26.783041, mean_q: -39.027821\n",
      " 46901/50000: episode: 222, duration: 1.765s, episode steps: 173, steps per second:  98, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 1.228810, mae: 26.939253, mean_q: -39.279545\n",
      " 47071/50000: episode: 223, duration: 1.744s, episode steps: 170, steps per second:  97, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 1.415787, mae: 26.820728, mean_q: -39.077316\n",
      " 47306/50000: episode: 224, duration: 2.393s, episode steps: 235, steps per second:  98, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 1.296747, mae: 26.922806, mean_q: -39.238934\n",
      " 47448/50000: episode: 225, duration: 1.457s, episode steps: 142, steps per second:  97, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 1.348992, mae: 26.919529, mean_q: -39.217304\n",
      " 47657/50000: episode: 226, duration: 2.127s, episode steps: 209, steps per second:  98, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.014 [0.000, 2.000],  loss: 1.244210, mae: 26.959274, mean_q: -39.342293\n",
      " 47808/50000: episode: 227, duration: 1.544s, episode steps: 151, steps per second:  98, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 1.332944, mae: 26.789032, mean_q: -39.052914\n",
      " 47984/50000: episode: 228, duration: 1.798s, episode steps: 176, steps per second:  98, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 1.269811, mae: 27.008667, mean_q: -39.371639\n",
      " 48183/50000: episode: 229, duration: 2.036s, episode steps: 199, steps per second:  98, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 1.164477, mae: 26.967651, mean_q: -39.326286\n",
      " 48407/50000: episode: 230, duration: 2.384s, episode steps: 224, steps per second:  94, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 1.168137, mae: 26.986593, mean_q: -39.366066\n",
      " 48577/50000: episode: 231, duration: 1.771s, episode steps: 170, steps per second:  96, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 1.428042, mae: 26.926735, mean_q: -39.252384\n",
      " 48723/50000: episode: 232, duration: 1.509s, episode steps: 146, steps per second:  97, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.272943, mae: 26.823780, mean_q: -39.096279\n",
      " 48897/50000: episode: 233, duration: 1.793s, episode steps: 174, steps per second:  97, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 1.174541, mae: 26.731810, mean_q: -38.969345\n",
      " 49095/50000: episode: 234, duration: 2.042s, episode steps: 198, steps per second:  97, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.240356, mae: 27.002974, mean_q: -39.356915\n",
      " 49277/50000: episode: 235, duration: 1.863s, episode steps: 182, steps per second:  98, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 1.267955, mae: 26.897793, mean_q: -39.179989\n",
      " 49503/50000: episode: 236, duration: 2.317s, episode steps: 226, steps per second:  98, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 1.374324, mae: 26.795553, mean_q: -39.024376\n",
      " 49679/50000: episode: 237, duration: 1.809s, episode steps: 176, steps per second:  97, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 1.222254, mae: 26.648571, mean_q: -38.799610\n",
      " 49872/50000: episode: 238, duration: 1.989s, episode steps: 193, steps per second:  97, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 1.112240, mae: 26.767986, mean_q: -39.027702\n",
      "done, took 495.852 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe759bad5b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9790ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -61.000, steps: 62\n",
      "Episode 2: reward: -61.000, steps: 62\n",
      "Episode 3: reward: -151.000, steps: 152\n",
      "Episode 4: reward: -69.000, steps: 70\n",
      "Episode 5: reward: -279.000, steps: 280\n",
      "Episode 6: reward: -82.000, steps: 83\n",
      "Episode 7: reward: -73.000, steps: 74\n",
      "Episode 8: reward: -193.000, steps: 194\n",
      "Episode 9: reward: -99.000, steps: 100\n",
      "Episode 10: reward: -128.000, steps: 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe759badaf0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acaa9d",
   "metadata": {},
   "source": [
    "# Box2D\n",
    "\n",
    "## Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67308296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8696e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    97/100000: episode: 1, duration: 0.797s, episode steps:  97, steps per second: 122, episode reward: -312.989, mean reward: -3.227 [-100.000, 40.537], mean action: 1.629 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   211/100000: episode: 2, duration: 3.646s, episode steps: 114, steps per second:  31, episode reward: -238.897, mean reward: -2.096 [-100.000,  8.177], mean action: 1.614 [0.000, 3.000],  loss: 53.887167, mae: 1.248642, mean_q: -0.260770\n",
      "   276/100000: episode: 3, duration: 0.720s, episode steps:  65, steps per second:  90, episode reward: -96.340, mean reward: -1.482 [-100.000,  6.120], mean action: 1.708 [0.000, 3.000],  loss: 48.265110, mae: 1.852698, mean_q: -0.831954\n",
      "   350/100000: episode: 4, duration: 0.823s, episode steps:  74, steps per second:  90, episode reward: -499.421, mean reward: -6.749 [-100.000,  0.699], mean action: 2.027 [0.000, 3.000],  loss: 34.889248, mae: 2.154367, mean_q: -0.889887\n",
      "   475/100000: episode: 5, duration: 1.375s, episode steps: 125, steps per second:  91, episode reward: -604.896, mean reward: -4.839 [-100.000,  1.876], mean action: 1.280 [0.000, 3.000],  loss: 49.093128, mae: 3.595279, mean_q: -2.387645\n",
      "   575/100000: episode: 6, duration: 1.098s, episode steps: 100, steps per second:  91, episode reward: -152.171, mean reward: -1.522 [-100.000,  6.445], mean action: 1.050 [0.000, 3.000],  loss: 41.697754, mae: 5.300356, mean_q: -4.488058\n",
      "   650/100000: episode: 7, duration: 0.841s, episode steps:  75, steps per second:  89, episode reward: -179.828, mean reward: -2.398 [-100.000, 28.824], mean action: 1.360 [0.000, 3.000],  loss: 31.188747, mae: 5.856516, mean_q: -5.450777\n",
      "   836/100000: episode: 8, duration: 2.110s, episode steps: 186, steps per second:  88, episode reward: -372.972, mean reward: -2.005 [-100.000,  2.804], mean action: 1.634 [0.000, 3.000],  loss: 22.930607, mae: 6.758510, mean_q: -6.893965\n",
      "   907/100000: episode: 9, duration: 0.796s, episode steps:  71, steps per second:  89, episode reward: -161.694, mean reward: -2.277 [-100.000,  9.135], mean action: 1.606 [0.000, 3.000],  loss: 21.328918, mae: 7.710488, mean_q: -8.221542\n",
      "  1053/100000: episode: 10, duration: 1.646s, episode steps: 146, steps per second:  89, episode reward: -97.193, mean reward: -0.666 [-100.000, 12.163], mean action: 1.459 [0.000, 3.000],  loss: 13.008494, mae: 8.826265, mean_q: -9.934117\n",
      "  1192/100000: episode: 11, duration: 1.540s, episode steps: 139, steps per second:  90, episode reward: -221.538, mean reward: -1.594 [-100.000, 11.822], mean action: 1.338 [0.000, 3.000],  loss: 11.296424, mae: 9.481888, mean_q: -10.599088\n",
      "  1334/100000: episode: 12, duration: 1.583s, episode steps: 142, steps per second:  90, episode reward: -81.513, mean reward: -0.574 [-100.000, 10.690], mean action: 1.641 [0.000, 3.000],  loss: 8.471068, mae: 9.769469, mean_q: -10.751884\n",
      "  1490/100000: episode: 13, duration: 1.746s, episode steps: 156, steps per second:  89, episode reward: -274.610, mean reward: -1.760 [-100.000, 15.327], mean action: 1.673 [0.000, 3.000],  loss: 10.899804, mae: 10.754625, mean_q: -11.694033\n",
      "  1695/100000: episode: 14, duration: 2.279s, episode steps: 205, steps per second:  90, episode reward: -258.042, mean reward: -1.259 [-100.000,  4.714], mean action: 1.488 [0.000, 3.000],  loss: 12.299057, mae: 10.321838, mean_q: -10.728878\n",
      "  1863/100000: episode: 15, duration: 1.883s, episode steps: 168, steps per second:  89, episode reward: -175.883, mean reward: -1.047 [-100.000,  7.931], mean action: 1.673 [0.000, 3.000],  loss: 13.000338, mae: 10.164721, mean_q: -10.243969\n",
      "  2019/100000: episode: 16, duration: 1.749s, episode steps: 156, steps per second:  89, episode reward: -399.765, mean reward: -2.563 [-100.000,  4.508], mean action: 1.513 [0.000, 3.000],  loss: 11.390783, mae: 10.558768, mean_q: -10.424855\n",
      "  2238/100000: episode: 17, duration: 2.587s, episode steps: 219, steps per second:  85, episode reward: -258.714, mean reward: -1.181 [-100.000,  4.147], mean action: 1.557 [0.000, 3.000],  loss: 9.837721, mae: 11.212421, mean_q: -10.596738\n",
      "  2479/100000: episode: 18, duration: 2.706s, episode steps: 241, steps per second:  89, episode reward: -69.169, mean reward: -0.287 [-100.000,  4.621], mean action: 1.585 [0.000, 3.000],  loss: 11.190464, mae: 12.335496, mean_q: -11.488373\n",
      "  2594/100000: episode: 19, duration: 1.284s, episode steps: 115, steps per second:  90, episode reward: -506.219, mean reward: -4.402 [-100.000,  3.074], mean action: 1.261 [0.000, 3.000],  loss: 12.560951, mae: 12.762592, mean_q: -11.383468\n",
      "  2799/100000: episode: 20, duration: 2.303s, episode steps: 205, steps per second:  89, episode reward: -153.985, mean reward: -0.751 [-100.000,  5.317], mean action: 1.673 [0.000, 3.000],  loss: 12.427115, mae: 14.491526, mean_q: -12.771167\n",
      "  2941/100000: episode: 21, duration: 1.691s, episode steps: 142, steps per second:  84, episode reward: -103.302, mean reward: -0.727 [-100.000, 24.277], mean action: 1.599 [0.000, 3.000],  loss: 9.721251, mae: 14.672772, mean_q: -12.134238\n",
      "  3065/100000: episode: 22, duration: 1.486s, episode steps: 124, steps per second:  83, episode reward: -139.440, mean reward: -1.125 [-100.000,  2.571], mean action: 1.750 [0.000, 3.000],  loss: 12.581288, mae: 15.661774, mean_q: -12.697680\n",
      "  3197/100000: episode: 23, duration: 1.487s, episode steps: 132, steps per second:  89, episode reward: -175.020, mean reward: -1.326 [-100.000, 25.695], mean action: 1.811 [0.000, 3.000],  loss: 12.132668, mae: 16.271751, mean_q: -13.220685\n",
      "  3314/100000: episode: 24, duration: 1.322s, episode steps: 117, steps per second:  89, episode reward: -44.005, mean reward: -0.376 [-100.000, 15.923], mean action: 1.855 [0.000, 3.000],  loss: 11.506145, mae: 17.436050, mean_q: -13.981803\n",
      "  3532/100000: episode: 25, duration: 2.455s, episode steps: 218, steps per second:  89, episode reward: -61.511, mean reward: -0.282 [-100.000,  7.395], mean action: 1.798 [0.000, 3.000],  loss: 9.732829, mae: 17.163803, mean_q: -13.034312\n",
      "  3661/100000: episode: 26, duration: 1.444s, episode steps: 129, steps per second:  89, episode reward: -155.865, mean reward: -1.208 [-100.000, 12.893], mean action: 1.667 [0.000, 3.000],  loss: 8.355989, mae: 17.753918, mean_q: -13.215149\n",
      "  3827/100000: episode: 27, duration: 1.860s, episode steps: 166, steps per second:  89, episode reward: -71.480, mean reward: -0.431 [-100.000,  6.068], mean action: 1.735 [0.000, 3.000],  loss: 7.229363, mae: 17.601873, mean_q: -12.383735\n",
      "  4020/100000: episode: 28, duration: 2.182s, episode steps: 193, steps per second:  88, episode reward: 49.513, mean reward:  0.257 [-100.000, 20.199], mean action: 1.772 [0.000, 3.000],  loss: 6.102687, mae: 17.909754, mean_q: -11.676250\n",
      "  4216/100000: episode: 29, duration: 2.194s, episode steps: 196, steps per second:  89, episode reward: -17.929, mean reward: -0.091 [-100.000, 24.754], mean action: 1.878 [0.000, 3.000],  loss: 7.223458, mae: 18.460016, mean_q: -10.945226\n",
      "  4377/100000: episode: 30, duration: 1.813s, episode steps: 161, steps per second:  89, episode reward: -101.696, mean reward: -0.632 [-100.000,  4.887], mean action: 1.534 [0.000, 3.000],  loss: 4.910758, mae: 19.000887, mean_q: -11.188464\n",
      "  4560/100000: episode: 31, duration: 2.071s, episode steps: 183, steps per second:  88, episode reward: -1.629, mean reward: -0.009 [-100.000, 14.395], mean action: 1.749 [0.000, 3.000],  loss: 6.729009, mae: 19.092600, mean_q: -10.818616\n",
      "  4743/100000: episode: 32, duration: 2.065s, episode steps: 183, steps per second:  89, episode reward: -39.430, mean reward: -0.215 [-100.000,  8.501], mean action: 1.770 [0.000, 3.000],  loss: 5.202312, mae: 19.794455, mean_q: -10.734985\n",
      "  5035/100000: episode: 33, duration: 3.402s, episode steps: 292, steps per second:  86, episode reward: -181.104, mean reward: -0.620 [-100.000,  4.773], mean action: 1.818 [0.000, 3.000],  loss: 6.187549, mae: 19.913822, mean_q: -9.709052\n",
      "  5398/100000: episode: 34, duration: 4.283s, episode steps: 363, steps per second:  85, episode reward: -194.342, mean reward: -0.535 [-100.000,  4.453], mean action: 1.708 [0.000, 3.000],  loss: 4.179468, mae: 20.657217, mean_q: -9.565819\n",
      "  5615/100000: episode: 35, duration: 2.479s, episode steps: 217, steps per second:  88, episode reward: -16.157, mean reward: -0.074 [-100.000, 27.281], mean action: 1.862 [0.000, 3.000],  loss: 4.695467, mae: 21.785706, mean_q: -10.199299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5841/100000: episode: 36, duration: 2.547s, episode steps: 226, steps per second:  89, episode reward: -79.463, mean reward: -0.352 [-100.000, 10.865], mean action: 1.664 [0.000, 3.000],  loss: 4.336106, mae: 21.461140, mean_q: -8.992249\n",
      "  6001/100000: episode: 37, duration: 1.798s, episode steps: 160, steps per second:  89, episode reward: -123.971, mean reward: -0.775 [-100.000,  6.092], mean action: 1.675 [0.000, 3.000],  loss: 3.917167, mae: 22.587315, mean_q: -9.615352\n",
      "  6298/100000: episode: 38, duration: 3.524s, episode steps: 297, steps per second:  84, episode reward: -41.794, mean reward: -0.141 [-100.000,  8.451], mean action: 1.892 [0.000, 3.000],  loss: 4.397918, mae: 22.369169, mean_q: -8.761622\n",
      "  6583/100000: episode: 39, duration: 3.257s, episode steps: 285, steps per second:  88, episode reward: -47.846, mean reward: -0.168 [-100.000, 13.311], mean action: 1.775 [0.000, 3.000],  loss: 3.878886, mae: 22.564608, mean_q: -7.465215\n",
      "  6741/100000: episode: 40, duration: 1.778s, episode steps: 158, steps per second:  89, episode reward: -31.746, mean reward: -0.201 [-100.000, 16.726], mean action: 1.835 [0.000, 3.000],  loss: 4.718489, mae: 22.973995, mean_q: -6.911385\n",
      "  6924/100000: episode: 41, duration: 2.057s, episode steps: 183, steps per second:  89, episode reward: -19.045, mean reward: -0.104 [-100.000, 13.201], mean action: 1.760 [0.000, 3.000],  loss: 4.026590, mae: 22.851389, mean_q: -6.923653\n",
      "  7251/100000: episode: 42, duration: 3.799s, episode steps: 327, steps per second:  86, episode reward: -168.395, mean reward: -0.515 [-100.000,  8.565], mean action: 1.627 [0.000, 3.000],  loss: 4.110461, mae: 22.801018, mean_q: -5.400401\n",
      "  7565/100000: episode: 43, duration: 3.665s, episode steps: 314, steps per second:  86, episode reward: -124.175, mean reward: -0.395 [-100.000, 20.426], mean action: 1.691 [0.000, 3.000],  loss: 4.498764, mae: 23.818064, mean_q: -5.443144\n",
      "  7762/100000: episode: 44, duration: 2.223s, episode steps: 197, steps per second:  89, episode reward: -104.302, mean reward: -0.529 [-100.000,  5.082], mean action: 1.736 [0.000, 3.000],  loss: 4.728827, mae: 24.422165, mean_q: -4.961735\n",
      "  7938/100000: episode: 45, duration: 1.986s, episode steps: 176, steps per second:  89, episode reward: 11.951, mean reward:  0.068 [-100.000, 12.621], mean action: 1.807 [0.000, 3.000],  loss: 3.934230, mae: 24.802898, mean_q: -5.460712\n",
      "  8130/100000: episode: 46, duration: 2.303s, episode steps: 192, steps per second:  83, episode reward: -36.323, mean reward: -0.189 [-100.000, 14.090], mean action: 1.698 [0.000, 3.000],  loss: 4.467256, mae: 24.761744, mean_q: -4.708879\n",
      "  8262/100000: episode: 47, duration: 1.559s, episode steps: 132, steps per second:  85, episode reward: -88.747, mean reward: -0.672 [-100.000, 16.012], mean action: 1.583 [0.000, 3.000],  loss: 3.771991, mae: 24.778782, mean_q: -2.990839\n",
      "  8567/100000: episode: 48, duration: 3.733s, episode steps: 305, steps per second:  82, episode reward: -91.895, mean reward: -0.301 [-100.000, 20.741], mean action: 1.643 [0.000, 3.000],  loss: 4.136007, mae: 25.390648, mean_q: -2.417062\n",
      "  8751/100000: episode: 49, duration: 2.092s, episode steps: 184, steps per second:  88, episode reward: -98.138, mean reward: -0.533 [-100.000,  4.751], mean action: 1.625 [0.000, 3.000],  loss: 3.933364, mae: 25.131708, mean_q: -1.169473\n",
      "  8934/100000: episode: 50, duration: 2.067s, episode steps: 183, steps per second:  89, episode reward: -26.298, mean reward: -0.144 [-100.000, 18.521], mean action: 1.667 [0.000, 3.000],  loss: 4.185337, mae: 26.518753, mean_q: -1.590878\n",
      "  9772/100000: episode: 51, duration: 10.649s, episode steps: 838, steps per second:  79, episode reward: -151.979, mean reward: -0.181 [-100.000, 16.403], mean action: 1.593 [0.000, 3.000],  loss: 4.788694, mae: 26.870331, mean_q: -1.698106\n",
      " 10064/100000: episode: 52, duration: 3.380s, episode steps: 292, steps per second:  86, episode reward: -106.693, mean reward: -0.365 [-100.000, 10.708], mean action: 1.709 [0.000, 3.000],  loss: 4.839822, mae: 27.922228, mean_q: -1.026652\n",
      " 11064/100000: episode: 53, duration: 13.121s, episode steps: 1000, steps per second:  76, episode reward: -22.137, mean reward: -0.022 [-22.339, 13.167], mean action: 1.688 [0.000, 3.000],  loss: 5.435151, mae: 27.709923, mean_q: 0.354407\n",
      " 12064/100000: episode: 54, duration: 12.615s, episode steps: 1000, steps per second:  79, episode reward: -69.036, mean reward: -0.069 [-4.604,  4.550], mean action: 1.667 [0.000, 3.000],  loss: 5.602406, mae: 27.096952, mean_q: 2.982534\n",
      " 13064/100000: episode: 55, duration: 13.890s, episode steps: 1000, steps per second:  72, episode reward: -64.916, mean reward: -0.065 [-4.785,  5.009], mean action: 1.688 [0.000, 3.000],  loss: 5.820294, mae: 26.965708, mean_q: 6.788090\n",
      " 14064/100000: episode: 56, duration: 12.368s, episode steps: 1000, steps per second:  81, episode reward: -66.337, mean reward: -0.066 [-4.530,  4.511], mean action: 1.704 [0.000, 3.000],  loss: 5.501480, mae: 26.552608, mean_q: 11.099174\n",
      " 15064/100000: episode: 57, duration: 13.144s, episode steps: 1000, steps per second:  76, episode reward: -18.775, mean reward: -0.019 [-4.912,  4.728], mean action: 1.728 [0.000, 3.000],  loss: 6.140526, mae: 26.648632, mean_q: 14.685042\n",
      " 16064/100000: episode: 58, duration: 13.538s, episode steps: 1000, steps per second:  74, episode reward: -20.490, mean reward: -0.020 [-4.984,  5.919], mean action: 1.753 [0.000, 3.000],  loss: 5.437362, mae: 26.490597, mean_q: 17.217289\n",
      " 16284/100000: episode: 59, duration: 2.551s, episode steps: 220, steps per second:  86, episode reward: -4.363, mean reward: -0.020 [-100.000, 19.342], mean action: 1.750 [0.000, 3.000],  loss: 6.177574, mae: 26.286888, mean_q: 19.343559\n",
      " 17284/100000: episode: 60, duration: 14.134s, episode steps: 1000, steps per second:  71, episode reward: -42.279, mean reward: -0.042 [-4.829,  5.098], mean action: 1.720 [0.000, 3.000],  loss: 5.560999, mae: 26.754957, mean_q: 20.774797\n",
      " 17568/100000: episode: 61, duration: 3.311s, episode steps: 284, steps per second:  86, episode reward: 15.026, mean reward:  0.053 [-100.000, 15.676], mean action: 1.708 [0.000, 3.000],  loss: 5.986368, mae: 26.994503, mean_q: 22.092564\n",
      " 18568/100000: episode: 62, duration: 12.302s, episode steps: 1000, steps per second:  81, episode reward: -12.107, mean reward: -0.012 [-4.868,  5.257], mean action: 1.705 [0.000, 3.000],  loss: 5.568587, mae: 27.153933, mean_q: 23.279583\n",
      " 19568/100000: episode: 63, duration: 14.409s, episode steps: 1000, steps per second:  69, episode reward: -33.589, mean reward: -0.034 [-4.793,  4.771], mean action: 1.671 [0.000, 3.000],  loss: 6.057442, mae: 27.457800, mean_q: 25.426046\n",
      " 20568/100000: episode: 64, duration: 13.307s, episode steps: 1000, steps per second:  75, episode reward: -36.339, mean reward: -0.036 [-4.655,  5.165], mean action: 1.705 [0.000, 3.000],  loss: 6.034875, mae: 28.067141, mean_q: 27.387007\n",
      " 21568/100000: episode: 65, duration: 13.258s, episode steps: 1000, steps per second:  75, episode reward: -9.745, mean reward: -0.010 [-4.711,  5.836], mean action: 1.644 [0.000, 3.000],  loss: 5.659822, mae: 28.404510, mean_q: 28.715034\n",
      " 22047/100000: episode: 66, duration: 5.891s, episode steps: 479, steps per second:  81, episode reward: -10.013, mean reward: -0.021 [-100.000, 17.317], mean action: 1.743 [0.000, 3.000],  loss: 5.342953, mae: 28.511681, mean_q: 29.186895\n",
      " 23047/100000: episode: 67, duration: 14.001s, episode steps: 1000, steps per second:  71, episode reward: -49.697, mean reward: -0.050 [-19.848, 26.473], mean action: 1.634 [0.000, 3.000],  loss: 5.983140, mae: 28.613668, mean_q: 29.180729\n",
      " 23359/100000: episode: 68, duration: 3.721s, episode steps: 312, steps per second:  84, episode reward: -74.683, mean reward: -0.239 [-100.000, 10.283], mean action: 1.731 [0.000, 3.000],  loss: 5.260904, mae: 28.429594, mean_q: 29.476822\n",
      " 24359/100000: episode: 69, duration: 13.028s, episode steps: 1000, steps per second:  77, episode reward: -1.714, mean reward: -0.002 [-22.049, 13.355], mean action: 1.534 [0.000, 3.000],  loss: 5.776200, mae: 28.665556, mean_q: 29.647371\n",
      " 24710/100000: episode: 70, duration: 4.252s, episode steps: 351, steps per second:  83, episode reward: -47.178, mean reward: -0.134 [-100.000, 17.386], mean action: 1.607 [0.000, 3.000],  loss: 6.794324, mae: 28.457115, mean_q: 29.792215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25710/100000: episode: 71, duration: 13.168s, episode steps: 1000, steps per second:  76, episode reward: 38.315, mean reward:  0.038 [-20.032, 22.813], mean action: 1.435 [0.000, 3.000],  loss: 6.779273, mae: 28.831795, mean_q: 30.547026\n",
      " 26710/100000: episode: 72, duration: 14.729s, episode steps: 1000, steps per second:  68, episode reward: 41.119, mean reward:  0.041 [-20.465, 13.407], mean action: 1.516 [0.000, 3.000],  loss: 6.363595, mae: 28.446726, mean_q: 30.544008\n",
      " 27710/100000: episode: 73, duration: 14.957s, episode steps: 1000, steps per second:  67, episode reward: 33.093, mean reward:  0.033 [-21.606, 25.646], mean action: 1.359 [0.000, 3.000],  loss: 6.526580, mae: 28.223207, mean_q: 29.880644\n",
      " 28710/100000: episode: 74, duration: 13.506s, episode steps: 1000, steps per second:  74, episode reward: -60.863, mean reward: -0.061 [-25.508, 21.668], mean action: 1.343 [0.000, 3.000],  loss: 6.734522, mae: 27.765703, mean_q: 29.347975\n",
      " 29710/100000: episode: 75, duration: 14.259s, episode steps: 1000, steps per second:  70, episode reward: 54.771, mean reward:  0.055 [-23.742, 14.429], mean action: 1.522 [0.000, 3.000],  loss: 7.396438, mae: 27.694004, mean_q: 29.592379\n",
      " 30710/100000: episode: 76, duration: 14.675s, episode steps: 1000, steps per second:  68, episode reward: -17.399, mean reward: -0.017 [-4.559,  4.843], mean action: 1.726 [0.000, 3.000],  loss: 6.533663, mae: 27.463367, mean_q: 30.032328\n",
      " 31710/100000: episode: 77, duration: 13.732s, episode steps: 1000, steps per second:  73, episode reward: 24.382, mean reward:  0.024 [-17.939, 12.927], mean action: 1.597 [0.000, 3.000],  loss: 6.736480, mae: 27.840734, mean_q: 31.297113\n",
      " 32710/100000: episode: 78, duration: 13.871s, episode steps: 1000, steps per second:  72, episode reward: 35.757, mean reward:  0.036 [-10.525, 11.560], mean action: 1.651 [0.000, 3.000],  loss: 5.436540, mae: 27.827179, mean_q: 32.087849\n",
      " 33710/100000: episode: 79, duration: 14.583s, episode steps: 1000, steps per second:  69, episode reward:  3.377, mean reward:  0.003 [-22.664, 23.886], mean action: 1.612 [0.000, 3.000],  loss: 5.684229, mae: 27.823254, mean_q: 32.678398\n",
      " 34710/100000: episode: 80, duration: 13.981s, episode steps: 1000, steps per second:  72, episode reward: -12.674, mean reward: -0.013 [-24.744, 24.341], mean action: 1.483 [0.000, 3.000],  loss: 5.235350, mae: 27.402857, mean_q: 32.329086\n",
      " 35710/100000: episode: 81, duration: 13.243s, episode steps: 1000, steps per second:  76, episode reward: 95.496, mean reward:  0.095 [-24.479, 25.193], mean action: 1.557 [0.000, 3.000],  loss: 7.616176, mae: 27.237883, mean_q: 32.635921\n",
      " 35978/100000: episode: 82, duration: 3.252s, episode steps: 268, steps per second:  82, episode reward: 28.231, mean reward:  0.105 [-100.000, 12.744], mean action: 1.619 [0.000, 3.000],  loss: 6.594265, mae: 27.656660, mean_q: 32.459873\n",
      " 36978/100000: episode: 83, duration: 14.502s, episode steps: 1000, steps per second:  69, episode reward: 117.416, mean reward:  0.117 [-23.940, 26.665], mean action: 1.522 [0.000, 3.000],  loss: 6.410285, mae: 27.563438, mean_q: 32.960026\n",
      " 37978/100000: episode: 84, duration: 14.648s, episode steps: 1000, steps per second:  68, episode reward: 97.898, mean reward:  0.098 [-23.836, 24.636], mean action: 1.538 [0.000, 3.000],  loss: 6.686395, mae: 27.604860, mean_q: 33.263268\n",
      " 38978/100000: episode: 85, duration: 13.487s, episode steps: 1000, steps per second:  74, episode reward: 52.295, mean reward:  0.052 [-20.815, 24.096], mean action: 1.487 [0.000, 3.000],  loss: 5.519780, mae: 27.985500, mean_q: 34.361191\n",
      " 39978/100000: episode: 86, duration: 13.695s, episode steps: 1000, steps per second:  73, episode reward: 107.533, mean reward:  0.108 [-24.403, 23.377], mean action: 1.513 [0.000, 3.000],  loss: 5.617530, mae: 28.273613, mean_q: 34.807423\n",
      " 40978/100000: episode: 87, duration: 14.151s, episode steps: 1000, steps per second:  71, episode reward: 58.396, mean reward:  0.058 [-24.010, 25.734], mean action: 1.490 [0.000, 3.000],  loss: 5.618322, mae: 28.663599, mean_q: 35.350933\n",
      " 41978/100000: episode: 88, duration: 14.863s, episode steps: 1000, steps per second:  67, episode reward: 38.888, mean reward:  0.039 [-21.481, 22.999], mean action: 1.493 [0.000, 3.000],  loss: 6.558503, mae: 29.140833, mean_q: 35.847713\n",
      " 42978/100000: episode: 89, duration: 13.640s, episode steps: 1000, steps per second:  73, episode reward: 64.882, mean reward:  0.065 [-20.214, 22.189], mean action: 1.592 [0.000, 3.000],  loss: 5.636170, mae: 28.855906, mean_q: 35.745785\n",
      " 43318/100000: episode: 90, duration: 4.237s, episode steps: 340, steps per second:  80, episode reward: -27.485, mean reward: -0.081 [-100.000, 13.241], mean action: 1.771 [0.000, 3.000],  loss: 6.314528, mae: 29.173306, mean_q: 36.244556\n",
      " 44318/100000: episode: 91, duration: 13.788s, episode steps: 1000, steps per second:  73, episode reward: 108.699, mean reward:  0.109 [-23.684, 22.924], mean action: 1.419 [0.000, 3.000],  loss: 5.458232, mae: 29.634172, mean_q: 36.894703\n",
      " 45318/100000: episode: 92, duration: 13.368s, episode steps: 1000, steps per second:  75, episode reward: 101.365, mean reward:  0.101 [-19.835, 23.028], mean action: 1.532 [0.000, 3.000],  loss: 6.395171, mae: 30.141668, mean_q: 37.646286\n",
      " 46318/100000: episode: 93, duration: 13.619s, episode steps: 1000, steps per second:  73, episode reward: 78.579, mean reward:  0.079 [-23.488, 24.131], mean action: 1.486 [0.000, 3.000],  loss: 6.177586, mae: 30.471718, mean_q: 38.072128\n",
      " 47318/100000: episode: 94, duration: 13.363s, episode steps: 1000, steps per second:  75, episode reward: 74.698, mean reward:  0.075 [-19.505, 24.395], mean action: 1.492 [0.000, 3.000],  loss: 5.929713, mae: 31.006153, mean_q: 38.050404\n",
      " 48318/100000: episode: 95, duration: 13.914s, episode steps: 1000, steps per second:  72, episode reward: 121.728, mean reward:  0.122 [-24.339, 25.004], mean action: 1.395 [0.000, 3.000],  loss: 7.276830, mae: 30.706211, mean_q: 38.067829\n",
      " 49318/100000: episode: 96, duration: 14.057s, episode steps: 1000, steps per second:  71, episode reward: 42.975, mean reward:  0.043 [-20.514, 26.782], mean action: 1.338 [0.000, 3.000],  loss: 6.848845, mae: 30.788010, mean_q: 38.323204\n",
      " 49801/100000: episode: 97, duration: 6.160s, episode steps: 483, steps per second:  78, episode reward: -102.002, mean reward: -0.211 [-100.000, 13.300], mean action: 1.708 [0.000, 3.000],  loss: 5.498189, mae: 31.027374, mean_q: 38.863483\n",
      " 50801/100000: episode: 98, duration: 13.588s, episode steps: 1000, steps per second:  74, episode reward: 56.136, mean reward:  0.056 [-23.140, 23.925], mean action: 1.502 [0.000, 3.000],  loss: 6.697199, mae: 30.801672, mean_q: 38.914734\n",
      " 51801/100000: episode: 99, duration: 14.074s, episode steps: 1000, steps per second:  71, episode reward: 100.387, mean reward:  0.100 [-21.535, 28.306], mean action: 1.452 [0.000, 3.000],  loss: 5.803726, mae: 30.336716, mean_q: 38.872787\n",
      " 52801/100000: episode: 100, duration: 13.872s, episode steps: 1000, steps per second:  72, episode reward: 76.945, mean reward:  0.077 [-19.374, 28.450], mean action: 1.466 [0.000, 3.000],  loss: 5.732110, mae: 29.577749, mean_q: 38.466251\n",
      " 53801/100000: episode: 101, duration: 13.306s, episode steps: 1000, steps per second:  75, episode reward: 76.310, mean reward:  0.076 [-20.474, 23.188], mean action: 1.375 [0.000, 3.000],  loss: 4.948117, mae: 29.090975, mean_q: 38.154121\n",
      " 54801/100000: episode: 102, duration: 14.836s, episode steps: 1000, steps per second:  67, episode reward: 129.022, mean reward:  0.129 [-21.312, 24.448], mean action: 1.488 [0.000, 3.000],  loss: 3.962662, mae: 28.626801, mean_q: 37.687019\n",
      " 55801/100000: episode: 103, duration: 13.764s, episode steps: 1000, steps per second:  73, episode reward: 133.348, mean reward:  0.133 [-20.021, 21.945], mean action: 1.397 [0.000, 3.000],  loss: 6.176487, mae: 28.293554, mean_q: 37.583775\n",
      " 56801/100000: episode: 104, duration: 14.136s, episode steps: 1000, steps per second:  71, episode reward: 121.305, mean reward:  0.121 [-21.709, 23.485], mean action: 1.369 [0.000, 3.000],  loss: 3.802879, mae: 27.548157, mean_q: 36.743626\n",
      " 57200/100000: episode: 105, duration: 5.148s, episode steps: 399, steps per second:  78, episode reward: -238.083, mean reward: -0.597 [-100.000, 32.139], mean action: 1.647 [0.000, 3.000],  loss: 3.226783, mae: 27.423035, mean_q: 36.760597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58200/100000: episode: 106, duration: 13.873s, episode steps: 1000, steps per second:  72, episode reward: 105.845, mean reward:  0.106 [-20.102, 22.789], mean action: 1.246 [0.000, 3.000],  loss: 2.842800, mae: 27.528204, mean_q: 36.940460\n",
      " 59200/100000: episode: 107, duration: 13.474s, episode steps: 1000, steps per second:  74, episode reward: 136.782, mean reward:  0.137 [-24.686, 23.750], mean action: 1.335 [0.000, 3.000],  loss: 3.586583, mae: 27.406395, mean_q: 36.946720\n",
      " 60200/100000: episode: 108, duration: 13.526s, episode steps: 1000, steps per second:  74, episode reward: 99.753, mean reward:  0.100 [-22.586, 23.237], mean action: 1.411 [0.000, 3.000],  loss: 2.927845, mae: 27.194309, mean_q: 36.708580\n",
      " 61200/100000: episode: 109, duration: 14.009s, episode steps: 1000, steps per second:  71, episode reward: 99.755, mean reward:  0.100 [-21.245, 25.013], mean action: 1.374 [0.000, 3.000],  loss: 3.411216, mae: 27.083761, mean_q: 36.571571\n",
      " 61398/100000: episode: 110, duration: 2.486s, episode steps: 198, steps per second:  80, episode reward: -25.549, mean reward: -0.129 [-100.000, 10.304], mean action: 1.732 [0.000, 3.000],  loss: 2.882589, mae: 27.157507, mean_q: 36.668797\n",
      " 62398/100000: episode: 111, duration: 14.049s, episode steps: 1000, steps per second:  71, episode reward: 136.819, mean reward:  0.137 [-23.007, 24.161], mean action: 1.466 [0.000, 3.000],  loss: 2.841659, mae: 26.784832, mean_q: 36.158821\n",
      " 63398/100000: episode: 112, duration: 13.766s, episode steps: 1000, steps per second:  73, episode reward: 116.359, mean reward:  0.116 [-22.468, 23.996], mean action: 1.388 [0.000, 3.000],  loss: 2.051924, mae: 26.726122, mean_q: 36.114883\n",
      " 64398/100000: episode: 113, duration: 14.530s, episode steps: 1000, steps per second:  69, episode reward: 123.576, mean reward:  0.124 [-21.945, 22.865], mean action: 1.368 [0.000, 3.000],  loss: 3.097565, mae: 26.913868, mean_q: 36.366833\n",
      " 65398/100000: episode: 114, duration: 13.502s, episode steps: 1000, steps per second:  74, episode reward: 55.788, mean reward:  0.056 [-21.154, 24.906], mean action: 1.487 [0.000, 3.000],  loss: 3.794540, mae: 27.258144, mean_q: 36.831638\n",
      " 65564/100000: episode: 115, duration: 2.105s, episode steps: 166, steps per second:  79, episode reward: -75.811, mean reward: -0.457 [-100.000, 14.576], mean action: 1.633 [0.000, 3.000],  loss: 1.753913, mae: 27.417604, mean_q: 37.010735\n",
      " 66564/100000: episode: 116, duration: 13.289s, episode steps: 1000, steps per second:  75, episode reward: 92.652, mean reward:  0.093 [-20.631, 24.556], mean action: 1.393 [0.000, 3.000],  loss: 3.700186, mae: 27.339441, mean_q: 36.907867\n",
      " 67564/100000: episode: 117, duration: 13.708s, episode steps: 1000, steps per second:  73, episode reward: -40.037, mean reward: -0.040 [-23.105, 23.765], mean action: 1.615 [0.000, 3.000],  loss: 2.901891, mae: 27.056070, mean_q: 36.538803\n",
      " 68564/100000: episode: 118, duration: 13.987s, episode steps: 1000, steps per second:  71, episode reward: 119.484, mean reward:  0.119 [-22.052, 22.810], mean action: 1.358 [0.000, 3.000],  loss: 2.640021, mae: 27.240351, mean_q: 36.799492\n",
      " 69564/100000: episode: 119, duration: 14.022s, episode steps: 1000, steps per second:  71, episode reward: 112.485, mean reward:  0.112 [-21.635, 25.504], mean action: 1.374 [0.000, 3.000],  loss: 3.464884, mae: 27.292028, mean_q: 36.889828\n",
      " 70564/100000: episode: 120, duration: 14.132s, episode steps: 1000, steps per second:  71, episode reward: 117.600, mean reward:  0.118 [-20.729, 23.490], mean action: 1.490 [0.000, 3.000],  loss: 2.738542, mae: 27.424347, mean_q: 37.047157\n",
      " 70773/100000: episode: 121, duration: 2.620s, episode steps: 209, steps per second:  80, episode reward: -402.474, mean reward: -1.926 [-100.000, 14.108], mean action: 1.651 [0.000, 3.000],  loss: 2.800648, mae: 27.422256, mean_q: 37.084148\n",
      " 71773/100000: episode: 122, duration: 14.163s, episode steps: 1000, steps per second:  71, episode reward: 154.260, mean reward:  0.154 [-22.384, 22.721], mean action: 1.327 [0.000, 3.000],  loss: 4.250163, mae: 27.366436, mean_q: 36.916523\n",
      " 72773/100000: episode: 123, duration: 13.696s, episode steps: 1000, steps per second:  73, episode reward: 109.646, mean reward:  0.110 [-19.890, 24.974], mean action: 1.320 [0.000, 3.000],  loss: 2.541461, mae: 27.373348, mean_q: 36.941952\n",
      " 73773/100000: episode: 124, duration: 13.910s, episode steps: 1000, steps per second:  72, episode reward: 147.465, mean reward:  0.147 [-19.702, 24.903], mean action: 1.421 [0.000, 3.000],  loss: 2.890155, mae: 27.326899, mean_q: 36.903671\n",
      " 74053/100000: episode: 125, duration: 3.518s, episode steps: 280, steps per second:  80, episode reward: -200.838, mean reward: -0.717 [-100.000, 17.629], mean action: 1.536 [0.000, 3.000],  loss: 3.766396, mae: 27.636429, mean_q: 37.330635\n",
      " 75053/100000: episode: 126, duration: 14.099s, episode steps: 1000, steps per second:  71, episode reward: 137.742, mean reward:  0.138 [-19.763, 23.153], mean action: 1.377 [0.000, 3.000],  loss: 2.468717, mae: 27.283258, mean_q: 36.841599\n",
      " 75178/100000: episode: 127, duration: 1.561s, episode steps: 125, steps per second:  80, episode reward: -165.838, mean reward: -1.327 [-100.000, 28.498], mean action: 1.464 [0.000, 3.000],  loss: 2.712259, mae: 27.113693, mean_q: 36.617615\n",
      " 76178/100000: episode: 128, duration: 13.935s, episode steps: 1000, steps per second:  72, episode reward: 26.000, mean reward:  0.026 [-25.342, 22.758], mean action: 1.768 [0.000, 3.000],  loss: 2.959894, mae: 27.489187, mean_q: 37.049866\n",
      " 76374/100000: episode: 129, duration: 2.475s, episode steps: 196, steps per second:  79, episode reward: -176.962, mean reward: -0.903 [-100.000, 10.422], mean action: 1.653 [0.000, 3.000],  loss: 2.094128, mae: 27.544554, mean_q: 37.213905\n",
      " 76724/100000: episode: 130, duration: 4.408s, episode steps: 350, steps per second:  79, episode reward: -3.797, mean reward: -0.011 [-100.000, 16.662], mean action: 1.726 [0.000, 3.000],  loss: 3.123814, mae: 27.596987, mean_q: 37.210789\n",
      " 77724/100000: episode: 131, duration: 14.200s, episode steps: 1000, steps per second:  70, episode reward: 118.302, mean reward:  0.118 [-20.786, 21.390], mean action: 1.363 [0.000, 3.000],  loss: 3.141157, mae: 27.867910, mean_q: 37.571953\n",
      " 78724/100000: episode: 132, duration: 13.379s, episode steps: 1000, steps per second:  75, episode reward: 131.304, mean reward:  0.131 [-20.921, 22.592], mean action: 1.276 [0.000, 3.000],  loss: 3.376618, mae: 27.999781, mean_q: 37.713936\n",
      " 79724/100000: episode: 133, duration: 13.190s, episode steps: 1000, steps per second:  76, episode reward: 78.759, mean reward:  0.079 [-22.065, 24.067], mean action: 1.347 [0.000, 3.000],  loss: 4.494418, mae: 27.826269, mean_q: 37.381630\n",
      " 80724/100000: episode: 134, duration: 13.815s, episode steps: 1000, steps per second:  72, episode reward: 46.131, mean reward:  0.046 [-23.221, 30.133], mean action: 1.948 [0.000, 3.000],  loss: 2.967981, mae: 27.659531, mean_q: 37.082687\n",
      " 81724/100000: episode: 135, duration: 14.648s, episode steps: 1000, steps per second:  68, episode reward: 91.167, mean reward:  0.091 [-20.955, 24.126], mean action: 1.353 [0.000, 3.000],  loss: 3.361611, mae: 27.811636, mean_q: 37.284492\n",
      " 82724/100000: episode: 136, duration: 13.615s, episode steps: 1000, steps per second:  73, episode reward: 130.005, mean reward:  0.130 [-20.065, 22.586], mean action: 1.405 [0.000, 3.000],  loss: 3.324456, mae: 27.538351, mean_q: 36.985260\n",
      " 83724/100000: episode: 137, duration: 14.411s, episode steps: 1000, steps per second:  69, episode reward: 68.227, mean reward:  0.068 [-22.505, 23.412], mean action: 1.479 [0.000, 3.000],  loss: 2.295255, mae: 27.516933, mean_q: 37.054783\n",
      " 84724/100000: episode: 138, duration: 13.270s, episode steps: 1000, steps per second:  75, episode reward: 131.645, mean reward:  0.132 [-21.601, 23.406], mean action: 1.506 [0.000, 3.000],  loss: 3.195195, mae: 27.842194, mean_q: 37.493000\n",
      " 85724/100000: episode: 139, duration: 13.285s, episode steps: 1000, steps per second:  75, episode reward: 109.129, mean reward:  0.109 [-18.765, 22.887], mean action: 1.324 [0.000, 3.000],  loss: 3.368903, mae: 27.825180, mean_q: 37.512886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86724/100000: episode: 140, duration: 13.772s, episode steps: 1000, steps per second:  73, episode reward: 127.718, mean reward:  0.128 [-20.146, 22.439], mean action: 1.457 [0.000, 3.000],  loss: 3.435793, mae: 27.901012, mean_q: 37.620975\n",
      " 87724/100000: episode: 141, duration: 13.329s, episode steps: 1000, steps per second:  75, episode reward: 84.727, mean reward:  0.085 [-24.078, 23.098], mean action: 1.513 [0.000, 3.000],  loss: 2.939423, mae: 27.819235, mean_q: 37.474495\n",
      " 88005/100000: episode: 142, duration: 3.553s, episode steps: 281, steps per second:  79, episode reward: -230.141, mean reward: -0.819 [-100.000, 52.874], mean action: 1.794 [0.000, 3.000],  loss: 4.143942, mae: 27.931124, mean_q: 37.460964\n",
      " 89005/100000: episode: 143, duration: 14.155s, episode steps: 1000, steps per second:  71, episode reward: 160.358, mean reward:  0.160 [-19.582, 13.047], mean action: 1.451 [0.000, 3.000],  loss: 3.731712, mae: 27.904253, mean_q: 37.435280\n",
      " 90005/100000: episode: 144, duration: 14.192s, episode steps: 1000, steps per second:  70, episode reward: 145.261, mean reward:  0.145 [-20.376, 25.471], mean action: 1.367 [0.000, 3.000],  loss: 4.041959, mae: 27.728918, mean_q: 37.026928\n",
      " 91005/100000: episode: 145, duration: 14.744s, episode steps: 1000, steps per second:  68, episode reward: 152.483, mean reward:  0.152 [-24.447, 23.167], mean action: 1.470 [0.000, 3.000],  loss: 3.460894, mae: 27.576454, mean_q: 36.748241\n",
      " 91146/100000: episode: 146, duration: 1.788s, episode steps: 141, steps per second:  79, episode reward:  5.541, mean reward:  0.039 [-100.000,  9.278], mean action: 1.652 [0.000, 3.000],  loss: 3.531719, mae: 27.465900, mean_q: 36.608685\n",
      " 92146/100000: episode: 147, duration: 13.413s, episode steps: 1000, steps per second:  75, episode reward: 106.884, mean reward:  0.107 [-20.718, 21.628], mean action: 1.820 [0.000, 3.000],  loss: 4.833030, mae: 27.816277, mean_q: 37.131676\n",
      " 93146/100000: episode: 148, duration: 13.924s, episode steps: 1000, steps per second:  72, episode reward: 135.267, mean reward:  0.135 [-20.390, 23.627], mean action: 1.411 [0.000, 3.000],  loss: 3.952597, mae: 27.723906, mean_q: 37.007664\n",
      " 93271/100000: episode: 149, duration: 1.556s, episode steps: 125, steps per second:  80, episode reward: -7.266, mean reward: -0.058 [-100.000,  9.392], mean action: 1.608 [0.000, 3.000],  loss: 1.896269, mae: 27.473118, mean_q: 36.891441\n",
      " 94271/100000: episode: 150, duration: 13.369s, episode steps: 1000, steps per second:  75, episode reward: 107.931, mean reward:  0.108 [-23.593, 24.387], mean action: 1.908 [0.000, 3.000],  loss: 4.027555, mae: 27.544628, mean_q: 36.889851\n",
      " 95271/100000: episode: 151, duration: 13.532s, episode steps: 1000, steps per second:  74, episode reward: 45.296, mean reward:  0.045 [-19.836, 23.274], mean action: 1.504 [0.000, 3.000],  loss: 2.864616, mae: 27.369566, mean_q: 36.793343\n",
      " 96271/100000: episode: 152, duration: 13.844s, episode steps: 1000, steps per second:  72, episode reward: 138.883, mean reward:  0.139 [-20.604, 23.267], mean action: 1.336 [0.000, 3.000],  loss: 3.017873, mae: 27.462780, mean_q: 36.863670\n",
      " 97271/100000: episode: 153, duration: 13.577s, episode steps: 1000, steps per second:  74, episode reward: 121.246, mean reward:  0.121 [-23.816, 23.063], mean action: 1.258 [0.000, 3.000],  loss: 3.112561, mae: 27.344809, mean_q: 36.739811\n",
      " 97394/100000: episode: 154, duration: 1.530s, episode steps: 123, steps per second:  80, episode reward: -112.009, mean reward: -0.911 [-100.000,  8.127], mean action: 1.236 [0.000, 3.000],  loss: 2.372310, mae: 26.581022, mean_q: 35.840591\n",
      " 98394/100000: episode: 155, duration: 13.961s, episode steps: 1000, steps per second:  72, episode reward: 104.316, mean reward:  0.104 [-23.501, 23.046], mean action: 1.470 [0.000, 3.000],  loss: 3.460130, mae: 27.271614, mean_q: 36.582317\n",
      " 99394/100000: episode: 156, duration: 13.797s, episode steps: 1000, steps per second:  72, episode reward: 136.125, mean reward:  0.136 [-24.295, 23.368], mean action: 1.351 [0.000, 3.000],  loss: 3.139434, mae: 27.139469, mean_q: 36.284664\n",
      "done, took 1352.175 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe753e98a90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce8a4e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -175.041, steps: 342\n",
      "Episode 2: reward: 23.638, steps: 220\n",
      "Episode 3: reward: -73.747, steps: 107\n",
      "Episode 4: reward: 123.541, steps: 1000\n",
      "Episode 5: reward: -325.697, steps: 345\n",
      "Episode 6: reward: 177.584, steps: 373\n",
      "Episode 7: reward: 257.463, steps: 291\n",
      "Episode 8: reward: 254.353, steps: 277\n",
      "Episode 9: reward: 164.730, steps: 388\n",
      "Episode 10: reward: 260.992, steps: 248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe74f91b700>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b8a5a",
   "metadata": {},
   "source": [
    "# Toy Text\n",
    "\n",
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f10ddb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edf53ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    15/50000: episode: 1, duration: 0.769s, episode steps:  15, steps per second:  19, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    34/50000: episode: 2, duration: 0.031s, episode steps:  19, steps per second: 618, episode reward:  1.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.263 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    38/50000: episode: 3, duration: 0.008s, episode steps:   4, steps per second: 501, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    44/50000: episode: 4, duration: 0.011s, episode steps:   6, steps per second: 551, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    50/50000: episode: 5, duration: 0.011s, episode steps:   6, steps per second: 552, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    53/50000: episode: 6, duration: 0.006s, episode steps:   3, steps per second: 493, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    58/50000: episode: 7, duration: 0.009s, episode steps:   5, steps per second: 564, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    71/50000: episode: 8, duration: 0.021s, episode steps:  13, steps per second: 614, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.077 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    76/50000: episode: 9, duration: 0.009s, episode steps:   5, steps per second: 545, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    91/50000: episode: 10, duration: 0.023s, episode steps:  15, steps per second: 638, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   102/50000: episode: 11, duration: 2.974s, episode steps:  11, steps per second:   4, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.197874, mae: 0.260762, mean_q: 0.594411\n",
      "   108/50000: episode: 12, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.186194, mae: 0.221708, mean_q: 0.426308\n",
      "   127/50000: episode: 13, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.102199, mae: 0.217833, mean_q: 0.424311\n",
      "   132/50000: episode: 14, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.079698, mae: 0.280471, mean_q: 0.447362\n",
      "   146/50000: episode: 15, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.068920, mae: 0.301750, mean_q: 0.457680\n",
      "   160/50000: episode: 16, duration: 0.164s, episode steps:  14, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.214 [0.000, 3.000],  loss: 0.056045, mae: 0.324043, mean_q: 0.460655\n",
      "   163/50000: episode: 17, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.070163, mae: 0.370462, mean_q: 0.520272\n",
      "   173/50000: episode: 18, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.068520, mae: 0.407134, mean_q: 0.550141\n",
      "   175/50000: episode: 19, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.079291, mae: 0.407616, mean_q: 0.528017\n",
      "   187/50000: episode: 20, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.063707, mae: 0.372583, mean_q: 0.461580\n",
      "   193/50000: episode: 21, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.071074, mae: 0.420419, mean_q: 0.527939\n",
      "   197/50000: episode: 22, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.063522, mae: 0.376111, mean_q: 0.452429\n",
      "   201/50000: episode: 23, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.056793, mae: 0.377211, mean_q: 0.470277\n",
      "   208/50000: episode: 24, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.052284, mae: 0.375511, mean_q: 0.483679\n",
      "   216/50000: episode: 25, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.050460, mae: 0.372125, mean_q: 0.479667\n",
      "   218/50000: episode: 26, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.039912, mae: 0.339202, mean_q: 0.437944\n",
      "   224/50000: episode: 27, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.046463, mae: 0.366144, mean_q: 0.475224\n",
      "   229/50000: episode: 28, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.042462, mae: 0.351252, mean_q: 0.445554\n",
      "   233/50000: episode: 29, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.046517, mae: 0.348040, mean_q: 0.419726\n",
      "   242/50000: episode: 30, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.044554, mae: 0.307526, mean_q: 0.373487\n",
      "   246/50000: episode: 31, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.043753, mae: 0.300245, mean_q: 0.366970\n",
      "   250/50000: episode: 32, duration: 0.053s, episode steps:   4, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.042400, mae: 0.305425, mean_q: 0.385090\n",
      "   253/50000: episode: 33, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.033481, mae: 0.348846, mean_q: 0.432891\n",
      "   256/50000: episode: 34, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.049734, mae: 0.297048, mean_q: 0.359415\n",
      "   268/50000: episode: 35, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.038653, mae: 0.305758, mean_q: 0.382656\n",
      "   271/50000: episode: 36, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.024243, mae: 0.286460, mean_q: 0.375863\n",
      "   274/50000: episode: 37, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.023293, mae: 0.279234, mean_q: 0.362959\n",
      "   276/50000: episode: 38, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.044747, mae: 0.268010, mean_q: 0.331549\n",
      "   285/50000: episode: 39, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.034555, mae: 0.273323, mean_q: 0.338735\n",
      "   299/50000: episode: 40, duration: 0.168s, episode steps:  14, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.028245, mae: 0.255035, mean_q: 0.332556\n",
      "   309/50000: episode: 41, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.026125, mae: 0.278643, mean_q: 0.351738\n",
      "   319/50000: episode: 42, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.025362, mae: 0.265112, mean_q: 0.346455\n",
      "   324/50000: episode: 43, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.036332, mae: 0.263699, mean_q: 0.344588\n",
      "   327/50000: episode: 44, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.027556, mae: 0.258821, mean_q: 0.350816\n",
      "   330/50000: episode: 45, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.020854, mae: 0.233377, mean_q: 0.322933\n",
      "   338/50000: episode: 46, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.026185, mae: 0.248107, mean_q: 0.337284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   344/50000: episode: 47, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.028374, mae: 0.261066, mean_q: 0.348317\n",
      "   348/50000: episode: 48, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.028657, mae: 0.252358, mean_q: 0.330545\n",
      "   353/50000: episode: 49, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.027451, mae: 0.256632, mean_q: 0.323180\n",
      "   359/50000: episode: 50, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.020540, mae: 0.262168, mean_q: 0.334407\n",
      "   376/50000: episode: 51, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.022597, mae: 0.242995, mean_q: 0.314732\n",
      "   395/50000: episode: 52, duration: 0.216s, episode steps:  19, steps per second:  88, episode reward:  1.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.211 [0.000, 3.000],  loss: 0.019204, mae: 0.240776, mean_q: 0.312448\n",
      "   398/50000: episode: 53, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.023904, mae: 0.247330, mean_q: 0.322507\n",
      "   410/50000: episode: 54, duration: 0.142s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.019182, mae: 0.245440, mean_q: 0.320487\n",
      "   416/50000: episode: 55, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.016692, mae: 0.234609, mean_q: 0.304949\n",
      "   421/50000: episode: 56, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.019523, mae: 0.257940, mean_q: 0.331517\n",
      "   427/50000: episode: 57, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.020926, mae: 0.256580, mean_q: 0.330283\n",
      "   430/50000: episode: 58, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.023829, mae: 0.263997, mean_q: 0.348332\n",
      "   436/50000: episode: 59, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.018558, mae: 0.246616, mean_q: 0.322869\n",
      "   442/50000: episode: 60, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.016518, mae: 0.229663, mean_q: 0.305799\n",
      "   444/50000: episode: 61, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.011479, mae: 0.224047, mean_q: 0.304531\n",
      "   461/50000: episode: 62, duration: 0.197s, episode steps:  17, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [1.000, 3.000],  loss: 0.018305, mae: 0.238182, mean_q: 0.324090\n",
      "   464/50000: episode: 63, duration: 0.042s, episode steps:   3, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.012144, mae: 0.247139, mean_q: 0.343366\n",
      "   475/50000: episode: 64, duration: 0.127s, episode steps:  11, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.014823, mae: 0.229627, mean_q: 0.309678\n",
      "   480/50000: episode: 65, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 2.000],  loss: 0.018001, mae: 0.239091, mean_q: 0.318430\n",
      "   487/50000: episode: 66, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.015927, mae: 0.222158, mean_q: 0.295038\n",
      "   490/50000: episode: 67, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.015903, mae: 0.226773, mean_q: 0.303536\n",
      "   492/50000: episode: 68, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.015181, mae: 0.221698, mean_q: 0.298945\n",
      "   503/50000: episode: 69, duration: 0.133s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.016904, mae: 0.240556, mean_q: 0.318661\n",
      "   512/50000: episode: 70, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.014987, mae: 0.236001, mean_q: 0.314741\n",
      "   515/50000: episode: 71, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.011955, mae: 0.229142, mean_q: 0.308597\n",
      "   525/50000: episode: 72, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.013751, mae: 0.245121, mean_q: 0.327377\n",
      "   528/50000: episode: 73, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.012617, mae: 0.227134, mean_q: 0.299588\n",
      "   537/50000: episode: 74, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.011512, mae: 0.224069, mean_q: 0.299547\n",
      "   548/50000: episode: 75, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.014878, mae: 0.247370, mean_q: 0.334188\n",
      "   554/50000: episode: 76, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.012336, mae: 0.245101, mean_q: 0.324321\n",
      "   560/50000: episode: 77, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.012653, mae: 0.237815, mean_q: 0.322222\n",
      "   563/50000: episode: 78, duration: 0.042s, episode steps:   3, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.011376, mae: 0.227537, mean_q: 0.313405\n",
      "   576/50000: episode: 79, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.010188, mae: 0.226486, mean_q: 0.311830\n",
      "   588/50000: episode: 80, duration: 0.140s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.011963, mae: 0.235487, mean_q: 0.325917\n",
      "   600/50000: episode: 81, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.012139, mae: 0.224539, mean_q: 0.298255\n",
      "   603/50000: episode: 82, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.009966, mae: 0.225469, mean_q: 0.295927\n",
      "   614/50000: episode: 83, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.011555, mae: 0.242042, mean_q: 0.325501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   616/50000: episode: 84, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.009555, mae: 0.219633, mean_q: 0.291733\n",
      "   623/50000: episode: 85, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.010200, mae: 0.223712, mean_q: 0.298976\n",
      "   628/50000: episode: 86, duration: 0.065s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.009256, mae: 0.222349, mean_q: 0.297424\n",
      "   640/50000: episode: 87, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.009927, mae: 0.235701, mean_q: 0.313784\n",
      "   650/50000: episode: 88, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.008840, mae: 0.229561, mean_q: 0.308379\n",
      "   652/50000: episode: 89, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005927, mae: 0.220979, mean_q: 0.301259\n",
      "   657/50000: episode: 90, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.010385, mae: 0.229578, mean_q: 0.305422\n",
      "   661/50000: episode: 91, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.009675, mae: 0.222062, mean_q: 0.293825\n",
      "   686/50000: episode: 92, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.009680, mae: 0.223227, mean_q: 0.297800\n",
      "   691/50000: episode: 93, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.008674, mae: 0.221984, mean_q: 0.294674\n",
      "   708/50000: episode: 94, duration: 0.199s, episode steps:  17, steps per second:  86, episode reward:  1.000, mean reward:  0.059 [ 0.000,  1.000], mean action: 1.235 [0.000, 3.000],  loss: 0.010947, mae: 0.222773, mean_q: 0.294321\n",
      "   716/50000: episode: 95, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.010461, mae: 0.239183, mean_q: 0.318801\n",
      "   718/50000: episode: 96, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.013096, mae: 0.236528, mean_q: 0.315272\n",
      "   725/50000: episode: 97, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.006438, mae: 0.226455, mean_q: 0.304864\n",
      "   733/50000: episode: 98, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.010431, mae: 0.225546, mean_q: 0.304512\n",
      "   736/50000: episode: 99, duration: 0.043s, episode steps:   3, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.005984, mae: 0.217530, mean_q: 0.296431\n",
      "   740/50000: episode: 100, duration: 0.053s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.008942, mae: 0.219686, mean_q: 0.295370\n",
      "   742/50000: episode: 101, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.010480, mae: 0.228551, mean_q: 0.306103\n",
      "   744/50000: episode: 102, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.009107, mae: 0.219742, mean_q: 0.305130\n",
      "   747/50000: episode: 103, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.007946, mae: 0.201236, mean_q: 0.268386\n",
      "   755/50000: episode: 104, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.008978, mae: 0.208456, mean_q: 0.280768\n",
      "   763/50000: episode: 105, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [0.000, 3.000],  loss: 0.009130, mae: 0.219175, mean_q: 0.295507\n",
      "   765/50000: episode: 106, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.009685, mae: 0.219335, mean_q: 0.287740\n",
      "   771/50000: episode: 107, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.006934, mae: 0.216483, mean_q: 0.288592\n",
      "   785/50000: episode: 108, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.007327, mae: 0.213470, mean_q: 0.292046\n",
      "   790/50000: episode: 109, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 3.000],  loss: 0.008063, mae: 0.196769, mean_q: 0.264676\n",
      "   801/50000: episode: 110, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.007522, mae: 0.197046, mean_q: 0.267867\n",
      "   809/50000: episode: 111, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [1.000, 3.000],  loss: 0.008110, mae: 0.205744, mean_q: 0.271265\n",
      "   813/50000: episode: 112, duration: 0.054s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.006893, mae: 0.202874, mean_q: 0.266107\n",
      "   821/50000: episode: 113, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.007324, mae: 0.206265, mean_q: 0.269355\n",
      "   823/50000: episode: 114, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003386, mae: 0.196389, mean_q: 0.268854\n",
      "   829/50000: episode: 115, duration: 0.081s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.007024, mae: 0.212805, mean_q: 0.286849\n",
      "   837/50000: episode: 116, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.007817, mae: 0.210621, mean_q: 0.283053\n",
      "   839/50000: episode: 117, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.007611, mae: 0.198957, mean_q: 0.267659\n",
      "   848/50000: episode: 118, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.004993, mae: 0.190791, mean_q: 0.259723\n",
      "   850/50000: episode: 119, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.007781, mae: 0.200568, mean_q: 0.263090\n",
      "   858/50000: episode: 120, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 2.000],  loss: 0.006904, mae: 0.205275, mean_q: 0.269712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   871/50000: episode: 121, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.006662, mae: 0.211783, mean_q: 0.280448\n",
      "   878/50000: episode: 122, duration: 0.085s, episode steps:   7, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.008727, mae: 0.197912, mean_q: 0.263681\n",
      "   881/50000: episode: 123, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.006907, mae: 0.187045, mean_q: 0.254913\n",
      "   890/50000: episode: 124, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.008309, mae: 0.192280, mean_q: 0.260749\n",
      "   892/50000: episode: 125, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005327, mae: 0.205027, mean_q: 0.278629\n",
      "   906/50000: episode: 126, duration: 0.161s, episode steps:  14, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.007056, mae: 0.208772, mean_q: 0.280780\n",
      "   910/50000: episode: 127, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.007026, mae: 0.196429, mean_q: 0.260499\n",
      "   919/50000: episode: 128, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.007133, mae: 0.193075, mean_q: 0.256182\n",
      "   923/50000: episode: 129, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.006048, mae: 0.179860, mean_q: 0.242899\n",
      "   936/50000: episode: 130, duration: 0.156s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.004937, mae: 0.189429, mean_q: 0.257104\n",
      "   938/50000: episode: 131, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002715, mae: 0.191688, mean_q: 0.263650\n",
      "   947/50000: episode: 132, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.006837, mae: 0.197630, mean_q: 0.270035\n",
      "   974/50000: episode: 133, duration: 0.307s, episode steps:  27, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.963 [0.000, 3.000],  loss: 0.005473, mae: 0.186429, mean_q: 0.256133\n",
      "   982/50000: episode: 134, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.006818, mae: 0.187151, mean_q: 0.247828\n",
      "   990/50000: episode: 135, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.005277, mae: 0.192218, mean_q: 0.258883\n",
      "   992/50000: episode: 136, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005867, mae: 0.189092, mean_q: 0.250894\n",
      "   996/50000: episode: 137, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.005963, mae: 0.194617, mean_q: 0.263119\n",
      "   998/50000: episode: 138, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.006268, mae: 0.200228, mean_q: 0.273782\n",
      "  1001/50000: episode: 139, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.005585, mae: 0.189646, mean_q: 0.260029\n",
      "  1009/50000: episode: 140, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.005095, mae: 0.188641, mean_q: 0.259235\n",
      "  1012/50000: episode: 141, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.008171, mae: 0.187015, mean_q: 0.252043\n",
      "  1028/50000: episode: 142, duration: 0.187s, episode steps:  16, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005530, mae: 0.181802, mean_q: 0.244892\n",
      "  1034/50000: episode: 143, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.005428, mae: 0.183782, mean_q: 0.250017\n",
      "  1044/50000: episode: 144, duration: 0.118s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 3.000],  loss: 0.005366, mae: 0.191746, mean_q: 0.263318\n",
      "  1049/50000: episode: 145, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.004277, mae: 0.187810, mean_q: 0.262732\n",
      "  1052/50000: episode: 146, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004964, mae: 0.181535, mean_q: 0.248667\n",
      "  1077/50000: episode: 147, duration: 0.284s, episode steps:  25, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.840 [0.000, 3.000],  loss: 0.004768, mae: 0.176674, mean_q: 0.238561\n",
      "  1091/50000: episode: 148, duration: 0.161s, episode steps:  14, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.004667, mae: 0.174551, mean_q: 0.235757\n",
      "  1104/50000: episode: 149, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.005339, mae: 0.171364, mean_q: 0.237225\n",
      "  1118/50000: episode: 150, duration: 0.159s, episode steps:  14, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.006144, mae: 0.177306, mean_q: 0.248194\n",
      "  1127/50000: episode: 151, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.005380, mae: 0.169897, mean_q: 0.228747\n",
      "  1131/50000: episode: 152, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.005560, mae: 0.165713, mean_q: 0.222307\n",
      "  1133/50000: episode: 153, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004470, mae: 0.173341, mean_q: 0.245292\n",
      "  1145/50000: episode: 154, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.003625, mae: 0.172195, mean_q: 0.240431\n",
      "  1148/50000: episode: 155, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.004029, mae: 0.166636, mean_q: 0.227563\n",
      "  1158/50000: episode: 156, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.005306, mae: 0.170345, mean_q: 0.231633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1161/50000: episode: 157, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.004453, mae: 0.162777, mean_q: 0.218730\n",
      "  1176/50000: episode: 158, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003737, mae: 0.169656, mean_q: 0.228907\n",
      "  1185/50000: episode: 159, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.004005, mae: 0.164458, mean_q: 0.221646\n",
      "  1189/50000: episode: 160, duration: 0.053s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.008487, mae: 0.166522, mean_q: 0.224180\n",
      "  1198/50000: episode: 161, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [0.000, 3.000],  loss: 0.004185, mae: 0.169688, mean_q: 0.236861\n",
      "  1212/50000: episode: 162, duration: 0.261s, episode steps:  14, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.004890, mae: 0.167029, mean_q: 0.231432\n",
      "  1215/50000: episode: 163, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.005281, mae: 0.164453, mean_q: 0.223786\n",
      "  1226/50000: episode: 164, duration: 0.129s, episode steps:  11, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003859, mae: 0.160578, mean_q: 0.224070\n",
      "  1239/50000: episode: 165, duration: 0.156s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.077 [0.000, 3.000],  loss: 0.005205, mae: 0.164893, mean_q: 0.228700\n",
      "  1269/50000: episode: 166, duration: 0.339s, episode steps:  30, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.004065, mae: 0.161748, mean_q: 0.220004\n",
      "  1276/50000: episode: 167, duration: 0.086s, episode steps:   7, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004711, mae: 0.153889, mean_q: 0.207346\n",
      "  1288/50000: episode: 168, duration: 0.142s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004038, mae: 0.161611, mean_q: 0.221684\n",
      "  1295/50000: episode: 169, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002813, mae: 0.157375, mean_q: 0.215352\n",
      "  1306/50000: episode: 170, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003340, mae: 0.152452, mean_q: 0.205823\n",
      "  1311/50000: episode: 171, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002998, mae: 0.154332, mean_q: 0.209380\n",
      "  1316/50000: episode: 172, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003237, mae: 0.155842, mean_q: 0.210244\n",
      "  1319/50000: episode: 173, duration: 0.042s, episode steps:   3, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001791, mae: 0.152289, mean_q: 0.209212\n",
      "  1323/50000: episode: 174, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002991, mae: 0.151833, mean_q: 0.206499\n",
      "  1336/50000: episode: 175, duration: 0.152s, episode steps:  13, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.003915, mae: 0.155298, mean_q: 0.207704\n",
      "  1347/50000: episode: 176, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.003013, mae: 0.156876, mean_q: 0.210461\n",
      "  1350/50000: episode: 177, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002632, mae: 0.156693, mean_q: 0.207372\n",
      "  1357/50000: episode: 178, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003577, mae: 0.153979, mean_q: 0.204942\n",
      "  1360/50000: episode: 179, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.007311, mae: 0.155851, mean_q: 0.207414\n",
      "  1366/50000: episode: 180, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003508, mae: 0.153587, mean_q: 0.209218\n",
      "  1376/50000: episode: 181, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.004282, mae: 0.152804, mean_q: 0.205648\n",
      "  1386/50000: episode: 182, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003029, mae: 0.152400, mean_q: 0.208483\n",
      "  1389/50000: episode: 183, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.006865, mae: 0.161009, mean_q: 0.217323\n",
      "  1393/50000: episode: 184, duration: 0.054s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003394, mae: 0.153229, mean_q: 0.211849\n",
      "  1395/50000: episode: 185, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003386, mae: 0.146832, mean_q: 0.199247\n",
      "  1407/50000: episode: 186, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003242, mae: 0.142289, mean_q: 0.192756\n",
      "  1410/50000: episode: 187, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003510, mae: 0.144146, mean_q: 0.193057\n",
      "  1413/50000: episode: 188, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003947, mae: 0.150202, mean_q: 0.210581\n",
      "  1416/50000: episode: 189, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002992, mae: 0.147367, mean_q: 0.205387\n",
      "  1419/50000: episode: 190, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003197, mae: 0.148297, mean_q: 0.209743\n",
      "  1426/50000: episode: 191, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 1.000],  loss: 0.002400, mae: 0.144706, mean_q: 0.200138\n",
      "  1437/50000: episode: 192, duration: 0.132s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003674, mae: 0.141807, mean_q: 0.192044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1443/50000: episode: 193, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003013, mae: 0.142934, mean_q: 0.193870\n",
      "  1452/50000: episode: 194, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003884, mae: 0.138926, mean_q: 0.187770\n",
      "  1461/50000: episode: 195, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002623, mae: 0.139958, mean_q: 0.194250\n",
      "  1466/50000: episode: 196, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002903, mae: 0.145743, mean_q: 0.202419\n",
      "  1468/50000: episode: 197, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001710, mae: 0.143696, mean_q: 0.207683\n",
      "  1485/50000: episode: 198, duration: 0.198s, episode steps:  17, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.003532, mae: 0.143497, mean_q: 0.199490\n",
      "  1497/50000: episode: 199, duration: 0.140s, episode steps:  12, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.004371, mae: 0.143438, mean_q: 0.190797\n",
      "  1505/50000: episode: 200, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003327, mae: 0.142417, mean_q: 0.196944\n",
      "  1507/50000: episode: 201, duration: 0.033s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004541, mae: 0.147904, mean_q: 0.200799\n",
      "  1514/50000: episode: 202, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003555, mae: 0.143410, mean_q: 0.195788\n",
      "  1528/50000: episode: 203, duration: 0.167s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002531, mae: 0.133365, mean_q: 0.178003\n",
      "  1534/50000: episode: 204, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001756, mae: 0.136159, mean_q: 0.184077\n",
      "  1537/50000: episode: 205, duration: 0.042s, episode steps:   3, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002239, mae: 0.142755, mean_q: 0.191431\n",
      "  1546/50000: episode: 206, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [1.000, 3.000],  loss: 0.004297, mae: 0.141679, mean_q: 0.187747\n",
      "  1552/50000: episode: 207, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002987, mae: 0.133886, mean_q: 0.175986\n",
      "  1554/50000: episode: 208, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005368, mae: 0.133563, mean_q: 0.177128\n",
      "  1558/50000: episode: 209, duration: 0.053s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.005407, mae: 0.137483, mean_q: 0.182829\n",
      "  1570/50000: episode: 210, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002532, mae: 0.140373, mean_q: 0.192164\n",
      "  1576/50000: episode: 211, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003895, mae: 0.139445, mean_q: 0.187824\n",
      "  1581/50000: episode: 212, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.003726, mae: 0.137240, mean_q: 0.181584\n",
      "  1585/50000: episode: 213, duration: 0.054s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002795, mae: 0.135254, mean_q: 0.180508\n",
      "  1589/50000: episode: 214, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002615, mae: 0.140866, mean_q: 0.187074\n",
      "  1594/50000: episode: 215, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002148, mae: 0.137592, mean_q: 0.185388\n",
      "  1602/50000: episode: 216, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002389, mae: 0.139864, mean_q: 0.188063\n",
      "  1609/50000: episode: 217, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.002780, mae: 0.136619, mean_q: 0.183039\n",
      "  1616/50000: episode: 218, duration: 0.086s, episode steps:   7, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002233, mae: 0.136038, mean_q: 0.186112\n",
      "  1620/50000: episode: 219, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004183, mae: 0.137079, mean_q: 0.186882\n",
      "  1642/50000: episode: 220, duration: 0.256s, episode steps:  22, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.003775, mae: 0.135826, mean_q: 0.187962\n",
      "  1658/50000: episode: 221, duration: 0.182s, episode steps:  16, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003066, mae: 0.135879, mean_q: 0.187346\n",
      "  1662/50000: episode: 222, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002354, mae: 0.129890, mean_q: 0.177069\n",
      "  1666/50000: episode: 223, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003482, mae: 0.130286, mean_q: 0.170988\n",
      "  1673/50000: episode: 224, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003434, mae: 0.124294, mean_q: 0.167278\n",
      "  1692/50000: episode: 225, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.003037, mae: 0.130545, mean_q: 0.183097\n",
      "  1696/50000: episode: 226, duration: 0.053s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003881, mae: 0.132579, mean_q: 0.190635\n",
      "  1702/50000: episode: 227, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002364, mae: 0.127398, mean_q: 0.175294\n",
      "  1704/50000: episode: 228, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002281, mae: 0.122181, mean_q: 0.168977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1714/50000: episode: 229, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002950, mae: 0.124098, mean_q: 0.178614\n",
      "  1723/50000: episode: 230, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002328, mae: 0.125253, mean_q: 0.177463\n",
      "  1726/50000: episode: 231, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003911, mae: 0.128608, mean_q: 0.173670\n",
      "  1732/50000: episode: 232, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.002633, mae: 0.129555, mean_q: 0.173312\n",
      "  1736/50000: episode: 233, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001774, mae: 0.127063, mean_q: 0.174069\n",
      "  1751/50000: episode: 234, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.933 [0.000, 3.000],  loss: 0.002646, mae: 0.125023, mean_q: 0.166997\n",
      "  1763/50000: episode: 235, duration: 0.141s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003055, mae: 0.127433, mean_q: 0.173337\n",
      "  1767/50000: episode: 236, duration: 0.054s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002399, mae: 0.134074, mean_q: 0.184081\n",
      "  1777/50000: episode: 237, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002111, mae: 0.120776, mean_q: 0.162132\n",
      "  1781/50000: episode: 238, duration: 0.053s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002451, mae: 0.115104, mean_q: 0.152140\n",
      "  1790/50000: episode: 239, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001694, mae: 0.113058, mean_q: 0.150459\n",
      "  1801/50000: episode: 240, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.001756, mae: 0.119791, mean_q: 0.163681\n",
      "  1803/50000: episode: 241, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002088, mae: 0.121800, mean_q: 0.164001\n",
      "  1813/50000: episode: 242, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002635, mae: 0.120104, mean_q: 0.162169\n",
      "  1816/50000: episode: 243, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002427, mae: 0.118990, mean_q: 0.160962\n",
      "  1820/50000: episode: 244, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001462, mae: 0.116294, mean_q: 0.159842\n",
      "  1823/50000: episode: 245, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001950, mae: 0.115773, mean_q: 0.159340\n",
      "  1828/50000: episode: 246, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001930, mae: 0.115260, mean_q: 0.153432\n",
      "  1832/50000: episode: 247, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001831, mae: 0.117506, mean_q: 0.157118\n",
      "  1842/50000: episode: 248, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.001698, mae: 0.113679, mean_q: 0.153700\n",
      "  1849/50000: episode: 249, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001548, mae: 0.112809, mean_q: 0.151896\n",
      "  1858/50000: episode: 250, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003581, mae: 0.119722, mean_q: 0.161286\n",
      "  1865/50000: episode: 251, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002894, mae: 0.124313, mean_q: 0.168266\n",
      "  1870/50000: episode: 252, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003334, mae: 0.120680, mean_q: 0.160147\n",
      "  1882/50000: episode: 253, duration: 0.140s, episode steps:  12, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001693, mae: 0.113915, mean_q: 0.155709\n",
      "  1898/50000: episode: 254, duration: 0.185s, episode steps:  16, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.003470, mae: 0.114284, mean_q: 0.158282\n",
      "  1906/50000: episode: 255, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002168, mae: 0.122357, mean_q: 0.171385\n",
      "  1923/50000: episode: 256, duration: 0.196s, episode steps:  17, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.003128, mae: 0.110541, mean_q: 0.152277\n",
      "  1932/50000: episode: 257, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002468, mae: 0.117346, mean_q: 0.163151\n",
      "  1936/50000: episode: 258, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002605, mae: 0.117244, mean_q: 0.160726\n",
      "  1940/50000: episode: 259, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001738, mae: 0.112658, mean_q: 0.150611\n",
      "  1942/50000: episode: 260, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005763, mae: 0.123679, mean_q: 0.167294\n",
      "  1946/50000: episode: 261, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002224, mae: 0.114974, mean_q: 0.152903\n",
      "  1979/50000: episode: 262, duration: 0.379s, episode steps:  33, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002162, mae: 0.111620, mean_q: 0.153500\n",
      "  1983/50000: episode: 263, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.005955, mae: 0.106993, mean_q: 0.144059\n",
      "  1991/50000: episode: 264, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002783, mae: 0.114167, mean_q: 0.159149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1998/50000: episode: 265, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002993, mae: 0.113588, mean_q: 0.158683\n",
      "  2006/50000: episode: 266, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002024, mae: 0.109984, mean_q: 0.152421\n",
      "  2019/50000: episode: 267, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003162, mae: 0.107108, mean_q: 0.144177\n",
      "  2025/50000: episode: 268, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002290, mae: 0.112080, mean_q: 0.150279\n",
      "  2031/50000: episode: 269, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001453, mae: 0.106610, mean_q: 0.148604\n",
      "  2048/50000: episode: 270, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002138, mae: 0.106031, mean_q: 0.147823\n",
      "  2055/50000: episode: 271, duration: 0.107s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002598, mae: 0.113200, mean_q: 0.163405\n",
      "  2060/50000: episode: 272, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.004355, mae: 0.116130, mean_q: 0.162428\n",
      "  2072/50000: episode: 273, duration: 0.188s, episode steps:  12, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002358, mae: 0.110367, mean_q: 0.156277\n",
      "  2080/50000: episode: 274, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002621, mae: 0.107841, mean_q: 0.146083\n",
      "  2084/50000: episode: 275, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001524, mae: 0.105234, mean_q: 0.143287\n",
      "  2092/50000: episode: 276, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.001428, mae: 0.104014, mean_q: 0.138618\n",
      "  2099/50000: episode: 277, duration: 0.086s, episode steps:   7, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.001491, mae: 0.105328, mean_q: 0.142404\n",
      "  2103/50000: episode: 278, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003002, mae: 0.105774, mean_q: 0.142057\n",
      "  2105/50000: episode: 279, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004749, mae: 0.107911, mean_q: 0.143866\n",
      "  2111/50000: episode: 280, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001404, mae: 0.114059, mean_q: 0.160764\n",
      "  2118/50000: episode: 281, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.001340, mae: 0.105882, mean_q: 0.148656\n",
      "  2147/50000: episode: 282, duration: 0.402s, episode steps:  29, steps per second:  72, episode reward:  1.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.103 [0.000, 3.000],  loss: 0.001461, mae: 0.102232, mean_q: 0.139602\n",
      "  2151/50000: episode: 283, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.004027, mae: 0.105166, mean_q: 0.141387\n",
      "  2158/50000: episode: 284, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.002804, mae: 0.108189, mean_q: 0.153711\n",
      "  2166/50000: episode: 285, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003474, mae: 0.111288, mean_q: 0.155158\n",
      "  2171/50000: episode: 286, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002633, mae: 0.104029, mean_q: 0.145595\n",
      "  2182/50000: episode: 287, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001591, mae: 0.102640, mean_q: 0.140699\n",
      "  2184/50000: episode: 288, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002501, mae: 0.105099, mean_q: 0.142084\n",
      "  2190/50000: episode: 289, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002464, mae: 0.100856, mean_q: 0.138654\n",
      "  2201/50000: episode: 290, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002886, mae: 0.109377, mean_q: 0.153337\n",
      "  2209/50000: episode: 291, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003079, mae: 0.114569, mean_q: 0.153043\n",
      "  2222/50000: episode: 292, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003915, mae: 0.109823, mean_q: 0.146132\n",
      "  2224/50000: episode: 293, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001035, mae: 0.098233, mean_q: 0.132979\n",
      "  2227/50000: episode: 294, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001477, mae: 0.096928, mean_q: 0.134269\n",
      "  2229/50000: episode: 295, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.005557, mae: 0.102925, mean_q: 0.137957\n",
      "  2233/50000: episode: 296, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001407, mae: 0.093912, mean_q: 0.125163\n",
      "  2242/50000: episode: 297, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003437, mae: 0.110931, mean_q: 0.154925\n",
      "  2244/50000: episode: 298, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001355, mae: 0.106741, mean_q: 0.149227\n",
      "  2246/50000: episode: 299, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003818, mae: 0.111401, mean_q: 0.151197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2256/50000: episode: 300, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002598, mae: 0.094719, mean_q: 0.125957\n",
      "  2267/50000: episode: 301, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001530, mae: 0.094159, mean_q: 0.126412\n",
      "  2274/50000: episode: 302, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002917, mae: 0.100861, mean_q: 0.135238\n",
      "  2281/50000: episode: 303, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002148, mae: 0.103744, mean_q: 0.138372\n",
      "  2283/50000: episode: 304, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.010010, mae: 0.105093, mean_q: 0.133501\n",
      "  2299/50000: episode: 305, duration: 0.194s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.062 [0.000, 3.000],  loss: 0.002775, mae: 0.106910, mean_q: 0.153617\n",
      "  2314/50000: episode: 306, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.001655, mae: 0.093576, mean_q: 0.133814\n",
      "  2326/50000: episode: 307, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.001428, mae: 0.098032, mean_q: 0.138545\n",
      "  2328/50000: episode: 308, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001987, mae: 0.097938, mean_q: 0.131194\n",
      "  2337/50000: episode: 309, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003668, mae: 0.096782, mean_q: 0.127351\n",
      "  2339/50000: episode: 310, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001966, mae: 0.103444, mean_q: 0.138303\n",
      "  2341/50000: episode: 311, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000943, mae: 0.099628, mean_q: 0.134571\n",
      "  2343/50000: episode: 312, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001202, mae: 0.099563, mean_q: 0.134893\n",
      "  2350/50000: episode: 313, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001882, mae: 0.104220, mean_q: 0.141905\n",
      "  2362/50000: episode: 314, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.001712, mae: 0.089935, mean_q: 0.119805\n",
      "  2364/50000: episode: 315, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.009472, mae: 0.107818, mean_q: 0.138044\n",
      "  2367/50000: episode: 316, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000933, mae: 0.100295, mean_q: 0.137553\n",
      "  2370/50000: episode: 317, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001190, mae: 0.107382, mean_q: 0.146150\n",
      "  2372/50000: episode: 318, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002021, mae: 0.108708, mean_q: 0.148501\n",
      "  2374/50000: episode: 319, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004963, mae: 0.102746, mean_q: 0.135449\n",
      "  2403/50000: episode: 320, duration: 0.335s, episode steps:  29, steps per second:  87, episode reward:  1.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.001620, mae: 0.091896, mean_q: 0.125142\n",
      "  2405/50000: episode: 321, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001417, mae: 0.098570, mean_q: 0.137258\n",
      "  2411/50000: episode: 322, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002577, mae: 0.092285, mean_q: 0.129188\n",
      "  2417/50000: episode: 323, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002497, mae: 0.093509, mean_q: 0.128723\n",
      "  2434/50000: episode: 324, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.059 [0.000, 3.000],  loss: 0.002155, mae: 0.095457, mean_q: 0.134417\n",
      "  2436/50000: episode: 325, duration: 0.045s, episode steps:   2, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001720, mae: 0.100043, mean_q: 0.141301\n",
      "  2452/50000: episode: 326, duration: 0.230s, episode steps:  16, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002774, mae: 0.096950, mean_q: 0.132643\n",
      "  2454/50000: episode: 327, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001322, mae: 0.092961, mean_q: 0.119966\n",
      "  2456/50000: episode: 328, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001404, mae: 0.093952, mean_q: 0.122218\n",
      "  2467/50000: episode: 329, duration: 0.182s, episode steps:  11, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002329, mae: 0.090300, mean_q: 0.118906\n",
      "  2471/50000: episode: 330, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003016, mae: 0.089828, mean_q: 0.124528\n",
      "  2479/50000: episode: 331, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002440, mae: 0.097642, mean_q: 0.139258\n",
      "  2486/50000: episode: 332, duration: 0.122s, episode steps:   7, steps per second:  58, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002743, mae: 0.097705, mean_q: 0.138862\n",
      "  2488/50000: episode: 333, duration: 0.042s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001897, mae: 0.098939, mean_q: 0.132698\n",
      "  2498/50000: episode: 334, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002798, mae: 0.097726, mean_q: 0.132298\n",
      "  2500/50000: episode: 335, duration: 0.031s, episode steps:   2, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004114, mae: 0.094749, mean_q: 0.130949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2524/50000: episode: 336, duration: 0.293s, episode steps:  24, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001797, mae: 0.091550, mean_q: 0.129075\n",
      "  2529/50000: episode: 337, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002219, mae: 0.095958, mean_q: 0.128618\n",
      "  2536/50000: episode: 338, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002904, mae: 0.097823, mean_q: 0.131287\n",
      "  2546/50000: episode: 339, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001476, mae: 0.091137, mean_q: 0.124868\n",
      "  2552/50000: episode: 340, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001372, mae: 0.091284, mean_q: 0.131354\n",
      "  2562/50000: episode: 341, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003796, mae: 0.094425, mean_q: 0.135811\n",
      "  2571/50000: episode: 342, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [1.000, 3.000],  loss: 0.001294, mae: 0.102074, mean_q: 0.141212\n",
      "  2573/50000: episode: 343, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002793, mae: 0.102566, mean_q: 0.134649\n",
      "  2576/50000: episode: 344, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001121, mae: 0.096853, mean_q: 0.132652\n",
      "  2582/50000: episode: 345, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001656, mae: 0.092953, mean_q: 0.126033\n",
      "  2590/50000: episode: 346, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.005156, mae: 0.097826, mean_q: 0.130563\n",
      "  2594/50000: episode: 347, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002540, mae: 0.102113, mean_q: 0.149282\n",
      "  2601/50000: episode: 348, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003391, mae: 0.109437, mean_q: 0.157939\n",
      "  2614/50000: episode: 349, duration: 0.188s, episode steps:  13, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.003285, mae: 0.097841, mean_q: 0.138735\n",
      "  2625/50000: episode: 350, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001688, mae: 0.089589, mean_q: 0.118506\n",
      "  2629/50000: episode: 351, duration: 0.054s, episode steps:   4, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003045, mae: 0.093320, mean_q: 0.122866\n",
      "  2639/50000: episode: 352, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002291, mae: 0.096028, mean_q: 0.132527\n",
      "  2641/50000: episode: 353, duration: 0.045s, episode steps:   2, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005354, mae: 0.095571, mean_q: 0.127968\n",
      "  2647/50000: episode: 354, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002395, mae: 0.097273, mean_q: 0.131970\n",
      "  2655/50000: episode: 355, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003140, mae: 0.096258, mean_q: 0.131491\n",
      "  2657/50000: episode: 356, duration: 0.044s, episode steps:   2, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000788, mae: 0.081587, mean_q: 0.112712\n",
      "  2663/50000: episode: 357, duration: 0.109s, episode steps:   6, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001633, mae: 0.092901, mean_q: 0.128557\n",
      "  2671/50000: episode: 358, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002586, mae: 0.085732, mean_q: 0.118514\n",
      "  2682/50000: episode: 359, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.003581, mae: 0.101855, mean_q: 0.152396\n",
      "  2689/50000: episode: 360, duration: 0.115s, episode steps:   7, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002680, mae: 0.091471, mean_q: 0.128059\n",
      "  2694/50000: episode: 361, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.003005, mae: 0.094532, mean_q: 0.131124\n",
      "  2699/50000: episode: 362, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003139, mae: 0.093365, mean_q: 0.130899\n",
      "  2706/50000: episode: 363, duration: 0.127s, episode steps:   7, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003033, mae: 0.098425, mean_q: 0.137802\n",
      "  2714/50000: episode: 364, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001247, mae: 0.086728, mean_q: 0.118309\n",
      "  2719/50000: episode: 365, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002411, mae: 0.094393, mean_q: 0.133255\n",
      "  2724/50000: episode: 366, duration: 0.093s, episode steps:   5, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000931, mae: 0.089422, mean_q: 0.125999\n",
      "  2733/50000: episode: 367, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.001930, mae: 0.096400, mean_q: 0.131652\n",
      "  2738/50000: episode: 368, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003440, mae: 0.101474, mean_q: 0.149395\n",
      "  2754/50000: episode: 369, duration: 0.230s, episode steps:  16, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002459, mae: 0.092844, mean_q: 0.131321\n",
      "  2757/50000: episode: 370, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.005508, mae: 0.096455, mean_q: 0.129941\n",
      "  2762/50000: episode: 371, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001454, mae: 0.090443, mean_q: 0.126431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2780/50000: episode: 372, duration: 0.263s, episode steps:  18, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003234, mae: 0.099827, mean_q: 0.134060\n",
      "  2786/50000: episode: 373, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002471, mae: 0.095056, mean_q: 0.127702\n",
      "  2791/50000: episode: 374, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002731, mae: 0.091771, mean_q: 0.124009\n",
      "  2800/50000: episode: 375, duration: 0.155s, episode steps:   9, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002716, mae: 0.093199, mean_q: 0.130710\n",
      "  2806/50000: episode: 376, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002277, mae: 0.097997, mean_q: 0.136459\n",
      "  2814/50000: episode: 377, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.001584, mae: 0.096136, mean_q: 0.131402\n",
      "  2824/50000: episode: 378, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002896, mae: 0.095902, mean_q: 0.127946\n",
      "  2833/50000: episode: 379, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 2.000],  loss: 0.002323, mae: 0.098613, mean_q: 0.133704\n",
      "  2837/50000: episode: 380, duration: 0.054s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002271, mae: 0.096488, mean_q: 0.131549\n",
      "  2850/50000: episode: 381, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.923 [0.000, 3.000],  loss: 0.003166, mae: 0.103531, mean_q: 0.141431\n",
      "  2864/50000: episode: 382, duration: 0.162s, episode steps:  14, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.003432, mae: 0.095707, mean_q: 0.131168\n",
      "  2872/50000: episode: 383, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004374, mae: 0.092051, mean_q: 0.124684\n",
      "  2876/50000: episode: 384, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.005644, mae: 0.103660, mean_q: 0.145733\n",
      "  2879/50000: episode: 385, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002640, mae: 0.108445, mean_q: 0.153335\n",
      "  2889/50000: episode: 386, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003588, mae: 0.104182, mean_q: 0.154685\n",
      "  2902/50000: episode: 387, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002566, mae: 0.091863, mean_q: 0.126282\n",
      "  2908/50000: episode: 388, duration: 0.103s, episode steps:   6, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001733, mae: 0.095847, mean_q: 0.128565\n",
      "  2911/50000: episode: 389, duration: 0.057s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001550, mae: 0.091240, mean_q: 0.123049\n",
      "  2916/50000: episode: 390, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.001831, mae: 0.099487, mean_q: 0.133993\n",
      "  2930/50000: episode: 391, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.002737, mae: 0.098072, mean_q: 0.131435\n",
      "  2937/50000: episode: 392, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [0.000, 3.000],  loss: 0.003496, mae: 0.093367, mean_q: 0.125571\n",
      "  2944/50000: episode: 393, duration: 0.085s, episode steps:   7, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001372, mae: 0.097068, mean_q: 0.132081\n",
      "  2950/50000: episode: 394, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002618, mae: 0.102315, mean_q: 0.143941\n",
      "  2957/50000: episode: 395, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002533, mae: 0.095324, mean_q: 0.134748\n",
      "  2960/50000: episode: 396, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002019, mae: 0.097207, mean_q: 0.137395\n",
      "  2966/50000: episode: 397, duration: 0.076s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002580, mae: 0.093797, mean_q: 0.130741\n",
      "  2979/50000: episode: 398, duration: 0.188s, episode steps:  13, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002700, mae: 0.098152, mean_q: 0.132418\n",
      "  2984/50000: episode: 399, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.005366, mae: 0.099296, mean_q: 0.134063\n",
      "  2988/50000: episode: 400, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001655, mae: 0.101153, mean_q: 0.136655\n",
      "  2999/50000: episode: 401, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.002179, mae: 0.094573, mean_q: 0.133316\n",
      "  3003/50000: episode: 402, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001989, mae: 0.097957, mean_q: 0.143314\n",
      "  3005/50000: episode: 403, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001596, mae: 0.095259, mean_q: 0.140930\n",
      "  3018/50000: episode: 404, duration: 0.151s, episode steps:  13, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.003228, mae: 0.100399, mean_q: 0.136708\n",
      "  3049/50000: episode: 405, duration: 0.350s, episode steps:  31, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.194 [0.000, 3.000],  loss: 0.002716, mae: 0.103228, mean_q: 0.144787\n",
      "  3051/50000: episode: 406, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.006604, mae: 0.111921, mean_q: 0.145525\n",
      "  3057/50000: episode: 407, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.003423, mae: 0.104463, mean_q: 0.141084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3068/50000: episode: 408, duration: 0.156s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001911, mae: 0.103362, mean_q: 0.140353\n",
      "  3072/50000: episode: 409, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003305, mae: 0.112062, mean_q: 0.153555\n",
      "  3080/50000: episode: 410, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003274, mae: 0.105987, mean_q: 0.144622\n",
      "  3099/50000: episode: 411, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.003888, mae: 0.106601, mean_q: 0.143292\n",
      "  3115/50000: episode: 412, duration: 0.210s, episode steps:  16, steps per second:  76, episode reward:  1.000, mean reward:  0.062 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.003066, mae: 0.105187, mean_q: 0.141438\n",
      "  3117/50000: episode: 413, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.006501, mae: 0.114000, mean_q: 0.152185\n",
      "  3128/50000: episode: 414, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003131, mae: 0.111157, mean_q: 0.164844\n",
      "  3137/50000: episode: 415, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002613, mae: 0.100059, mean_q: 0.136112\n",
      "  3140/50000: episode: 416, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002368, mae: 0.097096, mean_q: 0.126869\n",
      "  3147/50000: episode: 417, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.004051, mae: 0.106003, mean_q: 0.138370\n",
      "  3150/50000: episode: 418, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002076, mae: 0.100243, mean_q: 0.134769\n",
      "  3157/50000: episode: 419, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001384, mae: 0.097222, mean_q: 0.138101\n",
      "  3162/50000: episode: 420, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.002333, mae: 0.100801, mean_q: 0.141436\n",
      "  3167/50000: episode: 421, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.003700, mae: 0.104400, mean_q: 0.141742\n",
      "  3169/50000: episode: 422, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004386, mae: 0.118062, mean_q: 0.153231\n",
      "  3172/50000: episode: 423, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003283, mae: 0.109185, mean_q: 0.144273\n",
      "  3175/50000: episode: 424, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001709, mae: 0.092261, mean_q: 0.124948\n",
      "  3182/50000: episode: 425, duration: 0.118s, episode steps:   7, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002763, mae: 0.093995, mean_q: 0.127235\n",
      "  3184/50000: episode: 426, duration: 0.047s, episode steps:   2, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002713, mae: 0.102096, mean_q: 0.144828\n",
      "  3193/50000: episode: 427, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003897, mae: 0.108332, mean_q: 0.154422\n",
      "  3203/50000: episode: 428, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003559, mae: 0.110248, mean_q: 0.151323\n",
      "  3218/50000: episode: 429, duration: 0.212s, episode steps:  15, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.003520, mae: 0.107914, mean_q: 0.146299\n",
      "  3235/50000: episode: 430, duration: 0.280s, episode steps:  17, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002739, mae: 0.095284, mean_q: 0.131852\n",
      "  3243/50000: episode: 431, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003601, mae: 0.104557, mean_q: 0.139107\n",
      "  3245/50000: episode: 432, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001262, mae: 0.109250, mean_q: 0.150358\n",
      "  3249/50000: episode: 433, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002459, mae: 0.113807, mean_q: 0.158194\n",
      "  3252/50000: episode: 434, duration: 0.061s, episode steps:   3, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001712, mae: 0.105211, mean_q: 0.146286\n",
      "  3261/50000: episode: 435, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003299, mae: 0.105319, mean_q: 0.142432\n",
      "  3271/50000: episode: 436, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002165, mae: 0.097621, mean_q: 0.128076\n",
      "  3276/50000: episode: 437, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [0.000, 3.000],  loss: 0.003753, mae: 0.094822, mean_q: 0.124966\n",
      "  3280/50000: episode: 438, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002182, mae: 0.107394, mean_q: 0.148399\n",
      "  3297/50000: episode: 439, duration: 0.241s, episode steps:  17, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001915, mae: 0.100277, mean_q: 0.137981\n",
      "  3320/50000: episode: 440, duration: 0.313s, episode steps:  23, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002332, mae: 0.103601, mean_q: 0.137232\n",
      "  3325/50000: episode: 441, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001557, mae: 0.099357, mean_q: 0.134253\n",
      "  3340/50000: episode: 442, duration: 0.184s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.002413, mae: 0.100822, mean_q: 0.139272\n",
      "  3348/50000: episode: 443, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003672, mae: 0.101951, mean_q: 0.135775\n",
      "  3354/50000: episode: 444, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 1.000],  loss: 0.001794, mae: 0.096938, mean_q: 0.131205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3363/50000: episode: 445, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002814, mae: 0.104925, mean_q: 0.142200\n",
      "  3365/50000: episode: 446, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002255, mae: 0.101688, mean_q: 0.139757\n",
      "  3367/50000: episode: 447, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002565, mae: 0.110696, mean_q: 0.151111\n",
      "  3401/50000: episode: 448, duration: 0.385s, episode steps:  34, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.003261, mae: 0.107640, mean_q: 0.147076\n",
      "  3405/50000: episode: 449, duration: 0.073s, episode steps:   4, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003937, mae: 0.104663, mean_q: 0.145829\n",
      "  3420/50000: episode: 450, duration: 0.209s, episode steps:  15, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002694, mae: 0.100608, mean_q: 0.134918\n",
      "  3425/50000: episode: 451, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002378, mae: 0.106303, mean_q: 0.143669\n",
      "  3434/50000: episode: 452, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.002050, mae: 0.106781, mean_q: 0.155698\n",
      "  3436/50000: episode: 453, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002119, mae: 0.100840, mean_q: 0.146422\n",
      "  3441/50000: episode: 454, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 3.000],  loss: 0.003336, mae: 0.106827, mean_q: 0.147193\n",
      "  3464/50000: episode: 455, duration: 0.293s, episode steps:  23, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.002517, mae: 0.103765, mean_q: 0.140634\n",
      "  3471/50000: episode: 456, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001941, mae: 0.102358, mean_q: 0.148645\n",
      "  3476/50000: episode: 457, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003107, mae: 0.102256, mean_q: 0.151360\n",
      "  3478/50000: episode: 458, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002005, mae: 0.100437, mean_q: 0.146052\n",
      "  3483/50000: episode: 459, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001657, mae: 0.107916, mean_q: 0.149062\n",
      "  3486/50000: episode: 460, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003218, mae: 0.111468, mean_q: 0.145443\n",
      "  3489/50000: episode: 461, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.005532, mae: 0.120008, mean_q: 0.154868\n",
      "  3491/50000: episode: 462, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002531, mae: 0.115006, mean_q: 0.151433\n",
      "  3494/50000: episode: 463, duration: 0.042s, episode steps:   3, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002974, mae: 0.104922, mean_q: 0.140327\n",
      "  3505/50000: episode: 464, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.001883, mae: 0.097360, mean_q: 0.132446\n",
      "  3507/50000: episode: 465, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001367, mae: 0.095041, mean_q: 0.141228\n",
      "  3509/50000: episode: 466, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002119, mae: 0.099101, mean_q: 0.142053\n",
      "  3518/50000: episode: 467, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002201, mae: 0.103583, mean_q: 0.141710\n",
      "  3526/50000: episode: 468, duration: 0.139s, episode steps:   8, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002249, mae: 0.106975, mean_q: 0.142692\n",
      "  3530/50000: episode: 469, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001998, mae: 0.102709, mean_q: 0.136869\n",
      "  3541/50000: episode: 470, duration: 0.130s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002915, mae: 0.100247, mean_q: 0.133094\n",
      "  3553/50000: episode: 471, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.002280, mae: 0.107547, mean_q: 0.145400\n",
      "  3572/50000: episode: 472, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.002683, mae: 0.109709, mean_q: 0.151540\n",
      "  3577/50000: episode: 473, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001754, mae: 0.107451, mean_q: 0.142721\n",
      "  3583/50000: episode: 474, duration: 0.100s, episode steps:   6, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.003551, mae: 0.107021, mean_q: 0.140952\n",
      "  3586/50000: episode: 475, duration: 0.056s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001238, mae: 0.110731, mean_q: 0.148081\n",
      "  3592/50000: episode: 476, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.003385, mae: 0.113215, mean_q: 0.148094\n",
      "  3602/50000: episode: 477, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.002869, mae: 0.100444, mean_q: 0.138815\n",
      "  3608/50000: episode: 478, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003987, mae: 0.112109, mean_q: 0.156009\n",
      "  3626/50000: episode: 479, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.002785, mae: 0.106981, mean_q: 0.151622\n",
      "  3633/50000: episode: 480, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003159, mae: 0.096161, mean_q: 0.130312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3649/50000: episode: 481, duration: 0.191s, episode steps:  16, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.938 [0.000, 3.000],  loss: 0.003063, mae: 0.111061, mean_q: 0.150236\n",
      "  3671/50000: episode: 482, duration: 0.253s, episode steps:  22, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.273 [0.000, 3.000],  loss: 0.003204, mae: 0.104395, mean_q: 0.141161\n",
      "  3683/50000: episode: 483, duration: 0.142s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002675, mae: 0.098191, mean_q: 0.139417\n",
      "  3688/50000: episode: 484, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002226, mae: 0.111206, mean_q: 0.155137\n",
      "  3703/50000: episode: 485, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002020, mae: 0.101288, mean_q: 0.136218\n",
      "  3707/50000: episode: 486, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001670, mae: 0.095642, mean_q: 0.127930\n",
      "  3725/50000: episode: 487, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.003044, mae: 0.106119, mean_q: 0.146932\n",
      "  3730/50000: episode: 488, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002521, mae: 0.099967, mean_q: 0.148442\n",
      "  3740/50000: episode: 489, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.004153, mae: 0.107513, mean_q: 0.145886\n",
      "  3745/50000: episode: 490, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002572, mae: 0.101603, mean_q: 0.135318\n",
      "  3753/50000: episode: 491, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001956, mae: 0.102232, mean_q: 0.138587\n",
      "  3761/50000: episode: 492, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002582, mae: 0.101467, mean_q: 0.135741\n",
      "  3764/50000: episode: 493, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.002482, mae: 0.113255, mean_q: 0.156349\n",
      "  3770/50000: episode: 494, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002183, mae: 0.106931, mean_q: 0.149461\n",
      "  3808/50000: episode: 495, duration: 0.425s, episode steps:  38, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: 0.002475, mae: 0.100565, mean_q: 0.140316\n",
      "  3821/50000: episode: 496, duration: 0.149s, episode steps:  13, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.003100, mae: 0.103001, mean_q: 0.136654\n",
      "  3825/50000: episode: 497, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002409, mae: 0.106144, mean_q: 0.142849\n",
      "  3831/50000: episode: 498, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.003360, mae: 0.106860, mean_q: 0.142290\n",
      "  3851/50000: episode: 499, duration: 0.232s, episode steps:  20, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002750, mae: 0.098076, mean_q: 0.134105\n",
      "  3864/50000: episode: 500, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002044, mae: 0.112096, mean_q: 0.151658\n",
      "  3870/50000: episode: 501, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.004384, mae: 0.107350, mean_q: 0.145568\n",
      "  3877/50000: episode: 502, duration: 0.116s, episode steps:   7, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.002472, mae: 0.100155, mean_q: 0.135873\n",
      "  3882/50000: episode: 503, duration: 0.092s, episode steps:   5, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.004816, mae: 0.109945, mean_q: 0.150792\n",
      "  3886/50000: episode: 504, duration: 0.071s, episode steps:   4, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002197, mae: 0.107775, mean_q: 0.154879\n",
      "  3898/50000: episode: 505, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002196, mae: 0.103104, mean_q: 0.150155\n",
      "  3936/50000: episode: 506, duration: 0.439s, episode steps:  38, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.395 [0.000, 3.000],  loss: 0.002579, mae: 0.104905, mean_q: 0.147734\n",
      "  3950/50000: episode: 507, duration: 0.161s, episode steps:  14, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002642, mae: 0.104644, mean_q: 0.141794\n",
      "  3962/50000: episode: 508, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001984, mae: 0.104274, mean_q: 0.142828\n",
      "  3965/50000: episode: 509, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002540, mae: 0.109837, mean_q: 0.148639\n",
      "  3981/50000: episode: 510, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002433, mae: 0.099235, mean_q: 0.136988\n",
      "  3988/50000: episode: 511, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002512, mae: 0.107836, mean_q: 0.147759\n",
      "  3993/50000: episode: 512, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001700, mae: 0.105327, mean_q: 0.141588\n",
      "  4000/50000: episode: 513, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.001290, mae: 0.096934, mean_q: 0.131011\n",
      "  4008/50000: episode: 514, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.002654, mae: 0.103693, mean_q: 0.137850\n",
      "  4011/50000: episode: 515, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002200, mae: 0.101878, mean_q: 0.135223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4017/50000: episode: 516, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001628, mae: 0.101206, mean_q: 0.135051\n",
      "  4028/50000: episode: 517, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002232, mae: 0.098888, mean_q: 0.133596\n",
      "  4036/50000: episode: 518, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003406, mae: 0.106980, mean_q: 0.144004\n",
      "  4038/50000: episode: 519, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001281, mae: 0.101707, mean_q: 0.138126\n",
      "  4053/50000: episode: 520, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002874, mae: 0.104939, mean_q: 0.153281\n",
      "  4058/50000: episode: 521, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002613, mae: 0.091264, mean_q: 0.134771\n",
      "  4067/50000: episode: 522, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002520, mae: 0.116412, mean_q: 0.168067\n",
      "  4079/50000: episode: 523, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002443, mae: 0.105882, mean_q: 0.140613\n",
      "  4088/50000: episode: 524, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002222, mae: 0.096487, mean_q: 0.127656\n",
      "  4093/50000: episode: 525, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000834, mae: 0.093469, mean_q: 0.128821\n",
      "  4105/50000: episode: 526, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001846, mae: 0.101221, mean_q: 0.137603\n",
      "  4124/50000: episode: 527, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.002893, mae: 0.102006, mean_q: 0.146279\n",
      "  4131/50000: episode: 528, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002464, mae: 0.104284, mean_q: 0.140665\n",
      "  4139/50000: episode: 529, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002253, mae: 0.105750, mean_q: 0.147361\n",
      "  4163/50000: episode: 530, duration: 0.475s, episode steps:  24, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.002050, mae: 0.101983, mean_q: 0.139411\n",
      "  4168/50000: episode: 531, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002536, mae: 0.104349, mean_q: 0.141501\n",
      "  4179/50000: episode: 532, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003134, mae: 0.104019, mean_q: 0.146959\n",
      "  4194/50000: episode: 533, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002449, mae: 0.104801, mean_q: 0.144133\n",
      "  4207/50000: episode: 534, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002084, mae: 0.103814, mean_q: 0.141520\n",
      "  4219/50000: episode: 535, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001810, mae: 0.102633, mean_q: 0.138494\n",
      "  4221/50000: episode: 536, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005358, mae: 0.113942, mean_q: 0.154453\n",
      "  4224/50000: episode: 537, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001747, mae: 0.104437, mean_q: 0.140568\n",
      "  4231/50000: episode: 538, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002783, mae: 0.105622, mean_q: 0.141105\n",
      "  4234/50000: episode: 539, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001577, mae: 0.099168, mean_q: 0.136931\n",
      "  4242/50000: episode: 540, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001155, mae: 0.089709, mean_q: 0.124281\n",
      "  4245/50000: episode: 541, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003516, mae: 0.098400, mean_q: 0.131763\n",
      "  4264/50000: episode: 542, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward:  1.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.263 [0.000, 3.000],  loss: 0.002687, mae: 0.105328, mean_q: 0.143888\n",
      "  4266/50000: episode: 543, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000857, mae: 0.099430, mean_q: 0.134977\n",
      "  4270/50000: episode: 544, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004144, mae: 0.111988, mean_q: 0.147672\n",
      "  4285/50000: episode: 545, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.002438, mae: 0.104958, mean_q: 0.146160\n",
      "  4292/50000: episode: 546, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.001506, mae: 0.096779, mean_q: 0.140026\n",
      "  4296/50000: episode: 547, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001568, mae: 0.106650, mean_q: 0.148381\n",
      "  4308/50000: episode: 548, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 3.000],  loss: 0.002086, mae: 0.106535, mean_q: 0.147231\n",
      "  4314/50000: episode: 549, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001729, mae: 0.099909, mean_q: 0.139517\n",
      "  4317/50000: episode: 550, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.006445, mae: 0.108898, mean_q: 0.147252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4333/50000: episode: 551, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002258, mae: 0.105536, mean_q: 0.149509\n",
      "  4339/50000: episode: 552, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002920, mae: 0.112412, mean_q: 0.159245\n",
      "  4347/50000: episode: 553, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003679, mae: 0.117507, mean_q: 0.165955\n",
      "  4349/50000: episode: 554, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001843, mae: 0.119161, mean_q: 0.163840\n",
      "  4352/50000: episode: 555, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002323, mae: 0.110784, mean_q: 0.157466\n",
      "  4360/50000: episode: 556, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002841, mae: 0.107641, mean_q: 0.150649\n",
      "  4365/50000: episode: 557, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002340, mae: 0.096512, mean_q: 0.139460\n",
      "  4370/50000: episode: 558, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001826, mae: 0.100418, mean_q: 0.142579\n",
      "  4382/50000: episode: 559, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002762, mae: 0.107219, mean_q: 0.145538\n",
      "  4389/50000: episode: 560, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.001425, mae: 0.100374, mean_q: 0.138377\n",
      "  4396/50000: episode: 561, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.002889, mae: 0.102735, mean_q: 0.142239\n",
      "  4404/50000: episode: 562, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004540, mae: 0.108098, mean_q: 0.159974\n",
      "  4408/50000: episode: 563, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001730, mae: 0.105222, mean_q: 0.152447\n",
      "  4412/50000: episode: 564, duration: 0.052s, episode steps:   4, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001623, mae: 0.101994, mean_q: 0.144339\n",
      "  4420/50000: episode: 565, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002901, mae: 0.103178, mean_q: 0.144569\n",
      "  4430/50000: episode: 566, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002273, mae: 0.109264, mean_q: 0.151567\n",
      "  4435/50000: episode: 567, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002236, mae: 0.108092, mean_q: 0.151171\n",
      "  4440/50000: episode: 568, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.001764, mae: 0.101874, mean_q: 0.138359\n",
      "  4444/50000: episode: 569, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003862, mae: 0.108026, mean_q: 0.146946\n",
      "  4448/50000: episode: 570, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001911, mae: 0.098383, mean_q: 0.132846\n",
      "  4461/50000: episode: 571, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.077 [0.000, 3.000],  loss: 0.003041, mae: 0.104370, mean_q: 0.138755\n",
      "  4479/50000: episode: 572, duration: 0.208s, episode steps:  18, steps per second:  87, episode reward:  1.000, mean reward:  0.056 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001984, mae: 0.107206, mean_q: 0.151221\n",
      "  4485/50000: episode: 573, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001743, mae: 0.109929, mean_q: 0.150832\n",
      "  4496/50000: episode: 574, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002841, mae: 0.111227, mean_q: 0.152605\n",
      "  4500/50000: episode: 575, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001520, mae: 0.103196, mean_q: 0.146899\n",
      "  4512/50000: episode: 576, duration: 0.141s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.002248, mae: 0.112326, mean_q: 0.156113\n",
      "  4520/50000: episode: 577, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002032, mae: 0.104959, mean_q: 0.143986\n",
      "  4526/50000: episode: 578, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.002585, mae: 0.104361, mean_q: 0.150372\n",
      "  4533/50000: episode: 579, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002061, mae: 0.108007, mean_q: 0.149918\n",
      "  4548/50000: episode: 580, duration: 0.180s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001960, mae: 0.106793, mean_q: 0.142262\n",
      "  4565/50000: episode: 581, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002606, mae: 0.106763, mean_q: 0.144922\n",
      "  4576/50000: episode: 582, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002306, mae: 0.111617, mean_q: 0.154974\n",
      "  4599/50000: episode: 583, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.003765, mae: 0.113635, mean_q: 0.154855\n",
      "  4604/50000: episode: 584, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.600 [2.000, 3.000],  loss: 0.001560, mae: 0.100324, mean_q: 0.140319\n",
      "  4606/50000: episode: 585, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005059, mae: 0.124194, mean_q: 0.170771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4615/50000: episode: 586, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.001635, mae: 0.112575, mean_q: 0.158625\n",
      "  4618/50000: episode: 587, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003478, mae: 0.113526, mean_q: 0.151504\n",
      "  4624/50000: episode: 588, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002508, mae: 0.105639, mean_q: 0.139821\n",
      "  4630/50000: episode: 589, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002878, mae: 0.107921, mean_q: 0.143159\n",
      "  4646/50000: episode: 590, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002180, mae: 0.110059, mean_q: 0.149502\n",
      "  4667/50000: episode: 591, duration: 0.243s, episode steps:  21, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002408, mae: 0.110004, mean_q: 0.153363\n",
      "  4669/50000: episode: 592, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004170, mae: 0.106408, mean_q: 0.151863\n",
      "  4674/50000: episode: 593, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003836, mae: 0.112077, mean_q: 0.161209\n",
      "  4683/50000: episode: 594, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002904, mae: 0.114936, mean_q: 0.163824\n",
      "  4687/50000: episode: 595, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003703, mae: 0.122469, mean_q: 0.170245\n",
      "  4689/50000: episode: 596, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003806, mae: 0.102096, mean_q: 0.143833\n",
      "  4696/50000: episode: 597, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003116, mae: 0.101544, mean_q: 0.141860\n",
      "  4702/50000: episode: 598, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002682, mae: 0.106572, mean_q: 0.148537\n",
      "  4706/50000: episode: 599, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002403, mae: 0.107739, mean_q: 0.150918\n",
      "  4711/50000: episode: 600, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001521, mae: 0.101612, mean_q: 0.148434\n",
      "  4721/50000: episode: 601, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003113, mae: 0.105025, mean_q: 0.148682\n",
      "  4731/50000: episode: 602, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002744, mae: 0.109161, mean_q: 0.153028\n",
      "  4737/50000: episode: 603, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002345, mae: 0.116004, mean_q: 0.164660\n",
      "  4744/50000: episode: 604, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002924, mae: 0.111883, mean_q: 0.151817\n",
      "  4747/50000: episode: 605, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001596, mae: 0.111070, mean_q: 0.154409\n",
      "  4755/50000: episode: 606, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002522, mae: 0.107442, mean_q: 0.145645\n",
      "  4763/50000: episode: 607, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.002915, mae: 0.110598, mean_q: 0.155022\n",
      "  4769/50000: episode: 608, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002208, mae: 0.108661, mean_q: 0.151041\n",
      "  4771/50000: episode: 609, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001405, mae: 0.104312, mean_q: 0.145179\n",
      "  4773/50000: episode: 610, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001852, mae: 0.112180, mean_q: 0.153640\n",
      "  4781/50000: episode: 611, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.375 [2.000, 3.000],  loss: 0.002586, mae: 0.111017, mean_q: 0.153515\n",
      "  4789/50000: episode: 612, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.002757, mae: 0.111734, mean_q: 0.152009\n",
      "  4794/50000: episode: 613, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 3.000],  loss: 0.002235, mae: 0.110673, mean_q: 0.156367\n",
      "  4796/50000: episode: 614, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004813, mae: 0.111153, mean_q: 0.153912\n",
      "  4798/50000: episode: 615, duration: 0.042s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002185, mae: 0.125563, mean_q: 0.176933\n",
      "  4801/50000: episode: 616, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003013, mae: 0.116711, mean_q: 0.157649\n",
      "  4806/50000: episode: 617, duration: 0.065s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003321, mae: 0.121015, mean_q: 0.167542\n",
      "  4817/50000: episode: 618, duration: 0.133s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003104, mae: 0.111903, mean_q: 0.153318\n",
      "  4831/50000: episode: 619, duration: 0.163s, episode steps:  14, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [1.000, 3.000],  loss: 0.002773, mae: 0.109941, mean_q: 0.152118\n",
      "  4839/50000: episode: 620, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002656, mae: 0.115064, mean_q: 0.156748\n",
      "  4855/50000: episode: 621, duration: 0.187s, episode steps:  16, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002721, mae: 0.115519, mean_q: 0.157717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4857/50000: episode: 622, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002064, mae: 0.105133, mean_q: 0.147156\n",
      "  4859/50000: episode: 623, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005800, mae: 0.111555, mean_q: 0.156893\n",
      "  4867/50000: episode: 624, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.004164, mae: 0.117283, mean_q: 0.160172\n",
      "  4870/50000: episode: 625, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001864, mae: 0.127050, mean_q: 0.180187\n",
      "  4886/50000: episode: 626, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003055, mae: 0.113659, mean_q: 0.158233\n",
      "  4888/50000: episode: 627, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002251, mae: 0.112299, mean_q: 0.158583\n",
      "  4898/50000: episode: 628, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002111, mae: 0.112456, mean_q: 0.154050\n",
      "  4902/50000: episode: 629, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001975, mae: 0.111701, mean_q: 0.154149\n",
      "  4908/50000: episode: 630, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002712, mae: 0.111378, mean_q: 0.155424\n",
      "  4921/50000: episode: 631, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.002471, mae: 0.110733, mean_q: 0.153153\n",
      "  4925/50000: episode: 632, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002537, mae: 0.111928, mean_q: 0.152689\n",
      "  4930/50000: episode: 633, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.002559, mae: 0.109956, mean_q: 0.153913\n",
      "  4932/50000: episode: 634, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001487, mae: 0.115595, mean_q: 0.162717\n",
      "  4945/50000: episode: 635, duration: 0.154s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.923 [0.000, 3.000],  loss: 0.002803, mae: 0.124034, mean_q: 0.168027\n",
      "  4949/50000: episode: 636, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002680, mae: 0.118455, mean_q: 0.162877\n",
      "  4964/50000: episode: 637, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.003071, mae: 0.108936, mean_q: 0.149612\n",
      "  4970/50000: episode: 638, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002104, mae: 0.108729, mean_q: 0.149251\n",
      "  4977/50000: episode: 639, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002791, mae: 0.108745, mean_q: 0.152406\n",
      "  4985/50000: episode: 640, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003212, mae: 0.115889, mean_q: 0.162450\n",
      "  4995/50000: episode: 641, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002958, mae: 0.121831, mean_q: 0.169717\n",
      "  5005/50000: episode: 642, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001809, mae: 0.109529, mean_q: 0.150686\n",
      "  5025/50000: episode: 643, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002963, mae: 0.110297, mean_q: 0.149566\n",
      "  5047/50000: episode: 644, duration: 0.254s, episode steps:  22, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.003677, mae: 0.122432, mean_q: 0.172174\n",
      "  5062/50000: episode: 645, duration: 0.175s, episode steps:  15, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002731, mae: 0.121494, mean_q: 0.168444\n",
      "  5064/50000: episode: 646, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002500, mae: 0.118784, mean_q: 0.163625\n",
      "  5067/50000: episode: 647, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004228, mae: 0.119649, mean_q: 0.163341\n",
      "  5076/50000: episode: 648, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002800, mae: 0.125733, mean_q: 0.176329\n",
      "  5079/50000: episode: 649, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003439, mae: 0.125492, mean_q: 0.178318\n",
      "  5087/50000: episode: 650, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [0.000, 3.000],  loss: 0.003294, mae: 0.124091, mean_q: 0.175496\n",
      "  5097/50000: episode: 651, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002146, mae: 0.112616, mean_q: 0.157725\n",
      "  5100/50000: episode: 652, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.004298, mae: 0.120430, mean_q: 0.163775\n",
      "  5103/50000: episode: 653, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.003682, mae: 0.128356, mean_q: 0.175735\n",
      "  5114/50000: episode: 654, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002085, mae: 0.115005, mean_q: 0.157568\n",
      "  5116/50000: episode: 655, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002362, mae: 0.114905, mean_q: 0.156573\n",
      "  5119/50000: episode: 656, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001623, mae: 0.113479, mean_q: 0.154082\n",
      "  5122/50000: episode: 657, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002120, mae: 0.115247, mean_q: 0.159681\n",
      "  5125/50000: episode: 658, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003405, mae: 0.113826, mean_q: 0.154372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5137/50000: episode: 659, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002772, mae: 0.111514, mean_q: 0.150024\n",
      "  5142/50000: episode: 660, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003151, mae: 0.121630, mean_q: 0.166769\n",
      "  5146/50000: episode: 661, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003975, mae: 0.124257, mean_q: 0.165963\n",
      "  5171/50000: episode: 662, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.840 [0.000, 3.000],  loss: 0.003403, mae: 0.115171, mean_q: 0.157086\n",
      "  5174/50000: episode: 663, duration: 0.064s, episode steps:   3, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003033, mae: 0.121061, mean_q: 0.162330\n",
      "  5182/50000: episode: 664, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003735, mae: 0.120610, mean_q: 0.163582\n",
      "  5184/50000: episode: 665, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002619, mae: 0.116274, mean_q: 0.157251\n",
      "  5193/50000: episode: 666, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003510, mae: 0.115909, mean_q: 0.164650\n",
      "  5197/50000: episode: 667, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003499, mae: 0.118987, mean_q: 0.169558\n",
      "  5208/50000: episode: 668, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002528, mae: 0.119087, mean_q: 0.164090\n",
      "  5214/50000: episode: 669, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002420, mae: 0.113978, mean_q: 0.163680\n",
      "  5216/50000: episode: 670, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002296, mae: 0.104026, mean_q: 0.151982\n",
      "  5222/50000: episode: 671, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003470, mae: 0.115827, mean_q: 0.172878\n",
      "  5227/50000: episode: 672, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003093, mae: 0.119750, mean_q: 0.166396\n",
      "  5232/50000: episode: 673, duration: 0.126s, episode steps:   5, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002129, mae: 0.115355, mean_q: 0.159720\n",
      "  5245/50000: episode: 674, duration: 0.252s, episode steps:  13, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.003120, mae: 0.120090, mean_q: 0.165491\n",
      "  5247/50000: episode: 675, duration: 0.060s, episode steps:   2, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001241, mae: 0.117106, mean_q: 0.163365\n",
      "  5250/50000: episode: 676, duration: 0.065s, episode steps:   3, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001627, mae: 0.118370, mean_q: 0.166583\n",
      "  5254/50000: episode: 677, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.004287, mae: 0.125802, mean_q: 0.170947\n",
      "  5260/50000: episode: 678, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003575, mae: 0.121727, mean_q: 0.169188\n",
      "  5263/50000: episode: 679, duration: 0.054s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002253, mae: 0.117459, mean_q: 0.162689\n",
      "  5268/50000: episode: 680, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002482, mae: 0.116847, mean_q: 0.163600\n",
      "  5271/50000: episode: 681, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001828, mae: 0.113708, mean_q: 0.154321\n",
      "  5293/50000: episode: 682, duration: 0.294s, episode steps:  22, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.864 [0.000, 3.000],  loss: 0.003582, mae: 0.120359, mean_q: 0.169745\n",
      "  5295/50000: episode: 683, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004616, mae: 0.118223, mean_q: 0.165261\n",
      "  5299/50000: episode: 684, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.004246, mae: 0.118855, mean_q: 0.165571\n",
      "  5307/50000: episode: 685, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003447, mae: 0.117780, mean_q: 0.159756\n",
      "  5314/50000: episode: 686, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.571 [1.000, 3.000],  loss: 0.002366, mae: 0.114889, mean_q: 0.162426\n",
      "  5317/50000: episode: 687, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001946, mae: 0.118619, mean_q: 0.170602\n",
      "  5323/50000: episode: 688, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003207, mae: 0.117603, mean_q: 0.167828\n",
      "  5328/50000: episode: 689, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004625, mae: 0.123669, mean_q: 0.175536\n",
      "  5344/50000: episode: 690, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.002790, mae: 0.127833, mean_q: 0.175599\n",
      "  5348/50000: episode: 691, duration: 0.076s, episode steps:   4, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002965, mae: 0.122298, mean_q: 0.164836\n",
      "  5359/50000: episode: 692, duration: 0.152s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002656, mae: 0.117260, mean_q: 0.160178\n",
      "  5361/50000: episode: 693, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002168, mae: 0.118983, mean_q: 0.161218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5374/50000: episode: 694, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.003113, mae: 0.120862, mean_q: 0.162262\n",
      "  5383/50000: episode: 695, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002970, mae: 0.118101, mean_q: 0.158715\n",
      "  5387/50000: episode: 696, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001908, mae: 0.112235, mean_q: 0.159089\n",
      "  5393/50000: episode: 697, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.003370, mae: 0.117569, mean_q: 0.165801\n",
      "  5397/50000: episode: 698, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002877, mae: 0.130027, mean_q: 0.180845\n",
      "  5410/50000: episode: 699, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.154 [0.000, 3.000],  loss: 0.002321, mae: 0.122575, mean_q: 0.170820\n",
      "  5431/50000: episode: 700, duration: 0.346s, episode steps:  21, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002513, mae: 0.119373, mean_q: 0.163840\n",
      "  5437/50000: episode: 701, duration: 0.095s, episode steps:   6, steps per second:  63, episode reward:  1.000, mean reward:  0.167 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002949, mae: 0.126543, mean_q: 0.177091\n",
      "  5441/50000: episode: 702, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002081, mae: 0.117816, mean_q: 0.169447\n",
      "  5445/50000: episode: 703, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.001804, mae: 0.113729, mean_q: 0.160515\n",
      "  5463/50000: episode: 704, duration: 0.239s, episode steps:  18, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003073, mae: 0.124830, mean_q: 0.171308\n",
      "  5466/50000: episode: 705, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002039, mae: 0.122114, mean_q: 0.174359\n",
      "  5480/50000: episode: 706, duration: 0.207s, episode steps:  14, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.071 [0.000, 3.000],  loss: 0.005341, mae: 0.127974, mean_q: 0.180303\n",
      "  5483/50000: episode: 707, duration: 0.056s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002636, mae: 0.128886, mean_q: 0.178169\n",
      "  5498/50000: episode: 708, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002467, mae: 0.125159, mean_q: 0.174695\n",
      "  5503/50000: episode: 709, duration: 0.098s, episode steps:   5, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002345, mae: 0.131379, mean_q: 0.183696\n",
      "  5506/50000: episode: 710, duration: 0.054s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002805, mae: 0.124580, mean_q: 0.171305\n",
      "  5517/50000: episode: 711, duration: 0.198s, episode steps:  11, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.003291, mae: 0.124721, mean_q: 0.170998\n",
      "  5519/50000: episode: 712, duration: 0.049s, episode steps:   2, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002284, mae: 0.122890, mean_q: 0.175945\n",
      "  5521/50000: episode: 713, duration: 0.054s, episode steps:   2, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002173, mae: 0.119488, mean_q: 0.165638\n",
      "  5529/50000: episode: 714, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003693, mae: 0.124775, mean_q: 0.170593\n",
      "  5531/50000: episode: 715, duration: 0.064s, episode steps:   2, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004902, mae: 0.133051, mean_q: 0.181469\n",
      "  5533/50000: episode: 716, duration: 0.068s, episode steps:   2, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002357, mae: 0.121871, mean_q: 0.167624\n",
      "  5538/50000: episode: 717, duration: 0.098s, episode steps:   5, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002722, mae: 0.116799, mean_q: 0.159482\n",
      "  5543/50000: episode: 718, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002523, mae: 0.109308, mean_q: 0.148664\n",
      "  5551/50000: episode: 719, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002781, mae: 0.120655, mean_q: 0.164379\n",
      "  5555/50000: episode: 720, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002574, mae: 0.130259, mean_q: 0.175580\n",
      "  5560/50000: episode: 721, duration: 0.093s, episode steps:   5, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002637, mae: 0.131415, mean_q: 0.177419\n",
      "  5568/50000: episode: 722, duration: 0.147s, episode steps:   8, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003032, mae: 0.120191, mean_q: 0.170862\n",
      "  5575/50000: episode: 723, duration: 0.132s, episode steps:   7, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 2.000],  loss: 0.002353, mae: 0.118460, mean_q: 0.170391\n",
      "  5581/50000: episode: 724, duration: 0.120s, episode steps:   6, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002157, mae: 0.126197, mean_q: 0.173155\n",
      "  5586/50000: episode: 725, duration: 0.092s, episode steps:   5, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002063, mae: 0.119514, mean_q: 0.163939\n",
      "  5593/50000: episode: 726, duration: 0.120s, episode steps:   7, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002775, mae: 0.119656, mean_q: 0.168373\n",
      "  5595/50000: episode: 727, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002025, mae: 0.118349, mean_q: 0.169023\n",
      "  5603/50000: episode: 728, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002075, mae: 0.124610, mean_q: 0.172185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5605/50000: episode: 729, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004319, mae: 0.128076, mean_q: 0.169943\n",
      "  5607/50000: episode: 730, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001718, mae: 0.115419, mean_q: 0.155714\n",
      "  5610/50000: episode: 731, duration: 0.059s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001805, mae: 0.119132, mean_q: 0.163972\n",
      "  5615/50000: episode: 732, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003945, mae: 0.121570, mean_q: 0.167699\n",
      "  5618/50000: episode: 733, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002741, mae: 0.115850, mean_q: 0.156355\n",
      "  5634/50000: episode: 734, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002813, mae: 0.118701, mean_q: 0.161683\n",
      "  5645/50000: episode: 735, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.003038, mae: 0.123646, mean_q: 0.172519\n",
      "  5648/50000: episode: 736, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002091, mae: 0.116366, mean_q: 0.157840\n",
      "  5662/50000: episode: 737, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003941, mae: 0.120258, mean_q: 0.163159\n",
      "  5669/50000: episode: 738, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [1.000, 3.000],  loss: 0.002135, mae: 0.131156, mean_q: 0.175837\n",
      "  5675/50000: episode: 739, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002050, mae: 0.122401, mean_q: 0.167183\n",
      "  5680/50000: episode: 740, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001535, mae: 0.115949, mean_q: 0.164892\n",
      "  5694/50000: episode: 741, duration: 0.172s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.002244, mae: 0.117252, mean_q: 0.166720\n",
      "  5707/50000: episode: 742, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.923 [0.000, 3.000],  loss: 0.002264, mae: 0.117710, mean_q: 0.165008\n",
      "  5710/50000: episode: 743, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.005211, mae: 0.121439, mean_q: 0.169040\n",
      "  5714/50000: episode: 744, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001879, mae: 0.114243, mean_q: 0.155893\n",
      "  5723/50000: episode: 745, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003719, mae: 0.121557, mean_q: 0.166010\n",
      "  5730/50000: episode: 746, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003036, mae: 0.119391, mean_q: 0.161506\n",
      "  5736/50000: episode: 747, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002312, mae: 0.113850, mean_q: 0.160820\n",
      "  5743/50000: episode: 748, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.002202, mae: 0.112492, mean_q: 0.155802\n",
      "  5750/50000: episode: 749, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002738, mae: 0.124830, mean_q: 0.170020\n",
      "  5755/50000: episode: 750, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003651, mae: 0.130319, mean_q: 0.175314\n",
      "  5761/50000: episode: 751, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002430, mae: 0.121333, mean_q: 0.163631\n",
      "  5769/50000: episode: 752, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003818, mae: 0.117907, mean_q: 0.163866\n",
      "  5772/50000: episode: 753, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004896, mae: 0.126624, mean_q: 0.180544\n",
      "  5779/50000: episode: 754, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003156, mae: 0.114901, mean_q: 0.156357\n",
      "  5781/50000: episode: 755, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001994, mae: 0.108801, mean_q: 0.151386\n",
      "  5793/50000: episode: 756, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002568, mae: 0.116336, mean_q: 0.157309\n",
      "  5818/50000: episode: 757, duration: 0.302s, episode steps:  25, steps per second:  83, episode reward:  1.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002807, mae: 0.121586, mean_q: 0.166658\n",
      "  5823/50000: episode: 758, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003824, mae: 0.126675, mean_q: 0.178211\n",
      "  5826/50000: episode: 759, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.004301, mae: 0.121831, mean_q: 0.178445\n",
      "  5831/50000: episode: 760, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.001722, mae: 0.107865, mean_q: 0.162567\n",
      "  5850/50000: episode: 761, duration: 0.218s, episode steps:  19, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.003053, mae: 0.119458, mean_q: 0.164011\n",
      "  5867/50000: episode: 762, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002617, mae: 0.124123, mean_q: 0.169132\n",
      "  5881/50000: episode: 763, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002272, mae: 0.117031, mean_q: 0.160288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5901/50000: episode: 764, duration: 0.236s, episode steps:  20, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002267, mae: 0.112275, mean_q: 0.155553\n",
      "  5912/50000: episode: 765, duration: 0.132s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002954, mae: 0.117390, mean_q: 0.165759\n",
      "  5915/50000: episode: 766, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.006078, mae: 0.123538, mean_q: 0.170651\n",
      "  5919/50000: episode: 767, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.005961, mae: 0.127605, mean_q: 0.171846\n",
      "  5924/50000: episode: 768, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002785, mae: 0.131840, mean_q: 0.178289\n",
      "  5927/50000: episode: 769, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001680, mae: 0.116304, mean_q: 0.159655\n",
      "  5933/50000: episode: 770, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002535, mae: 0.116076, mean_q: 0.157807\n",
      "  5938/50000: episode: 771, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002176, mae: 0.117520, mean_q: 0.164347\n",
      "  5942/50000: episode: 772, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002160, mae: 0.124690, mean_q: 0.175765\n",
      "  5950/50000: episode: 773, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002361, mae: 0.118818, mean_q: 0.163251\n",
      "  5965/50000: episode: 774, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002744, mae: 0.115309, mean_q: 0.163822\n",
      "  5976/50000: episode: 775, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002789, mae: 0.119493, mean_q: 0.166587\n",
      "  5986/50000: episode: 776, duration: 0.140s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002096, mae: 0.107557, mean_q: 0.152755\n",
      "  5992/50000: episode: 777, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003173, mae: 0.110709, mean_q: 0.152709\n",
      "  6003/50000: episode: 778, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002591, mae: 0.121845, mean_q: 0.166993\n",
      "  6006/50000: episode: 779, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002138, mae: 0.122856, mean_q: 0.165131\n",
      "  6009/50000: episode: 780, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002004, mae: 0.119793, mean_q: 0.164208\n",
      "  6011/50000: episode: 781, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001625, mae: 0.111637, mean_q: 0.155078\n",
      "  6024/50000: episode: 782, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002425, mae: 0.114020, mean_q: 0.157131\n",
      "  6028/50000: episode: 783, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003277, mae: 0.115540, mean_q: 0.155894\n",
      "  6043/50000: episode: 784, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002050, mae: 0.116290, mean_q: 0.158804\n",
      "  6055/50000: episode: 785, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002672, mae: 0.118465, mean_q: 0.161333\n",
      "  6059/50000: episode: 786, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.001818, mae: 0.115440, mean_q: 0.156852\n",
      "  6067/50000: episode: 787, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [1.000, 3.000],  loss: 0.001794, mae: 0.115916, mean_q: 0.156359\n",
      "  6069/50000: episode: 788, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001592, mae: 0.111128, mean_q: 0.153722\n",
      "  6075/50000: episode: 789, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002289, mae: 0.107451, mean_q: 0.151218\n",
      "  6080/50000: episode: 790, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001442, mae: 0.106860, mean_q: 0.152369\n",
      "  6090/50000: episode: 791, duration: 0.153s, episode steps:  10, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001709, mae: 0.116792, mean_q: 0.162534\n",
      "  6093/50000: episode: 792, duration: 0.056s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003459, mae: 0.113743, mean_q: 0.156034\n",
      "  6095/50000: episode: 793, duration: 0.047s, episode steps:   2, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002190, mae: 0.120107, mean_q: 0.163841\n",
      "  6101/50000: episode: 794, duration: 0.095s, episode steps:   6, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002324, mae: 0.118047, mean_q: 0.160805\n",
      "  6109/50000: episode: 795, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.375 [0.000, 3.000],  loss: 0.001725, mae: 0.111669, mean_q: 0.153767\n",
      "  6111/50000: episode: 796, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003949, mae: 0.110562, mean_q: 0.154406\n",
      "  6115/50000: episode: 797, duration: 0.080s, episode steps:   4, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001132, mae: 0.105436, mean_q: 0.150000\n",
      "  6124/50000: episode: 798, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001903, mae: 0.113366, mean_q: 0.158362\n",
      "  6128/50000: episode: 799, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005182, mae: 0.129665, mean_q: 0.183557\n",
      "  6130/50000: episode: 800, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004389, mae: 0.135919, mean_q: 0.186319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6137/50000: episode: 801, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002802, mae: 0.116378, mean_q: 0.156074\n",
      "  6149/50000: episode: 802, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003161, mae: 0.110368, mean_q: 0.147725\n",
      "  6158/50000: episode: 803, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003297, mae: 0.113943, mean_q: 0.157010\n",
      "  6171/50000: episode: 804, duration: 0.170s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002376, mae: 0.117756, mean_q: 0.160014\n",
      "  6184/50000: episode: 805, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002734, mae: 0.111224, mean_q: 0.149463\n",
      "  6198/50000: episode: 806, duration: 0.184s, episode steps:  14, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002908, mae: 0.116181, mean_q: 0.159647\n",
      "  6202/50000: episode: 807, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.001977, mae: 0.112551, mean_q: 0.156475\n",
      "  6211/50000: episode: 808, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001601, mae: 0.110567, mean_q: 0.158094\n",
      "  6236/50000: episode: 809, duration: 0.327s, episode steps:  25, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.002732, mae: 0.116308, mean_q: 0.159686\n",
      "  6240/50000: episode: 810, duration: 0.073s, episode steps:   4, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004534, mae: 0.110667, mean_q: 0.151801\n",
      "  6246/50000: episode: 811, duration: 0.095s, episode steps:   6, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003004, mae: 0.115722, mean_q: 0.154295\n",
      "  6253/50000: episode: 812, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002725, mae: 0.111973, mean_q: 0.149695\n",
      "  6256/50000: episode: 813, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002297, mae: 0.119631, mean_q: 0.162981\n",
      "  6268/50000: episode: 814, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001971, mae: 0.110895, mean_q: 0.150892\n",
      "  6272/50000: episode: 815, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.003783, mae: 0.112702, mean_q: 0.152656\n",
      "  6276/50000: episode: 816, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.003968, mae: 0.118660, mean_q: 0.162675\n",
      "  6283/50000: episode: 817, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001837, mae: 0.115289, mean_q: 0.162023\n",
      "  6286/50000: episode: 818, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.004318, mae: 0.126163, mean_q: 0.171347\n",
      "  6288/50000: episode: 819, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004908, mae: 0.126469, mean_q: 0.174757\n",
      "  6294/50000: episode: 820, duration: 0.089s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002021, mae: 0.107035, mean_q: 0.149905\n",
      "  6299/50000: episode: 821, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004487, mae: 0.114298, mean_q: 0.155653\n",
      "  6310/50000: episode: 822, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002840, mae: 0.118798, mean_q: 0.162519\n",
      "  6317/50000: episode: 823, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002118, mae: 0.101453, mean_q: 0.140855\n",
      "  6326/50000: episode: 824, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001487, mae: 0.109326, mean_q: 0.151277\n",
      "  6332/50000: episode: 825, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002725, mae: 0.114494, mean_q: 0.157846\n",
      "  6337/50000: episode: 826, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003898, mae: 0.110187, mean_q: 0.153513\n",
      "  6339/50000: episode: 827, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002185, mae: 0.108917, mean_q: 0.152290\n",
      "  6356/50000: episode: 828, duration: 0.199s, episode steps:  17, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002707, mae: 0.108822, mean_q: 0.153200\n",
      "  6362/50000: episode: 829, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001819, mae: 0.111130, mean_q: 0.157751\n",
      "  6368/50000: episode: 830, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002567, mae: 0.112712, mean_q: 0.158635\n",
      "  6380/50000: episode: 831, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002056, mae: 0.110481, mean_q: 0.155464\n",
      "  6384/50000: episode: 832, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002469, mae: 0.110716, mean_q: 0.152548\n",
      "  6400/50000: episode: 833, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003353, mae: 0.112425, mean_q: 0.151552\n",
      "  6409/50000: episode: 834, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.002360, mae: 0.109867, mean_q: 0.146672\n",
      "  6416/50000: episode: 835, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002029, mae: 0.117139, mean_q: 0.159499\n",
      "  6422/50000: episode: 836, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003917, mae: 0.116104, mean_q: 0.154970\n",
      "  6431/50000: episode: 837, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.002050, mae: 0.105413, mean_q: 0.145355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6439/50000: episode: 838, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002030, mae: 0.105183, mean_q: 0.146086\n",
      "  6451/50000: episode: 839, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002591, mae: 0.107250, mean_q: 0.149925\n",
      "  6461/50000: episode: 840, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.001922, mae: 0.105561, mean_q: 0.147953\n",
      "  6463/50000: episode: 841, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001193, mae: 0.100654, mean_q: 0.142685\n",
      "  6472/50000: episode: 842, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.556 [0.000, 2.000],  loss: 0.002635, mae: 0.105602, mean_q: 0.149312\n",
      "  6475/50000: episode: 843, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001329, mae: 0.110810, mean_q: 0.155160\n",
      "  6482/50000: episode: 844, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001395, mae: 0.107975, mean_q: 0.147548\n",
      "  6495/50000: episode: 845, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001382, mae: 0.108489, mean_q: 0.150060\n",
      "  6514/50000: episode: 846, duration: 0.263s, episode steps:  19, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.001555, mae: 0.106864, mean_q: 0.146095\n",
      "  6524/50000: episode: 847, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002835, mae: 0.110638, mean_q: 0.146933\n",
      "  6528/50000: episode: 848, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002834, mae: 0.108241, mean_q: 0.141396\n",
      "  6533/50000: episode: 849, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001837, mae: 0.104587, mean_q: 0.139372\n",
      "  6539/50000: episode: 850, duration: 0.093s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003692, mae: 0.103598, mean_q: 0.138381\n",
      "  6544/50000: episode: 851, duration: 0.090s, episode steps:   5, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002649, mae: 0.110340, mean_q: 0.145912\n",
      "  6548/50000: episode: 852, duration: 0.087s, episode steps:   4, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003854, mae: 0.113247, mean_q: 0.154357\n",
      "  6550/50000: episode: 853, duration: 0.062s, episode steps:   2, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001014, mae: 0.103159, mean_q: 0.144118\n",
      "  6554/50000: episode: 854, duration: 0.087s, episode steps:   4, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001644, mae: 0.105731, mean_q: 0.148695\n",
      "  6556/50000: episode: 855, duration: 0.046s, episode steps:   2, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004362, mae: 0.116234, mean_q: 0.160262\n",
      "  6564/50000: episode: 856, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001861, mae: 0.111548, mean_q: 0.155178\n",
      "  6577/50000: episode: 857, duration: 0.187s, episode steps:  13, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.692 [0.000, 2.000],  loss: 0.001637, mae: 0.107153, mean_q: 0.150177\n",
      "  6581/50000: episode: 858, duration: 0.087s, episode steps:   4, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002219, mae: 0.103737, mean_q: 0.147277\n",
      "  6584/50000: episode: 859, duration: 0.069s, episode steps:   3, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001129, mae: 0.102095, mean_q: 0.142376\n",
      "  6593/50000: episode: 860, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002330, mae: 0.112641, mean_q: 0.157713\n",
      "  6605/50000: episode: 861, duration: 0.213s, episode steps:  12, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001851, mae: 0.102561, mean_q: 0.141609\n",
      "  6611/50000: episode: 862, duration: 0.133s, episode steps:   6, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.001846, mae: 0.106456, mean_q: 0.147178\n",
      "  6613/50000: episode: 863, duration: 0.056s, episode steps:   2, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002578, mae: 0.116343, mean_q: 0.162659\n",
      "  6619/50000: episode: 864, duration: 0.125s, episode steps:   6, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001858, mae: 0.106468, mean_q: 0.148940\n",
      "  6621/50000: episode: 865, duration: 0.051s, episode steps:   2, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001628, mae: 0.101102, mean_q: 0.145104\n",
      "  6628/50000: episode: 866, duration: 0.123s, episode steps:   7, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [2.000, 3.000],  loss: 0.003639, mae: 0.109000, mean_q: 0.148197\n",
      "  6634/50000: episode: 867, duration: 0.093s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002208, mae: 0.109823, mean_q: 0.147586\n",
      "  6655/50000: episode: 868, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002283, mae: 0.110462, mean_q: 0.148421\n",
      "  6660/50000: episode: 869, duration: 0.087s, episode steps:   5, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003213, mae: 0.096718, mean_q: 0.134919\n",
      "  6667/50000: episode: 870, duration: 0.130s, episode steps:   7, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.001838, mae: 0.100118, mean_q: 0.141262\n",
      "  6670/50000: episode: 871, duration: 0.058s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.000966, mae: 0.105873, mean_q: 0.148176\n",
      "  6679/50000: episode: 872, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001843, mae: 0.108932, mean_q: 0.150342\n",
      "  6684/50000: episode: 873, duration: 0.091s, episode steps:   5, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.002307, mae: 0.102278, mean_q: 0.138085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6691/50000: episode: 874, duration: 0.112s, episode steps:   7, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002740, mae: 0.100980, mean_q: 0.136455\n",
      "  6701/50000: episode: 875, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001546, mae: 0.103726, mean_q: 0.147530\n",
      "  6703/50000: episode: 876, duration: 0.045s, episode steps:   2, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000985, mae: 0.099850, mean_q: 0.145677\n",
      "  6708/50000: episode: 877, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.001847, mae: 0.098426, mean_q: 0.139181\n",
      "  6721/50000: episode: 878, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002124, mae: 0.102827, mean_q: 0.142147\n",
      "  6741/50000: episode: 879, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001785, mae: 0.106247, mean_q: 0.145774\n",
      "  6751/50000: episode: 880, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.001750, mae: 0.101809, mean_q: 0.141868\n",
      "  6767/50000: episode: 881, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002021, mae: 0.103119, mean_q: 0.144689\n",
      "  6791/50000: episode: 882, duration: 0.293s, episode steps:  24, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002545, mae: 0.106738, mean_q: 0.147811\n",
      "  6800/50000: episode: 883, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003391, mae: 0.107550, mean_q: 0.147685\n",
      "  6805/50000: episode: 884, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002923, mae: 0.108333, mean_q: 0.156815\n",
      "  6818/50000: episode: 885, duration: 0.179s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.154 [0.000, 3.000],  loss: 0.002415, mae: 0.107058, mean_q: 0.152630\n",
      "  6854/50000: episode: 886, duration: 0.479s, episode steps:  36, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.002421, mae: 0.107629, mean_q: 0.148590\n",
      "  6856/50000: episode: 887, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002905, mae: 0.107642, mean_q: 0.146665\n",
      "  6869/50000: episode: 888, duration: 0.181s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002555, mae: 0.109380, mean_q: 0.145421\n",
      "  6872/50000: episode: 889, duration: 0.063s, episode steps:   3, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003025, mae: 0.109575, mean_q: 0.148074\n",
      "  6881/50000: episode: 890, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002076, mae: 0.105616, mean_q: 0.142607\n",
      "  6887/50000: episode: 891, duration: 0.103s, episode steps:   6, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002355, mae: 0.103681, mean_q: 0.142040\n",
      "  6889/50000: episode: 892, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001944, mae: 0.105012, mean_q: 0.139737\n",
      "  6891/50000: episode: 893, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003785, mae: 0.109520, mean_q: 0.147151\n",
      "  6911/50000: episode: 894, duration: 0.268s, episode steps:  20, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002179, mae: 0.101756, mean_q: 0.140032\n",
      "  6922/50000: episode: 895, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002372, mae: 0.111678, mean_q: 0.155795\n",
      "  6931/50000: episode: 896, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002237, mae: 0.106638, mean_q: 0.144611\n",
      "  6935/50000: episode: 897, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003175, mae: 0.101618, mean_q: 0.136061\n",
      "  6937/50000: episode: 898, duration: 0.060s, episode steps:   2, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002318, mae: 0.103157, mean_q: 0.137058\n",
      "  6959/50000: episode: 899, duration: 0.267s, episode steps:  22, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.136 [0.000, 3.000],  loss: 0.002618, mae: 0.108781, mean_q: 0.149484\n",
      "  6980/50000: episode: 900, duration: 0.271s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002214, mae: 0.099659, mean_q: 0.134972\n",
      "  6987/50000: episode: 901, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003291, mae: 0.106589, mean_q: 0.143075\n",
      "  6992/50000: episode: 902, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001941, mae: 0.098044, mean_q: 0.130107\n",
      "  7004/50000: episode: 903, duration: 0.175s, episode steps:  12, steps per second:  69, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.003495, mae: 0.100203, mean_q: 0.134693\n",
      "  7014/50000: episode: 904, duration: 0.202s, episode steps:  10, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001976, mae: 0.102053, mean_q: 0.135779\n",
      "  7019/50000: episode: 905, duration: 0.097s, episode steps:   5, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001434, mae: 0.096161, mean_q: 0.133099\n",
      "  7023/50000: episode: 906, duration: 0.078s, episode steps:   4, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001662, mae: 0.093599, mean_q: 0.129201\n",
      "  7025/50000: episode: 907, duration: 0.051s, episode steps:   2, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002570, mae: 0.110913, mean_q: 0.149264\n",
      "  7031/50000: episode: 908, duration: 0.113s, episode steps:   6, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001124, mae: 0.098423, mean_q: 0.136012\n",
      "  7033/50000: episode: 909, duration: 0.050s, episode steps:   2, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.000848, mae: 0.095026, mean_q: 0.131722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7039/50000: episode: 910, duration: 0.125s, episode steps:   6, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002015, mae: 0.105578, mean_q: 0.141832\n",
      "  7048/50000: episode: 911, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003025, mae: 0.104012, mean_q: 0.137715\n",
      "  7061/50000: episode: 912, duration: 0.233s, episode steps:  13, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.001577, mae: 0.099658, mean_q: 0.136174\n",
      "  7064/50000: episode: 913, duration: 0.086s, episode steps:   3, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003770, mae: 0.113381, mean_q: 0.155575\n",
      "  7069/50000: episode: 914, duration: 0.080s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002277, mae: 0.102863, mean_q: 0.145425\n",
      "  7084/50000: episode: 915, duration: 0.223s, episode steps:  15, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002209, mae: 0.102876, mean_q: 0.148829\n",
      "  7087/50000: episode: 916, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001490, mae: 0.103344, mean_q: 0.143941\n",
      "  7093/50000: episode: 917, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.003103, mae: 0.111917, mean_q: 0.151905\n",
      "  7097/50000: episode: 918, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002002, mae: 0.104575, mean_q: 0.140282\n",
      "  7106/50000: episode: 919, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.002885, mae: 0.105910, mean_q: 0.146063\n",
      "  7108/50000: episode: 920, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003205, mae: 0.094461, mean_q: 0.135427\n",
      "  7114/50000: episode: 921, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001522, mae: 0.096970, mean_q: 0.139758\n",
      "  7123/50000: episode: 922, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001686, mae: 0.104942, mean_q: 0.149617\n",
      "  7134/50000: episode: 923, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002881, mae: 0.110358, mean_q: 0.149356\n",
      "  7141/50000: episode: 924, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [1.000, 3.000],  loss: 0.003006, mae: 0.102605, mean_q: 0.145523\n",
      "  7148/50000: episode: 925, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001782, mae: 0.105524, mean_q: 0.158983\n",
      "  7153/50000: episode: 926, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002727, mae: 0.108795, mean_q: 0.155281\n",
      "  7158/50000: episode: 927, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001904, mae: 0.099564, mean_q: 0.141432\n",
      "  7161/50000: episode: 928, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001698, mae: 0.103626, mean_q: 0.146297\n",
      "  7167/50000: episode: 929, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003029, mae: 0.108107, mean_q: 0.150248\n",
      "  7173/50000: episode: 930, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003279, mae: 0.104380, mean_q: 0.143164\n",
      "  7178/50000: episode: 931, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001566, mae: 0.096190, mean_q: 0.133745\n",
      "  7180/50000: episode: 932, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001077, mae: 0.089816, mean_q: 0.129328\n",
      "  7184/50000: episode: 933, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002142, mae: 0.096744, mean_q: 0.134142\n",
      "  7188/50000: episode: 934, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001147, mae: 0.097812, mean_q: 0.136348\n",
      "  7203/50000: episode: 935, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward:  1.000, mean reward:  0.067 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002489, mae: 0.109655, mean_q: 0.149976\n",
      "  7210/50000: episode: 936, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002053, mae: 0.108874, mean_q: 0.150503\n",
      "  7212/50000: episode: 937, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004744, mae: 0.123720, mean_q: 0.172701\n",
      "  7214/50000: episode: 938, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001403, mae: 0.101090, mean_q: 0.145929\n",
      "  7216/50000: episode: 939, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002451, mae: 0.102934, mean_q: 0.152118\n",
      "  7220/50000: episode: 940, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002136, mae: 0.106646, mean_q: 0.150522\n",
      "  7234/50000: episode: 941, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002008, mae: 0.107421, mean_q: 0.152009\n",
      "  7246/50000: episode: 942, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002465, mae: 0.106861, mean_q: 0.147013\n",
      "  7253/50000: episode: 943, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002630, mae: 0.105403, mean_q: 0.144049\n",
      "  7270/50000: episode: 944, duration: 0.206s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002432, mae: 0.104805, mean_q: 0.146375\n",
      "  7276/50000: episode: 945, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003144, mae: 0.121164, mean_q: 0.167050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7288/50000: episode: 946, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002416, mae: 0.107245, mean_q: 0.149048\n",
      "  7295/50000: episode: 947, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002495, mae: 0.101263, mean_q: 0.140555\n",
      "  7300/50000: episode: 948, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003078, mae: 0.100715, mean_q: 0.139203\n",
      "  7314/50000: episode: 949, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002873, mae: 0.105265, mean_q: 0.148033\n",
      "  7321/50000: episode: 950, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.571 [1.000, 3.000],  loss: 0.003165, mae: 0.105683, mean_q: 0.149483\n",
      "  7355/50000: episode: 951, duration: 0.386s, episode steps:  34, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002491, mae: 0.111764, mean_q: 0.153354\n",
      "  7357/50000: episode: 952, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001448, mae: 0.116464, mean_q: 0.162378\n",
      "  7365/50000: episode: 953, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002001, mae: 0.102834, mean_q: 0.145527\n",
      "  7367/50000: episode: 954, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.005901, mae: 0.111187, mean_q: 0.151324\n",
      "  7375/50000: episode: 955, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002680, mae: 0.103041, mean_q: 0.144221\n",
      "  7379/50000: episode: 956, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001425, mae: 0.094785, mean_q: 0.132611\n",
      "  7387/50000: episode: 957, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001315, mae: 0.096935, mean_q: 0.136757\n",
      "  7398/50000: episode: 958, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.001843, mae: 0.102375, mean_q: 0.140097\n",
      "  7408/50000: episode: 959, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002124, mae: 0.102817, mean_q: 0.141284\n",
      "  7413/50000: episode: 960, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002438, mae: 0.102168, mean_q: 0.141528\n",
      "  7416/50000: episode: 961, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001488, mae: 0.101737, mean_q: 0.139657\n",
      "  7434/50000: episode: 962, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001608, mae: 0.106030, mean_q: 0.143792\n",
      "  7443/50000: episode: 963, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001632, mae: 0.100765, mean_q: 0.136461\n",
      "  7445/50000: episode: 964, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001154, mae: 0.093350, mean_q: 0.127747\n",
      "  7452/50000: episode: 965, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002074, mae: 0.106821, mean_q: 0.145238\n",
      "  7457/50000: episode: 966, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002502, mae: 0.103843, mean_q: 0.141928\n",
      "  7461/50000: episode: 967, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.002300, mae: 0.107840, mean_q: 0.147580\n",
      "  7466/50000: episode: 968, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.001854, mae: 0.105566, mean_q: 0.143832\n",
      "  7469/50000: episode: 969, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003518, mae: 0.105927, mean_q: 0.148264\n",
      "  7472/50000: episode: 970, duration: 0.059s, episode steps:   3, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001776, mae: 0.104582, mean_q: 0.144913\n",
      "  7479/50000: episode: 971, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003006, mae: 0.109315, mean_q: 0.148958\n",
      "  7481/50000: episode: 972, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003691, mae: 0.109909, mean_q: 0.147629\n",
      "  7494/50000: episode: 973, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.001819, mae: 0.098438, mean_q: 0.136197\n",
      "  7504/50000: episode: 974, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002558, mae: 0.110223, mean_q: 0.153507\n",
      "  7512/50000: episode: 975, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [1.000, 3.000],  loss: 0.001946, mae: 0.108865, mean_q: 0.152931\n",
      "  7516/50000: episode: 976, duration: 0.077s, episode steps:   4, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002169, mae: 0.101847, mean_q: 0.138100\n",
      "  7523/50000: episode: 977, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001818, mae: 0.098041, mean_q: 0.132917\n",
      "  7532/50000: episode: 978, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 3.000],  loss: 0.002752, mae: 0.106022, mean_q: 0.145323\n",
      "  7543/50000: episode: 979, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002410, mae: 0.103153, mean_q: 0.142845\n",
      "  7545/50000: episode: 980, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002253, mae: 0.099007, mean_q: 0.133328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7558/50000: episode: 981, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002938, mae: 0.105813, mean_q: 0.142649\n",
      "  7561/50000: episode: 982, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000940, mae: 0.104914, mean_q: 0.146186\n",
      "  7565/50000: episode: 983, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002769, mae: 0.109974, mean_q: 0.146992\n",
      "  7570/50000: episode: 984, duration: 0.065s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002041, mae: 0.097360, mean_q: 0.131072\n",
      "  7576/50000: episode: 985, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.002191, mae: 0.097872, mean_q: 0.136546\n",
      "  7578/50000: episode: 986, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002057, mae: 0.100157, mean_q: 0.140511\n",
      "  7593/50000: episode: 987, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.002602, mae: 0.104733, mean_q: 0.146339\n",
      "  7597/50000: episode: 988, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002797, mae: 0.111602, mean_q: 0.152827\n",
      "  7602/50000: episode: 989, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001516, mae: 0.109180, mean_q: 0.156951\n",
      "  7621/50000: episode: 990, duration: 0.219s, episode steps:  19, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.737 [0.000, 3.000],  loss: 0.001937, mae: 0.096563, mean_q: 0.131877\n",
      "  7630/50000: episode: 991, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003046, mae: 0.102173, mean_q: 0.136068\n",
      "  7638/50000: episode: 992, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001183, mae: 0.098392, mean_q: 0.135290\n",
      "  7642/50000: episode: 993, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001576, mae: 0.100457, mean_q: 0.139204\n",
      "  7646/50000: episode: 994, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001989, mae: 0.100750, mean_q: 0.138684\n",
      "  7651/50000: episode: 995, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001703, mae: 0.100219, mean_q: 0.137552\n",
      "  7655/50000: episode: 996, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001709, mae: 0.103843, mean_q: 0.142802\n",
      "  7666/50000: episode: 997, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002820, mae: 0.103774, mean_q: 0.140376\n",
      "  7670/50000: episode: 998, duration: 0.066s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.003448, mae: 0.099596, mean_q: 0.138277\n",
      "  7672/50000: episode: 999, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001887, mae: 0.093298, mean_q: 0.129700\n",
      "  7678/50000: episode: 1000, duration: 0.113s, episode steps:   6, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002559, mae: 0.106223, mean_q: 0.151507\n",
      "  7680/50000: episode: 1001, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000886, mae: 0.097061, mean_q: 0.134454\n",
      "  7687/50000: episode: 1002, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001861, mae: 0.100553, mean_q: 0.139587\n",
      "  7691/50000: episode: 1003, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.001611, mae: 0.096421, mean_q: 0.134698\n",
      "  7702/50000: episode: 1004, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003236, mae: 0.104123, mean_q: 0.144797\n",
      "  7726/50000: episode: 1005, duration: 0.278s, episode steps:  24, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002052, mae: 0.099039, mean_q: 0.136326\n",
      "  7728/50000: episode: 1006, duration: 0.059s, episode steps:   2, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001146, mae: 0.098591, mean_q: 0.143006\n",
      "  7735/50000: episode: 1007, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003226, mae: 0.106128, mean_q: 0.146345\n",
      "  7740/50000: episode: 1008, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001563, mae: 0.106167, mean_q: 0.147305\n",
      "  7743/50000: episode: 1009, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001707, mae: 0.108883, mean_q: 0.150807\n",
      "  7748/50000: episode: 1010, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [2.000, 3.000],  loss: 0.001513, mae: 0.105262, mean_q: 0.146748\n",
      "  7758/50000: episode: 1011, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001965, mae: 0.104127, mean_q: 0.147626\n",
      "  7762/50000: episode: 1012, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002056, mae: 0.103069, mean_q: 0.142911\n",
      "  7781/50000: episode: 1013, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward:  1.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.316 [0.000, 3.000],  loss: 0.003103, mae: 0.108132, mean_q: 0.153111\n",
      "  7784/50000: episode: 1014, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002675, mae: 0.116519, mean_q: 0.170790\n",
      "  7793/50000: episode: 1015, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002475, mae: 0.102894, mean_q: 0.143436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7799/50000: episode: 1016, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002064, mae: 0.104370, mean_q: 0.142852\n",
      "  7817/50000: episode: 1017, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002220, mae: 0.103304, mean_q: 0.140295\n",
      "  7823/50000: episode: 1018, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002479, mae: 0.107745, mean_q: 0.150435\n",
      "  7826/50000: episode: 1019, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.004438, mae: 0.112540, mean_q: 0.153882\n",
      "  7834/50000: episode: 1020, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003622, mae: 0.108690, mean_q: 0.149843\n",
      "  7838/50000: episode: 1021, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002251, mae: 0.094320, mean_q: 0.133066\n",
      "  7846/50000: episode: 1022, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003597, mae: 0.104128, mean_q: 0.145014\n",
      "  7848/50000: episode: 1023, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.005047, mae: 0.136393, mean_q: 0.179609\n",
      "  7852/50000: episode: 1024, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003352, mae: 0.131083, mean_q: 0.177848\n",
      "  7859/50000: episode: 1025, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002787, mae: 0.116030, mean_q: 0.158570\n",
      "  7876/50000: episode: 1026, duration: 0.204s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.001938, mae: 0.103673, mean_q: 0.141892\n",
      "  7878/50000: episode: 1027, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001614, mae: 0.104903, mean_q: 0.146963\n",
      "  7880/50000: episode: 1028, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003240, mae: 0.102402, mean_q: 0.136618\n",
      "  7889/50000: episode: 1029, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.004149, mae: 0.105919, mean_q: 0.142480\n",
      "  7891/50000: episode: 1030, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001387, mae: 0.100940, mean_q: 0.138049\n",
      "  7895/50000: episode: 1031, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001513, mae: 0.100768, mean_q: 0.139512\n",
      "  7900/50000: episode: 1032, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002826, mae: 0.104993, mean_q: 0.141494\n",
      "  7910/50000: episode: 1033, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.003431, mae: 0.110464, mean_q: 0.152812\n",
      "  7939/50000: episode: 1034, duration: 0.332s, episode steps:  29, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.897 [0.000, 3.000],  loss: 0.002827, mae: 0.110145, mean_q: 0.157655\n",
      "  7944/50000: episode: 1035, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003866, mae: 0.099017, mean_q: 0.136133\n",
      "  7953/50000: episode: 1036, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.004345, mae: 0.112730, mean_q: 0.152506\n",
      "  7961/50000: episode: 1037, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002719, mae: 0.109879, mean_q: 0.149280\n",
      "  7963/50000: episode: 1038, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005185, mae: 0.119201, mean_q: 0.163059\n",
      "  7966/50000: episode: 1039, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002500, mae: 0.117218, mean_q: 0.160721\n",
      "  7969/50000: episode: 1040, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.005086, mae: 0.113655, mean_q: 0.150654\n",
      "  7983/50000: episode: 1041, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.929 [0.000, 3.000],  loss: 0.003065, mae: 0.107886, mean_q: 0.150143\n",
      "  7985/50000: episode: 1042, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003055, mae: 0.105229, mean_q: 0.151332\n",
      "  7991/50000: episode: 1043, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002612, mae: 0.106610, mean_q: 0.148923\n",
      "  7996/50000: episode: 1044, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002903, mae: 0.113391, mean_q: 0.158132\n",
      "  7998/50000: episode: 1045, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001823, mae: 0.115490, mean_q: 0.166405\n",
      "  8002/50000: episode: 1046, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001703, mae: 0.098971, mean_q: 0.136993\n",
      "  8015/50000: episode: 1047, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.154 [0.000, 3.000],  loss: 0.002476, mae: 0.107532, mean_q: 0.150683\n",
      "  8017/50000: episode: 1048, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001750, mae: 0.109691, mean_q: 0.150010\n",
      "  8019/50000: episode: 1049, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001712, mae: 0.112227, mean_q: 0.154496\n",
      "  8026/50000: episode: 1050, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 2.000],  loss: 0.003093, mae: 0.106355, mean_q: 0.147213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8035/50000: episode: 1051, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002639, mae: 0.102898, mean_q: 0.141270\n",
      "  8041/50000: episode: 1052, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002888, mae: 0.108801, mean_q: 0.147593\n",
      "  8052/50000: episode: 1053, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.003810, mae: 0.108986, mean_q: 0.146847\n",
      "  8056/50000: episode: 1054, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002364, mae: 0.101304, mean_q: 0.136024\n",
      "  8068/50000: episode: 1055, duration: 0.142s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.083 [0.000, 3.000],  loss: 0.004743, mae: 0.116758, mean_q: 0.156049\n",
      "  8071/50000: episode: 1056, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002746, mae: 0.113613, mean_q: 0.152136\n",
      "  8081/50000: episode: 1057, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001970, mae: 0.097570, mean_q: 0.140955\n",
      "  8084/50000: episode: 1058, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.001981, mae: 0.100445, mean_q: 0.144888\n",
      "  8089/50000: episode: 1059, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002846, mae: 0.107673, mean_q: 0.148682\n",
      "  8099/50000: episode: 1060, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003887, mae: 0.115041, mean_q: 0.157021\n",
      "  8101/50000: episode: 1061, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003117, mae: 0.107620, mean_q: 0.148038\n",
      "  8116/50000: episode: 1062, duration: 0.175s, episode steps:  15, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.003075, mae: 0.102935, mean_q: 0.141585\n",
      "  8123/50000: episode: 1063, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002363, mae: 0.105282, mean_q: 0.143601\n",
      "  8127/50000: episode: 1064, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.004017, mae: 0.112283, mean_q: 0.148764\n",
      "  8129/50000: episode: 1065, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002483, mae: 0.113661, mean_q: 0.159507\n",
      "  8137/50000: episode: 1066, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002804, mae: 0.116256, mean_q: 0.159501\n",
      "  8142/50000: episode: 1067, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003822, mae: 0.114542, mean_q: 0.157754\n",
      "  8183/50000: episode: 1068, duration: 0.481s, episode steps:  41, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002707, mae: 0.108124, mean_q: 0.149341\n",
      "  8188/50000: episode: 1069, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001740, mae: 0.102888, mean_q: 0.147449\n",
      "  8190/50000: episode: 1070, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001665, mae: 0.104602, mean_q: 0.151740\n",
      "  8200/50000: episode: 1071, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003434, mae: 0.109171, mean_q: 0.153853\n",
      "  8206/50000: episode: 1072, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002036, mae: 0.097667, mean_q: 0.140850\n",
      "  8220/50000: episode: 1073, duration: 0.168s, episode steps:  14, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002680, mae: 0.105934, mean_q: 0.147613\n",
      "  8224/50000: episode: 1074, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002958, mae: 0.112850, mean_q: 0.149902\n",
      "  8231/50000: episode: 1075, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002644, mae: 0.107902, mean_q: 0.148027\n",
      "  8233/50000: episode: 1076, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001363, mae: 0.099136, mean_q: 0.133580\n",
      "  8241/50000: episode: 1077, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002877, mae: 0.106252, mean_q: 0.142185\n",
      "  8248/50000: episode: 1078, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.002971, mae: 0.115144, mean_q: 0.158071\n",
      "  8252/50000: episode: 1079, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001877, mae: 0.104929, mean_q: 0.147854\n",
      "  8259/50000: episode: 1080, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002396, mae: 0.105605, mean_q: 0.149100\n",
      "  8268/50000: episode: 1081, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001853, mae: 0.105004, mean_q: 0.142416\n",
      "  8273/50000: episode: 1082, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.600 [2.000, 3.000],  loss: 0.002349, mae: 0.108985, mean_q: 0.148289\n",
      "  8278/50000: episode: 1083, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003333, mae: 0.113553, mean_q: 0.156503\n",
      "  8284/50000: episode: 1084, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002029, mae: 0.105575, mean_q: 0.146216\n",
      "  8289/50000: episode: 1085, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001982, mae: 0.104862, mean_q: 0.143940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8297/50000: episode: 1086, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.002375, mae: 0.110075, mean_q: 0.148841\n",
      "  8311/50000: episode: 1087, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.002380, mae: 0.110353, mean_q: 0.150039\n",
      "  8313/50000: episode: 1088, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002638, mae: 0.110771, mean_q: 0.157617\n",
      "  8318/50000: episode: 1089, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.004168, mae: 0.113926, mean_q: 0.157952\n",
      "  8321/50000: episode: 1090, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002558, mae: 0.107940, mean_q: 0.150489\n",
      "  8324/50000: episode: 1091, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003943, mae: 0.108520, mean_q: 0.157992\n",
      "  8335/50000: episode: 1092, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002141, mae: 0.104675, mean_q: 0.152187\n",
      "  8355/50000: episode: 1093, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003014, mae: 0.111997, mean_q: 0.160766\n",
      "  8364/50000: episode: 1094, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001850, mae: 0.110925, mean_q: 0.156612\n",
      "  8368/50000: episode: 1095, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002481, mae: 0.114066, mean_q: 0.152279\n",
      "  8375/50000: episode: 1096, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002771, mae: 0.112831, mean_q: 0.151900\n",
      "  8377/50000: episode: 1097, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003035, mae: 0.114819, mean_q: 0.154607\n",
      "  8379/50000: episode: 1098, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003800, mae: 0.117488, mean_q: 0.158134\n",
      "  8388/50000: episode: 1099, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002614, mae: 0.106787, mean_q: 0.148632\n",
      "  8403/50000: episode: 1100, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002894, mae: 0.103669, mean_q: 0.143299\n",
      "  8415/50000: episode: 1101, duration: 0.142s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002193, mae: 0.105200, mean_q: 0.143890\n",
      "  8423/50000: episode: 1102, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001936, mae: 0.102183, mean_q: 0.136959\n",
      "  8428/50000: episode: 1103, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001580, mae: 0.107904, mean_q: 0.146586\n",
      "  8438/50000: episode: 1104, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002133, mae: 0.104421, mean_q: 0.140903\n",
      "  8441/50000: episode: 1105, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003594, mae: 0.112471, mean_q: 0.154257\n",
      "  8452/50000: episode: 1106, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002720, mae: 0.108537, mean_q: 0.148043\n",
      "  8456/50000: episode: 1107, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002379, mae: 0.110073, mean_q: 0.155226\n",
      "  8459/50000: episode: 1108, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002020, mae: 0.102184, mean_q: 0.143448\n",
      "  8466/50000: episode: 1109, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002407, mae: 0.109601, mean_q: 0.154721\n",
      "  8469/50000: episode: 1110, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002353, mae: 0.104591, mean_q: 0.155345\n",
      "  8471/50000: episode: 1111, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004197, mae: 0.103976, mean_q: 0.151054\n",
      "  8479/50000: episode: 1112, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002815, mae: 0.109275, mean_q: 0.157501\n",
      "  8490/50000: episode: 1113, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.002435, mae: 0.110176, mean_q: 0.157882\n",
      "  8501/50000: episode: 1114, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002990, mae: 0.107311, mean_q: 0.149282\n",
      "  8504/50000: episode: 1115, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002376, mae: 0.115002, mean_q: 0.160350\n",
      "  8507/50000: episode: 1116, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.004477, mae: 0.109267, mean_q: 0.150383\n",
      "  8513/50000: episode: 1117, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001864, mae: 0.094200, mean_q: 0.134613\n",
      "  8516/50000: episode: 1118, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002709, mae: 0.105037, mean_q: 0.146535\n",
      "  8520/50000: episode: 1119, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002195, mae: 0.110351, mean_q: 0.150545\n",
      "  8522/50000: episode: 1120, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004028, mae: 0.114602, mean_q: 0.147150\n",
      "  8525/50000: episode: 1121, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003060, mae: 0.113738, mean_q: 0.151756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8538/50000: episode: 1122, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.846 [0.000, 3.000],  loss: 0.003111, mae: 0.110740, mean_q: 0.149545\n",
      "  8543/50000: episode: 1123, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002354, mae: 0.106164, mean_q: 0.141022\n",
      "  8548/50000: episode: 1124, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002921, mae: 0.117479, mean_q: 0.155834\n",
      "  8563/50000: episode: 1125, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002861, mae: 0.104311, mean_q: 0.138670\n",
      "  8567/50000: episode: 1126, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001994, mae: 0.107923, mean_q: 0.146243\n",
      "  8570/50000: episode: 1127, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002815, mae: 0.110656, mean_q: 0.146887\n",
      "  8576/50000: episode: 1128, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001067, mae: 0.103753, mean_q: 0.142835\n",
      "  8587/50000: episode: 1129, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002556, mae: 0.105495, mean_q: 0.144779\n",
      "  8597/50000: episode: 1130, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.001609, mae: 0.105678, mean_q: 0.147833\n",
      "  8601/50000: episode: 1131, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002032, mae: 0.102097, mean_q: 0.139327\n",
      "  8606/50000: episode: 1132, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002577, mae: 0.101891, mean_q: 0.137494\n",
      "  8610/50000: episode: 1133, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002914, mae: 0.096745, mean_q: 0.130835\n",
      "  8615/50000: episode: 1134, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.003250, mae: 0.098780, mean_q: 0.133668\n",
      "  8621/50000: episode: 1135, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001889, mae: 0.108045, mean_q: 0.145971\n",
      "  8629/50000: episode: 1136, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003418, mae: 0.110888, mean_q: 0.145685\n",
      "  8635/50000: episode: 1137, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002888, mae: 0.104455, mean_q: 0.141086\n",
      "  8638/50000: episode: 1138, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.004443, mae: 0.112348, mean_q: 0.153991\n",
      "  8645/50000: episode: 1139, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.001723, mae: 0.090390, mean_q: 0.125868\n",
      "  8659/50000: episode: 1140, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002343, mae: 0.095801, mean_q: 0.131781\n",
      "  8670/50000: episode: 1141, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002361, mae: 0.100359, mean_q: 0.137090\n",
      "  8685/50000: episode: 1142, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002184, mae: 0.103728, mean_q: 0.138035\n",
      "  8687/50000: episode: 1143, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003742, mae: 0.109755, mean_q: 0.141515\n",
      "  8692/50000: episode: 1144, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003087, mae: 0.097125, mean_q: 0.131951\n",
      "  8702/50000: episode: 1145, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002070, mae: 0.102998, mean_q: 0.139971\n",
      "  8713/50000: episode: 1146, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002517, mae: 0.101586, mean_q: 0.136984\n",
      "  8718/50000: episode: 1147, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001197, mae: 0.092332, mean_q: 0.127200\n",
      "  8721/50000: episode: 1148, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002240, mae: 0.097356, mean_q: 0.130619\n",
      "  8740/50000: episode: 1149, duration: 0.246s, episode steps:  19, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.002424, mae: 0.100888, mean_q: 0.136257\n",
      "  8750/50000: episode: 1150, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001817, mae: 0.097300, mean_q: 0.132581\n",
      "  8755/50000: episode: 1151, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001255, mae: 0.095827, mean_q: 0.131366\n",
      "  8759/50000: episode: 1152, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003319, mae: 0.106737, mean_q: 0.145098\n",
      "  8768/50000: episode: 1153, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002159, mae: 0.099419, mean_q: 0.140047\n",
      "  8770/50000: episode: 1154, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001470, mae: 0.091995, mean_q: 0.132288\n",
      "  8773/50000: episode: 1155, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001958, mae: 0.092203, mean_q: 0.127418\n",
      "  8779/50000: episode: 1156, duration: 0.077s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002001, mae: 0.096158, mean_q: 0.135800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8795/50000: episode: 1157, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001654, mae: 0.093876, mean_q: 0.134850\n",
      "  8803/50000: episode: 1158, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.002890, mae: 0.099402, mean_q: 0.141344\n",
      "  8807/50000: episode: 1159, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001978, mae: 0.103968, mean_q: 0.144340\n",
      "  8820/50000: episode: 1160, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002073, mae: 0.098360, mean_q: 0.135255\n",
      "  8825/50000: episode: 1161, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001942, mae: 0.097676, mean_q: 0.137009\n",
      "  8832/50000: episode: 1162, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.004295, mae: 0.102475, mean_q: 0.138692\n",
      "  8848/50000: episode: 1163, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001749, mae: 0.097047, mean_q: 0.136737\n",
      "  8867/50000: episode: 1164, duration: 0.235s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.895 [0.000, 3.000],  loss: 0.002021, mae: 0.101456, mean_q: 0.143543\n",
      "  8876/50000: episode: 1165, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [1.000, 3.000],  loss: 0.001965, mae: 0.099691, mean_q: 0.140181\n",
      "  8879/50000: episode: 1166, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003624, mae: 0.103946, mean_q: 0.141849\n",
      "  8893/50000: episode: 1167, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002496, mae: 0.102929, mean_q: 0.142373\n",
      "  8907/50000: episode: 1168, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.001791, mae: 0.094055, mean_q: 0.130480\n",
      "  8915/50000: episode: 1169, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.001754, mae: 0.100704, mean_q: 0.136564\n",
      "  8926/50000: episode: 1170, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002004, mae: 0.100218, mean_q: 0.137004\n",
      "  8934/50000: episode: 1171, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003461, mae: 0.104226, mean_q: 0.139133\n",
      "  8941/50000: episode: 1172, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.429 [2.000, 3.000],  loss: 0.002363, mae: 0.100143, mean_q: 0.136262\n",
      "  8948/50000: episode: 1173, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001953, mae: 0.098681, mean_q: 0.138146\n",
      "  8952/50000: episode: 1174, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001820, mae: 0.091738, mean_q: 0.130424\n",
      "  8962/50000: episode: 1175, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002894, mae: 0.100552, mean_q: 0.137066\n",
      "  8967/50000: episode: 1176, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.003663, mae: 0.099949, mean_q: 0.137760\n",
      "  8972/50000: episode: 1177, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003137, mae: 0.105766, mean_q: 0.143293\n",
      "  8980/50000: episode: 1178, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.001504, mae: 0.105089, mean_q: 0.142534\n",
      "  8988/50000: episode: 1179, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001699, mae: 0.093098, mean_q: 0.126544\n",
      "  8992/50000: episode: 1180, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001765, mae: 0.087062, mean_q: 0.120181\n",
      "  9017/50000: episode: 1181, duration: 0.290s, episode steps:  25, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.840 [0.000, 3.000],  loss: 0.002269, mae: 0.096498, mean_q: 0.131708\n",
      "  9019/50000: episode: 1182, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002672, mae: 0.099663, mean_q: 0.134591\n",
      "  9021/50000: episode: 1183, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.000975, mae: 0.093634, mean_q: 0.133282\n",
      "  9023/50000: episode: 1184, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004855, mae: 0.106526, mean_q: 0.142094\n",
      "  9029/50000: episode: 1185, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002873, mae: 0.105470, mean_q: 0.144108\n",
      "  9031/50000: episode: 1186, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001305, mae: 0.092718, mean_q: 0.131917\n",
      "  9048/50000: episode: 1187, duration: 0.206s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002664, mae: 0.097455, mean_q: 0.133093\n",
      "  9056/50000: episode: 1188, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001705, mae: 0.094162, mean_q: 0.126472\n",
      "  9064/50000: episode: 1189, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001969, mae: 0.096529, mean_q: 0.129092\n",
      "  9073/50000: episode: 1190, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001280, mae: 0.090350, mean_q: 0.126334\n",
      "  9079/50000: episode: 1191, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003220, mae: 0.101972, mean_q: 0.145869\n",
      "  9082/50000: episode: 1192, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000732, mae: 0.087347, mean_q: 0.127954\n",
      "  9085/50000: episode: 1193, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001182, mae: 0.092416, mean_q: 0.135450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9093/50000: episode: 1194, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002315, mae: 0.096127, mean_q: 0.136859\n",
      "  9097/50000: episode: 1195, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001525, mae: 0.094443, mean_q: 0.131495\n",
      "  9099/50000: episode: 1196, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001432, mae: 0.097305, mean_q: 0.135138\n",
      "  9107/50000: episode: 1197, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001948, mae: 0.093936, mean_q: 0.129315\n",
      "  9113/50000: episode: 1198, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002190, mae: 0.099260, mean_q: 0.135218\n",
      "  9116/50000: episode: 1199, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001037, mae: 0.091521, mean_q: 0.126946\n",
      "  9125/50000: episode: 1200, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002537, mae: 0.096781, mean_q: 0.133845\n",
      "  9128/50000: episode: 1201, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001830, mae: 0.091302, mean_q: 0.126353\n",
      "  9135/50000: episode: 1202, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.003606, mae: 0.094111, mean_q: 0.128550\n",
      "  9145/50000: episode: 1203, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001620, mae: 0.094359, mean_q: 0.128839\n",
      "  9153/50000: episode: 1204, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001703, mae: 0.092771, mean_q: 0.124535\n",
      "  9168/50000: episode: 1205, duration: 0.211s, episode steps:  15, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.001947, mae: 0.092010, mean_q: 0.127833\n",
      "  9170/50000: episode: 1206, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001552, mae: 0.089652, mean_q: 0.124588\n",
      "  9173/50000: episode: 1207, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001344, mae: 0.091668, mean_q: 0.128379\n",
      "  9185/50000: episode: 1208, duration: 0.142s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002639, mae: 0.090937, mean_q: 0.123928\n",
      "  9190/50000: episode: 1209, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001615, mae: 0.098132, mean_q: 0.136906\n",
      "  9196/50000: episode: 1210, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002087, mae: 0.098832, mean_q: 0.135810\n",
      "  9201/50000: episode: 1211, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002455, mae: 0.097115, mean_q: 0.136609\n",
      "  9204/50000: episode: 1212, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.000944, mae: 0.086921, mean_q: 0.124005\n",
      "  9206/50000: episode: 1213, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002665, mae: 0.096680, mean_q: 0.138348\n",
      "  9213/50000: episode: 1214, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 0.857 [0.000, 2.000],  loss: 0.002910, mae: 0.089244, mean_q: 0.124802\n",
      "  9220/50000: episode: 1215, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [2.000, 3.000],  loss: 0.003119, mae: 0.102324, mean_q: 0.143630\n",
      "  9229/50000: episode: 1216, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.004019, mae: 0.106931, mean_q: 0.150453\n",
      "  9231/50000: episode: 1217, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001671, mae: 0.088916, mean_q: 0.124062\n",
      "  9244/50000: episode: 1218, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.003197, mae: 0.100672, mean_q: 0.136798\n",
      "  9254/50000: episode: 1219, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001845, mae: 0.091031, mean_q: 0.129164\n",
      "  9262/50000: episode: 1220, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.002447, mae: 0.099494, mean_q: 0.139410\n",
      "  9265/50000: episode: 1221, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001523, mae: 0.093332, mean_q: 0.129473\n",
      "  9281/50000: episode: 1222, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001999, mae: 0.091100, mean_q: 0.130707\n",
      "  9295/50000: episode: 1223, duration: 0.163s, episode steps:  14, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002438, mae: 0.095587, mean_q: 0.134493\n",
      "  9305/50000: episode: 1224, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002096, mae: 0.094712, mean_q: 0.129324\n",
      "  9317/50000: episode: 1225, duration: 0.157s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001633, mae: 0.093809, mean_q: 0.126690\n",
      "  9321/50000: episode: 1226, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.001420, mae: 0.092526, mean_q: 0.127672\n",
      "  9330/50000: episode: 1227, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001898, mae: 0.087591, mean_q: 0.121608\n",
      "  9334/50000: episode: 1228, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001009, mae: 0.089160, mean_q: 0.124592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9343/50000: episode: 1229, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.004084, mae: 0.100224, mean_q: 0.135496\n",
      "  9345/50000: episode: 1230, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002118, mae: 0.103461, mean_q: 0.142213\n",
      "  9370/50000: episode: 1231, duration: 0.294s, episode steps:  25, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.002585, mae: 0.094777, mean_q: 0.128549\n",
      "  9382/50000: episode: 1232, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002384, mae: 0.099794, mean_q: 0.137639\n",
      "  9388/50000: episode: 1233, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.003367, mae: 0.099886, mean_q: 0.135759\n",
      "  9390/50000: episode: 1234, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003012, mae: 0.105718, mean_q: 0.142358\n",
      "  9395/50000: episode: 1235, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002427, mae: 0.100256, mean_q: 0.140123\n",
      "  9406/50000: episode: 1236, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002603, mae: 0.102479, mean_q: 0.144074\n",
      "  9412/50000: episode: 1237, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001329, mae: 0.093687, mean_q: 0.131616\n",
      "  9420/50000: episode: 1238, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002838, mae: 0.101335, mean_q: 0.138647\n",
      "  9428/50000: episode: 1239, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003361, mae: 0.105433, mean_q: 0.145535\n",
      "  9434/50000: episode: 1240, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.001655, mae: 0.088977, mean_q: 0.123081\n",
      "  9445/50000: episode: 1241, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002073, mae: 0.094686, mean_q: 0.127417\n",
      "  9447/50000: episode: 1242, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003750, mae: 0.098517, mean_q: 0.129523\n",
      "  9460/50000: episode: 1243, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.001671, mae: 0.093868, mean_q: 0.128181\n",
      "  9472/50000: episode: 1244, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.004092, mae: 0.099331, mean_q: 0.134545\n",
      "  9474/50000: episode: 1245, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001565, mae: 0.090375, mean_q: 0.125713\n",
      "  9483/50000: episode: 1246, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002053, mae: 0.102220, mean_q: 0.137945\n",
      "  9488/50000: episode: 1247, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.003424, mae: 0.107436, mean_q: 0.145515\n",
      "  9490/50000: episode: 1248, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001900, mae: 0.090285, mean_q: 0.122119\n",
      "  9507/50000: episode: 1249, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002706, mae: 0.097562, mean_q: 0.133426\n",
      "  9512/50000: episode: 1250, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001432, mae: 0.098191, mean_q: 0.138137\n",
      "  9517/50000: episode: 1251, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003356, mae: 0.097564, mean_q: 0.134285\n",
      "  9520/50000: episode: 1252, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.002710, mae: 0.096365, mean_q: 0.131899\n",
      "  9527/50000: episode: 1253, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002405, mae: 0.102676, mean_q: 0.140145\n",
      "  9531/50000: episode: 1254, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.004172, mae: 0.102600, mean_q: 0.138580\n",
      "  9535/50000: episode: 1255, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001841, mae: 0.095616, mean_q: 0.133524\n",
      "  9546/50000: episode: 1256, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002032, mae: 0.096702, mean_q: 0.132425\n",
      "  9552/50000: episode: 1257, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002727, mae: 0.094682, mean_q: 0.131966\n",
      "  9560/50000: episode: 1258, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.003609, mae: 0.096424, mean_q: 0.131477\n",
      "  9566/50000: episode: 1259, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002709, mae: 0.108246, mean_q: 0.146746\n",
      "  9570/50000: episode: 1260, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001626, mae: 0.091921, mean_q: 0.129423\n",
      "  9579/50000: episode: 1261, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001860, mae: 0.092304, mean_q: 0.127885\n",
      "  9581/50000: episode: 1262, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002901, mae: 0.092249, mean_q: 0.125158\n",
      "  9587/50000: episode: 1263, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002189, mae: 0.097429, mean_q: 0.132612\n",
      "  9590/50000: episode: 1264, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.005472, mae: 0.103824, mean_q: 0.141057\n",
      "  9594/50000: episode: 1265, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.001940, mae: 0.104404, mean_q: 0.145767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9603/50000: episode: 1266, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001849, mae: 0.100199, mean_q: 0.135492\n",
      "  9618/50000: episode: 1267, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003033, mae: 0.101553, mean_q: 0.138857\n",
      "  9621/50000: episode: 1268, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001168, mae: 0.091590, mean_q: 0.131043\n",
      "  9626/50000: episode: 1269, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002209, mae: 0.106544, mean_q: 0.147983\n",
      "  9636/50000: episode: 1270, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001816, mae: 0.100859, mean_q: 0.138153\n",
      "  9638/50000: episode: 1271, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004443, mae: 0.102871, mean_q: 0.142628\n",
      "  9641/50000: episode: 1272, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.004095, mae: 0.101803, mean_q: 0.140839\n",
      "  9656/50000: episode: 1273, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002642, mae: 0.090523, mean_q: 0.128235\n",
      "  9662/50000: episode: 1274, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001555, mae: 0.097918, mean_q: 0.133182\n",
      "  9665/50000: episode: 1275, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000900, mae: 0.095337, mean_q: 0.131756\n",
      "  9674/50000: episode: 1276, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002409, mae: 0.102884, mean_q: 0.141061\n",
      "  9689/50000: episode: 1277, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003233, mae: 0.098503, mean_q: 0.133234\n",
      "  9692/50000: episode: 1278, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003180, mae: 0.100655, mean_q: 0.137636\n",
      "  9695/50000: episode: 1279, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.006165, mae: 0.114218, mean_q: 0.145125\n",
      "  9699/50000: episode: 1280, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001753, mae: 0.094377, mean_q: 0.126915\n",
      "  9707/50000: episode: 1281, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002844, mae: 0.095424, mean_q: 0.128800\n",
      "  9722/50000: episode: 1282, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002965, mae: 0.103033, mean_q: 0.138026\n",
      "  9727/50000: episode: 1283, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003198, mae: 0.109204, mean_q: 0.147601\n",
      "  9730/50000: episode: 1284, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002058, mae: 0.092836, mean_q: 0.127538\n",
      "  9739/50000: episode: 1285, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002567, mae: 0.093579, mean_q: 0.134165\n",
      "  9752/50000: episode: 1286, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003392, mae: 0.109050, mean_q: 0.151776\n",
      "  9777/50000: episode: 1287, duration: 0.285s, episode steps:  25, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.002338, mae: 0.099125, mean_q: 0.136194\n",
      "  9782/50000: episode: 1288, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.001375, mae: 0.100229, mean_q: 0.137120\n",
      "  9787/50000: episode: 1289, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002176, mae: 0.100976, mean_q: 0.133414\n",
      "  9793/50000: episode: 1290, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002480, mae: 0.094899, mean_q: 0.128370\n",
      "  9796/50000: episode: 1291, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002265, mae: 0.097433, mean_q: 0.133367\n",
      "  9806/50000: episode: 1292, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002165, mae: 0.101365, mean_q: 0.141640\n",
      "  9815/50000: episode: 1293, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002020, mae: 0.102754, mean_q: 0.144113\n",
      "  9817/50000: episode: 1294, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004131, mae: 0.109490, mean_q: 0.149019\n",
      "  9843/50000: episode: 1295, duration: 0.304s, episode steps:  26, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.002692, mae: 0.094492, mean_q: 0.129810\n",
      "  9845/50000: episode: 1296, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002026, mae: 0.089903, mean_q: 0.124615\n",
      "  9858/50000: episode: 1297, duration: 0.152s, episode steps:  13, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.001991, mae: 0.094813, mean_q: 0.127023\n",
      "  9860/50000: episode: 1298, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001595, mae: 0.090822, mean_q: 0.126369\n",
      "  9869/50000: episode: 1299, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002851, mae: 0.099101, mean_q: 0.138452\n",
      "  9875/50000: episode: 1300, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002650, mae: 0.096284, mean_q: 0.135905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9881/50000: episode: 1301, duration: 0.115s, episode steps:   6, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003435, mae: 0.110420, mean_q: 0.156812\n",
      "  9887/50000: episode: 1302, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002999, mae: 0.110581, mean_q: 0.153955\n",
      "  9898/50000: episode: 1303, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002544, mae: 0.095761, mean_q: 0.136949\n",
      "  9910/50000: episode: 1304, duration: 0.142s, episode steps:  12, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002345, mae: 0.098635, mean_q: 0.138491\n",
      "  9914/50000: episode: 1305, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003855, mae: 0.107902, mean_q: 0.148018\n",
      "  9917/50000: episode: 1306, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003365, mae: 0.096819, mean_q: 0.135321\n",
      "  9920/50000: episode: 1307, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001354, mae: 0.087255, mean_q: 0.121853\n",
      "  9928/50000: episode: 1308, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003371, mae: 0.091039, mean_q: 0.125819\n",
      "  9934/50000: episode: 1309, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002320, mae: 0.097824, mean_q: 0.139230\n",
      "  9940/50000: episode: 1310, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 1.000],  loss: 0.003080, mae: 0.099493, mean_q: 0.139675\n",
      "  9957/50000: episode: 1311, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.003278, mae: 0.102301, mean_q: 0.142155\n",
      "  9962/50000: episode: 1312, duration: 0.089s, episode steps:   5, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002121, mae: 0.102166, mean_q: 0.146060\n",
      "  9970/50000: episode: 1313, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002873, mae: 0.106991, mean_q: 0.146130\n",
      "  9977/50000: episode: 1314, duration: 0.119s, episode steps:   7, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001910, mae: 0.098430, mean_q: 0.134867\n",
      "  9988/50000: episode: 1315, duration: 0.176s, episode steps:  11, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.727 [0.000, 3.000],  loss: 0.002529, mae: 0.095279, mean_q: 0.130125\n",
      "  9994/50000: episode: 1316, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002606, mae: 0.100439, mean_q: 0.139108\n",
      " 10005/50000: episode: 1317, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002408, mae: 0.105994, mean_q: 0.142904\n",
      " 10009/50000: episode: 1318, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.003005, mae: 0.098857, mean_q: 0.134220\n",
      " 10011/50000: episode: 1319, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002708, mae: 0.101151, mean_q: 0.138594\n",
      " 10013/50000: episode: 1320, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003761, mae: 0.095520, mean_q: 0.126435\n",
      " 10015/50000: episode: 1321, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003860, mae: 0.096541, mean_q: 0.127898\n",
      " 10021/50000: episode: 1322, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002971, mae: 0.102858, mean_q: 0.138583\n",
      " 10035/50000: episode: 1323, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002388, mae: 0.100141, mean_q: 0.135298\n",
      " 10039/50000: episode: 1324, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003733, mae: 0.097064, mean_q: 0.130946\n",
      " 10048/50000: episode: 1325, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002123, mae: 0.104780, mean_q: 0.141834\n",
      " 10051/50000: episode: 1326, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002170, mae: 0.105739, mean_q: 0.143373\n",
      " 10070/50000: episode: 1327, duration: 0.225s, episode steps:  19, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.002083, mae: 0.097180, mean_q: 0.133484\n",
      " 10073/50000: episode: 1328, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003319, mae: 0.095381, mean_q: 0.129004\n",
      " 10078/50000: episode: 1329, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001959, mae: 0.095276, mean_q: 0.129734\n",
      " 10080/50000: episode: 1330, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003549, mae: 0.108042, mean_q: 0.147367\n",
      " 10091/50000: episode: 1331, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002403, mae: 0.101225, mean_q: 0.137781\n",
      " 10094/50000: episode: 1332, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001465, mae: 0.095391, mean_q: 0.129277\n",
      " 10100/50000: episode: 1333, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001929, mae: 0.092743, mean_q: 0.125165\n",
      " 10114/50000: episode: 1334, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.002299, mae: 0.101213, mean_q: 0.138450\n",
      " 10122/50000: episode: 1335, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.001421, mae: 0.095172, mean_q: 0.129361\n",
      " 10130/50000: episode: 1336, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002154, mae: 0.099456, mean_q: 0.133890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10144/50000: episode: 1337, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002265, mae: 0.105561, mean_q: 0.145129\n",
      " 10147/50000: episode: 1338, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001111, mae: 0.088153, mean_q: 0.124937\n",
      " 10167/50000: episode: 1339, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002241, mae: 0.102159, mean_q: 0.142231\n",
      " 10170/50000: episode: 1340, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003634, mae: 0.095667, mean_q: 0.133562\n",
      " 10178/50000: episode: 1341, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003112, mae: 0.105011, mean_q: 0.146758\n",
      " 10181/50000: episode: 1342, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003060, mae: 0.101860, mean_q: 0.136757\n",
      " 10186/50000: episode: 1343, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 1.000],  loss: 0.003892, mae: 0.103109, mean_q: 0.139378\n",
      " 10188/50000: episode: 1344, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000343, mae: 0.088021, mean_q: 0.124417\n",
      " 10196/50000: episode: 1345, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002748, mae: 0.102539, mean_q: 0.142807\n",
      " 10226/50000: episode: 1346, duration: 0.349s, episode steps:  30, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002965, mae: 0.101696, mean_q: 0.140407\n",
      " 10235/50000: episode: 1347, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001893, mae: 0.096346, mean_q: 0.135518\n",
      " 10237/50000: episode: 1348, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002272, mae: 0.107097, mean_q: 0.150882\n",
      " 10251/50000: episode: 1349, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002441, mae: 0.100751, mean_q: 0.141034\n",
      " 10265/50000: episode: 1350, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003874, mae: 0.104617, mean_q: 0.144217\n",
      " 10278/50000: episode: 1351, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.003341, mae: 0.100697, mean_q: 0.139765\n",
      " 10288/50000: episode: 1352, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.300 [1.000, 3.000],  loss: 0.002461, mae: 0.099997, mean_q: 0.137982\n",
      " 10304/50000: episode: 1353, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002468, mae: 0.102459, mean_q: 0.140338\n",
      " 10320/50000: episode: 1354, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.812 [0.000, 3.000],  loss: 0.002727, mae: 0.100047, mean_q: 0.139419\n",
      " 10343/50000: episode: 1355, duration: 0.272s, episode steps:  23, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002533, mae: 0.106931, mean_q: 0.146121\n",
      " 10365/50000: episode: 1356, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.003294, mae: 0.104506, mean_q: 0.141551\n",
      " 10369/50000: episode: 1357, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.001732, mae: 0.092859, mean_q: 0.126180\n",
      " 10372/50000: episode: 1358, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002529, mae: 0.099621, mean_q: 0.132129\n",
      " 10378/50000: episode: 1359, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003171, mae: 0.105208, mean_q: 0.139353\n",
      " 10382/50000: episode: 1360, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002315, mae: 0.102373, mean_q: 0.138956\n",
      " 10385/50000: episode: 1361, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003449, mae: 0.101179, mean_q: 0.135658\n",
      " 10391/50000: episode: 1362, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004590, mae: 0.105049, mean_q: 0.138157\n",
      " 10395/50000: episode: 1363, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002679, mae: 0.102733, mean_q: 0.139859\n",
      " 10401/50000: episode: 1364, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002774, mae: 0.098173, mean_q: 0.134119\n",
      " 10403/50000: episode: 1365, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003726, mae: 0.109224, mean_q: 0.149082\n",
      " 10409/50000: episode: 1366, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002179, mae: 0.096750, mean_q: 0.132448\n",
      " 10421/50000: episode: 1367, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003245, mae: 0.105007, mean_q: 0.141668\n",
      " 10423/50000: episode: 1368, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003289, mae: 0.095764, mean_q: 0.133829\n",
      " 10428/50000: episode: 1369, duration: 0.065s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000998, mae: 0.085760, mean_q: 0.125940\n",
      " 10461/50000: episode: 1370, duration: 0.385s, episode steps:  33, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001983, mae: 0.097284, mean_q: 0.138025\n",
      " 10466/50000: episode: 1371, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002738, mae: 0.098032, mean_q: 0.134031\n",
      " 10469/50000: episode: 1372, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002027, mae: 0.100590, mean_q: 0.141713\n",
      " 10473/50000: episode: 1373, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001248, mae: 0.091126, mean_q: 0.128946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10478/50000: episode: 1374, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003116, mae: 0.102137, mean_q: 0.139595\n",
      " 10484/50000: episode: 1375, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001761, mae: 0.095786, mean_q: 0.135742\n",
      " 10486/50000: episode: 1376, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003544, mae: 0.119536, mean_q: 0.166812\n",
      " 10492/50000: episode: 1377, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002518, mae: 0.104507, mean_q: 0.143833\n",
      " 10497/50000: episode: 1378, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002343, mae: 0.101908, mean_q: 0.141980\n",
      " 10500/50000: episode: 1379, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004523, mae: 0.124268, mean_q: 0.166194\n",
      " 10507/50000: episode: 1380, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003120, mae: 0.111270, mean_q: 0.148110\n",
      " 10526/50000: episode: 1381, duration: 0.221s, episode steps:  19, steps per second:  86, episode reward:  1.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002627, mae: 0.099215, mean_q: 0.133344\n",
      " 10528/50000: episode: 1382, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001250, mae: 0.107801, mean_q: 0.152404\n",
      " 10535/50000: episode: 1383, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002728, mae: 0.106555, mean_q: 0.143551\n",
      " 10543/50000: episode: 1384, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002142, mae: 0.100621, mean_q: 0.137084\n",
      " 10547/50000: episode: 1385, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003672, mae: 0.107531, mean_q: 0.146889\n",
      " 10551/50000: episode: 1386, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.002380, mae: 0.097632, mean_q: 0.136155\n",
      " 10559/50000: episode: 1387, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003674, mae: 0.101614, mean_q: 0.143237\n",
      " 10571/50000: episode: 1388, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002566, mae: 0.099787, mean_q: 0.137007\n",
      " 10576/50000: episode: 1389, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002431, mae: 0.097827, mean_q: 0.132882\n",
      " 10583/50000: episode: 1390, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002903, mae: 0.101113, mean_q: 0.137009\n",
      " 10598/50000: episode: 1391, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward:  1.000, mean reward:  0.067 [ 0.000,  1.000], mean action: 1.267 [0.000, 3.000],  loss: 0.002007, mae: 0.091414, mean_q: 0.124279\n",
      " 10601/50000: episode: 1392, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002310, mae: 0.098465, mean_q: 0.135569\n",
      " 10605/50000: episode: 1393, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003155, mae: 0.100006, mean_q: 0.138960\n",
      " 10609/50000: episode: 1394, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.000906, mae: 0.092269, mean_q: 0.133435\n",
      " 10625/50000: episode: 1395, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.003128, mae: 0.107508, mean_q: 0.150162\n",
      " 10630/50000: episode: 1396, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.002757, mae: 0.090923, mean_q: 0.129600\n",
      " 10643/50000: episode: 1397, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002287, mae: 0.093312, mean_q: 0.134446\n",
      " 10645/50000: episode: 1398, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004042, mae: 0.096143, mean_q: 0.136508\n",
      " 10652/50000: episode: 1399, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002740, mae: 0.099615, mean_q: 0.135406\n",
      " 10669/50000: episode: 1400, duration: 0.214s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002400, mae: 0.097575, mean_q: 0.133722\n",
      " 10679/50000: episode: 1401, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003879, mae: 0.108420, mean_q: 0.148940\n",
      " 10684/50000: episode: 1402, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002604, mae: 0.112166, mean_q: 0.161915\n",
      " 10692/50000: episode: 1403, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002043, mae: 0.103172, mean_q: 0.147754\n",
      " 10702/50000: episode: 1404, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003761, mae: 0.099112, mean_q: 0.137975\n",
      " 10711/50000: episode: 1405, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002368, mae: 0.099044, mean_q: 0.136256\n",
      " 10714/50000: episode: 1406, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002619, mae: 0.099975, mean_q: 0.135242\n",
      " 10722/50000: episode: 1407, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002727, mae: 0.103436, mean_q: 0.141861\n",
      " 10731/50000: episode: 1408, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002061, mae: 0.104305, mean_q: 0.141652\n",
      " 10739/50000: episode: 1409, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002454, mae: 0.099108, mean_q: 0.134028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10748/50000: episode: 1410, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002500, mae: 0.097975, mean_q: 0.133583\n",
      " 10763/50000: episode: 1411, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.003799, mae: 0.104691, mean_q: 0.141706\n",
      " 10772/50000: episode: 1412, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002827, mae: 0.105602, mean_q: 0.145754\n",
      " 10776/50000: episode: 1413, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003292, mae: 0.096469, mean_q: 0.134605\n",
      " 10794/50000: episode: 1414, duration: 0.218s, episode steps:  18, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.056 [0.000, 3.000],  loss: 0.003290, mae: 0.104285, mean_q: 0.142125\n",
      " 10800/50000: episode: 1415, duration: 0.078s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002978, mae: 0.109330, mean_q: 0.149289\n",
      " 10806/50000: episode: 1416, duration: 0.078s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002621, mae: 0.102632, mean_q: 0.141546\n",
      " 10809/50000: episode: 1417, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003395, mae: 0.100054, mean_q: 0.132759\n",
      " 10817/50000: episode: 1418, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002726, mae: 0.093601, mean_q: 0.124955\n",
      " 10822/50000: episode: 1419, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002498, mae: 0.104893, mean_q: 0.137962\n",
      " 10826/50000: episode: 1420, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002859, mae: 0.102380, mean_q: 0.137063\n",
      " 10829/50000: episode: 1421, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001899, mae: 0.093043, mean_q: 0.128982\n",
      " 10845/50000: episode: 1422, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002776, mae: 0.096991, mean_q: 0.132484\n",
      " 10847/50000: episode: 1423, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002208, mae: 0.104525, mean_q: 0.143632\n",
      " 10854/50000: episode: 1424, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002034, mae: 0.101950, mean_q: 0.141203\n",
      " 10858/50000: episode: 1425, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001454, mae: 0.097805, mean_q: 0.136556\n",
      " 10862/50000: episode: 1426, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001155, mae: 0.095061, mean_q: 0.132607\n",
      " 10868/50000: episode: 1427, duration: 0.078s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.001523, mae: 0.100391, mean_q: 0.139171\n",
      " 10875/50000: episode: 1428, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002491, mae: 0.100519, mean_q: 0.138571\n",
      " 10885/50000: episode: 1429, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002585, mae: 0.097720, mean_q: 0.133271\n",
      " 10892/50000: episode: 1430, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002703, mae: 0.093779, mean_q: 0.129492\n",
      " 10899/50000: episode: 1431, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002172, mae: 0.097379, mean_q: 0.135206\n",
      " 10907/50000: episode: 1432, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002838, mae: 0.098225, mean_q: 0.132906\n",
      " 10915/50000: episode: 1433, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001482, mae: 0.093448, mean_q: 0.129213\n",
      " 10920/50000: episode: 1434, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001800, mae: 0.094232, mean_q: 0.130849\n",
      " 10927/50000: episode: 1435, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001660, mae: 0.094975, mean_q: 0.131480\n",
      " 10935/50000: episode: 1436, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001849, mae: 0.094241, mean_q: 0.130272\n",
      " 10941/50000: episode: 1437, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002893, mae: 0.110070, mean_q: 0.151858\n",
      " 10943/50000: episode: 1438, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003719, mae: 0.102116, mean_q: 0.138696\n",
      " 10947/50000: episode: 1439, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003729, mae: 0.111211, mean_q: 0.152732\n",
      " 10950/50000: episode: 1440, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001770, mae: 0.099102, mean_q: 0.136279\n",
      " 10952/50000: episode: 1441, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002045, mae: 0.096137, mean_q: 0.132803\n",
      " 10959/50000: episode: 1442, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.429 [0.000, 3.000],  loss: 0.003549, mae: 0.097287, mean_q: 0.131354\n",
      " 10961/50000: episode: 1443, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000732, mae: 0.082043, mean_q: 0.116467\n",
      " 10971/50000: episode: 1444, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002772, mae: 0.101130, mean_q: 0.137789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10976/50000: episode: 1445, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001519, mae: 0.096878, mean_q: 0.135108\n",
      " 10981/50000: episode: 1446, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002913, mae: 0.096541, mean_q: 0.134754\n",
      " 10996/50000: episode: 1447, duration: 0.191s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002382, mae: 0.096047, mean_q: 0.134370\n",
      " 10999/50000: episode: 1448, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002248, mae: 0.097401, mean_q: 0.133537\n",
      " 11007/50000: episode: 1449, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [0.000, 3.000],  loss: 0.002993, mae: 0.102634, mean_q: 0.136648\n",
      " 11023/50000: episode: 1450, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002108, mae: 0.094450, mean_q: 0.130855\n",
      " 11027/50000: episode: 1451, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001872, mae: 0.098019, mean_q: 0.139540\n",
      " 11031/50000: episode: 1452, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002931, mae: 0.105264, mean_q: 0.142781\n",
      " 11039/50000: episode: 1453, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003115, mae: 0.109047, mean_q: 0.149964\n",
      " 11046/50000: episode: 1454, duration: 0.107s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [2.000, 3.000],  loss: 0.002056, mae: 0.100073, mean_q: 0.137008\n",
      " 11058/50000: episode: 1455, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.001645, mae: 0.092398, mean_q: 0.129314\n",
      " 11060/50000: episode: 1456, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001291, mae: 0.089931, mean_q: 0.127759\n",
      " 11072/50000: episode: 1457, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002668, mae: 0.103605, mean_q: 0.140318\n",
      " 11083/50000: episode: 1458, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001760, mae: 0.092792, mean_q: 0.124582\n",
      " 11089/50000: episode: 1459, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001843, mae: 0.093944, mean_q: 0.126154\n",
      " 11092/50000: episode: 1460, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.004043, mae: 0.098729, mean_q: 0.131766\n",
      " 11102/50000: episode: 1461, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002234, mae: 0.097769, mean_q: 0.129984\n",
      " 11104/50000: episode: 1462, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001773, mae: 0.108296, mean_q: 0.143498\n",
      " 11109/50000: episode: 1463, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002735, mae: 0.099332, mean_q: 0.130754\n",
      " 11113/50000: episode: 1464, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.001191, mae: 0.088770, mean_q: 0.123300\n",
      " 11117/50000: episode: 1465, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.001714, mae: 0.090665, mean_q: 0.128157\n",
      " 11140/50000: episode: 1466, duration: 0.279s, episode steps:  23, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.002069, mae: 0.097995, mean_q: 0.137816\n",
      " 11158/50000: episode: 1467, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002429, mae: 0.094905, mean_q: 0.132281\n",
      " 11165/50000: episode: 1468, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.002549, mae: 0.101828, mean_q: 0.139987\n",
      " 11170/50000: episode: 1469, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002108, mae: 0.098690, mean_q: 0.133660\n",
      " 11173/50000: episode: 1470, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005374, mae: 0.106898, mean_q: 0.144471\n",
      " 11184/50000: episode: 1471, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003483, mae: 0.098683, mean_q: 0.131983\n",
      " 11198/50000: episode: 1472, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002318, mae: 0.095583, mean_q: 0.127047\n",
      " 11200/50000: episode: 1473, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000862, mae: 0.090675, mean_q: 0.122443\n",
      " 11209/50000: episode: 1474, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001780, mae: 0.095695, mean_q: 0.129679\n",
      " 11213/50000: episode: 1475, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002308, mae: 0.102064, mean_q: 0.140917\n",
      " 11216/50000: episode: 1476, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001055, mae: 0.091264, mean_q: 0.131583\n",
      " 11222/50000: episode: 1477, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002898, mae: 0.094016, mean_q: 0.131298\n",
      " 11227/50000: episode: 1478, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002825, mae: 0.091999, mean_q: 0.133370\n",
      " 11233/50000: episode: 1479, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001299, mae: 0.092973, mean_q: 0.133432\n",
      " 11237/50000: episode: 1480, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001338, mae: 0.098498, mean_q: 0.136472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11250/50000: episode: 1481, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002293, mae: 0.096744, mean_q: 0.136445\n",
      " 11253/50000: episode: 1482, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001286, mae: 0.088616, mean_q: 0.123053\n",
      " 11265/50000: episode: 1483, duration: 0.185s, episode steps:  12, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001865, mae: 0.091908, mean_q: 0.124704\n",
      " 11282/50000: episode: 1484, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002036, mae: 0.096542, mean_q: 0.131649\n",
      " 11285/50000: episode: 1485, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001474, mae: 0.092104, mean_q: 0.130749\n",
      " 11293/50000: episode: 1486, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.375 [0.000, 3.000],  loss: 0.002610, mae: 0.100934, mean_q: 0.142414\n",
      " 11299/50000: episode: 1487, duration: 0.078s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001906, mae: 0.099142, mean_q: 0.136691\n",
      " 11323/50000: episode: 1488, duration: 0.281s, episode steps:  24, steps per second:  86, episode reward:  1.000, mean reward:  0.042 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001929, mae: 0.097351, mean_q: 0.133381\n",
      " 11329/50000: episode: 1489, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004158, mae: 0.102469, mean_q: 0.136806\n",
      " 11337/50000: episode: 1490, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002156, mae: 0.094962, mean_q: 0.131319\n",
      " 11349/50000: episode: 1491, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002508, mae: 0.095995, mean_q: 0.131588\n",
      " 11369/50000: episode: 1492, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.850 [0.000, 3.000],  loss: 0.002424, mae: 0.101844, mean_q: 0.142127\n",
      " 11374/50000: episode: 1493, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002001, mae: 0.098382, mean_q: 0.137337\n",
      " 11380/50000: episode: 1494, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002832, mae: 0.099065, mean_q: 0.137183\n",
      " 11388/50000: episode: 1495, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.625 [0.000, 2.000],  loss: 0.003231, mae: 0.100751, mean_q: 0.136450\n",
      " 11404/50000: episode: 1496, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.002164, mae: 0.099945, mean_q: 0.134691\n",
      " 11410/50000: episode: 1497, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.004020, mae: 0.098174, mean_q: 0.131378\n",
      " 11422/50000: episode: 1498, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004764, mae: 0.106500, mean_q: 0.145491\n",
      " 11425/50000: episode: 1499, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002302, mae: 0.104445, mean_q: 0.142424\n",
      " 11437/50000: episode: 1500, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002046, mae: 0.094534, mean_q: 0.130894\n",
      " 11441/50000: episode: 1501, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001740, mae: 0.083106, mean_q: 0.113312\n",
      " 11445/50000: episode: 1502, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003224, mae: 0.089085, mean_q: 0.118812\n",
      " 11448/50000: episode: 1503, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002665, mae: 0.099880, mean_q: 0.135113\n",
      " 11458/50000: episode: 1504, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002019, mae: 0.096068, mean_q: 0.130345\n",
      " 11462/50000: episode: 1505, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003686, mae: 0.103195, mean_q: 0.137588\n",
      " 11470/50000: episode: 1506, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002246, mae: 0.093014, mean_q: 0.123645\n",
      " 11490/50000: episode: 1507, duration: 0.236s, episode steps:  20, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002953, mae: 0.100093, mean_q: 0.133945\n",
      " 11492/50000: episode: 1508, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001695, mae: 0.099627, mean_q: 0.138361\n",
      " 11499/50000: episode: 1509, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002080, mae: 0.096541, mean_q: 0.134414\n",
      " 11508/50000: episode: 1510, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [0.000, 3.000],  loss: 0.002112, mae: 0.093002, mean_q: 0.129503\n",
      " 11510/50000: episode: 1511, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002964, mae: 0.104205, mean_q: 0.140343\n",
      " 11514/50000: episode: 1512, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002298, mae: 0.111943, mean_q: 0.151338\n",
      " 11522/50000: episode: 1513, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002591, mae: 0.091883, mean_q: 0.126134\n",
      " 11532/50000: episode: 1514, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002998, mae: 0.099503, mean_q: 0.131439\n",
      " 11539/50000: episode: 1515, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002896, mae: 0.101384, mean_q: 0.133344\n",
      " 11544/50000: episode: 1516, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001176, mae: 0.091467, mean_q: 0.129523\n",
      " 11550/50000: episode: 1517, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002424, mae: 0.091123, mean_q: 0.129838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11567/50000: episode: 1518, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002009, mae: 0.097394, mean_q: 0.134438\n",
      " 11575/50000: episode: 1519, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 3.000],  loss: 0.003542, mae: 0.096373, mean_q: 0.129683\n",
      " 11582/50000: episode: 1520, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003095, mae: 0.096475, mean_q: 0.127851\n",
      " 11587/50000: episode: 1521, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002490, mae: 0.103140, mean_q: 0.137672\n",
      " 11594/50000: episode: 1522, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001705, mae: 0.091739, mean_q: 0.122104\n",
      " 11620/50000: episode: 1523, duration: 0.301s, episode steps:  26, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.002740, mae: 0.098337, mean_q: 0.131063\n",
      " 11624/50000: episode: 1524, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003130, mae: 0.099292, mean_q: 0.134511\n",
      " 11627/50000: episode: 1525, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001546, mae: 0.093529, mean_q: 0.130867\n",
      " 11631/50000: episode: 1526, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003403, mae: 0.088500, mean_q: 0.122135\n",
      " 11635/50000: episode: 1527, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001959, mae: 0.085327, mean_q: 0.117966\n",
      " 11642/50000: episode: 1528, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001750, mae: 0.094588, mean_q: 0.127474\n",
      " 11652/50000: episode: 1529, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003425, mae: 0.105351, mean_q: 0.139563\n",
      " 11657/50000: episode: 1530, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001519, mae: 0.089705, mean_q: 0.122628\n",
      " 11667/50000: episode: 1531, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002081, mae: 0.090611, mean_q: 0.124231\n",
      " 11669/50000: episode: 1532, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002050, mae: 0.090644, mean_q: 0.122869\n",
      " 11672/50000: episode: 1533, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002068, mae: 0.097411, mean_q: 0.135891\n",
      " 11688/50000: episode: 1534, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.002938, mae: 0.099903, mean_q: 0.139542\n",
      " 11692/50000: episode: 1535, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.002799, mae: 0.093911, mean_q: 0.129066\n",
      " 11698/50000: episode: 1536, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001908, mae: 0.094447, mean_q: 0.132261\n",
      " 11705/50000: episode: 1537, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001769, mae: 0.089571, mean_q: 0.129563\n",
      " 11711/50000: episode: 1538, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003403, mae: 0.101971, mean_q: 0.143526\n",
      " 11715/50000: episode: 1539, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002206, mae: 0.093200, mean_q: 0.130337\n",
      " 11719/50000: episode: 1540, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004014, mae: 0.106502, mean_q: 0.147868\n",
      " 11721/50000: episode: 1541, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002713, mae: 0.100173, mean_q: 0.139194\n",
      " 11724/50000: episode: 1542, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001439, mae: 0.093384, mean_q: 0.132218\n",
      " 11727/50000: episode: 1543, duration: 0.054s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002656, mae: 0.092488, mean_q: 0.128100\n",
      " 11737/50000: episode: 1544, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001737, mae: 0.097295, mean_q: 0.135041\n",
      " 11740/50000: episode: 1545, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001373, mae: 0.086413, mean_q: 0.117970\n",
      " 11745/50000: episode: 1546, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.004501, mae: 0.094435, mean_q: 0.126701\n",
      " 11750/50000: episode: 1547, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [2.000, 3.000],  loss: 0.003441, mae: 0.097037, mean_q: 0.130518\n",
      " 11758/50000: episode: 1548, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002774, mae: 0.099886, mean_q: 0.138172\n",
      " 11763/50000: episode: 1549, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001836, mae: 0.093817, mean_q: 0.136786\n",
      " 11776/50000: episode: 1550, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.002341, mae: 0.089149, mean_q: 0.127795\n",
      " 11781/50000: episode: 1551, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002745, mae: 0.095836, mean_q: 0.133271\n",
      " 11784/50000: episode: 1552, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002279, mae: 0.093646, mean_q: 0.130681\n",
      " 11790/50000: episode: 1553, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002496, mae: 0.103637, mean_q: 0.140232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11793/50000: episode: 1554, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002989, mae: 0.093387, mean_q: 0.122337\n",
      " 11795/50000: episode: 1555, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001877, mae: 0.086712, mean_q: 0.117946\n",
      " 11806/50000: episode: 1556, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003109, mae: 0.088890, mean_q: 0.119804\n",
      " 11810/50000: episode: 1557, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001199, mae: 0.093233, mean_q: 0.127016\n",
      " 11820/50000: episode: 1558, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.001602, mae: 0.087590, mean_q: 0.121057\n",
      " 11825/50000: episode: 1559, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.200 [0.000, 1.000],  loss: 0.002898, mae: 0.088970, mean_q: 0.124402\n",
      " 11836/50000: episode: 1560, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002402, mae: 0.089421, mean_q: 0.121906\n",
      " 11841/50000: episode: 1561, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003742, mae: 0.112787, mean_q: 0.156962\n",
      " 11847/50000: episode: 1562, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002042, mae: 0.091923, mean_q: 0.133224\n",
      " 11855/50000: episode: 1563, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003022, mae: 0.094443, mean_q: 0.136431\n",
      " 11864/50000: episode: 1564, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003745, mae: 0.107268, mean_q: 0.148983\n",
      " 11867/50000: episode: 1565, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002419, mae: 0.093521, mean_q: 0.130328\n",
      " 11873/50000: episode: 1566, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002928, mae: 0.090463, mean_q: 0.130408\n",
      " 11891/50000: episode: 1567, duration: 0.218s, episode steps:  18, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002048, mae: 0.094810, mean_q: 0.133792\n",
      " 11893/50000: episode: 1568, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002896, mae: 0.101088, mean_q: 0.139673\n",
      " 11904/50000: episode: 1569, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002784, mae: 0.094802, mean_q: 0.131935\n",
      " 11907/50000: episode: 1570, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003882, mae: 0.108816, mean_q: 0.150398\n",
      " 11913/50000: episode: 1571, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003775, mae: 0.104143, mean_q: 0.143681\n",
      " 11915/50000: episode: 1572, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001420, mae: 0.097792, mean_q: 0.137237\n",
      " 11919/50000: episode: 1573, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.003065, mae: 0.089449, mean_q: 0.120987\n",
      " 11921/50000: episode: 1574, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000782, mae: 0.082409, mean_q: 0.115901\n",
      " 11926/50000: episode: 1575, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001479, mae: 0.087439, mean_q: 0.120272\n",
      " 11943/50000: episode: 1576, duration: 0.198s, episode steps:  17, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002481, mae: 0.091878, mean_q: 0.127200\n",
      " 11945/50000: episode: 1577, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003683, mae: 0.090093, mean_q: 0.128300\n",
      " 11948/50000: episode: 1578, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001918, mae: 0.087935, mean_q: 0.124969\n",
      " 11954/50000: episode: 1579, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002801, mae: 0.098219, mean_q: 0.138582\n",
      " 11968/50000: episode: 1580, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002779, mae: 0.093583, mean_q: 0.131555\n",
      " 11975/50000: episode: 1581, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.001939, mae: 0.092831, mean_q: 0.127107\n",
      " 11981/50000: episode: 1582, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002510, mae: 0.085996, mean_q: 0.118100\n",
      " 11989/50000: episode: 1583, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003976, mae: 0.096234, mean_q: 0.136620\n",
      " 11995/50000: episode: 1584, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.005628, mae: 0.116586, mean_q: 0.160655\n",
      " 12000/50000: episode: 1585, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002861, mae: 0.107280, mean_q: 0.148494\n",
      " 12006/50000: episode: 1586, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003596, mae: 0.097432, mean_q: 0.133237\n",
      " 12012/50000: episode: 1587, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003921, mae: 0.099945, mean_q: 0.135740\n",
      " 12016/50000: episode: 1588, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.001153, mae: 0.094102, mean_q: 0.128489\n",
      " 12020/50000: episode: 1589, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002708, mae: 0.091734, mean_q: 0.124429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12028/50000: episode: 1590, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002014, mae: 0.095524, mean_q: 0.130719\n",
      " 12034/50000: episode: 1591, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.001241, mae: 0.094917, mean_q: 0.130680\n",
      " 12036/50000: episode: 1592, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001249, mae: 0.091021, mean_q: 0.122852\n",
      " 12049/50000: episode: 1593, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.003533, mae: 0.097207, mean_q: 0.131731\n",
      " 12054/50000: episode: 1594, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002651, mae: 0.096709, mean_q: 0.136354\n",
      " 12058/50000: episode: 1595, duration: 0.054s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002893, mae: 0.102920, mean_q: 0.141685\n",
      " 12061/50000: episode: 1596, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003186, mae: 0.103815, mean_q: 0.143312\n",
      " 12065/50000: episode: 1597, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001971, mae: 0.094555, mean_q: 0.132207\n",
      " 12068/50000: episode: 1598, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003660, mae: 0.097768, mean_q: 0.132230\n",
      " 12074/50000: episode: 1599, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  1.000, mean reward:  0.167 [ 0.000,  1.000], mean action: 1.833 [1.000, 2.000],  loss: 0.002214, mae: 0.094855, mean_q: 0.133551\n",
      " 12076/50000: episode: 1600, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002353, mae: 0.093782, mean_q: 0.126927\n",
      " 12078/50000: episode: 1601, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001321, mae: 0.086428, mean_q: 0.119236\n",
      " 12086/50000: episode: 1602, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002067, mae: 0.095344, mean_q: 0.131857\n",
      " 12089/50000: episode: 1603, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003737, mae: 0.104466, mean_q: 0.144501\n",
      " 12093/50000: episode: 1604, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003479, mae: 0.103772, mean_q: 0.144457\n",
      " 12107/50000: episode: 1605, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002535, mae: 0.099007, mean_q: 0.135816\n",
      " 12109/50000: episode: 1606, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003956, mae: 0.104958, mean_q: 0.145737\n",
      " 12130/50000: episode: 1607, duration: 0.273s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002648, mae: 0.092794, mean_q: 0.125134\n",
      " 12133/50000: episode: 1608, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001950, mae: 0.086756, mean_q: 0.122147\n",
      " 12142/50000: episode: 1609, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [1.000, 3.000],  loss: 0.002612, mae: 0.094601, mean_q: 0.132831\n",
      " 12149/50000: episode: 1610, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002273, mae: 0.097350, mean_q: 0.136172\n",
      " 12158/50000: episode: 1611, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002096, mae: 0.103722, mean_q: 0.145130\n",
      " 12166/50000: episode: 1612, duration: 0.101s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003448, mae: 0.104349, mean_q: 0.145515\n",
      " 12170/50000: episode: 1613, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.003918, mae: 0.105565, mean_q: 0.142284\n",
      " 12173/50000: episode: 1614, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003296, mae: 0.095925, mean_q: 0.130967\n",
      " 12178/50000: episode: 1615, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002486, mae: 0.092949, mean_q: 0.125732\n",
      " 12182/50000: episode: 1616, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.003083, mae: 0.099536, mean_q: 0.132169\n",
      " 12190/50000: episode: 1617, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.004317, mae: 0.105289, mean_q: 0.141605\n",
      " 12197/50000: episode: 1618, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002473, mae: 0.099320, mean_q: 0.133182\n",
      " 12212/50000: episode: 1619, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002666, mae: 0.098412, mean_q: 0.131395\n",
      " 12216/50000: episode: 1620, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003479, mae: 0.097697, mean_q: 0.133996\n",
      " 12226/50000: episode: 1621, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002682, mae: 0.093241, mean_q: 0.130741\n",
      " 12230/50000: episode: 1622, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001788, mae: 0.085585, mean_q: 0.119985\n",
      " 12234/50000: episode: 1623, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003282, mae: 0.100645, mean_q: 0.142147\n",
      " 12239/50000: episode: 1624, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002286, mae: 0.104700, mean_q: 0.144720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12248/50000: episode: 1625, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003087, mae: 0.097964, mean_q: 0.138549\n",
      " 12256/50000: episode: 1626, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002228, mae: 0.092717, mean_q: 0.135174\n",
      " 12259/50000: episode: 1627, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004311, mae: 0.103099, mean_q: 0.142466\n",
      " 12263/50000: episode: 1628, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001663, mae: 0.104766, mean_q: 0.143976\n",
      " 12271/50000: episode: 1629, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001961, mae: 0.098329, mean_q: 0.134139\n",
      " 12276/50000: episode: 1630, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001467, mae: 0.083558, mean_q: 0.114632\n",
      " 12279/50000: episode: 1631, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002715, mae: 0.092482, mean_q: 0.125115\n",
      " 12286/50000: episode: 1632, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002009, mae: 0.101659, mean_q: 0.140248\n",
      " 12289/50000: episode: 1633, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001678, mae: 0.100079, mean_q: 0.139386\n",
      " 12291/50000: episode: 1634, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003075, mae: 0.103358, mean_q: 0.143051\n",
      " 12293/50000: episode: 1635, duration: 0.033s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001700, mae: 0.101563, mean_q: 0.143600\n",
      " 12311/50000: episode: 1636, duration: 0.216s, episode steps:  18, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [0.000, 3.000],  loss: 0.002282, mae: 0.098410, mean_q: 0.139946\n",
      " 12318/50000: episode: 1637, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.003109, mae: 0.105221, mean_q: 0.147299\n",
      " 12325/50000: episode: 1638, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.002668, mae: 0.100825, mean_q: 0.137581\n",
      " 12330/50000: episode: 1639, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003487, mae: 0.109587, mean_q: 0.154526\n",
      " 12337/50000: episode: 1640, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004425, mae: 0.099953, mean_q: 0.138195\n",
      " 12354/50000: episode: 1641, duration: 0.211s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002814, mae: 0.107444, mean_q: 0.152551\n",
      " 12358/50000: episode: 1642, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003212, mae: 0.110678, mean_q: 0.153867\n",
      " 12360/50000: episode: 1643, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003845, mae: 0.108159, mean_q: 0.151439\n",
      " 12364/50000: episode: 1644, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002866, mae: 0.094211, mean_q: 0.130603\n",
      " 12373/50000: episode: 1645, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003055, mae: 0.095883, mean_q: 0.132338\n",
      " 12379/50000: episode: 1646, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002360, mae: 0.100227, mean_q: 0.139696\n",
      " 12385/50000: episode: 1647, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002173, mae: 0.103033, mean_q: 0.144221\n",
      " 12387/50000: episode: 1648, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.005277, mae: 0.121604, mean_q: 0.174285\n",
      " 12389/50000: episode: 1649, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005154, mae: 0.117583, mean_q: 0.173512\n",
      " 12400/50000: episode: 1650, duration: 0.130s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002824, mae: 0.102036, mean_q: 0.144942\n",
      " 12404/50000: episode: 1651, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001432, mae: 0.100248, mean_q: 0.138647\n",
      " 12409/50000: episode: 1652, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.005022, mae: 0.106415, mean_q: 0.140225\n",
      " 12414/50000: episode: 1653, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.003606, mae: 0.105387, mean_q: 0.139552\n",
      " 12422/50000: episode: 1654, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002408, mae: 0.104277, mean_q: 0.142718\n",
      " 12425/50000: episode: 1655, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004102, mae: 0.104063, mean_q: 0.143595\n",
      " 12447/50000: episode: 1656, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.004557, mae: 0.106193, mean_q: 0.151016\n",
      " 12451/50000: episode: 1657, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003023, mae: 0.111817, mean_q: 0.154067\n",
      " 12453/50000: episode: 1658, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001891, mae: 0.110350, mean_q: 0.152186\n",
      " 12465/50000: episode: 1659, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004432, mae: 0.112060, mean_q: 0.152513\n",
      " 12471/50000: episode: 1660, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003145, mae: 0.100624, mean_q: 0.136700\n",
      " 12475/50000: episode: 1661, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003192, mae: 0.099992, mean_q: 0.136210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12483/50000: episode: 1662, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.005064, mae: 0.114830, mean_q: 0.151680\n",
      " 12487/50000: episode: 1663, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003792, mae: 0.110501, mean_q: 0.147872\n",
      " 12490/50000: episode: 1664, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001640, mae: 0.093675, mean_q: 0.127314\n",
      " 12509/50000: episode: 1665, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.003448, mae: 0.109645, mean_q: 0.146068\n",
      " 12511/50000: episode: 1666, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004380, mae: 0.111827, mean_q: 0.146353\n",
      " 12515/50000: episode: 1667, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001123, mae: 0.096973, mean_q: 0.131299\n",
      " 12517/50000: episode: 1668, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003576, mae: 0.098522, mean_q: 0.129657\n",
      " 12520/50000: episode: 1669, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002847, mae: 0.096661, mean_q: 0.129705\n",
      " 12531/50000: episode: 1670, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.003927, mae: 0.105767, mean_q: 0.140780\n",
      " 12541/50000: episode: 1671, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003976, mae: 0.107788, mean_q: 0.147858\n",
      " 12543/50000: episode: 1672, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001029, mae: 0.098171, mean_q: 0.143696\n",
      " 12545/50000: episode: 1673, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002439, mae: 0.114526, mean_q: 0.165707\n",
      " 12552/50000: episode: 1674, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003921, mae: 0.114818, mean_q: 0.162734\n",
      " 12567/50000: episode: 1675, duration: 0.180s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002826, mae: 0.109542, mean_q: 0.154810\n",
      " 12576/50000: episode: 1676, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [1.000, 3.000],  loss: 0.002808, mae: 0.108555, mean_q: 0.153728\n",
      " 12578/50000: episode: 1677, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001779, mae: 0.095014, mean_q: 0.133669\n",
      " 12584/50000: episode: 1678, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002057, mae: 0.101626, mean_q: 0.145824\n",
      " 12588/50000: episode: 1679, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004877, mae: 0.117281, mean_q: 0.165111\n",
      " 12598/50000: episode: 1680, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002478, mae: 0.108074, mean_q: 0.149728\n",
      " 12600/50000: episode: 1681, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003471, mae: 0.111386, mean_q: 0.152749\n",
      " 12619/50000: episode: 1682, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.737 [0.000, 3.000],  loss: 0.003225, mae: 0.117076, mean_q: 0.163326\n",
      " 12635/50000: episode: 1683, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002666, mae: 0.101271, mean_q: 0.140601\n",
      " 12677/50000: episode: 1684, duration: 0.489s, episode steps:  42, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.262 [0.000, 3.000],  loss: 0.002800, mae: 0.105382, mean_q: 0.148170\n",
      " 12679/50000: episode: 1685, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002154, mae: 0.102308, mean_q: 0.137798\n",
      " 12693/50000: episode: 1686, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003493, mae: 0.110083, mean_q: 0.148936\n",
      " 12700/50000: episode: 1687, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [0.000, 3.000],  loss: 0.003506, mae: 0.110720, mean_q: 0.153077\n",
      " 12703/50000: episode: 1688, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001973, mae: 0.111981, mean_q: 0.156938\n",
      " 12707/50000: episode: 1689, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003175, mae: 0.111440, mean_q: 0.150405\n",
      " 12714/50000: episode: 1690, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002225, mae: 0.110059, mean_q: 0.148823\n",
      " 12723/50000: episode: 1691, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002437, mae: 0.105745, mean_q: 0.143400\n",
      " 12733/50000: episode: 1692, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003526, mae: 0.113862, mean_q: 0.153456\n",
      " 12752/50000: episode: 1693, duration: 0.228s, episode steps:  19, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.003292, mae: 0.102018, mean_q: 0.137431\n",
      " 12754/50000: episode: 1694, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002040, mae: 0.099122, mean_q: 0.137406\n",
      " 12762/50000: episode: 1695, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003432, mae: 0.111840, mean_q: 0.149451\n",
      " 12772/50000: episode: 1696, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.004157, mae: 0.116036, mean_q: 0.154552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12793/50000: episode: 1697, duration: 0.242s, episode steps:  21, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.381 [0.000, 3.000],  loss: 0.003944, mae: 0.113487, mean_q: 0.152705\n",
      " 12795/50000: episode: 1698, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002045, mae: 0.114010, mean_q: 0.159554\n",
      " 12799/50000: episode: 1699, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001628, mae: 0.114328, mean_q: 0.155150\n",
      " 12805/50000: episode: 1700, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003752, mae: 0.116738, mean_q: 0.155118\n",
      " 12811/50000: episode: 1701, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003045, mae: 0.106793, mean_q: 0.144285\n",
      " 12823/50000: episode: 1702, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.002644, mae: 0.105059, mean_q: 0.149056\n",
      " 12831/50000: episode: 1703, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002082, mae: 0.104148, mean_q: 0.144890\n",
      " 12833/50000: episode: 1704, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003521, mae: 0.112140, mean_q: 0.150027\n",
      " 12836/50000: episode: 1705, duration: 0.052s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.006658, mae: 0.124458, mean_q: 0.171618\n",
      " 12842/50000: episode: 1706, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002523, mae: 0.107926, mean_q: 0.152154\n",
      " 12845/50000: episode: 1707, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.003067, mae: 0.106487, mean_q: 0.149458\n",
      " 12856/50000: episode: 1708, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002755, mae: 0.111436, mean_q: 0.154743\n",
      " 12858/50000: episode: 1709, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002575, mae: 0.102122, mean_q: 0.140680\n",
      " 12876/50000: episode: 1710, duration: 0.254s, episode steps:  18, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002738, mae: 0.104041, mean_q: 0.143822\n",
      " 12878/50000: episode: 1711, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003593, mae: 0.115299, mean_q: 0.155802\n",
      " 12882/50000: episode: 1712, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003712, mae: 0.111728, mean_q: 0.148100\n",
      " 12891/50000: episode: 1713, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.003134, mae: 0.105969, mean_q: 0.145192\n",
      " 12907/50000: episode: 1714, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003483, mae: 0.108798, mean_q: 0.147963\n",
      " 12911/50000: episode: 1715, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002799, mae: 0.113395, mean_q: 0.156942\n",
      " 12917/50000: episode: 1716, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002178, mae: 0.107069, mean_q: 0.144667\n",
      " 12923/50000: episode: 1717, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.003035, mae: 0.112498, mean_q: 0.151738\n",
      " 12927/50000: episode: 1718, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.003615, mae: 0.111105, mean_q: 0.150391\n",
      " 12932/50000: episode: 1719, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002956, mae: 0.114534, mean_q: 0.155966\n",
      " 12938/50000: episode: 1720, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004901, mae: 0.110444, mean_q: 0.148092\n",
      " 12964/50000: episode: 1721, duration: 0.307s, episode steps:  26, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002882, mae: 0.106061, mean_q: 0.148618\n",
      " 12966/50000: episode: 1722, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003848, mae: 0.108217, mean_q: 0.151416\n",
      " 12976/50000: episode: 1723, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.001956, mae: 0.101563, mean_q: 0.145347\n",
      " 12984/50000: episode: 1724, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.001831, mae: 0.107079, mean_q: 0.150221\n",
      " 12988/50000: episode: 1725, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001614, mae: 0.107976, mean_q: 0.151901\n",
      " 12990/50000: episode: 1726, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004027, mae: 0.098293, mean_q: 0.132097\n",
      " 13003/50000: episode: 1727, duration: 0.158s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.004154, mae: 0.112534, mean_q: 0.149713\n",
      " 13005/50000: episode: 1728, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.006512, mae: 0.115549, mean_q: 0.153557\n",
      " 13012/50000: episode: 1729, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002120, mae: 0.099931, mean_q: 0.135010\n",
      " 13023/50000: episode: 1730, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.364 [0.000, 3.000],  loss: 0.002605, mae: 0.106391, mean_q: 0.143489\n",
      " 13037/50000: episode: 1731, duration: 0.174s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002677, mae: 0.105615, mean_q: 0.141809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13042/50000: episode: 1732, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003615, mae: 0.104775, mean_q: 0.143635\n",
      " 13045/50000: episode: 1733, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002711, mae: 0.099673, mean_q: 0.136953\n",
      " 13062/50000: episode: 1734, duration: 0.208s, episode steps:  17, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.003659, mae: 0.111823, mean_q: 0.153151\n",
      " 13070/50000: episode: 1735, duration: 0.101s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002419, mae: 0.106430, mean_q: 0.149412\n",
      " 13077/50000: episode: 1736, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001916, mae: 0.105807, mean_q: 0.149939\n",
      " 13085/50000: episode: 1737, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 3.000],  loss: 0.002431, mae: 0.112222, mean_q: 0.155107\n",
      " 13090/50000: episode: 1738, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002387, mae: 0.107339, mean_q: 0.146226\n",
      " 13093/50000: episode: 1739, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.000786, mae: 0.100767, mean_q: 0.139208\n",
      " 13099/50000: episode: 1740, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004599, mae: 0.110744, mean_q: 0.148773\n",
      " 13105/50000: episode: 1741, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003979, mae: 0.114488, mean_q: 0.152481\n",
      " 13107/50000: episode: 1742, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005498, mae: 0.120873, mean_q: 0.160110\n",
      " 13119/50000: episode: 1743, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.003648, mae: 0.103819, mean_q: 0.140171\n",
      " 13140/50000: episode: 1744, duration: 0.254s, episode steps:  21, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002668, mae: 0.105598, mean_q: 0.148735\n",
      " 13144/50000: episode: 1745, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002068, mae: 0.101620, mean_q: 0.141196\n",
      " 13154/50000: episode: 1746, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002502, mae: 0.104813, mean_q: 0.147353\n",
      " 13168/50000: episode: 1747, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.071 [0.000, 3.000],  loss: 0.003248, mae: 0.108380, mean_q: 0.147663\n",
      " 13178/50000: episode: 1748, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002626, mae: 0.102031, mean_q: 0.139239\n",
      " 13189/50000: episode: 1749, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003092, mae: 0.108456, mean_q: 0.146937\n",
      " 13195/50000: episode: 1750, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001952, mae: 0.107690, mean_q: 0.146906\n",
      " 13205/50000: episode: 1751, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.003597, mae: 0.115731, mean_q: 0.154259\n",
      " 13208/50000: episode: 1752, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.005622, mae: 0.119901, mean_q: 0.155626\n",
      " 13223/50000: episode: 1753, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002531, mae: 0.104917, mean_q: 0.142483\n",
      " 13249/50000: episode: 1754, duration: 0.300s, episode steps:  26, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.003379, mae: 0.106254, mean_q: 0.144651\n",
      " 13274/50000: episode: 1755, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.002799, mae: 0.104814, mean_q: 0.142377\n",
      " 13277/50000: episode: 1756, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001944, mae: 0.106834, mean_q: 0.145509\n",
      " 13280/50000: episode: 1757, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002156, mae: 0.106211, mean_q: 0.145841\n",
      " 13282/50000: episode: 1758, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003182, mae: 0.105050, mean_q: 0.140864\n",
      " 13285/50000: episode: 1759, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001964, mae: 0.097735, mean_q: 0.134918\n",
      " 13305/50000: episode: 1760, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.002880, mae: 0.110330, mean_q: 0.150252\n",
      " 13307/50000: episode: 1761, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001941, mae: 0.094974, mean_q: 0.133636\n",
      " 13322/50000: episode: 1762, duration: 0.191s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003767, mae: 0.110791, mean_q: 0.153341\n",
      " 13345/50000: episode: 1763, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002668, mae: 0.107329, mean_q: 0.145220\n",
      " 13347/50000: episode: 1764, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001649, mae: 0.104241, mean_q: 0.139036\n",
      " 13359/50000: episode: 1765, duration: 0.144s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002572, mae: 0.107439, mean_q: 0.147436\n",
      " 13373/50000: episode: 1766, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003473, mae: 0.109330, mean_q: 0.148611\n",
      " 13379/50000: episode: 1767, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002448, mae: 0.109856, mean_q: 0.150019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13390/50000: episode: 1768, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002298, mae: 0.105259, mean_q: 0.144095\n",
      " 13395/50000: episode: 1769, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003251, mae: 0.109456, mean_q: 0.151342\n",
      " 13402/50000: episode: 1770, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.571 [0.000, 3.000],  loss: 0.002860, mae: 0.104556, mean_q: 0.145960\n",
      " 13408/50000: episode: 1771, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003317, mae: 0.113171, mean_q: 0.158087\n",
      " 13414/50000: episode: 1772, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003240, mae: 0.110048, mean_q: 0.155640\n",
      " 13422/50000: episode: 1773, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002495, mae: 0.107016, mean_q: 0.150440\n",
      " 13425/50000: episode: 1774, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002123, mae: 0.103011, mean_q: 0.144033\n",
      " 13429/50000: episode: 1775, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.003781, mae: 0.107930, mean_q: 0.148219\n",
      " 13439/50000: episode: 1776, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002342, mae: 0.109410, mean_q: 0.152146\n",
      " 13447/50000: episode: 1777, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002569, mae: 0.106178, mean_q: 0.145950\n",
      " 13449/50000: episode: 1778, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003542, mae: 0.117860, mean_q: 0.159371\n",
      " 13456/50000: episode: 1779, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002640, mae: 0.104574, mean_q: 0.145599\n",
      " 13463/50000: episode: 1780, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002809, mae: 0.105612, mean_q: 0.147689\n",
      " 13466/50000: episode: 1781, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.005236, mae: 0.120999, mean_q: 0.168894\n",
      " 13479/50000: episode: 1782, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.003507, mae: 0.114786, mean_q: 0.158515\n",
      " 13498/50000: episode: 1783, duration: 0.222s, episode steps:  19, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002481, mae: 0.102036, mean_q: 0.140584\n",
      " 13500/50000: episode: 1784, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002070, mae: 0.098474, mean_q: 0.131601\n",
      " 13522/50000: episode: 1785, duration: 0.258s, episode steps:  22, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002594, mae: 0.109817, mean_q: 0.150322\n",
      " 13525/50000: episode: 1786, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.006773, mae: 0.121669, mean_q: 0.161088\n",
      " 13533/50000: episode: 1787, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003052, mae: 0.110553, mean_q: 0.152787\n",
      " 13539/50000: episode: 1788, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003311, mae: 0.102873, mean_q: 0.142720\n",
      " 13576/50000: episode: 1789, duration: 0.440s, episode steps:  37, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.002911, mae: 0.111458, mean_q: 0.153606\n",
      " 13584/50000: episode: 1790, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003172, mae: 0.112761, mean_q: 0.160912\n",
      " 13588/50000: episode: 1791, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002345, mae: 0.109681, mean_q: 0.157667\n",
      " 13597/50000: episode: 1792, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.004110, mae: 0.114832, mean_q: 0.165139\n",
      " 13600/50000: episode: 1793, duration: 0.063s, episode steps:   3, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002622, mae: 0.121108, mean_q: 0.174666\n",
      " 13602/50000: episode: 1794, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.006971, mae: 0.114672, mean_q: 0.153569\n",
      " 13609/50000: episode: 1795, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003561, mae: 0.118669, mean_q: 0.164589\n",
      " 13629/50000: episode: 1796, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003970, mae: 0.105732, mean_q: 0.143955\n",
      " 13637/50000: episode: 1797, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.001867, mae: 0.106125, mean_q: 0.149977\n",
      " 13641/50000: episode: 1798, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004721, mae: 0.118222, mean_q: 0.162068\n",
      " 13644/50000: episode: 1799, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003711, mae: 0.108837, mean_q: 0.151077\n",
      " 13650/50000: episode: 1800, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001513, mae: 0.101982, mean_q: 0.144485\n",
      " 13658/50000: episode: 1801, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002087, mae: 0.106856, mean_q: 0.149241\n",
      " 13661/50000: episode: 1802, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001854, mae: 0.108369, mean_q: 0.152916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13667/50000: episode: 1803, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003634, mae: 0.118414, mean_q: 0.166809\n",
      " 13669/50000: episode: 1804, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000963, mae: 0.097152, mean_q: 0.135953\n",
      " 13684/50000: episode: 1805, duration: 0.184s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.003422, mae: 0.108574, mean_q: 0.151295\n",
      " 13700/50000: episode: 1806, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003231, mae: 0.113445, mean_q: 0.158973\n",
      " 13711/50000: episode: 1807, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.004262, mae: 0.105761, mean_q: 0.143360\n",
      " 13715/50000: episode: 1808, duration: 0.054s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003623, mae: 0.109688, mean_q: 0.148258\n",
      " 13725/50000: episode: 1809, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002399, mae: 0.115063, mean_q: 0.158799\n",
      " 13731/50000: episode: 1810, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003140, mae: 0.111321, mean_q: 0.149353\n",
      " 13747/50000: episode: 1811, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002958, mae: 0.101169, mean_q: 0.136999\n",
      " 13770/50000: episode: 1812, duration: 0.269s, episode steps:  23, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.043 [0.000, 3.000],  loss: 0.003644, mae: 0.111538, mean_q: 0.150000\n",
      " 13783/50000: episode: 1813, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002652, mae: 0.111768, mean_q: 0.153648\n",
      " 13789/50000: episode: 1814, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.003317, mae: 0.114957, mean_q: 0.152523\n",
      " 13792/50000: episode: 1815, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004402, mae: 0.116546, mean_q: 0.154237\n",
      " 13802/50000: episode: 1816, duration: 0.123s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002841, mae: 0.103966, mean_q: 0.140327\n",
      " 13804/50000: episode: 1817, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003433, mae: 0.106414, mean_q: 0.146251\n",
      " 13807/50000: episode: 1818, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002499, mae: 0.108161, mean_q: 0.148021\n",
      " 13810/50000: episode: 1819, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001693, mae: 0.105460, mean_q: 0.143525\n",
      " 13819/50000: episode: 1820, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003914, mae: 0.111260, mean_q: 0.153149\n",
      " 13825/50000: episode: 1821, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002940, mae: 0.111895, mean_q: 0.153493\n",
      " 13830/50000: episode: 1822, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003560, mae: 0.123556, mean_q: 0.166653\n",
      " 13842/50000: episode: 1823, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002043, mae: 0.103831, mean_q: 0.142909\n",
      " 13848/50000: episode: 1824, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002663, mae: 0.107022, mean_q: 0.145684\n",
      " 13856/50000: episode: 1825, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003854, mae: 0.120051, mean_q: 0.160538\n",
      " 13858/50000: episode: 1826, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004012, mae: 0.120864, mean_q: 0.161420\n",
      " 13861/50000: episode: 1827, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001918, mae: 0.111754, mean_q: 0.152375\n",
      " 13868/50000: episode: 1828, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002609, mae: 0.110856, mean_q: 0.151096\n",
      " 13874/50000: episode: 1829, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002829, mae: 0.105169, mean_q: 0.140501\n",
      " 13878/50000: episode: 1830, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.003898, mae: 0.110796, mean_q: 0.148747\n",
      " 13880/50000: episode: 1831, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004857, mae: 0.116616, mean_q: 0.157656\n",
      " 13895/50000: episode: 1832, duration: 0.182s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002082, mae: 0.106505, mean_q: 0.145522\n",
      " 13899/50000: episode: 1833, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003470, mae: 0.105625, mean_q: 0.143443\n",
      " 13901/50000: episode: 1834, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003237, mae: 0.105309, mean_q: 0.142801\n",
      " 13909/50000: episode: 1835, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002998, mae: 0.114707, mean_q: 0.154455\n",
      " 13922/50000: episode: 1836, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.003290, mae: 0.111324, mean_q: 0.151549\n",
      " 13931/50000: episode: 1837, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002644, mae: 0.107284, mean_q: 0.144889\n",
      " 13937/50000: episode: 1838, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002129, mae: 0.101827, mean_q: 0.138493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13946/50000: episode: 1839, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.002912, mae: 0.103352, mean_q: 0.143142\n",
      " 13949/50000: episode: 1840, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002444, mae: 0.114702, mean_q: 0.156607\n",
      " 13953/50000: episode: 1841, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001924, mae: 0.118761, mean_q: 0.166706\n",
      " 13960/50000: episode: 1842, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002045, mae: 0.110036, mean_q: 0.154236\n",
      " 13967/50000: episode: 1843, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003854, mae: 0.114267, mean_q: 0.154504\n",
      " 13971/50000: episode: 1844, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003267, mae: 0.107755, mean_q: 0.144771\n",
      " 13979/50000: episode: 1845, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002648, mae: 0.101619, mean_q: 0.140186\n",
      " 13983/50000: episode: 1846, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005362, mae: 0.115106, mean_q: 0.153721\n",
      " 13989/50000: episode: 1847, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002680, mae: 0.114383, mean_q: 0.157640\n",
      " 13991/50000: episode: 1848, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002473, mae: 0.108484, mean_q: 0.150176\n",
      " 13998/50000: episode: 1849, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002018, mae: 0.106173, mean_q: 0.147385\n",
      " 14003/50000: episode: 1850, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002352, mae: 0.106206, mean_q: 0.148548\n",
      " 14020/50000: episode: 1851, duration: 0.200s, episode steps:  17, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.118 [0.000, 3.000],  loss: 0.003173, mae: 0.110037, mean_q: 0.152439\n",
      " 14024/50000: episode: 1852, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003299, mae: 0.111547, mean_q: 0.151265\n",
      " 14026/50000: episode: 1853, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002439, mae: 0.111797, mean_q: 0.153059\n",
      " 14032/50000: episode: 1854, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002081, mae: 0.112825, mean_q: 0.152664\n",
      " 14050/50000: episode: 1855, duration: 0.211s, episode steps:  18, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003612, mae: 0.103889, mean_q: 0.139317\n",
      " 14053/50000: episode: 1856, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001503, mae: 0.097717, mean_q: 0.134390\n",
      " 14059/50000: episode: 1857, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001526, mae: 0.097783, mean_q: 0.135411\n",
      " 14069/50000: episode: 1858, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005206, mae: 0.115738, mean_q: 0.154925\n",
      " 14076/50000: episode: 1859, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002185, mae: 0.111425, mean_q: 0.155277\n",
      " 14093/50000: episode: 1860, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.002709, mae: 0.105832, mean_q: 0.148760\n",
      " 14096/50000: episode: 1861, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.003195, mae: 0.111425, mean_q: 0.152595\n",
      " 14110/50000: episode: 1862, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002923, mae: 0.104660, mean_q: 0.143298\n",
      " 14121/50000: episode: 1863, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.818 [0.000, 3.000],  loss: 0.001846, mae: 0.100635, mean_q: 0.140635\n",
      " 14124/50000: episode: 1864, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001601, mae: 0.108577, mean_q: 0.153542\n",
      " 14135/50000: episode: 1865, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001857, mae: 0.102387, mean_q: 0.142443\n",
      " 14146/50000: episode: 1866, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003258, mae: 0.103376, mean_q: 0.138862\n",
      " 14152/50000: episode: 1867, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001667, mae: 0.101057, mean_q: 0.140649\n",
      " 14158/50000: episode: 1868, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003003, mae: 0.108338, mean_q: 0.148544\n",
      " 14166/50000: episode: 1869, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002878, mae: 0.101575, mean_q: 0.139505\n",
      " 14171/50000: episode: 1870, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002972, mae: 0.106220, mean_q: 0.143967\n",
      " 14183/50000: episode: 1871, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002847, mae: 0.111017, mean_q: 0.152107\n",
      " 14205/50000: episode: 1872, duration: 0.256s, episode steps:  22, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.136 [0.000, 3.000],  loss: 0.002907, mae: 0.103463, mean_q: 0.140166\n",
      " 14207/50000: episode: 1873, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001238, mae: 0.097115, mean_q: 0.132688\n",
      " 14212/50000: episode: 1874, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003134, mae: 0.103888, mean_q: 0.141313\n",
      " 14215/50000: episode: 1875, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002123, mae: 0.099903, mean_q: 0.137296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14222/50000: episode: 1876, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003501, mae: 0.110643, mean_q: 0.148640\n",
      " 14228/50000: episode: 1877, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001972, mae: 0.107036, mean_q: 0.150255\n",
      " 14236/50000: episode: 1878, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003893, mae: 0.110620, mean_q: 0.153230\n",
      " 14243/50000: episode: 1879, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002088, mae: 0.107727, mean_q: 0.156726\n",
      " 14249/50000: episode: 1880, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002400, mae: 0.112798, mean_q: 0.160240\n",
      " 14252/50000: episode: 1881, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001811, mae: 0.104819, mean_q: 0.147759\n",
      " 14257/50000: episode: 1882, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003650, mae: 0.103788, mean_q: 0.146695\n",
      " 14265/50000: episode: 1883, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002375, mae: 0.105146, mean_q: 0.147407\n",
      " 14305/50000: episode: 1884, duration: 0.470s, episode steps:  40, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002421, mae: 0.104938, mean_q: 0.143756\n",
      " 14310/50000: episode: 1885, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002328, mae: 0.098696, mean_q: 0.139374\n",
      " 14317/50000: episode: 1886, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001970, mae: 0.099382, mean_q: 0.140674\n",
      " 14328/50000: episode: 1887, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002359, mae: 0.105964, mean_q: 0.147254\n",
      " 14332/50000: episode: 1888, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003031, mae: 0.100299, mean_q: 0.138476\n",
      " 14334/50000: episode: 1889, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000794, mae: 0.104718, mean_q: 0.148687\n",
      " 14336/50000: episode: 1890, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004140, mae: 0.116721, mean_q: 0.159422\n",
      " 14345/50000: episode: 1891, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003734, mae: 0.101486, mean_q: 0.138984\n",
      " 14364/50000: episode: 1892, duration: 0.228s, episode steps:  19, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.003601, mae: 0.108107, mean_q: 0.146610\n",
      " 14366/50000: episode: 1893, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002082, mae: 0.098836, mean_q: 0.136751\n",
      " 14369/50000: episode: 1894, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002874, mae: 0.105039, mean_q: 0.141946\n",
      " 14386/50000: episode: 1895, duration: 0.206s, episode steps:  17, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.002704, mae: 0.104493, mean_q: 0.141620\n",
      " 14391/50000: episode: 1896, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001995, mae: 0.098777, mean_q: 0.135259\n",
      " 14395/50000: episode: 1897, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002152, mae: 0.103744, mean_q: 0.142705\n",
      " 14401/50000: episode: 1898, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002650, mae: 0.101368, mean_q: 0.139170\n",
      " 14404/50000: episode: 1899, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003298, mae: 0.101043, mean_q: 0.136970\n",
      " 14408/50000: episode: 1900, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002689, mae: 0.104662, mean_q: 0.142391\n",
      " 14413/50000: episode: 1901, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001288, mae: 0.100642, mean_q: 0.140449\n",
      " 14421/50000: episode: 1902, duration: 0.107s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.002237, mae: 0.103218, mean_q: 0.141677\n",
      " 14435/50000: episode: 1903, duration: 0.164s, episode steps:  14, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002292, mae: 0.103570, mean_q: 0.141894\n",
      " 14437/50000: episode: 1904, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001207, mae: 0.101888, mean_q: 0.141492\n",
      " 14439/50000: episode: 1905, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003570, mae: 0.108657, mean_q: 0.145546\n",
      " 14452/50000: episode: 1906, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002571, mae: 0.104054, mean_q: 0.142777\n",
      " 14461/50000: episode: 1907, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003489, mae: 0.103049, mean_q: 0.142972\n",
      " 14466/50000: episode: 1908, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002330, mae: 0.095985, mean_q: 0.135981\n",
      " 14477/50000: episode: 1909, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.003522, mae: 0.107974, mean_q: 0.146227\n",
      " 14482/50000: episode: 1910, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [2.000, 3.000],  loss: 0.001387, mae: 0.094599, mean_q: 0.130216\n",
      " 14484/50000: episode: 1911, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002020, mae: 0.097515, mean_q: 0.131803\n",
      " 14490/50000: episode: 1912, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001440, mae: 0.096940, mean_q: 0.132107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14496/50000: episode: 1913, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002074, mae: 0.101093, mean_q: 0.141243\n",
      " 14503/50000: episode: 1914, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.002985, mae: 0.095885, mean_q: 0.135646\n",
      " 14506/50000: episode: 1915, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001878, mae: 0.098535, mean_q: 0.137177\n",
      " 14516/50000: episode: 1916, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002335, mae: 0.101469, mean_q: 0.139845\n",
      " 14518/50000: episode: 1917, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002769, mae: 0.103373, mean_q: 0.141728\n",
      " 14527/50000: episode: 1918, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [1.000, 3.000],  loss: 0.002764, mae: 0.098038, mean_q: 0.135249\n",
      " 14539/50000: episode: 1919, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.083 [1.000, 3.000],  loss: 0.002545, mae: 0.101068, mean_q: 0.139133\n",
      " 14542/50000: episode: 1920, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003265, mae: 0.109557, mean_q: 0.146488\n",
      " 14547/50000: episode: 1921, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002335, mae: 0.100375, mean_q: 0.135611\n",
      " 14551/50000: episode: 1922, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001244, mae: 0.088309, mean_q: 0.121486\n",
      " 14566/50000: episode: 1923, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002191, mae: 0.099895, mean_q: 0.135895\n",
      " 14571/50000: episode: 1924, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002465, mae: 0.107853, mean_q: 0.151511\n",
      " 14579/50000: episode: 1925, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003289, mae: 0.105864, mean_q: 0.147646\n",
      " 14581/50000: episode: 1926, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001997, mae: 0.095633, mean_q: 0.135545\n",
      " 14586/50000: episode: 1927, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002875, mae: 0.099883, mean_q: 0.142101\n",
      " 14601/50000: episode: 1928, duration: 0.180s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003258, mae: 0.100406, mean_q: 0.140326\n",
      " 14612/50000: episode: 1929, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002411, mae: 0.104158, mean_q: 0.146611\n",
      " 14635/50000: episode: 1930, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.003126, mae: 0.110940, mean_q: 0.155432\n",
      " 14637/50000: episode: 1931, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002882, mae: 0.100379, mean_q: 0.140397\n",
      " 14659/50000: episode: 1932, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.002554, mae: 0.103727, mean_q: 0.144499\n",
      " 14666/50000: episode: 1933, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.002409, mae: 0.096630, mean_q: 0.134372\n",
      " 14692/50000: episode: 1934, duration: 0.302s, episode steps:  26, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.002879, mae: 0.103398, mean_q: 0.143063\n",
      " 14699/50000: episode: 1935, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.003735, mae: 0.116520, mean_q: 0.164147\n",
      " 14706/50000: episode: 1936, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.001947, mae: 0.100920, mean_q: 0.142151\n",
      " 14723/50000: episode: 1937, duration: 0.199s, episode steps:  17, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.003284, mae: 0.107665, mean_q: 0.148448\n",
      " 14726/50000: episode: 1938, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004445, mae: 0.113391, mean_q: 0.156248\n",
      " 14753/50000: episode: 1939, duration: 0.311s, episode steps:  27, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.003217, mae: 0.109021, mean_q: 0.152838\n",
      " 14763/50000: episode: 1940, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.003164, mae: 0.107267, mean_q: 0.150794\n",
      " 14765/50000: episode: 1941, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001774, mae: 0.100540, mean_q: 0.140520\n",
      " 14768/50000: episode: 1942, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001739, mae: 0.099320, mean_q: 0.135649\n",
      " 14778/50000: episode: 1943, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003496, mae: 0.110426, mean_q: 0.150966\n",
      " 14806/50000: episode: 1944, duration: 0.326s, episode steps:  28, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.003035, mae: 0.106249, mean_q: 0.146634\n",
      " 14815/50000: episode: 1945, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.003682, mae: 0.106150, mean_q: 0.141527\n",
      " 14842/50000: episode: 1946, duration: 0.323s, episode steps:  27, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.852 [0.000, 3.000],  loss: 0.002838, mae: 0.104444, mean_q: 0.143491\n",
      " 14852/50000: episode: 1947, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002667, mae: 0.107303, mean_q: 0.145332\n",
      " 14856/50000: episode: 1948, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002512, mae: 0.097178, mean_q: 0.130675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14859/50000: episode: 1949, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001944, mae: 0.099895, mean_q: 0.137111\n",
      " 14866/50000: episode: 1950, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002789, mae: 0.108818, mean_q: 0.151364\n",
      " 14873/50000: episode: 1951, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.001787, mae: 0.101959, mean_q: 0.144666\n",
      " 14880/50000: episode: 1952, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002109, mae: 0.099803, mean_q: 0.142415\n",
      " 14885/50000: episode: 1953, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.003705, mae: 0.109682, mean_q: 0.152301\n",
      " 14892/50000: episode: 1954, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.003391, mae: 0.102158, mean_q: 0.141684\n",
      " 14897/50000: episode: 1955, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003160, mae: 0.106074, mean_q: 0.146099\n",
      " 14909/50000: episode: 1956, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002015, mae: 0.098523, mean_q: 0.137006\n",
      " 14918/50000: episode: 1957, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [1.000, 2.000],  loss: 0.002066, mae: 0.101686, mean_q: 0.138935\n",
      " 14921/50000: episode: 1958, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004691, mae: 0.104520, mean_q: 0.143960\n",
      " 14947/50000: episode: 1959, duration: 0.300s, episode steps:  26, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002887, mae: 0.102280, mean_q: 0.142837\n",
      " 14952/50000: episode: 1960, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002616, mae: 0.096218, mean_q: 0.134819\n",
      " 14959/50000: episode: 1961, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002330, mae: 0.103922, mean_q: 0.143494\n",
      " 14963/50000: episode: 1962, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.004888, mae: 0.104673, mean_q: 0.138769\n",
      " 14966/50000: episode: 1963, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003149, mae: 0.102625, mean_q: 0.139862\n",
      " 14969/50000: episode: 1964, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004229, mae: 0.105702, mean_q: 0.140336\n",
      " 14974/50000: episode: 1965, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002758, mae: 0.096179, mean_q: 0.125237\n",
      " 14982/50000: episode: 1966, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002574, mae: 0.102304, mean_q: 0.135009\n",
      " 14984/50000: episode: 1967, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001466, mae: 0.103131, mean_q: 0.137260\n",
      " 14988/50000: episode: 1968, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001841, mae: 0.102428, mean_q: 0.135777\n",
      " 14994/50000: episode: 1969, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001186, mae: 0.094998, mean_q: 0.129063\n",
      " 15001/50000: episode: 1970, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002499, mae: 0.103861, mean_q: 0.138813\n",
      " 15010/50000: episode: 1971, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.778 [0.000, 3.000],  loss: 0.001518, mae: 0.096745, mean_q: 0.130930\n",
      " 15022/50000: episode: 1972, duration: 0.144s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002538, mae: 0.096852, mean_q: 0.132881\n",
      " 15026/50000: episode: 1973, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.001667, mae: 0.093704, mean_q: 0.128710\n",
      " 15028/50000: episode: 1974, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004511, mae: 0.109305, mean_q: 0.147000\n",
      " 15032/50000: episode: 1975, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001979, mae: 0.098320, mean_q: 0.135462\n",
      " 15050/50000: episode: 1976, duration: 0.215s, episode steps:  18, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002921, mae: 0.103517, mean_q: 0.139965\n",
      " 15063/50000: episode: 1977, duration: 0.154s, episode steps:  13, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002506, mae: 0.098218, mean_q: 0.132635\n",
      " 15073/50000: episode: 1978, duration: 0.126s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.002010, mae: 0.095368, mean_q: 0.128293\n",
      " 15075/50000: episode: 1979, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.000737, mae: 0.083420, mean_q: 0.114971\n",
      " 15086/50000: episode: 1980, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.002261, mae: 0.092524, mean_q: 0.127445\n",
      " 15114/50000: episode: 1981, duration: 0.323s, episode steps:  28, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.002656, mae: 0.098204, mean_q: 0.135798\n",
      " 15120/50000: episode: 1982, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001189, mae: 0.094799, mean_q: 0.132421\n",
      " 15127/50000: episode: 1983, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002531, mae: 0.100647, mean_q: 0.138466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15135/50000: episode: 1984, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002710, mae: 0.102123, mean_q: 0.140808\n",
      " 15157/50000: episode: 1985, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002755, mae: 0.095039, mean_q: 0.132903\n",
      " 15162/50000: episode: 1986, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001379, mae: 0.100170, mean_q: 0.139913\n",
      " 15171/50000: episode: 1987, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003308, mae: 0.109048, mean_q: 0.154515\n",
      " 15177/50000: episode: 1988, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001732, mae: 0.100486, mean_q: 0.143120\n",
      " 15194/50000: episode: 1989, duration: 0.199s, episode steps:  17, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.003452, mae: 0.101532, mean_q: 0.140651\n",
      " 15202/50000: episode: 1990, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002752, mae: 0.097800, mean_q: 0.133294\n",
      " 15206/50000: episode: 1991, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002948, mae: 0.102810, mean_q: 0.141278\n",
      " 15211/50000: episode: 1992, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002785, mae: 0.099789, mean_q: 0.141898\n",
      " 15215/50000: episode: 1993, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002263, mae: 0.095026, mean_q: 0.132462\n",
      " 15222/50000: episode: 1994, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002565, mae: 0.099755, mean_q: 0.139119\n",
      " 15224/50000: episode: 1995, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002851, mae: 0.096086, mean_q: 0.131880\n",
      " 15229/50000: episode: 1996, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002888, mae: 0.112576, mean_q: 0.159532\n",
      " 15238/50000: episode: 1997, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004251, mae: 0.098972, mean_q: 0.134882\n",
      " 15240/50000: episode: 1998, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001347, mae: 0.088178, mean_q: 0.123150\n",
      " 15243/50000: episode: 1999, duration: 0.050s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002126, mae: 0.087906, mean_q: 0.121905\n",
      " 15255/50000: episode: 2000, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002206, mae: 0.095020, mean_q: 0.132650\n",
      " 15266/50000: episode: 2001, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002341, mae: 0.095282, mean_q: 0.130949\n",
      " 15275/50000: episode: 2002, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003484, mae: 0.102479, mean_q: 0.140341\n",
      " 15292/50000: episode: 2003, duration: 0.203s, episode steps:  17, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002062, mae: 0.094729, mean_q: 0.133979\n",
      " 15304/50000: episode: 2004, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002393, mae: 0.100958, mean_q: 0.139740\n",
      " 15306/50000: episode: 2005, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003506, mae: 0.098369, mean_q: 0.137248\n",
      " 15336/50000: episode: 2006, duration: 0.354s, episode steps:  30, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002247, mae: 0.097977, mean_q: 0.138474\n",
      " 15341/50000: episode: 2007, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002098, mae: 0.085321, mean_q: 0.119958\n",
      " 15347/50000: episode: 2008, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.004648, mae: 0.100146, mean_q: 0.136309\n",
      " 15368/50000: episode: 2009, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001866, mae: 0.096734, mean_q: 0.134216\n",
      " 15373/50000: episode: 2010, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002260, mae: 0.094113, mean_q: 0.126891\n",
      " 15376/50000: episode: 2011, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.005076, mae: 0.108157, mean_q: 0.143336\n",
      " 15381/50000: episode: 2012, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.005321, mae: 0.111859, mean_q: 0.147266\n",
      " 15404/50000: episode: 2013, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.002180, mae: 0.093526, mean_q: 0.129511\n",
      " 15412/50000: episode: 2014, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003334, mae: 0.095965, mean_q: 0.134882\n",
      " 15427/50000: episode: 2015, duration: 0.180s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002525, mae: 0.096429, mean_q: 0.133874\n",
      " 15429/50000: episode: 2016, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002499, mae: 0.098252, mean_q: 0.135801\n",
      " 15432/50000: episode: 2017, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002497, mae: 0.094016, mean_q: 0.130450\n",
      " 15438/50000: episode: 2018, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002132, mae: 0.088830, mean_q: 0.121021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15442/50000: episode: 2019, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001664, mae: 0.084742, mean_q: 0.118147\n",
      " 15454/50000: episode: 2020, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002526, mae: 0.093063, mean_q: 0.129306\n",
      " 15457/50000: episode: 2021, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003307, mae: 0.092665, mean_q: 0.126960\n",
      " 15479/50000: episode: 2022, duration: 0.258s, episode steps:  22, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002080, mae: 0.094090, mean_q: 0.130111\n",
      " 15491/50000: episode: 2023, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001360, mae: 0.088255, mean_q: 0.122933\n",
      " 15495/50000: episode: 2024, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002922, mae: 0.092555, mean_q: 0.126352\n",
      " 15504/50000: episode: 2025, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003008, mae: 0.099058, mean_q: 0.136799\n",
      " 15510/50000: episode: 2026, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.001980, mae: 0.095338, mean_q: 0.130224\n",
      " 15515/50000: episode: 2027, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002187, mae: 0.091847, mean_q: 0.123510\n",
      " 15524/50000: episode: 2028, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002959, mae: 0.095522, mean_q: 0.128548\n",
      " 15532/50000: episode: 2029, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002055, mae: 0.097254, mean_q: 0.137391\n",
      " 15539/50000: episode: 2030, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001850, mae: 0.092293, mean_q: 0.129808\n",
      " 15549/50000: episode: 2031, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.002764, mae: 0.098732, mean_q: 0.137340\n",
      " 15554/50000: episode: 2032, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.002184, mae: 0.105742, mean_q: 0.149709\n",
      " 15557/50000: episode: 2033, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003461, mae: 0.098336, mean_q: 0.138916\n",
      " 15568/50000: episode: 2034, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002050, mae: 0.097856, mean_q: 0.136381\n",
      " 15570/50000: episode: 2035, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004800, mae: 0.105148, mean_q: 0.147556\n",
      " 15577/50000: episode: 2036, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001617, mae: 0.088241, mean_q: 0.122301\n",
      " 15582/50000: episode: 2037, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002253, mae: 0.089653, mean_q: 0.123222\n",
      " 15601/50000: episode: 2038, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.002620, mae: 0.101365, mean_q: 0.139759\n",
      " 15612/50000: episode: 2039, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002579, mae: 0.093411, mean_q: 0.127927\n",
      " 15620/50000: episode: 2040, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002564, mae: 0.096677, mean_q: 0.132473\n",
      " 15625/50000: episode: 2041, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002183, mae: 0.093626, mean_q: 0.127664\n",
      " 15643/50000: episode: 2042, duration: 0.217s, episode steps:  18, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002078, mae: 0.100946, mean_q: 0.141590\n",
      " 15656/50000: episode: 2043, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002592, mae: 0.089364, mean_q: 0.125160\n",
      " 15665/50000: episode: 2044, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001987, mae: 0.092457, mean_q: 0.128804\n",
      " 15669/50000: episode: 2045, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001982, mae: 0.091590, mean_q: 0.128238\n",
      " 15676/50000: episode: 2046, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001349, mae: 0.084933, mean_q: 0.119453\n",
      " 15683/50000: episode: 2047, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.003063, mae: 0.095116, mean_q: 0.129151\n",
      " 15686/50000: episode: 2048, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001726, mae: 0.091594, mean_q: 0.127054\n",
      " 15688/50000: episode: 2049, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002057, mae: 0.092981, mean_q: 0.128389\n",
      " 15692/50000: episode: 2050, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003347, mae: 0.101115, mean_q: 0.136521\n",
      " 15694/50000: episode: 2051, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001474, mae: 0.091004, mean_q: 0.125801\n",
      " 15701/50000: episode: 2052, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.004637, mae: 0.104007, mean_q: 0.142335\n",
      " 15707/50000: episode: 2053, duration: 0.103s, episode steps:   6, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003169, mae: 0.099380, mean_q: 0.135201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15718/50000: episode: 2054, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002224, mae: 0.093389, mean_q: 0.129413\n",
      " 15725/50000: episode: 2055, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003532, mae: 0.097909, mean_q: 0.134654\n",
      " 15727/50000: episode: 2056, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004023, mae: 0.096667, mean_q: 0.131354\n",
      " 15735/50000: episode: 2057, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002480, mae: 0.096337, mean_q: 0.134335\n",
      " 15740/50000: episode: 2058, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001497, mae: 0.092457, mean_q: 0.128202\n",
      " 15755/50000: episode: 2059, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001626, mae: 0.093227, mean_q: 0.130825\n",
      " 15762/50000: episode: 2060, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.001440, mae: 0.096017, mean_q: 0.132707\n",
      " 15766/50000: episode: 2061, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003594, mae: 0.099944, mean_q: 0.133837\n",
      " 15773/50000: episode: 2062, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001790, mae: 0.095863, mean_q: 0.131894\n",
      " 15780/50000: episode: 2063, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [1.000, 3.000],  loss: 0.002946, mae: 0.104908, mean_q: 0.144282\n",
      " 15803/50000: episode: 2064, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002230, mae: 0.091521, mean_q: 0.125973\n",
      " 15807/50000: episode: 2065, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.002634, mae: 0.096986, mean_q: 0.134731\n",
      " 15831/50000: episode: 2066, duration: 0.325s, episode steps:  24, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002353, mae: 0.096803, mean_q: 0.135171\n",
      " 15833/50000: episode: 2067, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002345, mae: 0.101851, mean_q: 0.140764\n",
      " 15838/50000: episode: 2068, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003139, mae: 0.111475, mean_q: 0.153795\n",
      " 15840/50000: episode: 2069, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002634, mae: 0.094703, mean_q: 0.129198\n",
      " 15854/50000: episode: 2070, duration: 0.191s, episode steps:  14, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002208, mae: 0.094213, mean_q: 0.127214\n",
      " 15867/50000: episode: 2071, duration: 0.226s, episode steps:  13, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.001917, mae: 0.092213, mean_q: 0.125212\n",
      " 15871/50000: episode: 2072, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001122, mae: 0.089859, mean_q: 0.124162\n",
      " 15877/50000: episode: 2073, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.001960, mae: 0.089186, mean_q: 0.122635\n",
      " 15890/50000: episode: 2074, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.001716, mae: 0.092291, mean_q: 0.127476\n",
      " 15918/50000: episode: 2075, duration: 0.353s, episode steps:  28, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.002922, mae: 0.096498, mean_q: 0.129570\n",
      " 15933/50000: episode: 2076, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002817, mae: 0.096455, mean_q: 0.131869\n",
      " 15937/50000: episode: 2077, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001686, mae: 0.094992, mean_q: 0.134642\n",
      " 15946/50000: episode: 2078, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001580, mae: 0.093859, mean_q: 0.130068\n",
      " 15953/50000: episode: 2079, duration: 0.126s, episode steps:   7, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002265, mae: 0.088489, mean_q: 0.120996\n",
      " 15974/50000: episode: 2080, duration: 0.300s, episode steps:  21, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002481, mae: 0.094546, mean_q: 0.131601\n",
      " 15979/50000: episode: 2081, duration: 0.095s, episode steps:   5, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002197, mae: 0.093203, mean_q: 0.132919\n",
      " 15981/50000: episode: 2082, duration: 0.048s, episode steps:   2, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001853, mae: 0.094836, mean_q: 0.138719\n",
      " 15989/50000: episode: 2083, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002320, mae: 0.094883, mean_q: 0.137477\n",
      " 15996/50000: episode: 2084, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001789, mae: 0.092284, mean_q: 0.131719\n",
      " 15999/50000: episode: 2085, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003019, mae: 0.101594, mean_q: 0.140495\n",
      " 16001/50000: episode: 2086, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002628, mae: 0.096981, mean_q: 0.130498\n",
      " 16007/50000: episode: 2087, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003248, mae: 0.102393, mean_q: 0.141742\n",
      " 16018/50000: episode: 2088, duration: 0.182s, episode steps:  11, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [1.000, 3.000],  loss: 0.004407, mae: 0.096601, mean_q: 0.131661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16020/50000: episode: 2089, duration: 0.051s, episode steps:   2, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003057, mae: 0.105208, mean_q: 0.146825\n",
      " 16040/50000: episode: 2090, duration: 0.255s, episode steps:  20, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001975, mae: 0.097776, mean_q: 0.132948\n",
      " 16045/50000: episode: 2091, duration: 0.100s, episode steps:   5, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001848, mae: 0.088655, mean_q: 0.119768\n",
      " 16048/50000: episode: 2092, duration: 0.067s, episode steps:   3, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003158, mae: 0.093688, mean_q: 0.125230\n",
      " 16052/50000: episode: 2093, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002780, mae: 0.096753, mean_q: 0.130361\n",
      " 16061/50000: episode: 2094, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002175, mae: 0.092665, mean_q: 0.127057\n",
      " 16069/50000: episode: 2095, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.625 [0.000, 3.000],  loss: 0.002690, mae: 0.099640, mean_q: 0.136352\n",
      " 16083/50000: episode: 2096, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.002234, mae: 0.100086, mean_q: 0.137984\n",
      " 16091/50000: episode: 2097, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003717, mae: 0.107081, mean_q: 0.146335\n",
      " 16097/50000: episode: 2098, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.003048, mae: 0.100139, mean_q: 0.140482\n",
      " 16099/50000: episode: 2099, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002340, mae: 0.093191, mean_q: 0.132033\n",
      " 16102/50000: episode: 2100, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.005076, mae: 0.093092, mean_q: 0.124248\n",
      " 16104/50000: episode: 2101, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003911, mae: 0.105328, mean_q: 0.145801\n",
      " 16109/50000: episode: 2102, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001734, mae: 0.085902, mean_q: 0.120434\n",
      " 16125/50000: episode: 2103, duration: 0.268s, episode steps:  16, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001889, mae: 0.100828, mean_q: 0.137453\n",
      " 16134/50000: episode: 2104, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003073, mae: 0.095843, mean_q: 0.129305\n",
      " 16136/50000: episode: 2105, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003513, mae: 0.101677, mean_q: 0.135435\n",
      " 16149/50000: episode: 2106, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.003215, mae: 0.101920, mean_q: 0.139999\n",
      " 16156/50000: episode: 2107, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001751, mae: 0.098267, mean_q: 0.145309\n",
      " 16166/50000: episode: 2108, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.002394, mae: 0.107547, mean_q: 0.156323\n",
      " 16173/50000: episode: 2109, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002773, mae: 0.096847, mean_q: 0.136975\n",
      " 16176/50000: episode: 2110, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001427, mae: 0.084137, mean_q: 0.119830\n",
      " 16187/50000: episode: 2111, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002514, mae: 0.093888, mean_q: 0.129629\n",
      " 16199/50000: episode: 2112, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002875, mae: 0.100380, mean_q: 0.136053\n",
      " 16217/50000: episode: 2113, duration: 0.272s, episode steps:  18, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002164, mae: 0.091287, mean_q: 0.123075\n",
      " 16219/50000: episode: 2114, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002133, mae: 0.094400, mean_q: 0.126783\n",
      " 16225/50000: episode: 2115, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002066, mae: 0.107320, mean_q: 0.142757\n",
      " 16233/50000: episode: 2116, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001580, mae: 0.089295, mean_q: 0.119625\n",
      " 16235/50000: episode: 2117, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001982, mae: 0.081883, mean_q: 0.109107\n",
      " 16240/50000: episode: 2118, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 1.000],  loss: 0.003980, mae: 0.100536, mean_q: 0.134018\n",
      " 16244/50000: episode: 2119, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003396, mae: 0.107546, mean_q: 0.144253\n",
      " 16255/50000: episode: 2120, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.003399, mae: 0.109584, mean_q: 0.146813\n",
      " 16257/50000: episode: 2121, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002247, mae: 0.097696, mean_q: 0.132014\n",
      " 16262/50000: episode: 2122, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.002270, mae: 0.095652, mean_q: 0.132548\n",
      " 16299/50000: episode: 2123, duration: 0.520s, episode steps:  37, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.027 [0.000, 3.000],  loss: 0.002338, mae: 0.094800, mean_q: 0.130668\n",
      " 16308/50000: episode: 2124, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002022, mae: 0.095396, mean_q: 0.130640\n",
      " 16313/50000: episode: 2125, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002633, mae: 0.089107, mean_q: 0.119722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16317/50000: episode: 2126, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.000920, mae: 0.089175, mean_q: 0.124355\n",
      " 16322/50000: episode: 2127, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002342, mae: 0.094043, mean_q: 0.127227\n",
      " 16325/50000: episode: 2128, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002760, mae: 0.095841, mean_q: 0.127051\n",
      " 16328/50000: episode: 2129, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002929, mae: 0.106981, mean_q: 0.144637\n",
      " 16331/50000: episode: 2130, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.002056, mae: 0.099468, mean_q: 0.136815\n",
      " 16342/50000: episode: 2131, duration: 0.140s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [1.000, 3.000],  loss: 0.001947, mae: 0.096612, mean_q: 0.132102\n",
      " 16346/50000: episode: 2132, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.001636, mae: 0.088501, mean_q: 0.122660\n",
      " 16355/50000: episode: 2133, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.003202, mae: 0.095702, mean_q: 0.131454\n",
      " 16361/50000: episode: 2134, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 3.000],  loss: 0.002651, mae: 0.101459, mean_q: 0.140668\n",
      " 16365/50000: episode: 2135, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002606, mae: 0.093047, mean_q: 0.129879\n",
      " 16384/50000: episode: 2136, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.002291, mae: 0.100961, mean_q: 0.138327\n",
      " 16391/50000: episode: 2137, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002664, mae: 0.096164, mean_q: 0.132849\n",
      " 16401/50000: episode: 2138, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003340, mae: 0.099030, mean_q: 0.138026\n",
      " 16416/50000: episode: 2139, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002862, mae: 0.095141, mean_q: 0.134041\n",
      " 16422/50000: episode: 2140, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003128, mae: 0.097003, mean_q: 0.139658\n",
      " 16433/50000: episode: 2141, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002086, mae: 0.094283, mean_q: 0.135320\n",
      " 16438/50000: episode: 2142, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001962, mae: 0.095146, mean_q: 0.134803\n",
      " 16445/50000: episode: 2143, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.429 [0.000, 2.000],  loss: 0.002984, mae: 0.096316, mean_q: 0.132995\n",
      " 16452/50000: episode: 2144, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003326, mae: 0.096311, mean_q: 0.132765\n",
      " 16458/50000: episode: 2145, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.003154, mae: 0.116264, mean_q: 0.157427\n",
      " 16464/50000: episode: 2146, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002680, mae: 0.110768, mean_q: 0.148964\n",
      " 16467/50000: episode: 2147, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003799, mae: 0.113861, mean_q: 0.156704\n",
      " 16473/50000: episode: 2148, duration: 0.077s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002630, mae: 0.090299, mean_q: 0.128082\n",
      " 16482/50000: episode: 2149, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002247, mae: 0.097945, mean_q: 0.134740\n",
      " 16486/50000: episode: 2150, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002514, mae: 0.104425, mean_q: 0.143231\n",
      " 16496/50000: episode: 2151, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002967, mae: 0.103201, mean_q: 0.142277\n",
      " 16507/50000: episode: 2152, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002784, mae: 0.090266, mean_q: 0.124010\n",
      " 16513/50000: episode: 2153, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002480, mae: 0.095306, mean_q: 0.134153\n",
      " 16516/50000: episode: 2154, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002534, mae: 0.098270, mean_q: 0.135802\n",
      " 16528/50000: episode: 2155, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.003053, mae: 0.102344, mean_q: 0.140425\n",
      " 16548/50000: episode: 2156, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003189, mae: 0.102442, mean_q: 0.142730\n",
      " 16560/50000: episode: 2157, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002783, mae: 0.098759, mean_q: 0.141064\n",
      " 16565/50000: episode: 2158, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002700, mae: 0.095482, mean_q: 0.132765\n",
      " 16575/50000: episode: 2159, duration: 0.138s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003236, mae: 0.094649, mean_q: 0.127711\n",
      " 16578/50000: episode: 2160, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001708, mae: 0.097750, mean_q: 0.133006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16582/50000: episode: 2161, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002914, mae: 0.100817, mean_q: 0.135178\n",
      " 16591/50000: episode: 2162, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003048, mae: 0.100382, mean_q: 0.137091\n",
      " 16601/50000: episode: 2163, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002419, mae: 0.092473, mean_q: 0.129549\n",
      " 16608/50000: episode: 2164, duration: 0.102s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003989, mae: 0.108596, mean_q: 0.151099\n",
      " 16615/50000: episode: 2165, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 2.000],  loss: 0.002877, mae: 0.103716, mean_q: 0.141803\n",
      " 16617/50000: episode: 2166, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001123, mae: 0.091907, mean_q: 0.129930\n",
      " 16627/50000: episode: 2167, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002018, mae: 0.094206, mean_q: 0.131857\n",
      " 16634/50000: episode: 2168, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.004079, mae: 0.116310, mean_q: 0.156728\n",
      " 16640/50000: episode: 2169, duration: 0.118s, episode steps:   6, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003255, mae: 0.119226, mean_q: 0.162142\n",
      " 16642/50000: episode: 2170, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003158, mae: 0.102963, mean_q: 0.137601\n",
      " 16644/50000: episode: 2171, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003833, mae: 0.101902, mean_q: 0.134241\n",
      " 16648/50000: episode: 2172, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003122, mae: 0.109546, mean_q: 0.149859\n",
      " 16683/50000: episode: 2173, duration: 0.412s, episode steps:  35, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002225, mae: 0.099782, mean_q: 0.136105\n",
      " 16689/50000: episode: 2174, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001821, mae: 0.097564, mean_q: 0.135093\n",
      " 16694/50000: episode: 2175, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002625, mae: 0.107068, mean_q: 0.144914\n",
      " 16704/50000: episode: 2176, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002792, mae: 0.096839, mean_q: 0.132714\n",
      " 16708/50000: episode: 2177, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002300, mae: 0.103916, mean_q: 0.142674\n",
      " 16733/50000: episode: 2178, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.160 [0.000, 3.000],  loss: 0.002508, mae: 0.096016, mean_q: 0.130436\n",
      " 16737/50000: episode: 2179, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002949, mae: 0.100935, mean_q: 0.137373\n",
      " 16746/50000: episode: 2180, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002834, mae: 0.103230, mean_q: 0.139856\n",
      " 16759/50000: episode: 2181, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002565, mae: 0.107004, mean_q: 0.147909\n",
      " 16771/50000: episode: 2182, duration: 0.144s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.003821, mae: 0.106590, mean_q: 0.146608\n",
      " 16784/50000: episode: 2183, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003619, mae: 0.106796, mean_q: 0.144755\n",
      " 16786/50000: episode: 2184, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.006127, mae: 0.112109, mean_q: 0.145162\n",
      " 16789/50000: episode: 2185, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.001978, mae: 0.102918, mean_q: 0.139486\n",
      " 16793/50000: episode: 2186, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002198, mae: 0.094436, mean_q: 0.127987\n",
      " 16795/50000: episode: 2187, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003232, mae: 0.094387, mean_q: 0.123777\n",
      " 16804/50000: episode: 2188, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.002272, mae: 0.095616, mean_q: 0.129413\n",
      " 16810/50000: episode: 2189, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003833, mae: 0.099635, mean_q: 0.131421\n",
      " 16816/50000: episode: 2190, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001977, mae: 0.101659, mean_q: 0.134894\n",
      " 16820/50000: episode: 2191, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001911, mae: 0.100118, mean_q: 0.134451\n",
      " 16823/50000: episode: 2192, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001889, mae: 0.093758, mean_q: 0.126327\n",
      " 16831/50000: episode: 2193, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001512, mae: 0.097374, mean_q: 0.132025\n",
      " 16840/50000: episode: 2194, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002340, mae: 0.096898, mean_q: 0.129380\n",
      " 16848/50000: episode: 2195, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002340, mae: 0.101102, mean_q: 0.135507\n",
      " 16852/50000: episode: 2196, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002679, mae: 0.103896, mean_q: 0.138645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16856/50000: episode: 2197, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003467, mae: 0.105738, mean_q: 0.140800\n",
      " 16867/50000: episode: 2198, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002401, mae: 0.100471, mean_q: 0.134186\n",
      " 16870/50000: episode: 2199, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001620, mae: 0.099657, mean_q: 0.135942\n",
      " 16882/50000: episode: 2200, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002430, mae: 0.097060, mean_q: 0.132384\n",
      " 16886/50000: episode: 2201, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002851, mae: 0.106238, mean_q: 0.145787\n",
      " 16892/50000: episode: 2202, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003488, mae: 0.114624, mean_q: 0.154993\n",
      " 16897/50000: episode: 2203, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.005553, mae: 0.105483, mean_q: 0.138528\n",
      " 16901/50000: episode: 2204, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.004385, mae: 0.108319, mean_q: 0.142928\n",
      " 16904/50000: episode: 2205, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001689, mae: 0.101641, mean_q: 0.139540\n",
      " 16907/50000: episode: 2206, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001537, mae: 0.097937, mean_q: 0.135765\n",
      " 16912/50000: episode: 2207, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002674, mae: 0.095972, mean_q: 0.130426\n",
      " 16918/50000: episode: 2208, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003234, mae: 0.094900, mean_q: 0.126083\n",
      " 16923/50000: episode: 2209, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.001956, mae: 0.091405, mean_q: 0.124255\n",
      " 16931/50000: episode: 2210, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002303, mae: 0.098783, mean_q: 0.134475\n",
      " 16942/50000: episode: 2211, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.003849, mae: 0.099802, mean_q: 0.135528\n",
      " 16955/50000: episode: 2212, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002077, mae: 0.098944, mean_q: 0.134968\n",
      " 16983/50000: episode: 2213, duration: 0.324s, episode steps:  28, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.107 [0.000, 3.000],  loss: 0.002995, mae: 0.094492, mean_q: 0.127699\n",
      " 16985/50000: episode: 2214, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002185, mae: 0.083631, mean_q: 0.114593\n",
      " 16996/50000: episode: 2215, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.001707, mae: 0.093309, mean_q: 0.127626\n",
      " 17004/50000: episode: 2216, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001807, mae: 0.093049, mean_q: 0.127335\n",
      " 17017/50000: episode: 2217, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002539, mae: 0.097072, mean_q: 0.134413\n",
      " 17023/50000: episode: 2218, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003696, mae: 0.102123, mean_q: 0.139415\n",
      " 17035/50000: episode: 2219, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002019, mae: 0.095947, mean_q: 0.132868\n",
      " 17037/50000: episode: 2220, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002444, mae: 0.095018, mean_q: 0.125773\n",
      " 17055/50000: episode: 2221, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002414, mae: 0.098835, mean_q: 0.135740\n",
      " 17072/50000: episode: 2222, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002645, mae: 0.098726, mean_q: 0.136381\n",
      " 17084/50000: episode: 2223, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002498, mae: 0.094656, mean_q: 0.129523\n",
      " 17088/50000: episode: 2224, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002441, mae: 0.097062, mean_q: 0.133554\n",
      " 17091/50000: episode: 2225, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001900, mae: 0.093025, mean_q: 0.128265\n",
      " 17102/50000: episode: 2226, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002812, mae: 0.096075, mean_q: 0.131434\n",
      " 17105/50000: episode: 2227, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001653, mae: 0.095860, mean_q: 0.132838\n",
      " 17107/50000: episode: 2228, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002673, mae: 0.097850, mean_q: 0.134885\n",
      " 17114/50000: episode: 2229, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002587, mae: 0.102510, mean_q: 0.141604\n",
      " 17116/50000: episode: 2230, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001977, mae: 0.110284, mean_q: 0.154869\n",
      " 17125/50000: episode: 2231, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 3.000],  loss: 0.001934, mae: 0.094606, mean_q: 0.129240\n",
      " 17138/50000: episode: 2232, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002122, mae: 0.095786, mean_q: 0.131906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17142/50000: episode: 2233, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001930, mae: 0.098779, mean_q: 0.139093\n",
      " 17147/50000: episode: 2234, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002400, mae: 0.092578, mean_q: 0.130068\n",
      " 17150/50000: episode: 2235, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001558, mae: 0.102484, mean_q: 0.142753\n",
      " 17158/50000: episode: 2236, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001320, mae: 0.084447, mean_q: 0.116662\n",
      " 17161/50000: episode: 2237, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004642, mae: 0.096422, mean_q: 0.132362\n",
      " 17164/50000: episode: 2238, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004065, mae: 0.102079, mean_q: 0.139194\n",
      " 17167/50000: episode: 2239, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002657, mae: 0.092924, mean_q: 0.128016\n",
      " 17174/50000: episode: 2240, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [0.000, 3.000],  loss: 0.002073, mae: 0.095312, mean_q: 0.134015\n",
      " 17183/50000: episode: 2241, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.001500, mae: 0.090244, mean_q: 0.126887\n",
      " 17192/50000: episode: 2242, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003786, mae: 0.095716, mean_q: 0.132764\n",
      " 17203/50000: episode: 2243, duration: 0.140s, episode steps:  11, steps per second:  79, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002972, mae: 0.095657, mean_q: 0.134838\n",
      " 17211/50000: episode: 2244, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 2.000],  loss: 0.001988, mae: 0.089714, mean_q: 0.127184\n",
      " 17217/50000: episode: 2245, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002640, mae: 0.088945, mean_q: 0.126216\n",
      " 17219/50000: episode: 2246, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.005772, mae: 0.100856, mean_q: 0.137690\n",
      " 17223/50000: episode: 2247, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001607, mae: 0.100121, mean_q: 0.140698\n",
      " 17234/50000: episode: 2248, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002697, mae: 0.095586, mean_q: 0.131364\n",
      " 17239/50000: episode: 2249, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002005, mae: 0.088657, mean_q: 0.122619\n",
      " 17241/50000: episode: 2250, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.000853, mae: 0.084522, mean_q: 0.117964\n",
      " 17244/50000: episode: 2251, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001744, mae: 0.089846, mean_q: 0.123598\n",
      " 17264/50000: episode: 2252, duration: 0.236s, episode steps:  20, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002199, mae: 0.094312, mean_q: 0.126802\n",
      " 17266/50000: episode: 2253, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001311, mae: 0.086751, mean_q: 0.117220\n",
      " 17273/50000: episode: 2254, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001817, mae: 0.095650, mean_q: 0.130905\n",
      " 17275/50000: episode: 2255, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.005747, mae: 0.103761, mean_q: 0.137078\n",
      " 17278/50000: episode: 2256, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002056, mae: 0.093365, mean_q: 0.127118\n",
      " 17300/50000: episode: 2257, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002023, mae: 0.093688, mean_q: 0.130116\n",
      " 17308/50000: episode: 2258, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003014, mae: 0.095220, mean_q: 0.128046\n",
      " 17315/50000: episode: 2259, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [0.000, 3.000],  loss: 0.003096, mae: 0.091003, mean_q: 0.121349\n",
      " 17333/50000: episode: 2260, duration: 0.221s, episode steps:  18, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [0.000, 3.000],  loss: 0.002812, mae: 0.090571, mean_q: 0.123822\n",
      " 17335/50000: episode: 2261, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002310, mae: 0.086943, mean_q: 0.121875\n",
      " 17346/50000: episode: 2262, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.003246, mae: 0.087321, mean_q: 0.123750\n",
      " 17354/50000: episode: 2263, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.000977, mae: 0.088712, mean_q: 0.125849\n",
      " 17358/50000: episode: 2264, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002954, mae: 0.084387, mean_q: 0.116826\n",
      " 17361/50000: episode: 2265, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002652, mae: 0.083650, mean_q: 0.116768\n",
      " 17367/50000: episode: 2266, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.001199, mae: 0.087190, mean_q: 0.120533\n",
      " 17369/50000: episode: 2267, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004206, mae: 0.110990, mean_q: 0.149264\n",
      " 17379/50000: episode: 2268, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001507, mae: 0.093508, mean_q: 0.128851\n",
      " 17381/50000: episode: 2269, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002291, mae: 0.087294, mean_q: 0.116832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17393/50000: episode: 2270, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001879, mae: 0.089617, mean_q: 0.120246\n",
      " 17396/50000: episode: 2271, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002733, mae: 0.088004, mean_q: 0.117677\n",
      " 17401/50000: episode: 2272, duration: 0.066s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.000950, mae: 0.084904, mean_q: 0.114891\n",
      " 17403/50000: episode: 2273, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.006084, mae: 0.094393, mean_q: 0.120777\n",
      " 17417/50000: episode: 2274, duration: 0.167s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.001991, mae: 0.086593, mean_q: 0.116658\n",
      " 17421/50000: episode: 2275, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001632, mae: 0.089327, mean_q: 0.123329\n",
      " 17442/50000: episode: 2276, duration: 0.245s, episode steps:  21, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001615, mae: 0.088225, mean_q: 0.121874\n",
      " 17450/50000: episode: 2277, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002084, mae: 0.089280, mean_q: 0.121906\n",
      " 17454/50000: episode: 2278, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.001589, mae: 0.086445, mean_q: 0.119045\n",
      " 17466/50000: episode: 2279, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001913, mae: 0.085219, mean_q: 0.117884\n",
      " 17479/50000: episode: 2280, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.001771, mae: 0.086805, mean_q: 0.117762\n",
      " 17487/50000: episode: 2281, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001850, mae: 0.086977, mean_q: 0.119154\n",
      " 17504/50000: episode: 2282, duration: 0.200s, episode steps:  17, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [1.000, 3.000],  loss: 0.001636, mae: 0.085838, mean_q: 0.118969\n",
      " 17506/50000: episode: 2283, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005315, mae: 0.104692, mean_q: 0.138908\n",
      " 17511/50000: episode: 2284, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003232, mae: 0.107049, mean_q: 0.142209\n",
      " 17525/50000: episode: 2285, duration: 0.170s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.001374, mae: 0.078735, mean_q: 0.109582\n",
      " 17530/50000: episode: 2286, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002471, mae: 0.087041, mean_q: 0.121834\n",
      " 17533/50000: episode: 2287, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001596, mae: 0.092210, mean_q: 0.125036\n",
      " 17537/50000: episode: 2288, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002494, mae: 0.094219, mean_q: 0.128470\n",
      " 17546/50000: episode: 2289, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002135, mae: 0.088896, mean_q: 0.119869\n",
      " 17549/50000: episode: 2290, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001820, mae: 0.083003, mean_q: 0.111929\n",
      " 17553/50000: episode: 2291, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002383, mae: 0.090240, mean_q: 0.126047\n",
      " 17562/50000: episode: 2292, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002205, mae: 0.091753, mean_q: 0.127617\n",
      " 17565/50000: episode: 2293, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002738, mae: 0.090051, mean_q: 0.122094\n",
      " 17570/50000: episode: 2294, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003398, mae: 0.094741, mean_q: 0.126646\n",
      " 17582/50000: episode: 2295, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001933, mae: 0.089354, mean_q: 0.126730\n",
      " 17586/50000: episode: 2296, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001006, mae: 0.081709, mean_q: 0.119252\n",
      " 17591/50000: episode: 2297, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000949, mae: 0.083450, mean_q: 0.120405\n",
      " 17594/50000: episode: 2298, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003338, mae: 0.091753, mean_q: 0.124905\n",
      " 17596/50000: episode: 2299, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001098, mae: 0.095470, mean_q: 0.133856\n",
      " 17605/50000: episode: 2300, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.001446, mae: 0.091723, mean_q: 0.126498\n",
      " 17622/50000: episode: 2301, duration: 0.241s, episode steps:  17, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001955, mae: 0.085980, mean_q: 0.116017\n",
      " 17659/50000: episode: 2302, duration: 0.517s, episode steps:  37, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002040, mae: 0.087689, mean_q: 0.121003\n",
      " 17661/50000: episode: 2303, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002213, mae: 0.092717, mean_q: 0.123412\n",
      " 17680/50000: episode: 2304, duration: 0.386s, episode steps:  19, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002184, mae: 0.085424, mean_q: 0.118605\n",
      " 17688/50000: episode: 2305, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002540, mae: 0.083173, mean_q: 0.118873\n",
      " 17690/50000: episode: 2306, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003342, mae: 0.096379, mean_q: 0.140575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17707/50000: episode: 2307, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.118 [0.000, 3.000],  loss: 0.002594, mae: 0.088284, mean_q: 0.125716\n",
      " 17719/50000: episode: 2308, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001906, mae: 0.087953, mean_q: 0.125259\n",
      " 17723/50000: episode: 2309, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001635, mae: 0.087230, mean_q: 0.125587\n",
      " 17726/50000: episode: 2310, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001541, mae: 0.087361, mean_q: 0.122089\n",
      " 17743/50000: episode: 2311, duration: 0.227s, episode steps:  17, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.001496, mae: 0.082636, mean_q: 0.114936\n",
      " 17752/50000: episode: 2312, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003478, mae: 0.089407, mean_q: 0.122197\n",
      " 17763/50000: episode: 2313, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002370, mae: 0.088058, mean_q: 0.122023\n",
      " 17774/50000: episode: 2314, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.001674, mae: 0.084339, mean_q: 0.116323\n",
      " 17778/50000: episode: 2315, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.003258, mae: 0.085777, mean_q: 0.118146\n",
      " 17780/50000: episode: 2316, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002728, mae: 0.085709, mean_q: 0.116443\n",
      " 17788/50000: episode: 2317, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.003314, mae: 0.087419, mean_q: 0.119235\n",
      " 17792/50000: episode: 2318, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002164, mae: 0.086687, mean_q: 0.118052\n",
      " 17811/50000: episode: 2319, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward:  1.000, mean reward:  0.053 [ 0.000,  1.000], mean action: 1.895 [0.000, 3.000],  loss: 0.002187, mae: 0.085533, mean_q: 0.117064\n",
      " 17816/50000: episode: 2320, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 2.000],  loss: 0.001501, mae: 0.080973, mean_q: 0.112869\n",
      " 17819/50000: episode: 2321, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002304, mae: 0.084085, mean_q: 0.115514\n",
      " 17823/50000: episode: 2322, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.002579, mae: 0.089745, mean_q: 0.124972\n",
      " 17837/50000: episode: 2323, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.001784, mae: 0.082699, mean_q: 0.115538\n",
      " 17843/50000: episode: 2324, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003254, mae: 0.082315, mean_q: 0.113675\n",
      " 17846/50000: episode: 2325, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.000957, mae: 0.086607, mean_q: 0.122848\n",
      " 17857/50000: episode: 2326, duration: 0.163s, episode steps:  11, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002109, mae: 0.089509, mean_q: 0.123786\n",
      " 17875/50000: episode: 2327, duration: 0.237s, episode steps:  18, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001629, mae: 0.085465, mean_q: 0.117398\n",
      " 17892/50000: episode: 2328, duration: 0.235s, episode steps:  17, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002870, mae: 0.091038, mean_q: 0.127205\n",
      " 17895/50000: episode: 2329, duration: 0.053s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002919, mae: 0.084313, mean_q: 0.116348\n",
      " 17899/50000: episode: 2330, duration: 0.069s, episode steps:   4, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002128, mae: 0.086794, mean_q: 0.119243\n",
      " 17917/50000: episode: 2331, duration: 0.254s, episode steps:  18, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [0.000, 3.000],  loss: 0.002437, mae: 0.085145, mean_q: 0.117752\n",
      " 17921/50000: episode: 2332, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001850, mae: 0.080635, mean_q: 0.110511\n",
      " 17930/50000: episode: 2333, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003010, mae: 0.089115, mean_q: 0.121782\n",
      " 17936/50000: episode: 2334, duration: 0.090s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.001742, mae: 0.088315, mean_q: 0.122581\n",
      " 17939/50000: episode: 2335, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001964, mae: 0.085869, mean_q: 0.117736\n",
      " 17942/50000: episode: 2336, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002678, mae: 0.093390, mean_q: 0.131407\n",
      " 17962/50000: episode: 2337, duration: 0.287s, episode steps:  20, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.350 [0.000, 3.000],  loss: 0.002291, mae: 0.084902, mean_q: 0.118340\n",
      " 17984/50000: episode: 2338, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002423, mae: 0.093308, mean_q: 0.128800\n",
      " 17987/50000: episode: 2339, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002458, mae: 0.091414, mean_q: 0.122126\n",
      " 17999/50000: episode: 2340, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001356, mae: 0.077996, mean_q: 0.110281\n",
      " 18007/50000: episode: 2341, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002219, mae: 0.090485, mean_q: 0.126900\n",
      " 18011/50000: episode: 2342, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002564, mae: 0.081769, mean_q: 0.113647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18016/50000: episode: 2343, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002264, mae: 0.089074, mean_q: 0.123269\n",
      " 18020/50000: episode: 2344, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001489, mae: 0.092265, mean_q: 0.128969\n",
      " 18032/50000: episode: 2345, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002473, mae: 0.088351, mean_q: 0.122207\n",
      " 18040/50000: episode: 2346, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001329, mae: 0.083244, mean_q: 0.117549\n",
      " 18043/50000: episode: 2347, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002002, mae: 0.085383, mean_q: 0.118282\n",
      " 18051/50000: episode: 2348, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003710, mae: 0.086144, mean_q: 0.118183\n",
      " 18056/50000: episode: 2349, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001148, mae: 0.080346, mean_q: 0.111279\n",
      " 18058/50000: episode: 2350, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002361, mae: 0.084088, mean_q: 0.114906\n",
      " 18060/50000: episode: 2351, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001569, mae: 0.077705, mean_q: 0.104675\n",
      " 18064/50000: episode: 2352, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002568, mae: 0.087410, mean_q: 0.119519\n",
      " 18066/50000: episode: 2353, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003025, mae: 0.089473, mean_q: 0.123651\n",
      " 18080/50000: episode: 2354, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002087, mae: 0.085294, mean_q: 0.116163\n",
      " 18101/50000: episode: 2355, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.810 [0.000, 3.000],  loss: 0.002599, mae: 0.086556, mean_q: 0.122076\n",
      " 18103/50000: episode: 2356, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001522, mae: 0.081729, mean_q: 0.115798\n",
      " 18116/50000: episode: 2357, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002269, mae: 0.092809, mean_q: 0.129520\n",
      " 18120/50000: episode: 2358, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001678, mae: 0.087002, mean_q: 0.122878\n",
      " 18129/50000: episode: 2359, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002876, mae: 0.091212, mean_q: 0.124837\n",
      " 18133/50000: episode: 2360, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.001369, mae: 0.087067, mean_q: 0.122354\n",
      " 18144/50000: episode: 2361, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.003196, mae: 0.099255, mean_q: 0.134346\n",
      " 18150/50000: episode: 2362, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001060, mae: 0.086251, mean_q: 0.118976\n",
      " 18153/50000: episode: 2363, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.004515, mae: 0.095351, mean_q: 0.131165\n",
      " 18162/50000: episode: 2364, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002302, mae: 0.095165, mean_q: 0.129801\n",
      " 18181/50000: episode: 2365, duration: 0.236s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.002405, mae: 0.087772, mean_q: 0.121932\n",
      " 18187/50000: episode: 2366, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002304, mae: 0.086026, mean_q: 0.118291\n",
      " 18211/50000: episode: 2367, duration: 0.286s, episode steps:  24, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001839, mae: 0.086964, mean_q: 0.118100\n",
      " 18214/50000: episode: 2368, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001637, mae: 0.086503, mean_q: 0.121027\n",
      " 18218/50000: episode: 2369, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002384, mae: 0.094955, mean_q: 0.129286\n",
      " 18230/50000: episode: 2370, duration: 0.151s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002046, mae: 0.091870, mean_q: 0.124778\n",
      " 18239/50000: episode: 2371, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001830, mae: 0.081393, mean_q: 0.113118\n",
      " 18243/50000: episode: 2372, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.750 [2.000, 3.000],  loss: 0.003091, mae: 0.085274, mean_q: 0.113508\n",
      " 18258/50000: episode: 2373, duration: 0.184s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.933 [0.000, 3.000],  loss: 0.002144, mae: 0.091519, mean_q: 0.123175\n",
      " 18262/50000: episode: 2374, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001563, mae: 0.086498, mean_q: 0.120063\n",
      " 18278/50000: episode: 2375, duration: 0.196s, episode steps:  16, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001519, mae: 0.086050, mean_q: 0.120816\n",
      " 18290/50000: episode: 2376, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002026, mae: 0.088351, mean_q: 0.120620\n",
      " 18293/50000: episode: 2377, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002786, mae: 0.092600, mean_q: 0.121815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18297/50000: episode: 2378, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002983, mae: 0.102595, mean_q: 0.140615\n",
      " 18300/50000: episode: 2379, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002607, mae: 0.094018, mean_q: 0.126454\n",
      " 18312/50000: episode: 2380, duration: 0.144s, episode steps:  12, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002936, mae: 0.097835, mean_q: 0.135083\n",
      " 18314/50000: episode: 2381, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002486, mae: 0.086897, mean_q: 0.117417\n",
      " 18316/50000: episode: 2382, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002633, mae: 0.088112, mean_q: 0.124108\n",
      " 18320/50000: episode: 2383, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001988, mae: 0.079457, mean_q: 0.111750\n",
      " 18329/50000: episode: 2384, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001828, mae: 0.091172, mean_q: 0.128632\n",
      " 18339/50000: episode: 2385, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003668, mae: 0.099598, mean_q: 0.136380\n",
      " 18345/50000: episode: 2386, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002208, mae: 0.090191, mean_q: 0.128041\n",
      " 18356/50000: episode: 2387, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002687, mae: 0.094358, mean_q: 0.133065\n",
      " 18359/50000: episode: 2388, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003457, mae: 0.101351, mean_q: 0.141243\n",
      " 18363/50000: episode: 2389, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.002894, mae: 0.096674, mean_q: 0.133361\n",
      " 18367/50000: episode: 2390, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001726, mae: 0.096987, mean_q: 0.136725\n",
      " 18377/50000: episode: 2391, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002517, mae: 0.092602, mean_q: 0.128217\n",
      " 18379/50000: episode: 2392, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001972, mae: 0.098447, mean_q: 0.137427\n",
      " 18394/50000: episode: 2393, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.933 [0.000, 3.000],  loss: 0.002583, mae: 0.088778, mean_q: 0.123190\n",
      " 18401/50000: episode: 2394, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002507, mae: 0.091663, mean_q: 0.126768\n",
      " 18415/50000: episode: 2395, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002229, mae: 0.097204, mean_q: 0.134341\n",
      " 18434/50000: episode: 2396, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.002591, mae: 0.095840, mean_q: 0.130418\n",
      " 18447/50000: episode: 2397, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.002773, mae: 0.098474, mean_q: 0.133222\n",
      " 18453/50000: episode: 2398, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003097, mae: 0.093760, mean_q: 0.124305\n",
      " 18457/50000: episode: 2399, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002829, mae: 0.093051, mean_q: 0.125875\n",
      " 18461/50000: episode: 2400, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002076, mae: 0.093300, mean_q: 0.123329\n",
      " 18484/50000: episode: 2401, duration: 0.269s, episode steps:  23, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002800, mae: 0.091934, mean_q: 0.125121\n",
      " 18487/50000: episode: 2402, duration: 0.044s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001263, mae: 0.087563, mean_q: 0.122379\n",
      " 18499/50000: episode: 2403, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002118, mae: 0.094170, mean_q: 0.132597\n",
      " 18501/50000: episode: 2404, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002811, mae: 0.093066, mean_q: 0.131031\n",
      " 18511/50000: episode: 2405, duration: 0.148s, episode steps:  10, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.300 [1.000, 3.000],  loss: 0.003016, mae: 0.091185, mean_q: 0.125258\n",
      " 18516/50000: episode: 2406, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004587, mae: 0.101794, mean_q: 0.136766\n",
      " 18534/50000: episode: 2407, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002561, mae: 0.086980, mean_q: 0.118878\n",
      " 18536/50000: episode: 2408, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000945, mae: 0.086011, mean_q: 0.117705\n",
      " 18550/50000: episode: 2409, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.002243, mae: 0.093741, mean_q: 0.129017\n",
      " 18557/50000: episode: 2410, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001789, mae: 0.091724, mean_q: 0.131639\n",
      " 18562/50000: episode: 2411, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001293, mae: 0.090769, mean_q: 0.129874\n",
      " 18564/50000: episode: 2412, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001728, mae: 0.086080, mean_q: 0.119988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18574/50000: episode: 2413, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.003061, mae: 0.098220, mean_q: 0.136263\n",
      " 18584/50000: episode: 2414, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002856, mae: 0.087721, mean_q: 0.123614\n",
      " 18589/50000: episode: 2415, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002048, mae: 0.087141, mean_q: 0.121054\n",
      " 18591/50000: episode: 2416, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001674, mae: 0.088886, mean_q: 0.127666\n",
      " 18595/50000: episode: 2417, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003811, mae: 0.093781, mean_q: 0.130210\n",
      " 18601/50000: episode: 2418, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.003165, mae: 0.097238, mean_q: 0.136936\n",
      " 18611/50000: episode: 2419, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.001500, mae: 0.095808, mean_q: 0.133051\n",
      " 18619/50000: episode: 2420, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002834, mae: 0.101088, mean_q: 0.138983\n",
      " 18622/50000: episode: 2421, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003548, mae: 0.097968, mean_q: 0.136320\n",
      " 18626/50000: episode: 2422, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.001550, mae: 0.084294, mean_q: 0.119141\n",
      " 18630/50000: episode: 2423, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004351, mae: 0.094275, mean_q: 0.128382\n",
      " 18634/50000: episode: 2424, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002926, mae: 0.095901, mean_q: 0.130162\n",
      " 18650/50000: episode: 2425, duration: 0.192s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002769, mae: 0.089071, mean_q: 0.121235\n",
      " 18667/50000: episode: 2426, duration: 0.203s, episode steps:  17, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002630, mae: 0.100894, mean_q: 0.141701\n",
      " 18678/50000: episode: 2427, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001800, mae: 0.093058, mean_q: 0.133131\n",
      " 18681/50000: episode: 2428, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001503, mae: 0.088481, mean_q: 0.128529\n",
      " 18685/50000: episode: 2429, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001522, mae: 0.089863, mean_q: 0.130022\n",
      " 18713/50000: episode: 2430, duration: 0.328s, episode steps:  28, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002966, mae: 0.100696, mean_q: 0.142132\n",
      " 18720/50000: episode: 2431, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002283, mae: 0.094903, mean_q: 0.131312\n",
      " 18728/50000: episode: 2432, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003308, mae: 0.099700, mean_q: 0.134722\n",
      " 18735/50000: episode: 2433, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002374, mae: 0.099773, mean_q: 0.137257\n",
      " 18737/50000: episode: 2434, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001874, mae: 0.098598, mean_q: 0.137123\n",
      " 18744/50000: episode: 2435, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003242, mae: 0.102043, mean_q: 0.139146\n",
      " 18747/50000: episode: 2436, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001753, mae: 0.091245, mean_q: 0.124388\n",
      " 18756/50000: episode: 2437, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [1.000, 3.000],  loss: 0.002885, mae: 0.105649, mean_q: 0.142855\n",
      " 18762/50000: episode: 2438, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001529, mae: 0.088042, mean_q: 0.121061\n",
      " 18765/50000: episode: 2439, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001145, mae: 0.082609, mean_q: 0.113000\n",
      " 18769/50000: episode: 2440, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001898, mae: 0.089870, mean_q: 0.122904\n",
      " 18771/50000: episode: 2441, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000825, mae: 0.081704, mean_q: 0.113537\n",
      " 18785/50000: episode: 2442, duration: 0.167s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003241, mae: 0.097189, mean_q: 0.131140\n",
      " 18788/50000: episode: 2443, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002239, mae: 0.095115, mean_q: 0.130365\n",
      " 18797/50000: episode: 2444, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.001151, mae: 0.088161, mean_q: 0.120883\n",
      " 18802/50000: episode: 2445, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000949, mae: 0.094233, mean_q: 0.133518\n",
      " 18807/50000: episode: 2446, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001104, mae: 0.092291, mean_q: 0.129880\n",
      " 18811/50000: episode: 2447, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.004369, mae: 0.110506, mean_q: 0.153223\n",
      " 18819/50000: episode: 2448, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002036, mae: 0.090504, mean_q: 0.124772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18824/50000: episode: 2449, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001153, mae: 0.089861, mean_q: 0.123771\n",
      " 18830/50000: episode: 2450, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002833, mae: 0.091349, mean_q: 0.122240\n",
      " 18832/50000: episode: 2451, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003218, mae: 0.109680, mean_q: 0.153876\n",
      " 18844/50000: episode: 2452, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001972, mae: 0.093297, mean_q: 0.126970\n",
      " 18863/50000: episode: 2453, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.105 [0.000, 3.000],  loss: 0.002707, mae: 0.089980, mean_q: 0.123947\n",
      " 18870/50000: episode: 2454, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002092, mae: 0.092916, mean_q: 0.127703\n",
      " 18875/50000: episode: 2455, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.001121, mae: 0.085993, mean_q: 0.120151\n",
      " 18886/50000: episode: 2456, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002977, mae: 0.093129, mean_q: 0.129329\n",
      " 18894/50000: episode: 2457, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.003029, mae: 0.101034, mean_q: 0.136180\n",
      " 18904/50000: episode: 2458, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.002010, mae: 0.091250, mean_q: 0.127215\n",
      " 18927/50000: episode: 2459, duration: 0.277s, episode steps:  23, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.174 [0.000, 3.000],  loss: 0.002776, mae: 0.094939, mean_q: 0.130461\n",
      " 18930/50000: episode: 2460, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001822, mae: 0.086981, mean_q: 0.121371\n",
      " 18936/50000: episode: 2461, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001841, mae: 0.095194, mean_q: 0.134872\n",
      " 18943/50000: episode: 2462, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 0.857 [0.000, 3.000],  loss: 0.001950, mae: 0.095806, mean_q: 0.134989\n",
      " 18945/50000: episode: 2463, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001008, mae: 0.087351, mean_q: 0.121855\n",
      " 18958/50000: episode: 2464, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.001667, mae: 0.087580, mean_q: 0.120834\n",
      " 18962/50000: episode: 2465, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002905, mae: 0.100980, mean_q: 0.134182\n",
      " 18975/50000: episode: 2466, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.002664, mae: 0.104796, mean_q: 0.144767\n",
      " 18991/50000: episode: 2467, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002592, mae: 0.100136, mean_q: 0.137297\n",
      " 18997/50000: episode: 2468, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001761, mae: 0.094612, mean_q: 0.129621\n",
      " 19008/50000: episode: 2469, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002983, mae: 0.101395, mean_q: 0.137210\n",
      " 19011/50000: episode: 2470, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001052, mae: 0.090488, mean_q: 0.123340\n",
      " 19021/50000: episode: 2471, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002652, mae: 0.098509, mean_q: 0.132387\n",
      " 19027/50000: episode: 2472, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003542, mae: 0.098765, mean_q: 0.132356\n",
      " 19038/50000: episode: 2473, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.003052, mae: 0.102349, mean_q: 0.139799\n",
      " 19046/50000: episode: 2474, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002252, mae: 0.090742, mean_q: 0.125329\n",
      " 19048/50000: episode: 2475, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003559, mae: 0.094830, mean_q: 0.127252\n",
      " 19053/50000: episode: 2476, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.600 [2.000, 3.000],  loss: 0.001682, mae: 0.090106, mean_q: 0.125821\n",
      " 19057/50000: episode: 2477, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002031, mae: 0.099208, mean_q: 0.133410\n",
      " 19063/50000: episode: 2478, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002183, mae: 0.095175, mean_q: 0.127670\n",
      " 19069/50000: episode: 2479, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003010, mae: 0.099784, mean_q: 0.133387\n",
      " 19076/50000: episode: 2480, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001956, mae: 0.096694, mean_q: 0.131694\n",
      " 19078/50000: episode: 2481, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001033, mae: 0.093937, mean_q: 0.129779\n",
      " 19083/50000: episode: 2482, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002236, mae: 0.104516, mean_q: 0.145583\n",
      " 19092/50000: episode: 2483, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003413, mae: 0.096478, mean_q: 0.132086\n",
      " 19106/50000: episode: 2484, duration: 0.172s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002711, mae: 0.088571, mean_q: 0.123316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19109/50000: episode: 2485, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001545, mae: 0.099357, mean_q: 0.136870\n",
      " 19112/50000: episode: 2486, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002459, mae: 0.095203, mean_q: 0.128763\n",
      " 19134/50000: episode: 2487, duration: 0.298s, episode steps:  22, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002149, mae: 0.089055, mean_q: 0.123562\n",
      " 19138/50000: episode: 2488, duration: 0.067s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002324, mae: 0.098199, mean_q: 0.134397\n",
      " 19145/50000: episode: 2489, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.001716, mae: 0.093155, mean_q: 0.127899\n",
      " 19149/50000: episode: 2490, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002490, mae: 0.093474, mean_q: 0.125312\n",
      " 19153/50000: episode: 2491, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.002670, mae: 0.094721, mean_q: 0.130905\n",
      " 19155/50000: episode: 2492, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003276, mae: 0.093372, mean_q: 0.124229\n",
      " 19158/50000: episode: 2493, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001114, mae: 0.088141, mean_q: 0.122974\n",
      " 19161/50000: episode: 2494, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002829, mae: 0.096632, mean_q: 0.129357\n",
      " 19164/50000: episode: 2495, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002300, mae: 0.098836, mean_q: 0.133026\n",
      " 19174/50000: episode: 2496, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002115, mae: 0.097871, mean_q: 0.131192\n",
      " 19182/50000: episode: 2497, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002332, mae: 0.090251, mean_q: 0.120611\n",
      " 19188/50000: episode: 2498, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001758, mae: 0.089848, mean_q: 0.123027\n",
      " 19190/50000: episode: 2499, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001886, mae: 0.092839, mean_q: 0.125466\n",
      " 19195/50000: episode: 2500, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002023, mae: 0.093939, mean_q: 0.127114\n",
      " 19199/50000: episode: 2501, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002415, mae: 0.091099, mean_q: 0.123785\n",
      " 19203/50000: episode: 2502, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001735, mae: 0.090281, mean_q: 0.122881\n",
      " 19217/50000: episode: 2503, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003029, mae: 0.092212, mean_q: 0.124566\n",
      " 19220/50000: episode: 2504, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002358, mae: 0.092735, mean_q: 0.128084\n",
      " 19227/50000: episode: 2505, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002121, mae: 0.094549, mean_q: 0.133700\n",
      " 19230/50000: episode: 2506, duration: 0.061s, episode steps:   3, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003918, mae: 0.099635, mean_q: 0.136904\n",
      " 19242/50000: episode: 2507, duration: 0.186s, episode steps:  12, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001847, mae: 0.089624, mean_q: 0.122929\n",
      " 19244/50000: episode: 2508, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003968, mae: 0.096354, mean_q: 0.126596\n",
      " 19249/50000: episode: 2509, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001811, mae: 0.089512, mean_q: 0.122097\n",
      " 19256/50000: episode: 2510, duration: 0.107s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.001891, mae: 0.095261, mean_q: 0.131024\n",
      " 19270/50000: episode: 2511, duration: 0.184s, episode steps:  14, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001930, mae: 0.092819, mean_q: 0.128716\n",
      " 19274/50000: episode: 2512, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.001369, mae: 0.087916, mean_q: 0.119238\n",
      " 19278/50000: episode: 2513, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.003282, mae: 0.097043, mean_q: 0.131202\n",
      " 19286/50000: episode: 2514, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.625 [0.000, 2.000],  loss: 0.002459, mae: 0.094820, mean_q: 0.128466\n",
      " 19294/50000: episode: 2515, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001962, mae: 0.088675, mean_q: 0.122171\n",
      " 19297/50000: episode: 2516, duration: 0.068s, episode steps:   3, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001634, mae: 0.091874, mean_q: 0.129099\n",
      " 19300/50000: episode: 2517, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001050, mae: 0.088810, mean_q: 0.122122\n",
      " 19308/50000: episode: 2518, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.625 [0.000, 3.000],  loss: 0.003092, mae: 0.096086, mean_q: 0.132441\n",
      " 19312/50000: episode: 2519, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002482, mae: 0.092920, mean_q: 0.128052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19315/50000: episode: 2520, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002100, mae: 0.091832, mean_q: 0.127637\n",
      " 19322/50000: episode: 2521, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002153, mae: 0.090884, mean_q: 0.128101\n",
      " 19329/50000: episode: 2522, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.003120, mae: 0.097398, mean_q: 0.137870\n",
      " 19341/50000: episode: 2523, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001855, mae: 0.091610, mean_q: 0.129696\n",
      " 19389/50000: episode: 2524, duration: 0.586s, episode steps:  48, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: 0.002363, mae: 0.093697, mean_q: 0.130607\n",
      " 19392/50000: episode: 2525, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001258, mae: 0.082383, mean_q: 0.121131\n",
      " 19416/50000: episode: 2526, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002305, mae: 0.097151, mean_q: 0.133677\n",
      " 19420/50000: episode: 2527, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002135, mae: 0.087310, mean_q: 0.118799\n",
      " 19422/50000: episode: 2528, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002314, mae: 0.101048, mean_q: 0.140024\n",
      " 19426/50000: episode: 2529, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002544, mae: 0.090185, mean_q: 0.121865\n",
      " 19432/50000: episode: 2530, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001946, mae: 0.087935, mean_q: 0.119631\n",
      " 19435/50000: episode: 2531, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003325, mae: 0.089734, mean_q: 0.124267\n",
      " 19437/50000: episode: 2532, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003041, mae: 0.088149, mean_q: 0.120359\n",
      " 19447/50000: episode: 2533, duration: 0.144s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002219, mae: 0.091480, mean_q: 0.127460\n",
      " 19449/50000: episode: 2534, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001059, mae: 0.082414, mean_q: 0.119240\n",
      " 19454/50000: episode: 2535, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001772, mae: 0.087443, mean_q: 0.121517\n",
      " 19463/50000: episode: 2536, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002411, mae: 0.092984, mean_q: 0.129070\n",
      " 19465/50000: episode: 2537, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002467, mae: 0.085730, mean_q: 0.121015\n",
      " 19468/50000: episode: 2538, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.003087, mae: 0.090261, mean_q: 0.127924\n",
      " 19472/50000: episode: 2539, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003277, mae: 0.087686, mean_q: 0.124058\n",
      " 19489/50000: episode: 2540, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.001750, mae: 0.092817, mean_q: 0.128844\n",
      " 19496/50000: episode: 2541, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.002149, mae: 0.091970, mean_q: 0.127052\n",
      " 19508/50000: episode: 2542, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002373, mae: 0.094503, mean_q: 0.130089\n",
      " 19513/50000: episode: 2543, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002300, mae: 0.088089, mean_q: 0.122098\n",
      " 19527/50000: episode: 2544, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.001896, mae: 0.090209, mean_q: 0.128814\n",
      " 19540/50000: episode: 2545, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002164, mae: 0.089109, mean_q: 0.124621\n",
      " 19546/50000: episode: 2546, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001719, mae: 0.101470, mean_q: 0.137812\n",
      " 19558/50000: episode: 2547, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 3.000],  loss: 0.001870, mae: 0.091730, mean_q: 0.127366\n",
      " 19574/50000: episode: 2548, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002535, mae: 0.094849, mean_q: 0.128400\n",
      " 19584/50000: episode: 2549, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001775, mae: 0.095609, mean_q: 0.132224\n",
      " 19596/50000: episode: 2550, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002731, mae: 0.095190, mean_q: 0.133118\n",
      " 19599/50000: episode: 2551, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002537, mae: 0.096390, mean_q: 0.135242\n",
      " 19602/50000: episode: 2552, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001083, mae: 0.090060, mean_q: 0.129063\n",
      " 19608/50000: episode: 2553, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004382, mae: 0.100423, mean_q: 0.138644\n",
      " 19623/50000: episode: 2554, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.002707, mae: 0.095240, mean_q: 0.131097\n",
      " 19628/50000: episode: 2555, duration: 0.164s, episode steps:   5, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003266, mae: 0.102685, mean_q: 0.139454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19640/50000: episode: 2556, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001883, mae: 0.093692, mean_q: 0.128315\n",
      " 19647/50000: episode: 2557, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 3.000],  loss: 0.003289, mae: 0.097407, mean_q: 0.133006\n",
      " 19655/50000: episode: 2558, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002857, mae: 0.090761, mean_q: 0.124886\n",
      " 19658/50000: episode: 2559, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003363, mae: 0.094085, mean_q: 0.128018\n",
      " 19662/50000: episode: 2560, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002318, mae: 0.090729, mean_q: 0.123890\n",
      " 19670/50000: episode: 2561, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.002406, mae: 0.090428, mean_q: 0.123806\n",
      " 19682/50000: episode: 2562, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002907, mae: 0.100341, mean_q: 0.134690\n",
      " 19688/50000: episode: 2563, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001043, mae: 0.082400, mean_q: 0.110200\n",
      " 19693/50000: episode: 2564, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.001500, mae: 0.086772, mean_q: 0.116624\n",
      " 19706/50000: episode: 2565, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.001976, mae: 0.091231, mean_q: 0.122651\n",
      " 19711/50000: episode: 2566, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001393, mae: 0.083872, mean_q: 0.115022\n",
      " 19714/50000: episode: 2567, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.002633, mae: 0.088855, mean_q: 0.118830\n",
      " 19722/50000: episode: 2568, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002769, mae: 0.091894, mean_q: 0.129411\n",
      " 19733/50000: episode: 2569, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001366, mae: 0.083316, mean_q: 0.116774\n",
      " 19736/50000: episode: 2570, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001996, mae: 0.091702, mean_q: 0.126456\n",
      " 19739/50000: episode: 2571, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002752, mae: 0.097082, mean_q: 0.131721\n",
      " 19756/50000: episode: 2572, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001266, mae: 0.087083, mean_q: 0.120720\n",
      " 19761/50000: episode: 2573, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002469, mae: 0.092664, mean_q: 0.125042\n",
      " 19770/50000: episode: 2574, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002347, mae: 0.100248, mean_q: 0.135543\n",
      " 19776/50000: episode: 2575, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001452, mae: 0.087530, mean_q: 0.121847\n",
      " 19778/50000: episode: 2576, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001140, mae: 0.086281, mean_q: 0.123762\n",
      " 19782/50000: episode: 2577, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.750 [2.000, 3.000],  loss: 0.002493, mae: 0.089314, mean_q: 0.121870\n",
      " 19790/50000: episode: 2578, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001353, mae: 0.087385, mean_q: 0.123728\n",
      " 19795/50000: episode: 2579, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000899, mae: 0.092582, mean_q: 0.129479\n",
      " 19800/50000: episode: 2580, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001243, mae: 0.087851, mean_q: 0.121780\n",
      " 19803/50000: episode: 2581, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.003090, mae: 0.095670, mean_q: 0.128621\n",
      " 19806/50000: episode: 2582, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002278, mae: 0.082798, mean_q: 0.113129\n",
      " 19808/50000: episode: 2583, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.000789, mae: 0.083120, mean_q: 0.118518\n",
      " 19819/50000: episode: 2584, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001403, mae: 0.087770, mean_q: 0.124293\n",
      " 19828/50000: episode: 2585, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003092, mae: 0.091322, mean_q: 0.126841\n",
      " 19835/50000: episode: 2586, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001831, mae: 0.087467, mean_q: 0.124205\n",
      " 19843/50000: episode: 2587, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001721, mae: 0.087526, mean_q: 0.123613\n",
      " 19845/50000: episode: 2588, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001874, mae: 0.086439, mean_q: 0.119341\n",
      " 19850/50000: episode: 2589, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001488, mae: 0.090490, mean_q: 0.124753\n",
      " 19853/50000: episode: 2590, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001196, mae: 0.084309, mean_q: 0.115586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19862/50000: episode: 2591, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001730, mae: 0.087549, mean_q: 0.120494\n",
      " 19865/50000: episode: 2592, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002238, mae: 0.094566, mean_q: 0.132627\n",
      " 19877/50000: episode: 2593, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.002062, mae: 0.084298, mean_q: 0.115796\n",
      " 19886/50000: episode: 2594, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002075, mae: 0.090886, mean_q: 0.123204\n",
      " 19889/50000: episode: 2595, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002240, mae: 0.091110, mean_q: 0.123488\n",
      " 19901/50000: episode: 2596, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001242, mae: 0.087153, mean_q: 0.118877\n",
      " 19904/50000: episode: 2597, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001114, mae: 0.082957, mean_q: 0.113184\n",
      " 19907/50000: episode: 2598, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001969, mae: 0.089377, mean_q: 0.119267\n",
      " 19919/50000: episode: 2599, duration: 0.169s, episode steps:  12, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001334, mae: 0.088741, mean_q: 0.119878\n",
      " 19921/50000: episode: 2600, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001338, mae: 0.084965, mean_q: 0.112977\n",
      " 19946/50000: episode: 2601, duration: 0.349s, episode steps:  25, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001807, mae: 0.089243, mean_q: 0.119975\n",
      " 19951/50000: episode: 2602, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000871, mae: 0.081063, mean_q: 0.109202\n",
      " 19955/50000: episode: 2603, duration: 0.066s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001317, mae: 0.089686, mean_q: 0.120807\n",
      " 19962/50000: episode: 2604, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003373, mae: 0.101392, mean_q: 0.136536\n",
      " 19967/50000: episode: 2605, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001519, mae: 0.097904, mean_q: 0.135803\n",
      " 19971/50000: episode: 2606, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004094, mae: 0.097301, mean_q: 0.130370\n",
      " 19980/50000: episode: 2607, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002763, mae: 0.092107, mean_q: 0.126316\n",
      " 19983/50000: episode: 2608, duration: 0.056s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001755, mae: 0.085449, mean_q: 0.122099\n",
      " 19988/50000: episode: 2609, duration: 0.090s, episode steps:   5, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.001229, mae: 0.083016, mean_q: 0.116314\n",
      " 19991/50000: episode: 2610, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003098, mae: 0.095588, mean_q: 0.131721\n",
      " 20006/50000: episode: 2611, duration: 0.184s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.933 [0.000, 3.000],  loss: 0.001876, mae: 0.092837, mean_q: 0.128819\n",
      " 20012/50000: episode: 2612, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002247, mae: 0.093557, mean_q: 0.129449\n",
      " 20027/50000: episode: 2613, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002984, mae: 0.096555, mean_q: 0.129391\n",
      " 20030/50000: episode: 2614, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.001495, mae: 0.087692, mean_q: 0.119778\n",
      " 20039/50000: episode: 2615, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [1.000, 3.000],  loss: 0.003860, mae: 0.097760, mean_q: 0.130874\n",
      " 20042/50000: episode: 2616, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002438, mae: 0.095383, mean_q: 0.131972\n",
      " 20049/50000: episode: 2617, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001514, mae: 0.096768, mean_q: 0.135116\n",
      " 20057/50000: episode: 2618, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003004, mae: 0.096330, mean_q: 0.130787\n",
      " 20060/50000: episode: 2619, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001555, mae: 0.091907, mean_q: 0.128701\n",
      " 20066/50000: episode: 2620, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002531, mae: 0.095750, mean_q: 0.130260\n",
      " 20083/50000: episode: 2621, duration: 0.206s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002816, mae: 0.091931, mean_q: 0.123719\n",
      " 20091/50000: episode: 2622, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003240, mae: 0.094269, mean_q: 0.126727\n",
      " 20107/50000: episode: 2623, duration: 0.195s, episode steps:  16, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002698, mae: 0.088308, mean_q: 0.124787\n",
      " 20127/50000: episode: 2624, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.002613, mae: 0.089607, mean_q: 0.122743\n",
      " 20131/50000: episode: 2625, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002662, mae: 0.085938, mean_q: 0.119633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20158/50000: episode: 2626, duration: 0.353s, episode steps:  27, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002186, mae: 0.092207, mean_q: 0.127948\n",
      " 20161/50000: episode: 2627, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002861, mae: 0.103487, mean_q: 0.142582\n",
      " 20177/50000: episode: 2628, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.812 [0.000, 3.000],  loss: 0.003552, mae: 0.092883, mean_q: 0.127924\n",
      " 20181/50000: episode: 2629, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.001797, mae: 0.091043, mean_q: 0.125289\n",
      " 20183/50000: episode: 2630, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002664, mae: 0.094641, mean_q: 0.127761\n",
      " 20188/50000: episode: 2631, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003459, mae: 0.090778, mean_q: 0.124847\n",
      " 20190/50000: episode: 2632, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004109, mae: 0.090659, mean_q: 0.124570\n",
      " 20197/50000: episode: 2633, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001405, mae: 0.092579, mean_q: 0.130103\n",
      " 20201/50000: episode: 2634, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001302, mae: 0.091441, mean_q: 0.127159\n",
      " 20209/50000: episode: 2635, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.002063, mae: 0.086202, mean_q: 0.122097\n",
      " 20211/50000: episode: 2636, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002355, mae: 0.094781, mean_q: 0.132720\n",
      " 20215/50000: episode: 2637, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002656, mae: 0.102145, mean_q: 0.146422\n",
      " 20218/50000: episode: 2638, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001571, mae: 0.087344, mean_q: 0.124037\n",
      " 20220/50000: episode: 2639, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001759, mae: 0.088341, mean_q: 0.125078\n",
      " 20222/50000: episode: 2640, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002568, mae: 0.090962, mean_q: 0.128931\n",
      " 20224/50000: episode: 2641, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001356, mae: 0.092658, mean_q: 0.131751\n",
      " 20229/50000: episode: 2642, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.002796, mae: 0.089441, mean_q: 0.124793\n",
      " 20247/50000: episode: 2643, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001988, mae: 0.090187, mean_q: 0.126045\n",
      " 20251/50000: episode: 2644, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001540, mae: 0.089341, mean_q: 0.128112\n",
      " 20270/50000: episode: 2645, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.002474, mae: 0.092954, mean_q: 0.134332\n",
      " 20278/50000: episode: 2646, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002547, mae: 0.094790, mean_q: 0.132373\n",
      " 20281/50000: episode: 2647, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003032, mae: 0.094295, mean_q: 0.130749\n",
      " 20283/50000: episode: 2648, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005368, mae: 0.101572, mean_q: 0.139685\n",
      " 20292/50000: episode: 2649, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001963, mae: 0.092829, mean_q: 0.126576\n",
      " 20297/50000: episode: 2650, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003958, mae: 0.097852, mean_q: 0.131164\n",
      " 20302/50000: episode: 2651, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002909, mae: 0.094445, mean_q: 0.127103\n",
      " 20311/50000: episode: 2652, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.002725, mae: 0.094145, mean_q: 0.130626\n",
      " 20318/50000: episode: 2653, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003190, mae: 0.096932, mean_q: 0.133874\n",
      " 20328/50000: episode: 2654, duration: 0.123s, episode steps:  10, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002155, mae: 0.090685, mean_q: 0.125311\n",
      " 20331/50000: episode: 2655, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002774, mae: 0.099333, mean_q: 0.135692\n",
      " 20333/50000: episode: 2656, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003163, mae: 0.098943, mean_q: 0.135903\n",
      " 20336/50000: episode: 2657, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002310, mae: 0.094419, mean_q: 0.129463\n",
      " 20346/50000: episode: 2658, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002035, mae: 0.097092, mean_q: 0.135362\n",
      " 20348/50000: episode: 2659, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002252, mae: 0.092166, mean_q: 0.126568\n",
      " 20350/50000: episode: 2660, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000536, mae: 0.078687, mean_q: 0.109145\n",
      " 20353/50000: episode: 2661, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001745, mae: 0.088509, mean_q: 0.120654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20361/50000: episode: 2662, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002099, mae: 0.086318, mean_q: 0.117585\n",
      " 20366/50000: episode: 2663, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001631, mae: 0.085054, mean_q: 0.118706\n",
      " 20371/50000: episode: 2664, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003247, mae: 0.101470, mean_q: 0.139051\n",
      " 20383/50000: episode: 2665, duration: 0.180s, episode steps:  12, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002834, mae: 0.099840, mean_q: 0.139783\n",
      " 20397/50000: episode: 2666, duration: 0.167s, episode steps:  14, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001986, mae: 0.093760, mean_q: 0.129691\n",
      " 20423/50000: episode: 2667, duration: 0.312s, episode steps:  26, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002332, mae: 0.093193, mean_q: 0.127693\n",
      " 20434/50000: episode: 2668, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002263, mae: 0.091049, mean_q: 0.122874\n",
      " 20443/50000: episode: 2669, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002302, mae: 0.094074, mean_q: 0.127005\n",
      " 20447/50000: episode: 2670, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002318, mae: 0.090265, mean_q: 0.121884\n",
      " 20454/50000: episode: 2671, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002836, mae: 0.096849, mean_q: 0.130397\n",
      " 20456/50000: episode: 2672, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002732, mae: 0.096708, mean_q: 0.130256\n",
      " 20470/50000: episode: 2673, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002824, mae: 0.094962, mean_q: 0.128738\n",
      " 20492/50000: episode: 2674, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002013, mae: 0.096396, mean_q: 0.135624\n",
      " 20504/50000: episode: 2675, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001425, mae: 0.093296, mean_q: 0.129572\n",
      " 20507/50000: episode: 2676, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003360, mae: 0.094954, mean_q: 0.133262\n",
      " 20512/50000: episode: 2677, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003554, mae: 0.097780, mean_q: 0.135147\n",
      " 20518/50000: episode: 2678, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002487, mae: 0.091408, mean_q: 0.125391\n",
      " 20529/50000: episode: 2679, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001590, mae: 0.091402, mean_q: 0.128932\n",
      " 20535/50000: episode: 2680, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.002118, mae: 0.096284, mean_q: 0.134586\n",
      " 20541/50000: episode: 2681, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.001433, mae: 0.091506, mean_q: 0.126785\n",
      " 20553/50000: episode: 2682, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002496, mae: 0.098198, mean_q: 0.136280\n",
      " 20562/50000: episode: 2683, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002856, mae: 0.091994, mean_q: 0.129930\n",
      " 20564/50000: episode: 2684, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005162, mae: 0.100951, mean_q: 0.142400\n",
      " 20572/50000: episode: 2685, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.002216, mae: 0.108726, mean_q: 0.151598\n",
      " 20574/50000: episode: 2686, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004813, mae: 0.121441, mean_q: 0.166766\n",
      " 20577/50000: episode: 2687, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002522, mae: 0.102637, mean_q: 0.139430\n",
      " 20580/50000: episode: 2688, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001479, mae: 0.099226, mean_q: 0.136636\n",
      " 20585/50000: episode: 2689, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002497, mae: 0.094644, mean_q: 0.128097\n",
      " 20592/50000: episode: 2690, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001943, mae: 0.089677, mean_q: 0.121542\n",
      " 20600/50000: episode: 2691, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002124, mae: 0.094213, mean_q: 0.128827\n",
      " 20602/50000: episode: 2692, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003462, mae: 0.098520, mean_q: 0.129900\n",
      " 20608/50000: episode: 2693, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.001464, mae: 0.090451, mean_q: 0.124248\n",
      " 20615/50000: episode: 2694, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.001700, mae: 0.094239, mean_q: 0.131039\n",
      " 20629/50000: episode: 2695, duration: 0.168s, episode steps:  14, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.002279, mae: 0.099421, mean_q: 0.136075\n",
      " 20639/50000: episode: 2696, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002153, mae: 0.099306, mean_q: 0.138746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20648/50000: episode: 2697, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.001872, mae: 0.097215, mean_q: 0.135451\n",
      " 20661/50000: episode: 2698, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.002369, mae: 0.097496, mean_q: 0.136297\n",
      " 20675/50000: episode: 2699, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002671, mae: 0.098692, mean_q: 0.136079\n",
      " 20689/50000: episode: 2700, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002865, mae: 0.101474, mean_q: 0.139980\n",
      " 20697/50000: episode: 2701, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.003220, mae: 0.098642, mean_q: 0.136321\n",
      " 20720/50000: episode: 2702, duration: 0.277s, episode steps:  23, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.348 [0.000, 3.000],  loss: 0.002285, mae: 0.094489, mean_q: 0.129223\n",
      " 20724/50000: episode: 2703, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.000848, mae: 0.095682, mean_q: 0.133320\n",
      " 20733/50000: episode: 2704, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002188, mae: 0.096759, mean_q: 0.133871\n",
      " 20737/50000: episode: 2705, duration: 0.103s, episode steps:   4, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.002542, mae: 0.101431, mean_q: 0.139156\n",
      " 20756/50000: episode: 2706, duration: 0.230s, episode steps:  19, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002227, mae: 0.093056, mean_q: 0.127583\n",
      " 20759/50000: episode: 2707, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002713, mae: 0.099720, mean_q: 0.138657\n",
      " 20769/50000: episode: 2708, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002248, mae: 0.090222, mean_q: 0.124533\n",
      " 20775/50000: episode: 2709, duration: 0.098s, episode steps:   6, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002355, mae: 0.096458, mean_q: 0.130532\n",
      " 20781/50000: episode: 2710, duration: 0.104s, episode steps:   6, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.001434, mae: 0.093857, mean_q: 0.126931\n",
      " 20785/50000: episode: 2711, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.004055, mae: 0.102545, mean_q: 0.135417\n",
      " 20788/50000: episode: 2712, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001758, mae: 0.093393, mean_q: 0.126476\n",
      " 20797/50000: episode: 2713, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.001499, mae: 0.089077, mean_q: 0.120877\n",
      " 20803/50000: episode: 2714, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002404, mae: 0.090436, mean_q: 0.122466\n",
      " 20836/50000: episode: 2715, duration: 0.395s, episode steps:  33, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.002161, mae: 0.094259, mean_q: 0.131178\n",
      " 20843/50000: episode: 2716, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [1.000, 3.000],  loss: 0.003220, mae: 0.097248, mean_q: 0.131047\n",
      " 20848/50000: episode: 2717, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.004987, mae: 0.102039, mean_q: 0.137034\n",
      " 20851/50000: episode: 2718, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001827, mae: 0.093879, mean_q: 0.128098\n",
      " 20861/50000: episode: 2719, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001752, mae: 0.085231, mean_q: 0.117408\n",
      " 20875/50000: episode: 2720, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001701, mae: 0.089932, mean_q: 0.121691\n",
      " 20880/50000: episode: 2721, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.000755, mae: 0.082961, mean_q: 0.115490\n",
      " 20920/50000: episode: 2722, duration: 0.479s, episode steps:  40, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001913, mae: 0.090664, mean_q: 0.124334\n",
      " 20929/50000: episode: 2723, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001272, mae: 0.090779, mean_q: 0.123753\n",
      " 20939/50000: episode: 2724, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001855, mae: 0.091261, mean_q: 0.122505\n",
      " 20944/50000: episode: 2725, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000807, mae: 0.086359, mean_q: 0.117253\n",
      " 20947/50000: episode: 2726, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001893, mae: 0.090578, mean_q: 0.121655\n",
      " 20952/50000: episode: 2727, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001503, mae: 0.090059, mean_q: 0.120727\n",
      " 20966/50000: episode: 2728, duration: 0.168s, episode steps:  14, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002432, mae: 0.088203, mean_q: 0.119401\n",
      " 20973/50000: episode: 2729, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002402, mae: 0.097973, mean_q: 0.133343\n",
      " 20980/50000: episode: 2730, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002146, mae: 0.089170, mean_q: 0.120502\n",
      " 20988/50000: episode: 2731, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003138, mae: 0.086132, mean_q: 0.115777\n",
      " 20994/50000: episode: 2732, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001337, mae: 0.084827, mean_q: 0.116623\n",
      " 20999/50000: episode: 2733, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002787, mae: 0.102657, mean_q: 0.137011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21005/50000: episode: 2734, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001484, mae: 0.090240, mean_q: 0.124438\n",
      " 21014/50000: episode: 2735, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001561, mae: 0.087910, mean_q: 0.118920\n",
      " 21022/50000: episode: 2736, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001299, mae: 0.083855, mean_q: 0.113329\n",
      " 21027/50000: episode: 2737, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.003194, mae: 0.090176, mean_q: 0.123796\n",
      " 21041/50000: episode: 2738, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002150, mae: 0.091324, mean_q: 0.125901\n",
      " 21043/50000: episode: 2739, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001995, mae: 0.087579, mean_q: 0.118634\n",
      " 21052/50000: episode: 2740, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002860, mae: 0.095115, mean_q: 0.129295\n",
      " 21054/50000: episode: 2741, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.000857, mae: 0.086794, mean_q: 0.117293\n",
      " 21057/50000: episode: 2742, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002599, mae: 0.093969, mean_q: 0.126029\n",
      " 21064/50000: episode: 2743, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001796, mae: 0.088699, mean_q: 0.122355\n",
      " 21079/50000: episode: 2744, duration: 0.213s, episode steps:  15, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [1.000, 3.000],  loss: 0.002099, mae: 0.092581, mean_q: 0.126295\n",
      " 21119/50000: episode: 2745, duration: 0.516s, episode steps:  40, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.275 [0.000, 3.000],  loss: 0.001740, mae: 0.088670, mean_q: 0.121113\n",
      " 21132/50000: episode: 2746, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001419, mae: 0.087161, mean_q: 0.118912\n",
      " 21138/50000: episode: 2747, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002284, mae: 0.088044, mean_q: 0.119852\n",
      " 21146/50000: episode: 2748, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001359, mae: 0.087737, mean_q: 0.121102\n",
      " 21149/50000: episode: 2749, duration: 0.070s, episode steps:   3, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.002251, mae: 0.091835, mean_q: 0.123683\n",
      " 21155/50000: episode: 2750, duration: 0.105s, episode steps:   6, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002168, mae: 0.088129, mean_q: 0.117350\n",
      " 21158/50000: episode: 2751, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.000775, mae: 0.079848, mean_q: 0.108517\n",
      " 21168/50000: episode: 2752, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002447, mae: 0.084928, mean_q: 0.111823\n",
      " 21170/50000: episode: 2753, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001715, mae: 0.078032, mean_q: 0.106758\n",
      " 21173/50000: episode: 2754, duration: 0.060s, episode steps:   3, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001585, mae: 0.085047, mean_q: 0.117224\n",
      " 21191/50000: episode: 2755, duration: 0.251s, episode steps:  18, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.001822, mae: 0.087963, mean_q: 0.120940\n",
      " 21198/50000: episode: 2756, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002212, mae: 0.086933, mean_q: 0.117241\n",
      " 21205/50000: episode: 2757, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.714 [2.000, 3.000],  loss: 0.002565, mae: 0.083516, mean_q: 0.111071\n",
      " 21211/50000: episode: 2758, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002080, mae: 0.091962, mean_q: 0.121688\n",
      " 21217/50000: episode: 2759, duration: 0.105s, episode steps:   6, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002776, mae: 0.087868, mean_q: 0.118110\n",
      " 21221/50000: episode: 2760, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001174, mae: 0.078774, mean_q: 0.107862\n",
      " 21227/50000: episode: 2761, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003028, mae: 0.085826, mean_q: 0.116511\n",
      " 21241/50000: episode: 2762, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002306, mae: 0.085483, mean_q: 0.117948\n",
      " 21245/50000: episode: 2763, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002192, mae: 0.095536, mean_q: 0.130673\n",
      " 21252/50000: episode: 2764, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002303, mae: 0.085781, mean_q: 0.118446\n",
      " 21259/50000: episode: 2765, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001376, mae: 0.086206, mean_q: 0.121825\n",
      " 21261/50000: episode: 2766, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002291, mae: 0.081328, mean_q: 0.115412\n",
      " 21291/50000: episode: 2767, duration: 0.451s, episode steps:  30, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.002016, mae: 0.084697, mean_q: 0.115792\n",
      " 21293/50000: episode: 2768, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001582, mae: 0.083785, mean_q: 0.112387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21309/50000: episode: 2769, duration: 0.204s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002127, mae: 0.088692, mean_q: 0.119561\n",
      " 21312/50000: episode: 2770, duration: 0.088s, episode steps:   3, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.000911, mae: 0.080846, mean_q: 0.108630\n",
      " 21316/50000: episode: 2771, duration: 0.084s, episode steps:   4, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001598, mae: 0.076661, mean_q: 0.103923\n",
      " 21327/50000: episode: 2772, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.001528, mae: 0.077855, mean_q: 0.107063\n",
      " 21334/50000: episode: 2773, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001317, mae: 0.080847, mean_q: 0.109016\n",
      " 21337/50000: episode: 2774, duration: 0.056s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001677, mae: 0.081545, mean_q: 0.108620\n",
      " 21339/50000: episode: 2775, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001266, mae: 0.083856, mean_q: 0.111368\n",
      " 21356/50000: episode: 2776, duration: 0.265s, episode steps:  17, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.059 [0.000, 3.000],  loss: 0.001584, mae: 0.080311, mean_q: 0.107574\n",
      " 21362/50000: episode: 2777, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003769, mae: 0.088331, mean_q: 0.116925\n",
      " 21371/50000: episode: 2778, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 0.889 [0.000, 3.000],  loss: 0.002497, mae: 0.076572, mean_q: 0.107272\n",
      " 21376/50000: episode: 2779, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001489, mae: 0.079670, mean_q: 0.112698\n",
      " 21392/50000: episode: 2780, duration: 0.231s, episode steps:  16, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001493, mae: 0.082912, mean_q: 0.115521\n",
      " 21400/50000: episode: 2781, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002914, mae: 0.079093, mean_q: 0.107460\n",
      " 21404/50000: episode: 2782, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001048, mae: 0.078953, mean_q: 0.111492\n",
      " 21408/50000: episode: 2783, duration: 0.069s, episode steps:   4, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000940, mae: 0.075595, mean_q: 0.107656\n",
      " 21411/50000: episode: 2784, duration: 0.054s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002398, mae: 0.081235, mean_q: 0.111931\n",
      " 21431/50000: episode: 2785, duration: 0.271s, episode steps:  20, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.001283, mae: 0.079359, mean_q: 0.110102\n",
      " 21445/50000: episode: 2786, duration: 0.196s, episode steps:  14, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.001506, mae: 0.081193, mean_q: 0.113498\n",
      " 21461/50000: episode: 2787, duration: 0.221s, episode steps:  16, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001883, mae: 0.079247, mean_q: 0.109583\n",
      " 21469/50000: episode: 2788, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002101, mae: 0.085000, mean_q: 0.117662\n",
      " 21479/50000: episode: 2789, duration: 0.148s, episode steps:  10, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000931, mae: 0.075694, mean_q: 0.105777\n",
      " 21485/50000: episode: 2790, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001274, mae: 0.079303, mean_q: 0.109465\n",
      " 21489/50000: episode: 2791, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001559, mae: 0.084204, mean_q: 0.118425\n",
      " 21502/50000: episode: 2792, duration: 0.186s, episode steps:  13, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001857, mae: 0.077209, mean_q: 0.106882\n",
      " 21505/50000: episode: 2793, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003058, mae: 0.081096, mean_q: 0.112494\n",
      " 21526/50000: episode: 2794, duration: 0.286s, episode steps:  21, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001796, mae: 0.082440, mean_q: 0.115586\n",
      " 21535/50000: episode: 2795, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001575, mae: 0.082703, mean_q: 0.112107\n",
      " 21538/50000: episode: 2796, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001456, mae: 0.081667, mean_q: 0.111303\n",
      " 21546/50000: episode: 2797, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001674, mae: 0.084444, mean_q: 0.115017\n",
      " 21556/50000: episode: 2798, duration: 0.138s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001580, mae: 0.082947, mean_q: 0.113150\n",
      " 21574/50000: episode: 2799, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward:  1.000, mean reward:  0.056 [ 0.000,  1.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001919, mae: 0.080106, mean_q: 0.110686\n",
      " 21579/50000: episode: 2800, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.001131, mae: 0.078247, mean_q: 0.111174\n",
      " 21598/50000: episode: 2801, duration: 0.263s, episode steps:  19, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.002118, mae: 0.082856, mean_q: 0.117633\n",
      " 21600/50000: episode: 2802, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003668, mae: 0.087931, mean_q: 0.121395\n",
      " 21605/50000: episode: 2803, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.002162, mae: 0.082322, mean_q: 0.119279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21621/50000: episode: 2804, duration: 0.221s, episode steps:  16, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002295, mae: 0.084587, mean_q: 0.116212\n",
      " 21623/50000: episode: 2805, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003531, mae: 0.094146, mean_q: 0.128004\n",
      " 21634/50000: episode: 2806, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.001234, mae: 0.081130, mean_q: 0.110574\n",
      " 21637/50000: episode: 2807, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001969, mae: 0.081835, mean_q: 0.111909\n",
      " 21643/50000: episode: 2808, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001254, mae: 0.080027, mean_q: 0.111670\n",
      " 21652/50000: episode: 2809, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002079, mae: 0.077990, mean_q: 0.107311\n",
      " 21658/50000: episode: 2810, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001383, mae: 0.084399, mean_q: 0.116723\n",
      " 21664/50000: episode: 2811, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001549, mae: 0.077931, mean_q: 0.106447\n",
      " 21672/50000: episode: 2812, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001281, mae: 0.075325, mean_q: 0.103863\n",
      " 21685/50000: episode: 2813, duration: 0.186s, episode steps:  13, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.001814, mae: 0.083019, mean_q: 0.116066\n",
      " 21694/50000: episode: 2814, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001487, mae: 0.077948, mean_q: 0.111386\n",
      " 21696/50000: episode: 2815, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002004, mae: 0.072081, mean_q: 0.100891\n",
      " 21700/50000: episode: 2816, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002893, mae: 0.080323, mean_q: 0.112498\n",
      " 21704/50000: episode: 2817, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.001733, mae: 0.076696, mean_q: 0.107849\n",
      " 21708/50000: episode: 2818, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002344, mae: 0.090690, mean_q: 0.124997\n",
      " 21715/50000: episode: 2819, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001946, mae: 0.082698, mean_q: 0.116548\n",
      " 21717/50000: episode: 2820, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000753, mae: 0.071885, mean_q: 0.102050\n",
      " 21728/50000: episode: 2821, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.003095, mae: 0.082783, mean_q: 0.114419\n",
      " 21730/50000: episode: 2822, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001341, mae: 0.081748, mean_q: 0.115610\n",
      " 21738/50000: episode: 2823, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001856, mae: 0.086212, mean_q: 0.118584\n",
      " 21745/50000: episode: 2824, duration: 0.196s, episode steps:   7, steps per second:  36, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 1.286 [1.000, 2.000],  loss: 0.004375, mae: 0.092571, mean_q: 0.125058\n",
      " 21747/50000: episode: 2825, duration: 0.048s, episode steps:   2, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001264, mae: 0.081481, mean_q: 0.117063\n",
      " 21749/50000: episode: 2826, duration: 0.040s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001044, mae: 0.079052, mean_q: 0.114927\n",
      " 21756/50000: episode: 2827, duration: 0.114s, episode steps:   7, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001112, mae: 0.083621, mean_q: 0.122819\n",
      " 21761/50000: episode: 2828, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002162, mae: 0.083493, mean_q: 0.121054\n",
      " 21770/50000: episode: 2829, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001056, mae: 0.080293, mean_q: 0.113764\n",
      " 21775/50000: episode: 2830, duration: 0.102s, episode steps:   5, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001662, mae: 0.083545, mean_q: 0.111843\n",
      " 21781/50000: episode: 2831, duration: 0.121s, episode steps:   6, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003408, mae: 0.085341, mean_q: 0.112676\n",
      " 21797/50000: episode: 2832, duration: 0.254s, episode steps:  16, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001477, mae: 0.078222, mean_q: 0.104121\n",
      " 21806/50000: episode: 2833, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002148, mae: 0.076882, mean_q: 0.102116\n",
      " 21814/50000: episode: 2834, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001775, mae: 0.080272, mean_q: 0.107512\n",
      " 21822/50000: episode: 2835, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002481, mae: 0.082417, mean_q: 0.110158\n",
      " 21836/50000: episode: 2836, duration: 0.205s, episode steps:  14, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.001169, mae: 0.081971, mean_q: 0.114351\n",
      " 21840/50000: episode: 2837, duration: 0.067s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.001047, mae: 0.077624, mean_q: 0.107964\n",
      " 21845/50000: episode: 2838, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.000873, mae: 0.078265, mean_q: 0.108291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21850/50000: episode: 2839, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.000937, mae: 0.074766, mean_q: 0.104051\n",
      " 21855/50000: episode: 2840, duration: 0.080s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001169, mae: 0.080100, mean_q: 0.112315\n",
      " 21859/50000: episode: 2841, duration: 0.074s, episode steps:   4, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003096, mae: 0.076234, mean_q: 0.103892\n",
      " 21864/50000: episode: 2842, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.001001, mae: 0.078592, mean_q: 0.106459\n",
      " 21873/50000: episode: 2843, duration: 0.131s, episode steps:   9, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001368, mae: 0.083554, mean_q: 0.113201\n",
      " 21879/50000: episode: 2844, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001100, mae: 0.075032, mean_q: 0.100798\n",
      " 21884/50000: episode: 2845, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.002136, mae: 0.077077, mean_q: 0.103627\n",
      " 21894/50000: episode: 2846, duration: 0.148s, episode steps:  10, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001842, mae: 0.080917, mean_q: 0.108068\n",
      " 21897/50000: episode: 2847, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001534, mae: 0.075047, mean_q: 0.102288\n",
      " 21899/50000: episode: 2848, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.000723, mae: 0.072885, mean_q: 0.101122\n",
      " 21906/50000: episode: 2849, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 0.714 [0.000, 2.000],  loss: 0.001729, mae: 0.079758, mean_q: 0.107724\n",
      " 21918/50000: episode: 2850, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.917 [0.000, 3.000],  loss: 0.001494, mae: 0.077280, mean_q: 0.106108\n",
      " 21927/50000: episode: 2851, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001886, mae: 0.079114, mean_q: 0.106350\n",
      " 21943/50000: episode: 2852, duration: 0.222s, episode steps:  16, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002059, mae: 0.087191, mean_q: 0.120297\n",
      " 21950/50000: episode: 2853, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001387, mae: 0.077897, mean_q: 0.108248\n",
      " 21954/50000: episode: 2854, duration: 0.075s, episode steps:   4, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.000857, mae: 0.068823, mean_q: 0.095859\n",
      " 21971/50000: episode: 2855, duration: 0.270s, episode steps:  17, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.001192, mae: 0.074605, mean_q: 0.102396\n",
      " 21976/50000: episode: 2856, duration: 0.084s, episode steps:   5, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002324, mae: 0.081726, mean_q: 0.111720\n",
      " 21995/50000: episode: 2857, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.001729, mae: 0.079090, mean_q: 0.108320\n",
      " 21997/50000: episode: 2858, duration: 0.043s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001054, mae: 0.070804, mean_q: 0.096174\n",
      " 21999/50000: episode: 2859, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001007, mae: 0.073053, mean_q: 0.101848\n",
      " 22009/50000: episode: 2860, duration: 0.197s, episode steps:  10, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001012, mae: 0.074657, mean_q: 0.103042\n",
      " 22012/50000: episode: 2861, duration: 0.061s, episode steps:   3, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002232, mae: 0.076920, mean_q: 0.103442\n",
      " 22016/50000: episode: 2862, duration: 0.073s, episode steps:   4, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001512, mae: 0.082280, mean_q: 0.111987\n",
      " 22022/50000: episode: 2863, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001395, mae: 0.079340, mean_q: 0.106647\n",
      " 22024/50000: episode: 2864, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002446, mae: 0.084320, mean_q: 0.112344\n",
      " 22028/50000: episode: 2865, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002810, mae: 0.085861, mean_q: 0.115088\n",
      " 22030/50000: episode: 2866, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002269, mae: 0.082317, mean_q: 0.107850\n",
      " 22036/50000: episode: 2867, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001398, mae: 0.075889, mean_q: 0.101296\n",
      " 22040/50000: episode: 2868, duration: 0.071s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.001389, mae: 0.077276, mean_q: 0.105981\n",
      " 22042/50000: episode: 2869, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001450, mae: 0.079058, mean_q: 0.107027\n",
      " 22059/50000: episode: 2870, duration: 0.229s, episode steps:  17, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.002074, mae: 0.081967, mean_q: 0.111828\n",
      " 22065/50000: episode: 2871, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001296, mae: 0.077295, mean_q: 0.108278\n",
      " 22075/50000: episode: 2872, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.100 [0.000, 3.000],  loss: 0.001441, mae: 0.076134, mean_q: 0.104121\n",
      " 22097/50000: episode: 2873, duration: 0.285s, episode steps:  22, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.045 [0.000, 3.000],  loss: 0.001569, mae: 0.073094, mean_q: 0.100825\n",
      " 22105/50000: episode: 2874, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001269, mae: 0.077166, mean_q: 0.105177\n",
      " 22107/50000: episode: 2875, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000664, mae: 0.067972, mean_q: 0.092496\n",
      " 22109/50000: episode: 2876, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004257, mae: 0.077525, mean_q: 0.103430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22114/50000: episode: 2877, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 1.000],  loss: 0.001158, mae: 0.075335, mean_q: 0.101797\n",
      " 22122/50000: episode: 2878, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001376, mae: 0.077771, mean_q: 0.106361\n",
      " 22124/50000: episode: 2879, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000754, mae: 0.073036, mean_q: 0.099900\n",
      " 22132/50000: episode: 2880, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002027, mae: 0.076588, mean_q: 0.105805\n",
      " 22135/50000: episode: 2881, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001045, mae: 0.067247, mean_q: 0.097325\n",
      " 22140/50000: episode: 2882, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002804, mae: 0.082720, mean_q: 0.115444\n",
      " 22149/50000: episode: 2883, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002234, mae: 0.086260, mean_q: 0.119064\n",
      " 22151/50000: episode: 2884, duration: 0.045s, episode steps:   2, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000437, mae: 0.069892, mean_q: 0.100595\n",
      " 22154/50000: episode: 2885, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001006, mae: 0.077512, mean_q: 0.109686\n",
      " 22185/50000: episode: 2886, duration: 0.376s, episode steps:  31, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.323 [0.000, 3.000],  loss: 0.001560, mae: 0.077294, mean_q: 0.106443\n",
      " 22188/50000: episode: 2887, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002106, mae: 0.078548, mean_q: 0.107355\n",
      " 22197/50000: episode: 2888, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001232, mae: 0.076540, mean_q: 0.107336\n",
      " 22202/50000: episode: 2889, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.001178, mae: 0.074975, mean_q: 0.105850\n",
      " 22206/50000: episode: 2890, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002129, mae: 0.078889, mean_q: 0.109757\n",
      " 22215/50000: episode: 2891, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002046, mae: 0.076515, mean_q: 0.106523\n",
      " 22222/50000: episode: 2892, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000718, mae: 0.075628, mean_q: 0.106470\n",
      " 22227/50000: episode: 2893, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001188, mae: 0.078852, mean_q: 0.109412\n",
      " 22230/50000: episode: 2894, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.000768, mae: 0.080906, mean_q: 0.115289\n",
      " 22254/50000: episode: 2895, duration: 0.295s, episode steps:  24, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002086, mae: 0.079679, mean_q: 0.109900\n",
      " 22265/50000: episode: 2896, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001667, mae: 0.080290, mean_q: 0.110681\n",
      " 22277/50000: episode: 2897, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002189, mae: 0.084563, mean_q: 0.118343\n",
      " 22285/50000: episode: 2898, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002000, mae: 0.086465, mean_q: 0.121393\n",
      " 22288/50000: episode: 2899, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002978, mae: 0.093264, mean_q: 0.130881\n",
      " 22292/50000: episode: 2900, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002193, mae: 0.081138, mean_q: 0.112664\n",
      " 22294/50000: episode: 2901, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001459, mae: 0.073369, mean_q: 0.100399\n",
      " 22298/50000: episode: 2902, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001703, mae: 0.075511, mean_q: 0.104274\n",
      " 22304/50000: episode: 2903, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.003525, mae: 0.090225, mean_q: 0.123659\n",
      " 22314/50000: episode: 2904, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002694, mae: 0.085843, mean_q: 0.119263\n",
      " 22323/50000: episode: 2905, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002449, mae: 0.084314, mean_q: 0.117411\n",
      " 22329/50000: episode: 2906, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001178, mae: 0.070686, mean_q: 0.097109\n",
      " 22335/50000: episode: 2907, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002744, mae: 0.075860, mean_q: 0.102975\n",
      " 22345/50000: episode: 2908, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.001596, mae: 0.082643, mean_q: 0.112507\n",
      " 22348/50000: episode: 2909, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001818, mae: 0.084194, mean_q: 0.115681\n",
      " 22353/50000: episode: 2910, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [2.000, 3.000],  loss: 0.001589, mae: 0.081003, mean_q: 0.109816\n",
      " 22369/50000: episode: 2911, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.003091, mae: 0.079027, mean_q: 0.107046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22373/50000: episode: 2912, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003362, mae: 0.087365, mean_q: 0.121146\n",
      " 22384/50000: episode: 2913, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.001568, mae: 0.083827, mean_q: 0.115782\n",
      " 22408/50000: episode: 2914, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001742, mae: 0.078365, mean_q: 0.107840\n",
      " 22410/50000: episode: 2915, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001245, mae: 0.078295, mean_q: 0.107977\n",
      " 22416/50000: episode: 2916, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.001381, mae: 0.082261, mean_q: 0.117990\n",
      " 22421/50000: episode: 2917, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002194, mae: 0.086174, mean_q: 0.124371\n",
      " 22434/50000: episode: 2918, duration: 0.168s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001821, mae: 0.082114, mean_q: 0.115692\n",
      " 22445/50000: episode: 2919, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.545 [0.000, 2.000],  loss: 0.001206, mae: 0.078419, mean_q: 0.109653\n",
      " 22447/50000: episode: 2920, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002796, mae: 0.084285, mean_q: 0.114513\n",
      " 22472/50000: episode: 2921, duration: 0.333s, episode steps:  25, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.002159, mae: 0.081567, mean_q: 0.112962\n",
      " 22475/50000: episode: 2922, duration: 0.057s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001385, mae: 0.081099, mean_q: 0.115274\n",
      " 22480/50000: episode: 2923, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002273, mae: 0.085848, mean_q: 0.118600\n",
      " 22482/50000: episode: 2924, duration: 0.056s, episode steps:   2, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001599, mae: 0.079068, mean_q: 0.109336\n",
      " 22488/50000: episode: 2925, duration: 0.144s, episode steps:   6, steps per second:  42, episode reward:  1.000, mean reward:  0.167 [ 0.000,  1.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002469, mae: 0.089109, mean_q: 0.126262\n",
      " 22492/50000: episode: 2926, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002392, mae: 0.080297, mean_q: 0.113421\n",
      " 22497/50000: episode: 2927, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002145, mae: 0.082825, mean_q: 0.121371\n",
      " 22507/50000: episode: 2928, duration: 0.138s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002046, mae: 0.078012, mean_q: 0.111985\n",
      " 22510/50000: episode: 2929, duration: 0.054s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001235, mae: 0.083257, mean_q: 0.116041\n",
      " 22519/50000: episode: 2930, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002396, mae: 0.084964, mean_q: 0.118190\n",
      " 22530/50000: episode: 2931, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002122, mae: 0.088132, mean_q: 0.123396\n",
      " 22537/50000: episode: 2932, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002337, mae: 0.085952, mean_q: 0.121861\n",
      " 22546/50000: episode: 2933, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002313, mae: 0.088383, mean_q: 0.121123\n",
      " 22555/50000: episode: 2934, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003199, mae: 0.090598, mean_q: 0.122185\n",
      " 22563/50000: episode: 2935, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002032, mae: 0.087278, mean_q: 0.117868\n",
      " 22566/50000: episode: 2936, duration: 0.053s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.000769, mae: 0.076825, mean_q: 0.105474\n",
      " 22571/50000: episode: 2937, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.004705, mae: 0.083787, mean_q: 0.112771\n",
      " 22574/50000: episode: 2938, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003112, mae: 0.089722, mean_q: 0.119451\n",
      " 22576/50000: episode: 2939, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002635, mae: 0.084835, mean_q: 0.114313\n",
      " 22581/50000: episode: 2940, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002589, mae: 0.085230, mean_q: 0.116275\n",
      " 22586/50000: episode: 2941, duration: 0.076s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001820, mae: 0.084634, mean_q: 0.121388\n",
      " 22592/50000: episode: 2942, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001093, mae: 0.084858, mean_q: 0.123813\n",
      " 22597/50000: episode: 2943, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 1.000],  loss: 0.000912, mae: 0.082561, mean_q: 0.118448\n",
      " 22601/50000: episode: 2944, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001349, mae: 0.077844, mean_q: 0.108625\n",
      " 22603/50000: episode: 2945, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001248, mae: 0.073898, mean_q: 0.103957\n",
      " 22605/50000: episode: 2946, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004358, mae: 0.092780, mean_q: 0.132044\n",
      " 22612/50000: episode: 2947, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002872, mae: 0.081632, mean_q: 0.113069\n",
      " 22614/50000: episode: 2948, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000936, mae: 0.080116, mean_q: 0.112842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22626/50000: episode: 2949, duration: 0.165s, episode steps:  12, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002559, mae: 0.092400, mean_q: 0.126219\n",
      " 22632/50000: episode: 2950, duration: 0.109s, episode steps:   6, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002631, mae: 0.091635, mean_q: 0.121384\n",
      " 22638/50000: episode: 2951, duration: 0.143s, episode steps:   6, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.003622, mae: 0.080156, mean_q: 0.103370\n",
      " 22644/50000: episode: 2952, duration: 0.124s, episode steps:   6, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002200, mae: 0.077999, mean_q: 0.105100\n",
      " 22650/50000: episode: 2953, duration: 0.100s, episode steps:   6, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001183, mae: 0.079222, mean_q: 0.112741\n",
      " 22653/50000: episode: 2954, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002485, mae: 0.084446, mean_q: 0.117822\n",
      " 22655/50000: episode: 2955, duration: 0.042s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001044, mae: 0.078908, mean_q: 0.110302\n",
      " 22663/50000: episode: 2956, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001859, mae: 0.085721, mean_q: 0.115468\n",
      " 22676/50000: episode: 2957, duration: 0.196s, episode steps:  13, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002266, mae: 0.086197, mean_q: 0.117801\n",
      " 22680/50000: episode: 2958, duration: 0.114s, episode steps:   4, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001747, mae: 0.087737, mean_q: 0.121618\n",
      " 22687/50000: episode: 2959, duration: 0.161s, episode steps:   7, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.003115, mae: 0.090112, mean_q: 0.122512\n",
      " 22713/50000: episode: 2960, duration: 0.392s, episode steps:  26, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.001670, mae: 0.083258, mean_q: 0.115132\n",
      " 22723/50000: episode: 2961, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002605, mae: 0.083144, mean_q: 0.113454\n",
      " 22726/50000: episode: 2962, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003474, mae: 0.088643, mean_q: 0.119804\n",
      " 22731/50000: episode: 2963, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001794, mae: 0.094482, mean_q: 0.128925\n",
      " 22740/50000: episode: 2964, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001445, mae: 0.081288, mean_q: 0.111403\n",
      " 22746/50000: episode: 2965, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001892, mae: 0.075744, mean_q: 0.105101\n",
      " 22748/50000: episode: 2966, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001329, mae: 0.083176, mean_q: 0.118442\n",
      " 22760/50000: episode: 2967, duration: 0.170s, episode steps:  12, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002265, mae: 0.083416, mean_q: 0.115647\n",
      " 22769/50000: episode: 2968, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001528, mae: 0.081393, mean_q: 0.113446\n",
      " 22781/50000: episode: 2969, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001195, mae: 0.081231, mean_q: 0.112779\n",
      " 22794/50000: episode: 2970, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 2.000],  loss: 0.002005, mae: 0.084072, mean_q: 0.117868\n",
      " 22798/50000: episode: 2971, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001942, mae: 0.081918, mean_q: 0.113471\n",
      " 22808/50000: episode: 2972, duration: 0.126s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001954, mae: 0.082236, mean_q: 0.112115\n",
      " 22810/50000: episode: 2973, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005708, mae: 0.097049, mean_q: 0.129502\n",
      " 22817/50000: episode: 2974, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002074, mae: 0.084511, mean_q: 0.116140\n",
      " 22843/50000: episode: 2975, duration: 0.518s, episode steps:  26, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.001662, mae: 0.084375, mean_q: 0.115321\n",
      " 22846/50000: episode: 2976, duration: 0.077s, episode steps:   3, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001385, mae: 0.086457, mean_q: 0.116383\n",
      " 22849/50000: episode: 2977, duration: 0.069s, episode steps:   3, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000817, mae: 0.077480, mean_q: 0.105940\n",
      " 22854/50000: episode: 2978, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001299, mae: 0.077107, mean_q: 0.105051\n",
      " 22863/50000: episode: 2979, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001596, mae: 0.075791, mean_q: 0.106093\n",
      " 22869/50000: episode: 2980, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.001481, mae: 0.086993, mean_q: 0.121042\n",
      " 22877/50000: episode: 2981, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002005, mae: 0.086909, mean_q: 0.120786\n",
      " 22881/50000: episode: 2982, duration: 0.089s, episode steps:   4, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002943, mae: 0.096563, mean_q: 0.134224\n",
      " 22884/50000: episode: 2983, duration: 0.070s, episode steps:   3, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.000869, mae: 0.080931, mean_q: 0.111984\n",
      " 22886/50000: episode: 2984, duration: 0.046s, episode steps:   2, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001922, mae: 0.092406, mean_q: 0.124895\n",
      " 22889/50000: episode: 2985, duration: 0.066s, episode steps:   3, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.000627, mae: 0.083347, mean_q: 0.115093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22894/50000: episode: 2986, duration: 0.102s, episode steps:   5, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002943, mae: 0.091285, mean_q: 0.124268\n",
      " 22896/50000: episode: 2987, duration: 0.050s, episode steps:   2, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001793, mae: 0.088063, mean_q: 0.122705\n",
      " 22900/50000: episode: 2988, duration: 0.077s, episode steps:   4, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002280, mae: 0.082980, mean_q: 0.115725\n",
      " 22902/50000: episode: 2989, duration: 0.042s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001241, mae: 0.080056, mean_q: 0.112477\n",
      " 22904/50000: episode: 2990, duration: 0.061s, episode steps:   2, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002102, mae: 0.075655, mean_q: 0.106474\n",
      " 22909/50000: episode: 2991, duration: 0.109s, episode steps:   5, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002258, mae: 0.083216, mean_q: 0.115854\n",
      " 22911/50000: episode: 2992, duration: 0.055s, episode steps:   2, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004738, mae: 0.100580, mean_q: 0.138287\n",
      " 22920/50000: episode: 2993, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.001972, mae: 0.093728, mean_q: 0.129515\n",
      " 22922/50000: episode: 2994, duration: 0.048s, episode steps:   2, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001624, mae: 0.086158, mean_q: 0.116390\n",
      " 22926/50000: episode: 2995, duration: 0.076s, episode steps:   4, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002303, mae: 0.089566, mean_q: 0.122293\n",
      " 22934/50000: episode: 2996, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001875, mae: 0.085456, mean_q: 0.115253\n",
      " 22948/50000: episode: 2997, duration: 0.214s, episode steps:  14, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002548, mae: 0.089068, mean_q: 0.125169\n",
      " 22953/50000: episode: 2998, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001060, mae: 0.086528, mean_q: 0.125059\n",
      " 22957/50000: episode: 2999, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001311, mae: 0.089170, mean_q: 0.125702\n",
      " 22960/50000: episode: 3000, duration: 0.067s, episode steps:   3, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002014, mae: 0.083571, mean_q: 0.115707\n",
      " 22963/50000: episode: 3001, duration: 0.057s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000658, mae: 0.080535, mean_q: 0.111253\n",
      " 22991/50000: episode: 3002, duration: 0.415s, episode steps:  28, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.002307, mae: 0.089530, mean_q: 0.120961\n",
      " 23003/50000: episode: 3003, duration: 0.186s, episode steps:  12, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001565, mae: 0.090414, mean_q: 0.123272\n",
      " 23014/50000: episode: 3004, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.001661, mae: 0.087986, mean_q: 0.121652\n",
      " 23026/50000: episode: 3005, duration: 0.172s, episode steps:  12, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001481, mae: 0.081739, mean_q: 0.115129\n",
      " 23032/50000: episode: 3006, duration: 0.104s, episode steps:   6, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001605, mae: 0.086113, mean_q: 0.119298\n",
      " 23051/50000: episode: 3007, duration: 0.280s, episode steps:  19, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.158 [0.000, 3.000],  loss: 0.002456, mae: 0.089729, mean_q: 0.123548\n",
      " 23056/50000: episode: 3008, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002081, mae: 0.084307, mean_q: 0.113882\n",
      " 23061/50000: episode: 3009, duration: 0.081s, episode steps:   5, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.001782, mae: 0.086994, mean_q: 0.119132\n",
      " 23073/50000: episode: 3010, duration: 0.174s, episode steps:  12, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.917 [0.000, 3.000],  loss: 0.002607, mae: 0.090840, mean_q: 0.124888\n",
      " 23085/50000: episode: 3011, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001935, mae: 0.082337, mean_q: 0.112839\n",
      " 23098/50000: episode: 3012, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.001929, mae: 0.085179, mean_q: 0.116651\n",
      " 23107/50000: episode: 3013, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002031, mae: 0.088793, mean_q: 0.120723\n",
      " 23111/50000: episode: 3014, duration: 0.097s, episode steps:   4, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.000839, mae: 0.083437, mean_q: 0.116129\n",
      " 23117/50000: episode: 3015, duration: 0.128s, episode steps:   6, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001802, mae: 0.093000, mean_q: 0.128943\n",
      " 23119/50000: episode: 3016, duration: 0.065s, episode steps:   2, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.000671, mae: 0.079825, mean_q: 0.112357\n",
      " 23141/50000: episode: 3017, duration: 0.362s, episode steps:  22, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001816, mae: 0.083282, mean_q: 0.117073\n",
      " 23146/50000: episode: 3018, duration: 0.088s, episode steps:   5, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001985, mae: 0.084459, mean_q: 0.119832\n",
      " 23149/50000: episode: 3019, duration: 0.058s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001494, mae: 0.086911, mean_q: 0.124231\n",
      " 23156/50000: episode: 3020, duration: 0.123s, episode steps:   7, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001145, mae: 0.081430, mean_q: 0.117470\n",
      " 23160/50000: episode: 3021, duration: 0.073s, episode steps:   4, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001807, mae: 0.086069, mean_q: 0.121083\n",
      " 23163/50000: episode: 3022, duration: 0.057s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001717, mae: 0.085644, mean_q: 0.121734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23168/50000: episode: 3023, duration: 0.088s, episode steps:   5, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001724, mae: 0.087194, mean_q: 0.122859\n",
      " 23177/50000: episode: 3024, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001909, mae: 0.082612, mean_q: 0.116166\n",
      " 23182/50000: episode: 3025, duration: 0.097s, episode steps:   5, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.600 [2.000, 3.000],  loss: 0.001423, mae: 0.090499, mean_q: 0.125821\n",
      " 23184/50000: episode: 3026, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001843, mae: 0.089846, mean_q: 0.124907\n",
      " 23190/50000: episode: 3027, duration: 0.100s, episode steps:   6, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003220, mae: 0.090972, mean_q: 0.126920\n",
      " 23194/50000: episode: 3028, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003467, mae: 0.095298, mean_q: 0.133888\n",
      " 23219/50000: episode: 3029, duration: 0.361s, episode steps:  25, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.001778, mae: 0.085446, mean_q: 0.118334\n",
      " 23221/50000: episode: 3030, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003414, mae: 0.091030, mean_q: 0.123147\n",
      " 23224/50000: episode: 3031, duration: 0.054s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001577, mae: 0.086025, mean_q: 0.120675\n",
      " 23236/50000: episode: 3032, duration: 0.186s, episode steps:  12, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002421, mae: 0.093786, mean_q: 0.129764\n",
      " 23243/50000: episode: 3033, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001618, mae: 0.095077, mean_q: 0.132399\n",
      " 23264/50000: episode: 3034, duration: 0.310s, episode steps:  21, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001734, mae: 0.083874, mean_q: 0.117561\n",
      " 23278/50000: episode: 3035, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.001858, mae: 0.092992, mean_q: 0.129187\n",
      " 23280/50000: episode: 3036, duration: 0.043s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001526, mae: 0.087232, mean_q: 0.122602\n",
      " 23293/50000: episode: 3037, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.002449, mae: 0.093685, mean_q: 0.129041\n",
      " 23310/50000: episode: 3038, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002140, mae: 0.085394, mean_q: 0.119491\n",
      " 23314/50000: episode: 3039, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002589, mae: 0.089926, mean_q: 0.123978\n",
      " 23318/50000: episode: 3040, duration: 0.074s, episode steps:   4, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.001378, mae: 0.087959, mean_q: 0.122483\n",
      " 23320/50000: episode: 3041, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001716, mae: 0.090431, mean_q: 0.127859\n",
      " 23328/50000: episode: 3042, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001926, mae: 0.085178, mean_q: 0.117776\n",
      " 23330/50000: episode: 3043, duration: 0.048s, episode steps:   2, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001109, mae: 0.092687, mean_q: 0.131155\n",
      " 23345/50000: episode: 3044, duration: 0.233s, episode steps:  15, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002887, mae: 0.095777, mean_q: 0.131250\n",
      " 23357/50000: episode: 3045, duration: 0.184s, episode steps:  12, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002943, mae: 0.087914, mean_q: 0.123550\n",
      " 23364/50000: episode: 3046, duration: 0.113s, episode steps:   7, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 3.000],  loss: 0.001271, mae: 0.089697, mean_q: 0.123344\n",
      " 23371/50000: episode: 3047, duration: 0.116s, episode steps:   7, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002334, mae: 0.095100, mean_q: 0.130040\n",
      " 23376/50000: episode: 3048, duration: 0.089s, episode steps:   5, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001979, mae: 0.093556, mean_q: 0.127981\n",
      " 23383/50000: episode: 3049, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001358, mae: 0.086897, mean_q: 0.120952\n",
      " 23387/50000: episode: 3050, duration: 0.080s, episode steps:   4, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001736, mae: 0.095988, mean_q: 0.132930\n",
      " 23394/50000: episode: 3051, duration: 0.125s, episode steps:   7, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001503, mae: 0.082912, mean_q: 0.115966\n",
      " 23398/50000: episode: 3052, duration: 0.071s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001743, mae: 0.088323, mean_q: 0.127453\n",
      " 23414/50000: episode: 3053, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.001827, mae: 0.091965, mean_q: 0.131478\n",
      " 23422/50000: episode: 3054, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [0.000, 3.000],  loss: 0.003028, mae: 0.096772, mean_q: 0.134736\n",
      " 23431/50000: episode: 3055, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001534, mae: 0.088245, mean_q: 0.124505\n",
      " 23433/50000: episode: 3056, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.006484, mae: 0.105717, mean_q: 0.140282\n",
      " 23451/50000: episode: 3057, duration: 0.310s, episode steps:  18, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001855, mae: 0.089297, mean_q: 0.123496\n",
      " 23459/50000: episode: 3058, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001853, mae: 0.090897, mean_q: 0.124415\n",
      " 23463/50000: episode: 3059, duration: 0.067s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002373, mae: 0.091515, mean_q: 0.124873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23472/50000: episode: 3060, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.002180, mae: 0.089781, mean_q: 0.125009\n",
      " 23485/50000: episode: 3061, duration: 0.181s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.003204, mae: 0.097954, mean_q: 0.136415\n",
      " 23492/50000: episode: 3062, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.002137, mae: 0.097584, mean_q: 0.136264\n",
      " 23496/50000: episode: 3063, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001244, mae: 0.092987, mean_q: 0.131053\n",
      " 23505/50000: episode: 3064, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002108, mae: 0.100494, mean_q: 0.137866\n",
      " 23513/50000: episode: 3065, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002159, mae: 0.098821, mean_q: 0.133808\n",
      " 23516/50000: episode: 3066, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.000909, mae: 0.090624, mean_q: 0.124432\n",
      " 23534/50000: episode: 3067, duration: 0.239s, episode steps:  18, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001865, mae: 0.091994, mean_q: 0.129781\n",
      " 23541/50000: episode: 3068, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001654, mae: 0.094963, mean_q: 0.131097\n",
      " 23546/50000: episode: 3069, duration: 0.081s, episode steps:   5, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001593, mae: 0.089738, mean_q: 0.124093\n",
      " 23559/50000: episode: 3070, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.001697, mae: 0.090377, mean_q: 0.127125\n",
      " 23574/50000: episode: 3071, duration: 0.201s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002536, mae: 0.097800, mean_q: 0.135468\n",
      " 23582/50000: episode: 3072, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001569, mae: 0.093572, mean_q: 0.130754\n",
      " 23596/50000: episode: 3073, duration: 0.193s, episode steps:  14, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001286, mae: 0.091754, mean_q: 0.128669\n",
      " 23598/50000: episode: 3074, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001347, mae: 0.098376, mean_q: 0.138775\n",
      " 23600/50000: episode: 3075, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001987, mae: 0.100456, mean_q: 0.144999\n",
      " 23607/50000: episode: 3076, duration: 0.109s, episode steps:   7, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.000989, mae: 0.094647, mean_q: 0.138251\n",
      " 23609/50000: episode: 3077, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001275, mae: 0.103385, mean_q: 0.149737\n",
      " 23616/50000: episode: 3078, duration: 0.099s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001312, mae: 0.091467, mean_q: 0.132816\n",
      " 23619/50000: episode: 3079, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000910, mae: 0.089090, mean_q: 0.125466\n",
      " 23625/50000: episode: 3080, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004577, mae: 0.111258, mean_q: 0.149080\n",
      " 23632/50000: episode: 3081, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002886, mae: 0.101115, mean_q: 0.138033\n",
      " 23651/50000: episode: 3082, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002024, mae: 0.092996, mean_q: 0.128799\n",
      " 23656/50000: episode: 3083, duration: 0.080s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001800, mae: 0.095261, mean_q: 0.129857\n",
      " 23666/50000: episode: 3084, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003330, mae: 0.103191, mean_q: 0.139186\n",
      " 23669/50000: episode: 3085, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002148, mae: 0.092596, mean_q: 0.127301\n",
      " 23680/50000: episode: 3086, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002124, mae: 0.099492, mean_q: 0.138061\n",
      " 23697/50000: episode: 3087, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002506, mae: 0.101198, mean_q: 0.139386\n",
      " 23706/50000: episode: 3088, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.001920, mae: 0.090056, mean_q: 0.125431\n",
      " 23720/50000: episode: 3089, duration: 0.193s, episode steps:  14, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002421, mae: 0.096307, mean_q: 0.133382\n",
      " 23726/50000: episode: 3090, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002807, mae: 0.092326, mean_q: 0.127843\n",
      " 23741/50000: episode: 3091, duration: 0.201s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002100, mae: 0.094132, mean_q: 0.132015\n",
      " 23745/50000: episode: 3092, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001689, mae: 0.095635, mean_q: 0.132940\n",
      " 23750/50000: episode: 3093, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001512, mae: 0.091773, mean_q: 0.127442\n",
      " 23761/50000: episode: 3094, duration: 0.158s, episode steps:  11, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.001603, mae: 0.096140, mean_q: 0.134592\n",
      " 23765/50000: episode: 3095, duration: 0.072s, episode steps:   4, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001972, mae: 0.100299, mean_q: 0.140860\n",
      " 23770/50000: episode: 3096, duration: 0.081s, episode steps:   5, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003145, mae: 0.097641, mean_q: 0.132992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23778/50000: episode: 3097, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002176, mae: 0.094792, mean_q: 0.128158\n",
      " 23786/50000: episode: 3098, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002771, mae: 0.099088, mean_q: 0.136349\n",
      " 23796/50000: episode: 3099, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001789, mae: 0.095304, mean_q: 0.131861\n",
      " 23800/50000: episode: 3100, duration: 0.070s, episode steps:   4, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002422, mae: 0.102048, mean_q: 0.138956\n",
      " 23808/50000: episode: 3101, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003091, mae: 0.101545, mean_q: 0.136521\n",
      " 23831/50000: episode: 3102, duration: 0.334s, episode steps:  23, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001630, mae: 0.092466, mean_q: 0.126403\n",
      " 23833/50000: episode: 3103, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004143, mae: 0.110509, mean_q: 0.152150\n",
      " 23843/50000: episode: 3104, duration: 0.148s, episode steps:  10, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002512, mae: 0.104106, mean_q: 0.143769\n",
      " 23847/50000: episode: 3105, duration: 0.071s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002561, mae: 0.101672, mean_q: 0.138972\n",
      " 23852/50000: episode: 3106, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001135, mae: 0.098889, mean_q: 0.139971\n",
      " 23855/50000: episode: 3107, duration: 0.059s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001012, mae: 0.095835, mean_q: 0.133732\n",
      " 23870/50000: episode: 3108, duration: 0.229s, episode steps:  15, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002441, mae: 0.099779, mean_q: 0.139692\n",
      " 23880/50000: episode: 3109, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.002357, mae: 0.092131, mean_q: 0.129710\n",
      " 23885/50000: episode: 3110, duration: 0.097s, episode steps:   5, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.001080, mae: 0.092051, mean_q: 0.130154\n",
      " 23890/50000: episode: 3111, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002541, mae: 0.102183, mean_q: 0.138320\n",
      " 23893/50000: episode: 3112, duration: 0.059s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001801, mae: 0.104950, mean_q: 0.142179\n",
      " 23902/50000: episode: 3113, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001998, mae: 0.097134, mean_q: 0.134157\n",
      " 23906/50000: episode: 3114, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001409, mae: 0.092240, mean_q: 0.130010\n",
      " 23917/50000: episode: 3115, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.818 [0.000, 3.000],  loss: 0.001573, mae: 0.099205, mean_q: 0.138747\n",
      " 23920/50000: episode: 3116, duration: 0.058s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003072, mae: 0.101150, mean_q: 0.140124\n",
      " 23932/50000: episode: 3117, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002401, mae: 0.099488, mean_q: 0.142634\n",
      " 23936/50000: episode: 3118, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001925, mae: 0.100625, mean_q: 0.146464\n",
      " 23939/50000: episode: 3119, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001813, mae: 0.096566, mean_q: 0.138885\n",
      " 23943/50000: episode: 3120, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001717, mae: 0.094757, mean_q: 0.132797\n",
      " 23954/50000: episode: 3121, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001592, mae: 0.098666, mean_q: 0.135470\n",
      " 23959/50000: episode: 3122, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003696, mae: 0.099004, mean_q: 0.134596\n",
      " 23965/50000: episode: 3123, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002320, mae: 0.095923, mean_q: 0.131104\n",
      " 23967/50000: episode: 3124, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003254, mae: 0.106755, mean_q: 0.148111\n",
      " 23972/50000: episode: 3125, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001999, mae: 0.098306, mean_q: 0.137941\n",
      " 23991/50000: episode: 3126, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001727, mae: 0.099589, mean_q: 0.140933\n",
      " 23993/50000: episode: 3127, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004713, mae: 0.099950, mean_q: 0.135548\n",
      " 24006/50000: episode: 3128, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002402, mae: 0.109684, mean_q: 0.151666\n",
      " 24014/50000: episode: 3129, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002159, mae: 0.098206, mean_q: 0.136824\n",
      " 24020/50000: episode: 3130, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002224, mae: 0.095497, mean_q: 0.132628\n",
      " 24037/50000: episode: 3131, duration: 0.204s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.002689, mae: 0.105592, mean_q: 0.144214\n",
      " 24050/50000: episode: 3132, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002210, mae: 0.094200, mean_q: 0.129190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24055/50000: episode: 3133, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.001754, mae: 0.100276, mean_q: 0.138881\n",
      " 24059/50000: episode: 3134, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002411, mae: 0.101719, mean_q: 0.140236\n",
      " 24062/50000: episode: 3135, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001640, mae: 0.098260, mean_q: 0.137826\n",
      " 24079/50000: episode: 3136, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.176 [0.000, 3.000],  loss: 0.002249, mae: 0.098520, mean_q: 0.138132\n",
      " 24087/50000: episode: 3137, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002127, mae: 0.103281, mean_q: 0.144289\n",
      " 24094/50000: episode: 3138, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [1.000, 2.000],  loss: 0.002197, mae: 0.101178, mean_q: 0.145374\n",
      " 24106/50000: episode: 3139, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002590, mae: 0.108865, mean_q: 0.151948\n",
      " 24117/50000: episode: 3140, duration: 0.140s, episode steps:  11, steps per second:  79, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.909 [1.000, 3.000],  loss: 0.001982, mae: 0.098061, mean_q: 0.137300\n",
      " 24120/50000: episode: 3141, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001298, mae: 0.103871, mean_q: 0.142988\n",
      " 24124/50000: episode: 3142, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002328, mae: 0.109505, mean_q: 0.152894\n",
      " 24128/50000: episode: 3143, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002342, mae: 0.099960, mean_q: 0.138866\n",
      " 24132/50000: episode: 3144, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003336, mae: 0.112068, mean_q: 0.155793\n",
      " 24138/50000: episode: 3145, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002876, mae: 0.106648, mean_q: 0.149162\n",
      " 24140/50000: episode: 3146, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001174, mae: 0.106180, mean_q: 0.150828\n",
      " 24148/50000: episode: 3147, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002759, mae: 0.107067, mean_q: 0.147525\n",
      " 24155/50000: episode: 3148, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001939, mae: 0.097866, mean_q: 0.136178\n",
      " 24157/50000: episode: 3149, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001078, mae: 0.095960, mean_q: 0.135686\n",
      " 24160/50000: episode: 3150, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001156, mae: 0.096712, mean_q: 0.133242\n",
      " 24163/50000: episode: 3151, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000807, mae: 0.098421, mean_q: 0.138839\n",
      " 24176/50000: episode: 3152, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.001555, mae: 0.101675, mean_q: 0.139690\n",
      " 24199/50000: episode: 3153, duration: 0.280s, episode steps:  23, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.002259, mae: 0.101082, mean_q: 0.138372\n",
      " 24208/50000: episode: 3154, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001771, mae: 0.101955, mean_q: 0.144526\n",
      " 24214/50000: episode: 3155, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002171, mae: 0.097477, mean_q: 0.137681\n",
      " 24218/50000: episode: 3156, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004559, mae: 0.103995, mean_q: 0.142754\n",
      " 24234/50000: episode: 3157, duration: 0.208s, episode steps:  16, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002003, mae: 0.102537, mean_q: 0.142379\n",
      " 24243/50000: episode: 3158, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.222 [0.000, 1.000],  loss: 0.001866, mae: 0.097692, mean_q: 0.138126\n",
      " 24249/50000: episode: 3159, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.002875, mae: 0.105926, mean_q: 0.147534\n",
      " 24253/50000: episode: 3160, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002779, mae: 0.111641, mean_q: 0.153253\n",
      " 24265/50000: episode: 3161, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002010, mae: 0.103942, mean_q: 0.143930\n",
      " 24269/50000: episode: 3162, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002018, mae: 0.102888, mean_q: 0.145511\n",
      " 24281/50000: episode: 3163, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002436, mae: 0.113138, mean_q: 0.155375\n",
      " 24292/50000: episode: 3164, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002629, mae: 0.096175, mean_q: 0.132109\n",
      " 24295/50000: episode: 3165, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001278, mae: 0.084669, mean_q: 0.121641\n",
      " 24305/50000: episode: 3166, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003471, mae: 0.096802, mean_q: 0.135619\n",
      " 24313/50000: episode: 3167, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002937, mae: 0.113475, mean_q: 0.154969\n",
      " 24321/50000: episode: 3168, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002263, mae: 0.099544, mean_q: 0.135891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24329/50000: episode: 3169, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002031, mae: 0.093857, mean_q: 0.132876\n",
      " 24336/50000: episode: 3170, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002468, mae: 0.102324, mean_q: 0.146477\n",
      " 24347/50000: episode: 3171, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.001603, mae: 0.108412, mean_q: 0.149639\n",
      " 24363/50000: episode: 3172, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002190, mae: 0.099818, mean_q: 0.138201\n",
      " 24369/50000: episode: 3173, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001876, mae: 0.109371, mean_q: 0.153597\n",
      " 24379/50000: episode: 3174, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001853, mae: 0.105454, mean_q: 0.149784\n",
      " 24385/50000: episode: 3175, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002611, mae: 0.102435, mean_q: 0.142208\n",
      " 24409/50000: episode: 3176, duration: 0.288s, episode steps:  24, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001968, mae: 0.098694, mean_q: 0.139772\n",
      " 24420/50000: episode: 3177, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001946, mae: 0.101402, mean_q: 0.140605\n",
      " 24440/50000: episode: 3178, duration: 0.242s, episode steps:  20, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.002366, mae: 0.104344, mean_q: 0.142948\n",
      " 24451/50000: episode: 3179, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.001652, mae: 0.099736, mean_q: 0.140506\n",
      " 24461/50000: episode: 3180, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002024, mae: 0.100457, mean_q: 0.144749\n",
      " 24465/50000: episode: 3181, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [0.000, 3.000],  loss: 0.003019, mae: 0.109109, mean_q: 0.153443\n",
      " 24471/50000: episode: 3182, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004419, mae: 0.116166, mean_q: 0.162477\n",
      " 24481/50000: episode: 3183, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002859, mae: 0.103279, mean_q: 0.144853\n",
      " 24484/50000: episode: 3184, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002209, mae: 0.097473, mean_q: 0.137333\n",
      " 24492/50000: episode: 3185, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002973, mae: 0.107959, mean_q: 0.148959\n",
      " 24494/50000: episode: 3186, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001589, mae: 0.107633, mean_q: 0.152183\n",
      " 24496/50000: episode: 3187, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001664, mae: 0.097949, mean_q: 0.140762\n",
      " 24508/50000: episode: 3188, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002776, mae: 0.102051, mean_q: 0.146557\n",
      " 24510/50000: episode: 3189, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001159, mae: 0.109639, mean_q: 0.157683\n",
      " 24519/50000: episode: 3190, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.001456, mae: 0.102575, mean_q: 0.145803\n",
      " 24529/50000: episode: 3191, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001876, mae: 0.100863, mean_q: 0.138436\n",
      " 24533/50000: episode: 3192, duration: 0.083s, episode steps:   4, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.001031, mae: 0.097823, mean_q: 0.133481\n",
      " 24555/50000: episode: 3193, duration: 0.260s, episode steps:  22, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001675, mae: 0.099905, mean_q: 0.138531\n",
      " 24565/50000: episode: 3194, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.002220, mae: 0.110290, mean_q: 0.149941\n",
      " 24567/50000: episode: 3195, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002650, mae: 0.110918, mean_q: 0.154050\n",
      " 24579/50000: episode: 3196, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002282, mae: 0.101420, mean_q: 0.144195\n",
      " 24589/50000: episode: 3197, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.001883, mae: 0.098689, mean_q: 0.140525\n",
      " 24602/50000: episode: 3198, duration: 0.179s, episode steps:  13, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001480, mae: 0.104313, mean_q: 0.147793\n",
      " 24614/50000: episode: 3199, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002974, mae: 0.106344, mean_q: 0.148352\n",
      " 24618/50000: episode: 3200, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002340, mae: 0.105492, mean_q: 0.147454\n",
      " 24626/50000: episode: 3201, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002422, mae: 0.102891, mean_q: 0.142791\n",
      " 24632/50000: episode: 3202, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001993, mae: 0.102693, mean_q: 0.142991\n",
      " 24634/50000: episode: 3203, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001668, mae: 0.099782, mean_q: 0.138832\n",
      " 24640/50000: episode: 3204, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003752, mae: 0.106991, mean_q: 0.148700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24649/50000: episode: 3205, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [1.000, 3.000],  loss: 0.001926, mae: 0.103736, mean_q: 0.146024\n",
      " 24673/50000: episode: 3206, duration: 0.382s, episode steps:  24, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.001936, mae: 0.100922, mean_q: 0.138065\n",
      " 24679/50000: episode: 3207, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002975, mae: 0.103911, mean_q: 0.139623\n",
      " 24686/50000: episode: 3208, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001574, mae: 0.096433, mean_q: 0.132629\n",
      " 24688/50000: episode: 3209, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001633, mae: 0.103620, mean_q: 0.145066\n",
      " 24692/50000: episode: 3210, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001938, mae: 0.102108, mean_q: 0.138387\n",
      " 24709/50000: episode: 3211, duration: 0.253s, episode steps:  17, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.002188, mae: 0.100401, mean_q: 0.138224\n",
      " 24712/50000: episode: 3212, duration: 0.056s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002375, mae: 0.116291, mean_q: 0.161166\n",
      " 24719/50000: episode: 3213, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002028, mae: 0.106356, mean_q: 0.145909\n",
      " 24721/50000: episode: 3214, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001041, mae: 0.101667, mean_q: 0.143077\n",
      " 24723/50000: episode: 3215, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001279, mae: 0.100301, mean_q: 0.139001\n",
      " 24734/50000: episode: 3216, duration: 0.179s, episode steps:  11, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002181, mae: 0.100314, mean_q: 0.138316\n",
      " 24742/50000: episode: 3217, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002747, mae: 0.099646, mean_q: 0.139638\n",
      " 24748/50000: episode: 3218, duration: 0.099s, episode steps:   6, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002511, mae: 0.108767, mean_q: 0.149832\n",
      " 24752/50000: episode: 3219, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002399, mae: 0.103489, mean_q: 0.144339\n",
      " 24758/50000: episode: 3220, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001642, mae: 0.099717, mean_q: 0.139369\n",
      " 24764/50000: episode: 3221, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002323, mae: 0.098651, mean_q: 0.139301\n",
      " 24771/50000: episode: 3222, duration: 0.114s, episode steps:   7, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 1.000],  loss: 0.003582, mae: 0.101605, mean_q: 0.142420\n",
      " 24782/50000: episode: 3223, duration: 0.152s, episode steps:  11, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002498, mae: 0.109390, mean_q: 0.151846\n",
      " 24788/50000: episode: 3224, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002035, mae: 0.099580, mean_q: 0.141795\n",
      " 24795/50000: episode: 3225, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002840, mae: 0.102644, mean_q: 0.145195\n",
      " 24798/50000: episode: 3226, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002566, mae: 0.098272, mean_q: 0.135967\n",
      " 24800/50000: episode: 3227, duration: 0.045s, episode steps:   2, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.007533, mae: 0.116111, mean_q: 0.156768\n",
      " 24802/50000: episode: 3228, duration: 0.044s, episode steps:   2, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001775, mae: 0.100261, mean_q: 0.139602\n",
      " 24812/50000: episode: 3229, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002882, mae: 0.098727, mean_q: 0.138545\n",
      " 24822/50000: episode: 3230, duration: 0.142s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001816, mae: 0.098680, mean_q: 0.140350\n",
      " 24826/50000: episode: 3231, duration: 0.076s, episode steps:   4, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001453, mae: 0.097988, mean_q: 0.138878\n",
      " 24829/50000: episode: 3232, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.005996, mae: 0.113876, mean_q: 0.155545\n",
      " 24835/50000: episode: 3233, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.001431, mae: 0.099635, mean_q: 0.141958\n",
      " 24844/50000: episode: 3234, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.002426, mae: 0.104098, mean_q: 0.146514\n",
      " 24849/50000: episode: 3235, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001655, mae: 0.099259, mean_q: 0.142351\n",
      " 24853/50000: episode: 3236, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001088, mae: 0.098496, mean_q: 0.137877\n",
      " 24858/50000: episode: 3237, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001201, mae: 0.100324, mean_q: 0.142649\n",
      " 24861/50000: episode: 3238, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003085, mae: 0.101100, mean_q: 0.141641\n",
      " 24867/50000: episode: 3239, duration: 0.122s, episode steps:   6, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002461, mae: 0.102763, mean_q: 0.142366\n",
      " 24870/50000: episode: 3240, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003131, mae: 0.115424, mean_q: 0.160357\n",
      " 24873/50000: episode: 3241, duration: 0.057s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001302, mae: 0.104582, mean_q: 0.145620\n",
      " 24878/50000: episode: 3242, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001625, mae: 0.103751, mean_q: 0.145759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24880/50000: episode: 3243, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.000973, mae: 0.090430, mean_q: 0.129233\n",
      " 24889/50000: episode: 3244, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001847, mae: 0.095195, mean_q: 0.133023\n",
      " 24895/50000: episode: 3245, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001729, mae: 0.095278, mean_q: 0.131515\n",
      " 24899/50000: episode: 3246, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.001858, mae: 0.108438, mean_q: 0.148166\n",
      " 24906/50000: episode: 3247, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001923, mae: 0.100264, mean_q: 0.136611\n",
      " 24908/50000: episode: 3248, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002039, mae: 0.103043, mean_q: 0.139798\n",
      " 24913/50000: episode: 3249, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002201, mae: 0.097163, mean_q: 0.134304\n",
      " 24925/50000: episode: 3250, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002376, mae: 0.103727, mean_q: 0.143096\n",
      " 24934/50000: episode: 3251, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002529, mae: 0.109474, mean_q: 0.148888\n",
      " 24942/50000: episode: 3252, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001961, mae: 0.103947, mean_q: 0.143101\n",
      " 24945/50000: episode: 3253, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001743, mae: 0.093932, mean_q: 0.130213\n",
      " 24948/50000: episode: 3254, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001678, mae: 0.097198, mean_q: 0.133654\n",
      " 24952/50000: episode: 3255, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001195, mae: 0.103062, mean_q: 0.142149\n",
      " 24956/50000: episode: 3256, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001412, mae: 0.099378, mean_q: 0.135892\n",
      " 24965/50000: episode: 3257, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001814, mae: 0.098994, mean_q: 0.137894\n",
      " 24989/50000: episode: 3258, duration: 0.287s, episode steps:  24, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002441, mae: 0.101320, mean_q: 0.140664\n",
      " 24992/50000: episode: 3259, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001912, mae: 0.112278, mean_q: 0.155247\n",
      " 25016/50000: episode: 3260, duration: 0.292s, episode steps:  24, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002316, mae: 0.102795, mean_q: 0.140262\n",
      " 25023/50000: episode: 3261, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002130, mae: 0.102181, mean_q: 0.140045\n",
      " 25036/50000: episode: 3262, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.001533, mae: 0.099730, mean_q: 0.139319\n",
      " 25043/50000: episode: 3263, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002473, mae: 0.103276, mean_q: 0.142115\n",
      " 25055/50000: episode: 3264, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001583, mae: 0.095254, mean_q: 0.135040\n",
      " 25062/50000: episode: 3265, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.002137, mae: 0.095929, mean_q: 0.135856\n",
      " 25064/50000: episode: 3266, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000873, mae: 0.094892, mean_q: 0.133902\n",
      " 25069/50000: episode: 3267, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001534, mae: 0.103121, mean_q: 0.146650\n",
      " 25076/50000: episode: 3268, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002311, mae: 0.106876, mean_q: 0.148437\n",
      " 25088/50000: episode: 3269, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001982, mae: 0.100313, mean_q: 0.139915\n",
      " 25091/50000: episode: 3270, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002270, mae: 0.102402, mean_q: 0.143837\n",
      " 25093/50000: episode: 3271, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002245, mae: 0.099599, mean_q: 0.139383\n",
      " 25097/50000: episode: 3272, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002930, mae: 0.108019, mean_q: 0.147183\n",
      " 25109/50000: episode: 3273, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002531, mae: 0.114646, mean_q: 0.156871\n",
      " 25115/50000: episode: 3274, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.001468, mae: 0.094127, mean_q: 0.128813\n",
      " 25118/50000: episode: 3275, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002213, mae: 0.094638, mean_q: 0.131848\n",
      " 25127/50000: episode: 3276, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.001551, mae: 0.097616, mean_q: 0.137017\n",
      " 25132/50000: episode: 3277, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001546, mae: 0.101674, mean_q: 0.142114\n",
      " 25140/50000: episode: 3278, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003256, mae: 0.107923, mean_q: 0.147839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25144/50000: episode: 3279, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001646, mae: 0.101527, mean_q: 0.142969\n",
      " 25163/50000: episode: 3280, duration: 0.252s, episode steps:  19, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.895 [0.000, 3.000],  loss: 0.001825, mae: 0.099809, mean_q: 0.140918\n",
      " 25183/50000: episode: 3281, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001693, mae: 0.099848, mean_q: 0.140454\n",
      " 25187/50000: episode: 3282, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002741, mae: 0.102431, mean_q: 0.142617\n",
      " 25195/50000: episode: 3283, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001844, mae: 0.102689, mean_q: 0.144901\n",
      " 25201/50000: episode: 3284, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002263, mae: 0.107645, mean_q: 0.152397\n",
      " 25220/50000: episode: 3285, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.002039, mae: 0.104973, mean_q: 0.149247\n",
      " 25234/50000: episode: 3286, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002960, mae: 0.108578, mean_q: 0.154316\n",
      " 25241/50000: episode: 3287, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001753, mae: 0.106592, mean_q: 0.149292\n",
      " 25252/50000: episode: 3288, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.001692, mae: 0.100748, mean_q: 0.137721\n",
      " 25255/50000: episode: 3289, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001745, mae: 0.099159, mean_q: 0.136709\n",
      " 25271/50000: episode: 3290, duration: 0.191s, episode steps:  16, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.002183, mae: 0.103179, mean_q: 0.142877\n",
      " 25273/50000: episode: 3291, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001533, mae: 0.097311, mean_q: 0.137064\n",
      " 25276/50000: episode: 3292, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001358, mae: 0.099940, mean_q: 0.141215\n",
      " 25287/50000: episode: 3293, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001959, mae: 0.101141, mean_q: 0.141897\n",
      " 25322/50000: episode: 3294, duration: 0.419s, episode steps:  35, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.002384, mae: 0.103885, mean_q: 0.146164\n",
      " 25325/50000: episode: 3295, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001457, mae: 0.098648, mean_q: 0.139816\n",
      " 25334/50000: episode: 3296, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001492, mae: 0.100038, mean_q: 0.140927\n",
      " 25340/50000: episode: 3297, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.005613, mae: 0.112031, mean_q: 0.154373\n",
      " 25346/50000: episode: 3298, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003278, mae: 0.111840, mean_q: 0.154220\n",
      " 25348/50000: episode: 3299, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004850, mae: 0.111403, mean_q: 0.155032\n",
      " 25356/50000: episode: 3300, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002296, mae: 0.104627, mean_q: 0.143779\n",
      " 25358/50000: episode: 3301, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002471, mae: 0.102658, mean_q: 0.142934\n",
      " 25373/50000: episode: 3302, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.001751, mae: 0.099966, mean_q: 0.138085\n",
      " 25377/50000: episode: 3303, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001609, mae: 0.099738, mean_q: 0.140146\n",
      " 25381/50000: episode: 3304, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001577, mae: 0.102123, mean_q: 0.141119\n",
      " 25383/50000: episode: 3305, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005320, mae: 0.118818, mean_q: 0.160136\n",
      " 25402/50000: episode: 3306, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.002322, mae: 0.105388, mean_q: 0.145820\n",
      " 25404/50000: episode: 3307, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002094, mae: 0.103788, mean_q: 0.140857\n",
      " 25415/50000: episode: 3308, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.909 [0.000, 3.000],  loss: 0.001787, mae: 0.103436, mean_q: 0.144949\n",
      " 25423/50000: episode: 3309, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001654, mae: 0.100302, mean_q: 0.139093\n",
      " 25429/50000: episode: 3310, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002240, mae: 0.109223, mean_q: 0.152542\n",
      " 25444/50000: episode: 3311, duration: 0.197s, episode steps:  15, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002606, mae: 0.101371, mean_q: 0.141935\n",
      " 25449/50000: episode: 3312, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001694, mae: 0.103107, mean_q: 0.142321\n",
      " 25453/50000: episode: 3313, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003845, mae: 0.110868, mean_q: 0.152867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25462/50000: episode: 3314, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003675, mae: 0.109454, mean_q: 0.154244\n",
      " 25483/50000: episode: 3315, duration: 0.293s, episode steps:  21, steps per second:  72, episode reward:  1.000, mean reward:  0.048 [ 0.000,  1.000], mean action: 2.048 [0.000, 3.000],  loss: 0.002790, mae: 0.113054, mean_q: 0.156237\n",
      " 25488/50000: episode: 3316, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001591, mae: 0.106475, mean_q: 0.152568\n",
      " 25497/50000: episode: 3317, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003219, mae: 0.104442, mean_q: 0.145793\n",
      " 25505/50000: episode: 3318, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001800, mae: 0.097956, mean_q: 0.139178\n",
      " 25515/50000: episode: 3319, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003822, mae: 0.123622, mean_q: 0.169377\n",
      " 25517/50000: episode: 3320, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.000722, mae: 0.111080, mean_q: 0.157170\n",
      " 25519/50000: episode: 3321, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002615, mae: 0.111799, mean_q: 0.150876\n",
      " 25527/50000: episode: 3322, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003093, mae: 0.107828, mean_q: 0.148162\n",
      " 25531/50000: episode: 3323, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002170, mae: 0.102557, mean_q: 0.139080\n",
      " 25538/50000: episode: 3324, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001918, mae: 0.104216, mean_q: 0.142284\n",
      " 25548/50000: episode: 3325, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001394, mae: 0.104806, mean_q: 0.144848\n",
      " 25551/50000: episode: 3326, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001214, mae: 0.096681, mean_q: 0.139056\n",
      " 25554/50000: episode: 3327, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001101, mae: 0.100430, mean_q: 0.144618\n",
      " 25557/50000: episode: 3328, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001680, mae: 0.104347, mean_q: 0.149319\n",
      " 25561/50000: episode: 3329, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.001372, mae: 0.099831, mean_q: 0.141784\n",
      " 25563/50000: episode: 3330, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001290, mae: 0.103695, mean_q: 0.147563\n",
      " 25567/50000: episode: 3331, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001993, mae: 0.112520, mean_q: 0.157514\n",
      " 25574/50000: episode: 3332, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001941, mae: 0.106964, mean_q: 0.148166\n",
      " 25578/50000: episode: 3333, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001160, mae: 0.103704, mean_q: 0.146075\n",
      " 25581/50000: episode: 3334, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002674, mae: 0.113752, mean_q: 0.161408\n",
      " 25583/50000: episode: 3335, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002104, mae: 0.114974, mean_q: 0.156808\n",
      " 25587/50000: episode: 3336, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.003007, mae: 0.114386, mean_q: 0.156891\n",
      " 25590/50000: episode: 3337, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001304, mae: 0.105603, mean_q: 0.144645\n",
      " 25593/50000: episode: 3338, duration: 0.047s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.002088, mae: 0.104835, mean_q: 0.145751\n",
      " 25595/50000: episode: 3339, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001686, mae: 0.106810, mean_q: 0.144767\n",
      " 25603/50000: episode: 3340, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002237, mae: 0.110166, mean_q: 0.150645\n",
      " 25607/50000: episode: 3341, duration: 0.069s, episode steps:   4, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002905, mae: 0.105855, mean_q: 0.147613\n",
      " 25623/50000: episode: 3342, duration: 0.215s, episode steps:  16, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.938 [0.000, 3.000],  loss: 0.002374, mae: 0.109265, mean_q: 0.153938\n",
      " 25630/50000: episode: 3343, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003816, mae: 0.115868, mean_q: 0.158752\n",
      " 25638/50000: episode: 3344, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.001957, mae: 0.099963, mean_q: 0.138058\n",
      " 25647/50000: episode: 3345, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001397, mae: 0.099817, mean_q: 0.139733\n",
      " 25649/50000: episode: 3346, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002759, mae: 0.105032, mean_q: 0.146173\n",
      " 25658/50000: episode: 3347, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002931, mae: 0.108042, mean_q: 0.150478\n",
      " 25672/50000: episode: 3348, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.214 [0.000, 3.000],  loss: 0.002084, mae: 0.103307, mean_q: 0.143359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25679/50000: episode: 3349, duration: 0.141s, episode steps:   7, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 3.000],  loss: 0.002257, mae: 0.105358, mean_q: 0.145992\n",
      " 25686/50000: episode: 3350, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001692, mae: 0.101683, mean_q: 0.144738\n",
      " 25691/50000: episode: 3351, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 1.000],  loss: 0.002934, mae: 0.105335, mean_q: 0.150631\n",
      " 25703/50000: episode: 3352, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002485, mae: 0.112146, mean_q: 0.157369\n",
      " 25715/50000: episode: 3353, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001841, mae: 0.103069, mean_q: 0.144435\n",
      " 25723/50000: episode: 3354, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001498, mae: 0.103170, mean_q: 0.143711\n",
      " 25725/50000: episode: 3355, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001137, mae: 0.102747, mean_q: 0.142827\n",
      " 25727/50000: episode: 3356, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002200, mae: 0.109000, mean_q: 0.151512\n",
      " 25729/50000: episode: 3357, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001674, mae: 0.103240, mean_q: 0.140915\n",
      " 25731/50000: episode: 3358, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003189, mae: 0.113392, mean_q: 0.155925\n",
      " 25735/50000: episode: 3359, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002084, mae: 0.108385, mean_q: 0.150835\n",
      " 25740/50000: episode: 3360, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002216, mae: 0.103198, mean_q: 0.140035\n",
      " 25744/50000: episode: 3361, duration: 0.076s, episode steps:   4, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.001758, mae: 0.105646, mean_q: 0.146494\n",
      " 25746/50000: episode: 3362, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001106, mae: 0.093530, mean_q: 0.133821\n",
      " 25751/50000: episode: 3363, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.003315, mae: 0.104241, mean_q: 0.146219\n",
      " 25756/50000: episode: 3364, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003069, mae: 0.107298, mean_q: 0.150283\n",
      " 25766/50000: episode: 3365, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.300 [0.000, 3.000],  loss: 0.003100, mae: 0.108910, mean_q: 0.150003\n",
      " 25781/50000: episode: 3366, duration: 0.216s, episode steps:  15, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.133 [0.000, 3.000],  loss: 0.002432, mae: 0.107937, mean_q: 0.150234\n",
      " 25783/50000: episode: 3367, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003617, mae: 0.118318, mean_q: 0.161320\n",
      " 25787/50000: episode: 3368, duration: 0.066s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001840, mae: 0.110080, mean_q: 0.151470\n",
      " 25791/50000: episode: 3369, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001877, mae: 0.107166, mean_q: 0.148894\n",
      " 25793/50000: episode: 3370, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001880, mae: 0.103306, mean_q: 0.145727\n",
      " 25809/50000: episode: 3371, duration: 0.234s, episode steps:  16, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002260, mae: 0.107348, mean_q: 0.150396\n",
      " 25818/50000: episode: 3372, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002203, mae: 0.106282, mean_q: 0.148664\n",
      " 25835/50000: episode: 3373, duration: 0.235s, episode steps:  17, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.003140, mae: 0.113562, mean_q: 0.156954\n",
      " 25846/50000: episode: 3374, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.001921, mae: 0.100072, mean_q: 0.144687\n",
      " 25855/50000: episode: 3375, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [1.000, 3.000],  loss: 0.001817, mae: 0.108052, mean_q: 0.150719\n",
      " 25860/50000: episode: 3376, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002373, mae: 0.111542, mean_q: 0.152760\n",
      " 25872/50000: episode: 3377, duration: 0.168s, episode steps:  12, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002505, mae: 0.105844, mean_q: 0.146361\n",
      " 25879/50000: episode: 3378, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002728, mae: 0.103903, mean_q: 0.145876\n",
      " 25886/50000: episode: 3379, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002098, mae: 0.096989, mean_q: 0.138138\n",
      " 25888/50000: episode: 3380, duration: 0.037s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001196, mae: 0.096621, mean_q: 0.138630\n",
      " 25896/50000: episode: 3381, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001853, mae: 0.099904, mean_q: 0.141640\n",
      " 25912/50000: episode: 3382, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001547, mae: 0.106015, mean_q: 0.146590\n",
      " 25924/50000: episode: 3383, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001541, mae: 0.101967, mean_q: 0.141702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25936/50000: episode: 3384, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001481, mae: 0.097866, mean_q: 0.137598\n",
      " 25940/50000: episode: 3385, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003036, mae: 0.104864, mean_q: 0.144198\n",
      " 25951/50000: episode: 3386, duration: 0.154s, episode steps:  11, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.001805, mae: 0.104645, mean_q: 0.145344\n",
      " 25958/50000: episode: 3387, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001115, mae: 0.099336, mean_q: 0.141184\n",
      " 25964/50000: episode: 3388, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003554, mae: 0.111544, mean_q: 0.153065\n",
      " 25976/50000: episode: 3389, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002366, mae: 0.112095, mean_q: 0.154596\n",
      " 25992/50000: episode: 3390, duration: 0.215s, episode steps:  16, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002492, mae: 0.109032, mean_q: 0.151412\n",
      " 26012/50000: episode: 3391, duration: 0.269s, episode steps:  20, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002317, mae: 0.105384, mean_q: 0.147177\n",
      " 26019/50000: episode: 3392, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002195, mae: 0.109829, mean_q: 0.151818\n",
      " 26022/50000: episode: 3393, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002176, mae: 0.105440, mean_q: 0.144974\n",
      " 26024/50000: episode: 3394, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002342, mae: 0.103295, mean_q: 0.145058\n",
      " 26034/50000: episode: 3395, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.001856, mae: 0.101605, mean_q: 0.143108\n",
      " 26040/50000: episode: 3396, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001847, mae: 0.107529, mean_q: 0.150473\n",
      " 26049/50000: episode: 3397, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002104, mae: 0.106501, mean_q: 0.147160\n",
      " 26051/50000: episode: 3398, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000991, mae: 0.100455, mean_q: 0.144117\n",
      " 26073/50000: episode: 3399, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001900, mae: 0.100649, mean_q: 0.142694\n",
      " 26094/50000: episode: 3400, duration: 0.276s, episode steps:  21, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.810 [0.000, 3.000],  loss: 0.002045, mae: 0.104945, mean_q: 0.146787\n",
      " 26097/50000: episode: 3401, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001782, mae: 0.108932, mean_q: 0.158595\n",
      " 26099/50000: episode: 3402, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001576, mae: 0.107108, mean_q: 0.153073\n",
      " 26123/50000: episode: 3403, duration: 0.322s, episode steps:  24, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001929, mae: 0.102952, mean_q: 0.146447\n",
      " 26127/50000: episode: 3404, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001287, mae: 0.097550, mean_q: 0.139011\n",
      " 26138/50000: episode: 3405, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.001326, mae: 0.100161, mean_q: 0.141551\n",
      " 26140/50000: episode: 3406, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003211, mae: 0.101731, mean_q: 0.143019\n",
      " 26145/50000: episode: 3407, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.001800, mae: 0.098768, mean_q: 0.141008\n",
      " 26147/50000: episode: 3408, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002024, mae: 0.101580, mean_q: 0.144188\n",
      " 26154/50000: episode: 3409, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.001444, mae: 0.103430, mean_q: 0.146073\n",
      " 26159/50000: episode: 3410, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.001356, mae: 0.106426, mean_q: 0.149960\n",
      " 26165/50000: episode: 3411, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002565, mae: 0.107309, mean_q: 0.147369\n",
      " 26178/50000: episode: 3412, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.001572, mae: 0.102775, mean_q: 0.142352\n",
      " 26184/50000: episode: 3413, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.001350, mae: 0.099343, mean_q: 0.138899\n",
      " 26188/50000: episode: 3414, duration: 0.090s, episode steps:   4, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001904, mae: 0.105946, mean_q: 0.145347\n",
      " 26192/50000: episode: 3415, duration: 0.077s, episode steps:   4, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001677, mae: 0.102518, mean_q: 0.140773\n",
      " 26197/50000: episode: 3416, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001900, mae: 0.105465, mean_q: 0.143089\n",
      " 26200/50000: episode: 3417, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001197, mae: 0.105238, mean_q: 0.148654\n",
      " 26202/50000: episode: 3418, duration: 0.042s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.000986, mae: 0.095889, mean_q: 0.133600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26221/50000: episode: 3419, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.001882, mae: 0.103408, mean_q: 0.143891\n",
      " 26228/50000: episode: 3420, duration: 0.110s, episode steps:   7, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.002231, mae: 0.110183, mean_q: 0.153065\n",
      " 26243/50000: episode: 3421, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002088, mae: 0.101368, mean_q: 0.140416\n",
      " 26245/50000: episode: 3422, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001429, mae: 0.105970, mean_q: 0.148941\n",
      " 26248/50000: episode: 3423, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001805, mae: 0.106500, mean_q: 0.149264\n",
      " 26255/50000: episode: 3424, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002957, mae: 0.103238, mean_q: 0.143675\n",
      " 26258/50000: episode: 3425, duration: 0.073s, episode steps:   3, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001796, mae: 0.096787, mean_q: 0.135786\n",
      " 26267/50000: episode: 3426, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.001756, mae: 0.098644, mean_q: 0.143345\n",
      " 26279/50000: episode: 3427, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002393, mae: 0.102991, mean_q: 0.146762\n",
      " 26281/50000: episode: 3428, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001988, mae: 0.107226, mean_q: 0.150225\n",
      " 26295/50000: episode: 3429, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002634, mae: 0.104456, mean_q: 0.145194\n",
      " 26306/50000: episode: 3430, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001199, mae: 0.096088, mean_q: 0.136651\n",
      " 26313/50000: episode: 3431, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003078, mae: 0.107313, mean_q: 0.148049\n",
      " 26327/50000: episode: 3432, duration: 0.170s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001890, mae: 0.103128, mean_q: 0.145973\n",
      " 26329/50000: episode: 3433, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001297, mae: 0.094718, mean_q: 0.135124\n",
      " 26341/50000: episode: 3434, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002173, mae: 0.100242, mean_q: 0.142525\n",
      " 26346/50000: episode: 3435, duration: 0.091s, episode steps:   5, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002817, mae: 0.109319, mean_q: 0.153544\n",
      " 26351/50000: episode: 3436, duration: 0.092s, episode steps:   5, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002445, mae: 0.107575, mean_q: 0.151731\n",
      " 26357/50000: episode: 3437, duration: 0.100s, episode steps:   6, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002343, mae: 0.107195, mean_q: 0.151286\n",
      " 26368/50000: episode: 3438, duration: 0.146s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002639, mae: 0.111631, mean_q: 0.156255\n",
      " 26370/50000: episode: 3439, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001100, mae: 0.108831, mean_q: 0.155272\n",
      " 26374/50000: episode: 3440, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002316, mae: 0.111921, mean_q: 0.160937\n",
      " 26376/50000: episode: 3441, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003043, mae: 0.118399, mean_q: 0.168288\n",
      " 26384/50000: episode: 3442, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003844, mae: 0.110118, mean_q: 0.156117\n",
      " 26394/50000: episode: 3443, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003938, mae: 0.105361, mean_q: 0.149063\n",
      " 26397/50000: episode: 3444, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001711, mae: 0.099228, mean_q: 0.143055\n",
      " 26403/50000: episode: 3445, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001515, mae: 0.103270, mean_q: 0.146487\n",
      " 26406/50000: episode: 3446, duration: 0.068s, episode steps:   3, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000539, mae: 0.097778, mean_q: 0.141694\n",
      " 26421/50000: episode: 3447, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.133 [0.000, 3.000],  loss: 0.002957, mae: 0.109860, mean_q: 0.150448\n",
      " 26425/50000: episode: 3448, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002565, mae: 0.110963, mean_q: 0.153497\n",
      " 26427/50000: episode: 3449, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001564, mae: 0.108439, mean_q: 0.152488\n",
      " 26446/50000: episode: 3450, duration: 0.269s, episode steps:  19, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.105 [0.000, 3.000],  loss: 0.001914, mae: 0.106244, mean_q: 0.148485\n",
      " 26455/50000: episode: 3451, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [0.000, 3.000],  loss: 0.002042, mae: 0.105355, mean_q: 0.146946\n",
      " 26467/50000: episode: 3452, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.083 [1.000, 3.000],  loss: 0.001927, mae: 0.101758, mean_q: 0.144123\n",
      " 26474/50000: episode: 3453, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002430, mae: 0.107495, mean_q: 0.145597\n",
      " 26478/50000: episode: 3454, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002231, mae: 0.111369, mean_q: 0.152145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26498/50000: episode: 3455, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.950 [0.000, 3.000],  loss: 0.001711, mae: 0.102193, mean_q: 0.142437\n",
      " 26504/50000: episode: 3456, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001702, mae: 0.098697, mean_q: 0.139128\n",
      " 26510/50000: episode: 3457, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002419, mae: 0.109931, mean_q: 0.154944\n",
      " 26513/50000: episode: 3458, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001498, mae: 0.107604, mean_q: 0.151652\n",
      " 26519/50000: episode: 3459, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001696, mae: 0.110073, mean_q: 0.152794\n",
      " 26529/50000: episode: 3460, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001942, mae: 0.105372, mean_q: 0.146150\n",
      " 26543/50000: episode: 3461, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002204, mae: 0.104456, mean_q: 0.145205\n",
      " 26546/50000: episode: 3462, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002929, mae: 0.109503, mean_q: 0.151647\n",
      " 26565/50000: episode: 3463, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.002580, mae: 0.105936, mean_q: 0.147505\n",
      " 26568/50000: episode: 3464, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002683, mae: 0.111141, mean_q: 0.158346\n",
      " 26579/50000: episode: 3465, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002056, mae: 0.109784, mean_q: 0.151533\n",
      " 26581/50000: episode: 3466, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003119, mae: 0.119953, mean_q: 0.165608\n",
      " 26594/50000: episode: 3467, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.002128, mae: 0.105879, mean_q: 0.146819\n",
      " 26599/50000: episode: 3468, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.001608, mae: 0.104578, mean_q: 0.150251\n",
      " 26601/50000: episode: 3469, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001407, mae: 0.097644, mean_q: 0.138836\n",
      " 26605/50000: episode: 3470, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003666, mae: 0.109314, mean_q: 0.153586\n",
      " 26608/50000: episode: 3471, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003182, mae: 0.110051, mean_q: 0.153595\n",
      " 26620/50000: episode: 3472, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001841, mae: 0.105668, mean_q: 0.146785\n",
      " 26626/50000: episode: 3473, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002403, mae: 0.101344, mean_q: 0.139095\n",
      " 26634/50000: episode: 3474, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002330, mae: 0.105951, mean_q: 0.147706\n",
      " 26641/50000: episode: 3475, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002207, mae: 0.104631, mean_q: 0.146758\n",
      " 26646/50000: episode: 3476, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001970, mae: 0.104768, mean_q: 0.145595\n",
      " 26664/50000: episode: 3477, duration: 0.232s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002533, mae: 0.108460, mean_q: 0.149158\n",
      " 26675/50000: episode: 3478, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001527, mae: 0.097273, mean_q: 0.138773\n",
      " 26680/50000: episode: 3479, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.001427, mae: 0.098279, mean_q: 0.140619\n",
      " 26694/50000: episode: 3480, duration: 0.172s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002566, mae: 0.111883, mean_q: 0.153768\n",
      " 26699/50000: episode: 3481, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.001794, mae: 0.100664, mean_q: 0.143426\n",
      " 26701/50000: episode: 3482, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002358, mae: 0.101232, mean_q: 0.142518\n",
      " 26713/50000: episode: 3483, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002147, mae: 0.108277, mean_q: 0.151689\n",
      " 26716/50000: episode: 3484, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003192, mae: 0.123163, mean_q: 0.168267\n",
      " 26719/50000: episode: 3485, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001698, mae: 0.108803, mean_q: 0.150383\n",
      " 26722/50000: episode: 3486, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003681, mae: 0.119774, mean_q: 0.162966\n",
      " 26726/50000: episode: 3487, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.002112, mae: 0.115135, mean_q: 0.157791\n",
      " 26731/50000: episode: 3488, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002206, mae: 0.102840, mean_q: 0.145064\n",
      " 26734/50000: episode: 3489, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001773, mae: 0.096738, mean_q: 0.138251\n",
      " 26745/50000: episode: 3490, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001401, mae: 0.100879, mean_q: 0.145638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26762/50000: episode: 3491, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002441, mae: 0.112567, mean_q: 0.155950\n",
      " 26766/50000: episode: 3492, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003088, mae: 0.111555, mean_q: 0.153602\n",
      " 26768/50000: episode: 3493, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001226, mae: 0.099550, mean_q: 0.140101\n",
      " 26772/50000: episode: 3494, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002454, mae: 0.099745, mean_q: 0.141254\n",
      " 26776/50000: episode: 3495, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001751, mae: 0.102540, mean_q: 0.147007\n",
      " 26781/50000: episode: 3496, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003061, mae: 0.112801, mean_q: 0.156699\n",
      " 26783/50000: episode: 3497, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001498, mae: 0.103166, mean_q: 0.143597\n",
      " 26787/50000: episode: 3498, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.001659, mae: 0.106889, mean_q: 0.148504\n",
      " 26793/50000: episode: 3499, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002482, mae: 0.105579, mean_q: 0.146576\n",
      " 26797/50000: episode: 3500, duration: 0.072s, episode steps:   4, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002440, mae: 0.108966, mean_q: 0.152315\n",
      " 26799/50000: episode: 3501, duration: 0.042s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001628, mae: 0.110888, mean_q: 0.150967\n",
      " 26802/50000: episode: 3502, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001773, mae: 0.101655, mean_q: 0.142680\n",
      " 26806/50000: episode: 3503, duration: 0.080s, episode steps:   4, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002511, mae: 0.106309, mean_q: 0.146089\n",
      " 26808/50000: episode: 3504, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.000805, mae: 0.099115, mean_q: 0.138108\n",
      " 26812/50000: episode: 3505, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002278, mae: 0.105171, mean_q: 0.147709\n",
      " 26816/50000: episode: 3506, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001804, mae: 0.105449, mean_q: 0.149193\n",
      " 26822/50000: episode: 3507, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.002439, mae: 0.108560, mean_q: 0.150164\n",
      " 26825/50000: episode: 3508, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001392, mae: 0.101092, mean_q: 0.145142\n",
      " 26833/50000: episode: 3509, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001863, mae: 0.105866, mean_q: 0.148396\n",
      " 26841/50000: episode: 3510, duration: 0.140s, episode steps:   8, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002339, mae: 0.104044, mean_q: 0.144058\n",
      " 26846/50000: episode: 3511, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001672, mae: 0.103795, mean_q: 0.148098\n",
      " 26852/50000: episode: 3512, duration: 0.090s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001708, mae: 0.110566, mean_q: 0.156350\n",
      " 26860/50000: episode: 3513, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.002149, mae: 0.112218, mean_q: 0.156445\n",
      " 26874/50000: episode: 3514, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001913, mae: 0.105223, mean_q: 0.148031\n",
      " 26898/50000: episode: 3515, duration: 0.286s, episode steps:  24, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002198, mae: 0.110808, mean_q: 0.154166\n",
      " 26905/50000: episode: 3516, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001012, mae: 0.108716, mean_q: 0.155784\n",
      " 26907/50000: episode: 3517, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001308, mae: 0.109407, mean_q: 0.150936\n",
      " 26909/50000: episode: 3518, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.005138, mae: 0.114088, mean_q: 0.155331\n",
      " 26919/50000: episode: 3519, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002671, mae: 0.113493, mean_q: 0.157241\n",
      " 26924/50000: episode: 3520, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002693, mae: 0.112055, mean_q: 0.159071\n",
      " 26931/50000: episode: 3521, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.002291, mae: 0.102360, mean_q: 0.143703\n",
      " 26938/50000: episode: 3522, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002138, mae: 0.110401, mean_q: 0.152526\n",
      " 26945/50000: episode: 3523, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002453, mae: 0.111227, mean_q: 0.154471\n",
      " 26953/50000: episode: 3524, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002285, mae: 0.111750, mean_q: 0.154807\n",
      " 26955/50000: episode: 3525, duration: 0.037s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001289, mae: 0.098661, mean_q: 0.141109\n",
      " 26960/50000: episode: 3526, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003684, mae: 0.115831, mean_q: 0.160584\n",
      " 26964/50000: episode: 3527, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002323, mae: 0.103006, mean_q: 0.145475\n",
      " 26969/50000: episode: 3528, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002714, mae: 0.112072, mean_q: 0.157464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26982/50000: episode: 3529, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.077 [0.000, 3.000],  loss: 0.001922, mae: 0.105462, mean_q: 0.152418\n",
      " 26985/50000: episode: 3530, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002382, mae: 0.105427, mean_q: 0.148172\n",
      " 27002/50000: episode: 3531, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002100, mae: 0.117925, mean_q: 0.164289\n",
      " 27004/50000: episode: 3532, duration: 0.043s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002747, mae: 0.115493, mean_q: 0.157661\n",
      " 27015/50000: episode: 3533, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002627, mae: 0.117246, mean_q: 0.159314\n",
      " 27019/50000: episode: 3534, duration: 0.072s, episode steps:   4, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001683, mae: 0.109163, mean_q: 0.152359\n",
      " 27037/50000: episode: 3535, duration: 0.231s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.002423, mae: 0.110154, mean_q: 0.152399\n",
      " 27041/50000: episode: 3536, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002997, mae: 0.113279, mean_q: 0.156626\n",
      " 27047/50000: episode: 3537, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.003340, mae: 0.113915, mean_q: 0.156528\n",
      " 27057/50000: episode: 3538, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002567, mae: 0.110482, mean_q: 0.153352\n",
      " 27060/50000: episode: 3539, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003408, mae: 0.106843, mean_q: 0.145117\n",
      " 27069/50000: episode: 3540, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002233, mae: 0.112386, mean_q: 0.155262\n",
      " 27081/50000: episode: 3541, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.002105, mae: 0.109308, mean_q: 0.155993\n",
      " 27088/50000: episode: 3542, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002215, mae: 0.118275, mean_q: 0.163790\n",
      " 27105/50000: episode: 3543, duration: 0.211s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002194, mae: 0.112474, mean_q: 0.154770\n",
      " 27107/50000: episode: 3544, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001817, mae: 0.109416, mean_q: 0.153433\n",
      " 27114/50000: episode: 3545, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [1.000, 3.000],  loss: 0.002464, mae: 0.114295, mean_q: 0.158754\n",
      " 27116/50000: episode: 3546, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001916, mae: 0.103514, mean_q: 0.145271\n",
      " 27119/50000: episode: 3547, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002153, mae: 0.106562, mean_q: 0.152039\n",
      " 27124/50000: episode: 3548, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002549, mae: 0.109669, mean_q: 0.157077\n",
      " 27140/50000: episode: 3549, duration: 0.195s, episode steps:  16, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002503, mae: 0.114909, mean_q: 0.161323\n",
      " 27150/50000: episode: 3550, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001279, mae: 0.107156, mean_q: 0.151830\n",
      " 27154/50000: episode: 3551, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.004749, mae: 0.117417, mean_q: 0.162664\n",
      " 27161/50000: episode: 3552, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002611, mae: 0.115575, mean_q: 0.164994\n",
      " 27176/50000: episode: 3553, duration: 0.195s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.002506, mae: 0.114287, mean_q: 0.161250\n",
      " 27180/50000: episode: 3554, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001495, mae: 0.109818, mean_q: 0.155850\n",
      " 27189/50000: episode: 3555, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003347, mae: 0.111138, mean_q: 0.158298\n",
      " 27192/50000: episode: 3556, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002458, mae: 0.109962, mean_q: 0.158296\n",
      " 27199/50000: episode: 3557, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003450, mae: 0.113895, mean_q: 0.160222\n",
      " 27214/50000: episode: 3558, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.002760, mae: 0.115872, mean_q: 0.160929\n",
      " 27216/50000: episode: 3559, duration: 0.050s, episode steps:   2, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002194, mae: 0.106231, mean_q: 0.153863\n",
      " 27220/50000: episode: 3560, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.003493, mae: 0.119853, mean_q: 0.167222\n",
      " 27235/50000: episode: 3561, duration: 0.196s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002820, mae: 0.115165, mean_q: 0.159241\n",
      " 27244/50000: episode: 3562, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002142, mae: 0.111320, mean_q: 0.155933\n",
      " 27250/50000: episode: 3563, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002877, mae: 0.112402, mean_q: 0.155191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27255/50000: episode: 3564, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002582, mae: 0.116358, mean_q: 0.161777\n",
      " 27259/50000: episode: 3565, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001445, mae: 0.112679, mean_q: 0.156486\n",
      " 27276/50000: episode: 3566, duration: 0.225s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.001955, mae: 0.113012, mean_q: 0.156278\n",
      " 27283/50000: episode: 3567, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002371, mae: 0.113897, mean_q: 0.158920\n",
      " 27286/50000: episode: 3568, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.004458, mae: 0.123119, mean_q: 0.171176\n",
      " 27290/50000: episode: 3569, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002139, mae: 0.114184, mean_q: 0.160734\n",
      " 27295/50000: episode: 3570, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002118, mae: 0.115571, mean_q: 0.163374\n",
      " 27303/50000: episode: 3571, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002110, mae: 0.113861, mean_q: 0.156655\n",
      " 27316/50000: episode: 3572, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.308 [0.000, 3.000],  loss: 0.002783, mae: 0.112358, mean_q: 0.156574\n",
      " 27324/50000: episode: 3573, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002279, mae: 0.114270, mean_q: 0.160443\n",
      " 27330/50000: episode: 3574, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002348, mae: 0.110826, mean_q: 0.155124\n",
      " 27334/50000: episode: 3575, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.002358, mae: 0.114764, mean_q: 0.158617\n",
      " 27345/50000: episode: 3576, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002264, mae: 0.115419, mean_q: 0.162743\n",
      " 27352/50000: episode: 3577, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002894, mae: 0.114190, mean_q: 0.157937\n",
      " 27356/50000: episode: 3578, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003803, mae: 0.121377, mean_q: 0.169399\n",
      " 27359/50000: episode: 3579, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002003, mae: 0.111521, mean_q: 0.156934\n",
      " 27368/50000: episode: 3580, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002610, mae: 0.111708, mean_q: 0.155689\n",
      " 27379/50000: episode: 3581, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.818 [0.000, 3.000],  loss: 0.002237, mae: 0.109982, mean_q: 0.153901\n",
      " 27381/50000: episode: 3582, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001206, mae: 0.108553, mean_q: 0.151242\n",
      " 27390/50000: episode: 3583, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.003043, mae: 0.115894, mean_q: 0.159473\n",
      " 27400/50000: episode: 3584, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.002153, mae: 0.111360, mean_q: 0.154948\n",
      " 27402/50000: episode: 3585, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003337, mae: 0.121512, mean_q: 0.167225\n",
      " 27406/50000: episode: 3586, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001378, mae: 0.112418, mean_q: 0.158964\n",
      " 27416/50000: episode: 3587, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003102, mae: 0.113109, mean_q: 0.160302\n",
      " 27418/50000: episode: 3588, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002215, mae: 0.114759, mean_q: 0.158244\n",
      " 27422/50000: episode: 3589, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003929, mae: 0.122940, mean_q: 0.170349\n",
      " 27434/50000: episode: 3590, duration: 0.165s, episode steps:  12, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002908, mae: 0.112826, mean_q: 0.159182\n",
      " 27445/50000: episode: 3591, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002720, mae: 0.115768, mean_q: 0.162990\n",
      " 27459/50000: episode: 3592, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.786 [0.000, 3.000],  loss: 0.002776, mae: 0.117765, mean_q: 0.164270\n",
      " 27469/50000: episode: 3593, duration: 0.126s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002431, mae: 0.111219, mean_q: 0.154868\n",
      " 27474/50000: episode: 3594, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003488, mae: 0.114872, mean_q: 0.159530\n",
      " 27485/50000: episode: 3595, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.001316, mae: 0.111742, mean_q: 0.158218\n",
      " 27488/50000: episode: 3596, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003702, mae: 0.117279, mean_q: 0.161641\n",
      " 27490/50000: episode: 3597, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001097, mae: 0.110186, mean_q: 0.158104\n",
      " 27494/50000: episode: 3598, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001540, mae: 0.108201, mean_q: 0.153029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27506/50000: episode: 3599, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002156, mae: 0.109349, mean_q: 0.154409\n",
      " 27517/50000: episode: 3600, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002896, mae: 0.118572, mean_q: 0.165330\n",
      " 27520/50000: episode: 3601, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003008, mae: 0.116323, mean_q: 0.162128\n",
      " 27524/50000: episode: 3602, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.002118, mae: 0.117472, mean_q: 0.162469\n",
      " 27538/50000: episode: 3603, duration: 0.181s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002625, mae: 0.113424, mean_q: 0.159742\n",
      " 27541/50000: episode: 3604, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003031, mae: 0.114986, mean_q: 0.161032\n",
      " 27544/50000: episode: 3605, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000812, mae: 0.108249, mean_q: 0.158471\n",
      " 27562/50000: episode: 3606, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.001622, mae: 0.112425, mean_q: 0.159081\n",
      " 27564/50000: episode: 3607, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001825, mae: 0.104084, mean_q: 0.146923\n",
      " 27568/50000: episode: 3608, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.004710, mae: 0.116197, mean_q: 0.162070\n",
      " 27570/50000: episode: 3609, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.006501, mae: 0.131029, mean_q: 0.175998\n",
      " 27579/50000: episode: 3610, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002548, mae: 0.115426, mean_q: 0.161254\n",
      " 27586/50000: episode: 3611, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [1.000, 3.000],  loss: 0.001967, mae: 0.112941, mean_q: 0.158123\n",
      " 27588/50000: episode: 3612, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003111, mae: 0.120332, mean_q: 0.166820\n",
      " 27590/50000: episode: 3613, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003302, mae: 0.123817, mean_q: 0.170970\n",
      " 27595/50000: episode: 3614, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002089, mae: 0.113633, mean_q: 0.158783\n",
      " 27597/50000: episode: 3615, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002968, mae: 0.116144, mean_q: 0.157695\n",
      " 27599/50000: episode: 3616, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001439, mae: 0.111981, mean_q: 0.154969\n",
      " 27603/50000: episode: 3617, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.002017, mae: 0.115440, mean_q: 0.161583\n",
      " 27606/50000: episode: 3618, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001306, mae: 0.112331, mean_q: 0.164080\n",
      " 27613/50000: episode: 3619, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [1.000, 2.000],  loss: 0.002684, mae: 0.118892, mean_q: 0.168703\n",
      " 27619/50000: episode: 3620, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002469, mae: 0.114208, mean_q: 0.163646\n",
      " 27623/50000: episode: 3621, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002249, mae: 0.118455, mean_q: 0.170852\n",
      " 27632/50000: episode: 3622, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001647, mae: 0.118038, mean_q: 0.168180\n",
      " 27641/50000: episode: 3623, duration: 0.131s, episode steps:   9, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002899, mae: 0.122535, mean_q: 0.170036\n",
      " 27652/50000: episode: 3624, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.727 [0.000, 3.000],  loss: 0.002259, mae: 0.109342, mean_q: 0.151364\n",
      " 27666/50000: episode: 3625, duration: 0.172s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.002035, mae: 0.115869, mean_q: 0.160833\n",
      " 27678/50000: episode: 3626, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002171, mae: 0.114379, mean_q: 0.160014\n",
      " 27690/50000: episode: 3627, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002922, mae: 0.118815, mean_q: 0.164944\n",
      " 27693/50000: episode: 3628, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002606, mae: 0.113300, mean_q: 0.157148\n",
      " 27699/50000: episode: 3629, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.004203, mae: 0.118549, mean_q: 0.166845\n",
      " 27714/50000: episode: 3630, duration: 0.184s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001777, mae: 0.109650, mean_q: 0.159645\n",
      " 27721/50000: episode: 3631, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002328, mae: 0.121006, mean_q: 0.172735\n",
      " 27728/50000: episode: 3632, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002667, mae: 0.124609, mean_q: 0.174750\n",
      " 27739/50000: episode: 3633, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002149, mae: 0.112929, mean_q: 0.159111\n",
      " 27750/50000: episode: 3634, duration: 0.175s, episode steps:  11, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002426, mae: 0.121642, mean_q: 0.170207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27759/50000: episode: 3635, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002026, mae: 0.122311, mean_q: 0.172482\n",
      " 27761/50000: episode: 3636, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002696, mae: 0.113624, mean_q: 0.162081\n",
      " 27771/50000: episode: 3637, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002237, mae: 0.114135, mean_q: 0.162197\n",
      " 27780/50000: episode: 3638, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [1.000, 3.000],  loss: 0.001898, mae: 0.114429, mean_q: 0.164195\n",
      " 27790/50000: episode: 3639, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002919, mae: 0.117637, mean_q: 0.166472\n",
      " 27797/50000: episode: 3640, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002602, mae: 0.118767, mean_q: 0.167141\n",
      " 27816/50000: episode: 3641, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.211 [0.000, 3.000],  loss: 0.002244, mae: 0.120521, mean_q: 0.169905\n",
      " 27818/50000: episode: 3642, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002301, mae: 0.137635, mean_q: 0.196445\n",
      " 27821/50000: episode: 3643, duration: 0.061s, episode steps:   3, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002099, mae: 0.113205, mean_q: 0.162197\n",
      " 27833/50000: episode: 3644, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002346, mae: 0.116505, mean_q: 0.165161\n",
      " 27850/50000: episode: 3645, duration: 0.202s, episode steps:  17, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.001671, mae: 0.111684, mean_q: 0.156752\n",
      " 27856/50000: episode: 3646, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002598, mae: 0.123051, mean_q: 0.172773\n",
      " 27868/50000: episode: 3647, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002442, mae: 0.117864, mean_q: 0.165171\n",
      " 27870/50000: episode: 3648, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002290, mae: 0.110671, mean_q: 0.156184\n",
      " 27885/50000: episode: 3649, duration: 0.182s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.001734, mae: 0.109701, mean_q: 0.154430\n",
      " 27890/50000: episode: 3650, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001838, mae: 0.112978, mean_q: 0.158564\n",
      " 27892/50000: episode: 3651, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002085, mae: 0.123974, mean_q: 0.178214\n",
      " 27904/50000: episode: 3652, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002577, mae: 0.116556, mean_q: 0.162356\n",
      " 27914/50000: episode: 3653, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002371, mae: 0.116810, mean_q: 0.166543\n",
      " 27917/50000: episode: 3654, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002697, mae: 0.114952, mean_q: 0.164652\n",
      " 27924/50000: episode: 3655, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002580, mae: 0.120262, mean_q: 0.172342\n",
      " 27927/50000: episode: 3656, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003140, mae: 0.119240, mean_q: 0.171203\n",
      " 27933/50000: episode: 3657, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001597, mae: 0.115011, mean_q: 0.164859\n",
      " 27946/50000: episode: 3658, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002341, mae: 0.119387, mean_q: 0.170260\n",
      " 27952/50000: episode: 3659, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002011, mae: 0.118186, mean_q: 0.167955\n",
      " 27955/50000: episode: 3660, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001735, mae: 0.116009, mean_q: 0.165176\n",
      " 27957/50000: episode: 3661, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001950, mae: 0.114906, mean_q: 0.164532\n",
      " 27972/50000: episode: 3662, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002333, mae: 0.119348, mean_q: 0.169402\n",
      " 27976/50000: episode: 3663, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002834, mae: 0.119044, mean_q: 0.167774\n",
      " 27980/50000: episode: 3664, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.003076, mae: 0.119683, mean_q: 0.167063\n",
      " 27987/50000: episode: 3665, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002173, mae: 0.115267, mean_q: 0.162516\n",
      " 28010/50000: episode: 3666, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002267, mae: 0.121882, mean_q: 0.172559\n",
      " 28022/50000: episode: 3667, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002406, mae: 0.122543, mean_q: 0.172580\n",
      " 28028/50000: episode: 3668, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002401, mae: 0.116148, mean_q: 0.164570\n",
      " 28042/50000: episode: 3669, duration: 0.197s, episode steps:  14, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.002496, mae: 0.115380, mean_q: 0.163493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28046/50000: episode: 3670, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002061, mae: 0.116015, mean_q: 0.165268\n",
      " 28049/50000: episode: 3671, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002371, mae: 0.119030, mean_q: 0.171899\n",
      " 28054/50000: episode: 3672, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003114, mae: 0.126810, mean_q: 0.176671\n",
      " 28060/50000: episode: 3673, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002589, mae: 0.116730, mean_q: 0.167119\n",
      " 28071/50000: episode: 3674, duration: 0.158s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002447, mae: 0.120765, mean_q: 0.169447\n",
      " 28078/50000: episode: 3675, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001906, mae: 0.120176, mean_q: 0.168200\n",
      " 28083/50000: episode: 3676, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001726, mae: 0.117250, mean_q: 0.165712\n",
      " 28089/50000: episode: 3677, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002059, mae: 0.117659, mean_q: 0.169023\n",
      " 28105/50000: episode: 3678, duration: 0.228s, episode steps:  16, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002559, mae: 0.118414, mean_q: 0.169254\n",
      " 28112/50000: episode: 3679, duration: 0.109s, episode steps:   7, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002500, mae: 0.120160, mean_q: 0.167083\n",
      " 28120/50000: episode: 3680, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004071, mae: 0.120653, mean_q: 0.164431\n",
      " 28122/50000: episode: 3681, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001860, mae: 0.123064, mean_q: 0.175106\n",
      " 28136/50000: episode: 3682, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002594, mae: 0.115935, mean_q: 0.166213\n",
      " 28143/50000: episode: 3683, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002326, mae: 0.122254, mean_q: 0.174426\n",
      " 28145/50000: episode: 3684, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001535, mae: 0.116395, mean_q: 0.165594\n",
      " 28151/50000: episode: 3685, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002619, mae: 0.129046, mean_q: 0.179835\n",
      " 28161/50000: episode: 3686, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003169, mae: 0.126838, mean_q: 0.176047\n",
      " 28169/50000: episode: 3687, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002908, mae: 0.117003, mean_q: 0.163794\n",
      " 28185/50000: episode: 3688, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002856, mae: 0.123551, mean_q: 0.173859\n",
      " 28188/50000: episode: 3689, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001919, mae: 0.123336, mean_q: 0.173075\n",
      " 28203/50000: episode: 3690, duration: 0.182s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002157, mae: 0.114512, mean_q: 0.162974\n",
      " 28207/50000: episode: 3691, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002273, mae: 0.117885, mean_q: 0.169106\n",
      " 28222/50000: episode: 3692, duration: 0.182s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.002251, mae: 0.122126, mean_q: 0.173213\n",
      " 28227/50000: episode: 3693, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003787, mae: 0.127896, mean_q: 0.177271\n",
      " 28236/50000: episode: 3694, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 3.000],  loss: 0.003354, mae: 0.120188, mean_q: 0.170489\n",
      " 28248/50000: episode: 3695, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002782, mae: 0.116377, mean_q: 0.170019\n",
      " 28251/50000: episode: 3696, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003369, mae: 0.124757, mean_q: 0.179639\n",
      " 28253/50000: episode: 3697, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002030, mae: 0.119107, mean_q: 0.173808\n",
      " 28257/50000: episode: 3698, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002068, mae: 0.124435, mean_q: 0.180710\n",
      " 28265/50000: episode: 3699, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002289, mae: 0.118289, mean_q: 0.169323\n",
      " 28268/50000: episode: 3700, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005281, mae: 0.127832, mean_q: 0.177616\n",
      " 28277/50000: episode: 3701, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001608, mae: 0.119210, mean_q: 0.165158\n",
      " 28282/50000: episode: 3702, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 1.000],  loss: 0.002026, mae: 0.127002, mean_q: 0.178279\n",
      " 28293/50000: episode: 3703, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.818 [0.000, 3.000],  loss: 0.003221, mae: 0.124740, mean_q: 0.172745\n",
      " 28307/50000: episode: 3704, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002530, mae: 0.126580, mean_q: 0.174786\n",
      " 28316/50000: episode: 3705, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002409, mae: 0.126536, mean_q: 0.177140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28323/50000: episode: 3706, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002072, mae: 0.121326, mean_q: 0.173200\n",
      " 28334/50000: episode: 3707, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002285, mae: 0.126996, mean_q: 0.177923\n",
      " 28344/50000: episode: 3708, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002496, mae: 0.122691, mean_q: 0.171466\n",
      " 28346/50000: episode: 3709, duration: 0.070s, episode steps:   2, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001131, mae: 0.120810, mean_q: 0.171316\n",
      " 28349/50000: episode: 3710, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001440, mae: 0.118716, mean_q: 0.168402\n",
      " 28352/50000: episode: 3711, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003743, mae: 0.127300, mean_q: 0.178746\n",
      " 28355/50000: episode: 3712, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002350, mae: 0.121788, mean_q: 0.172846\n",
      " 28357/50000: episode: 3713, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001584, mae: 0.109005, mean_q: 0.162162\n",
      " 28373/50000: episode: 3714, duration: 0.219s, episode steps:  16, steps per second:  73, episode reward:  1.000, mean reward:  0.062 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002264, mae: 0.119799, mean_q: 0.174902\n",
      " 28378/50000: episode: 3715, duration: 0.081s, episode steps:   5, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.003278, mae: 0.128854, mean_q: 0.184430\n",
      " 28385/50000: episode: 3716, duration: 0.137s, episode steps:   7, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003337, mae: 0.139746, mean_q: 0.197665\n",
      " 28390/50000: episode: 3717, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 1.000],  loss: 0.004004, mae: 0.132285, mean_q: 0.182050\n",
      " 28397/50000: episode: 3718, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002149, mae: 0.128731, mean_q: 0.181687\n",
      " 28407/50000: episode: 3719, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002119, mae: 0.119461, mean_q: 0.169025\n",
      " 28421/50000: episode: 3720, duration: 0.170s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002637, mae: 0.132600, mean_q: 0.186765\n",
      " 28432/50000: episode: 3721, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [1.000, 3.000],  loss: 0.002257, mae: 0.128020, mean_q: 0.180516\n",
      " 28434/50000: episode: 3722, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001751, mae: 0.131574, mean_q: 0.189524\n",
      " 28446/50000: episode: 3723, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002829, mae: 0.126546, mean_q: 0.177096\n",
      " 28456/50000: episode: 3724, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002705, mae: 0.130594, mean_q: 0.181028\n",
      " 28464/50000: episode: 3725, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003625, mae: 0.130012, mean_q: 0.179281\n",
      " 28477/50000: episode: 3726, duration: 0.183s, episode steps:  13, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.003306, mae: 0.129092, mean_q: 0.177505\n",
      " 28483/50000: episode: 3727, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 2.000],  loss: 0.002415, mae: 0.125124, mean_q: 0.174468\n",
      " 28491/50000: episode: 3728, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003021, mae: 0.128752, mean_q: 0.178010\n",
      " 28498/50000: episode: 3729, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003740, mae: 0.135158, mean_q: 0.186234\n",
      " 28510/50000: episode: 3730, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002358, mae: 0.128064, mean_q: 0.178674\n",
      " 28532/50000: episode: 3731, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002737, mae: 0.128965, mean_q: 0.181731\n",
      " 28543/50000: episode: 3732, duration: 0.197s, episode steps:  11, steps per second:  56, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 0.818 [0.000, 3.000],  loss: 0.001859, mae: 0.122059, mean_q: 0.172574\n",
      " 28551/50000: episode: 3733, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003843, mae: 0.129850, mean_q: 0.179808\n",
      " 28564/50000: episode: 3734, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002110, mae: 0.123148, mean_q: 0.172888\n",
      " 28568/50000: episode: 3735, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002022, mae: 0.122836, mean_q: 0.170755\n",
      " 28571/50000: episode: 3736, duration: 0.057s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002649, mae: 0.134043, mean_q: 0.184227\n",
      " 28580/50000: episode: 3737, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 3.000],  loss: 0.002847, mae: 0.127082, mean_q: 0.178142\n",
      " 28583/50000: episode: 3738, duration: 0.056s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002922, mae: 0.123828, mean_q: 0.169881\n",
      " 28585/50000: episode: 3739, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001827, mae: 0.115412, mean_q: 0.165476\n",
      " 28600/50000: episode: 3740, duration: 0.184s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002378, mae: 0.125113, mean_q: 0.176249\n",
      " 28606/50000: episode: 3741, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.004125, mae: 0.130130, mean_q: 0.183368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28623/50000: episode: 3742, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002402, mae: 0.120866, mean_q: 0.170341\n",
      " 28634/50000: episode: 3743, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002283, mae: 0.125575, mean_q: 0.176902\n",
      " 28638/50000: episode: 3744, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001796, mae: 0.121821, mean_q: 0.170670\n",
      " 28644/50000: episode: 3745, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002830, mae: 0.119167, mean_q: 0.166913\n",
      " 28648/50000: episode: 3746, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001846, mae: 0.121416, mean_q: 0.171600\n",
      " 28652/50000: episode: 3747, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002549, mae: 0.134406, mean_q: 0.186536\n",
      " 28656/50000: episode: 3748, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002246, mae: 0.127098, mean_q: 0.175993\n",
      " 28665/50000: episode: 3749, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.003237, mae: 0.123040, mean_q: 0.174958\n",
      " 28680/50000: episode: 3750, duration: 0.182s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002585, mae: 0.126078, mean_q: 0.181781\n",
      " 28694/50000: episode: 3751, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002304, mae: 0.123392, mean_q: 0.174836\n",
      " 28698/50000: episode: 3752, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001701, mae: 0.119897, mean_q: 0.168465\n",
      " 28701/50000: episode: 3753, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003809, mae: 0.133350, mean_q: 0.183879\n",
      " 28704/50000: episode: 3754, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001936, mae: 0.112668, mean_q: 0.161818\n",
      " 28723/50000: episode: 3755, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.002486, mae: 0.123962, mean_q: 0.175676\n",
      " 28733/50000: episode: 3756, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002542, mae: 0.125597, mean_q: 0.179787\n",
      " 28745/50000: episode: 3757, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.003203, mae: 0.127746, mean_q: 0.179861\n",
      " 28749/50000: episode: 3758, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.001882, mae: 0.127773, mean_q: 0.181590\n",
      " 28753/50000: episode: 3759, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001773, mae: 0.120303, mean_q: 0.169204\n",
      " 28761/50000: episode: 3760, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002497, mae: 0.126323, mean_q: 0.175037\n",
      " 28767/50000: episode: 3761, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001487, mae: 0.121241, mean_q: 0.172387\n",
      " 28770/50000: episode: 3762, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001873, mae: 0.120786, mean_q: 0.167315\n",
      " 28785/50000: episode: 3763, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.003478, mae: 0.131188, mean_q: 0.183487\n",
      " 28790/50000: episode: 3764, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [2.000, 3.000],  loss: 0.001833, mae: 0.127476, mean_q: 0.181517\n",
      " 28807/50000: episode: 3765, duration: 0.208s, episode steps:  17, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.003069, mae: 0.127726, mean_q: 0.181258\n",
      " 28814/50000: episode: 3766, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002278, mae: 0.122250, mean_q: 0.174166\n",
      " 28816/50000: episode: 3767, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001563, mae: 0.119215, mean_q: 0.168580\n",
      " 28819/50000: episode: 3768, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002647, mae: 0.124072, mean_q: 0.175081\n",
      " 28825/50000: episode: 3769, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002843, mae: 0.130371, mean_q: 0.182488\n",
      " 28831/50000: episode: 3770, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002980, mae: 0.129599, mean_q: 0.177979\n",
      " 28835/50000: episode: 3771, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003187, mae: 0.126506, mean_q: 0.181535\n",
      " 28839/50000: episode: 3772, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002050, mae: 0.121660, mean_q: 0.176628\n",
      " 28841/50000: episode: 3773, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002157, mae: 0.126211, mean_q: 0.181905\n",
      " 28852/50000: episode: 3774, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002436, mae: 0.127129, mean_q: 0.178845\n",
      " 28858/50000: episode: 3775, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002355, mae: 0.131656, mean_q: 0.183071\n",
      " 28860/50000: episode: 3776, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002090, mae: 0.126286, mean_q: 0.178354\n",
      " 28866/50000: episode: 3777, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002001, mae: 0.121114, mean_q: 0.170889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28881/50000: episode: 3778, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  1.000, mean reward:  0.067 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.003109, mae: 0.125418, mean_q: 0.178159\n",
      " 28893/50000: episode: 3779, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002689, mae: 0.123247, mean_q: 0.180030\n",
      " 28904/50000: episode: 3780, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002830, mae: 0.129894, mean_q: 0.184668\n",
      " 28909/50000: episode: 3781, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003013, mae: 0.129188, mean_q: 0.181669\n",
      " 28917/50000: episode: 3782, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002959, mae: 0.126656, mean_q: 0.175949\n",
      " 28922/50000: episode: 3783, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001909, mae: 0.119619, mean_q: 0.170077\n",
      " 28935/50000: episode: 3784, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002348, mae: 0.124219, mean_q: 0.176229\n",
      " 28937/50000: episode: 3785, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001121, mae: 0.118316, mean_q: 0.166597\n",
      " 28939/50000: episode: 3786, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.008877, mae: 0.142526, mean_q: 0.192675\n",
      " 28947/50000: episode: 3787, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002323, mae: 0.130081, mean_q: 0.182444\n",
      " 28949/50000: episode: 3788, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003835, mae: 0.136926, mean_q: 0.191630\n",
      " 28952/50000: episode: 3789, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001226, mae: 0.123939, mean_q: 0.174414\n",
      " 28955/50000: episode: 3790, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001430, mae: 0.124037, mean_q: 0.176519\n",
      " 28961/50000: episode: 3791, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002608, mae: 0.135936, mean_q: 0.188642\n",
      " 28966/50000: episode: 3792, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001873, mae: 0.123241, mean_q: 0.173095\n",
      " 28969/50000: episode: 3793, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002632, mae: 0.131040, mean_q: 0.183542\n",
      " 28971/50000: episode: 3794, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003105, mae: 0.144409, mean_q: 0.199534\n",
      " 28973/50000: episode: 3795, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001980, mae: 0.142457, mean_q: 0.197027\n",
      " 28982/50000: episode: 3796, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002396, mae: 0.134863, mean_q: 0.189062\n",
      " 28987/50000: episode: 3797, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002458, mae: 0.138599, mean_q: 0.193992\n",
      " 28999/50000: episode: 3798, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002901, mae: 0.130352, mean_q: 0.184005\n",
      " 29017/50000: episode: 3799, duration: 0.224s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.002763, mae: 0.128021, mean_q: 0.179346\n",
      " 29019/50000: episode: 3800, duration: 0.033s, episode steps:   2, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004767, mae: 0.136167, mean_q: 0.183486\n",
      " 29024/50000: episode: 3801, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002379, mae: 0.124498, mean_q: 0.176032\n",
      " 29036/50000: episode: 3802, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003410, mae: 0.129383, mean_q: 0.181714\n",
      " 29038/50000: episode: 3803, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002678, mae: 0.119485, mean_q: 0.174382\n",
      " 29042/50000: episode: 3804, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001399, mae: 0.120975, mean_q: 0.172432\n",
      " 29060/50000: episode: 3805, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001997, mae: 0.127105, mean_q: 0.180670\n",
      " 29066/50000: episode: 3806, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002622, mae: 0.133672, mean_q: 0.187269\n",
      " 29087/50000: episode: 3807, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002637, mae: 0.125329, mean_q: 0.177022\n",
      " 29089/50000: episode: 3808, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001383, mae: 0.124631, mean_q: 0.183657\n",
      " 29091/50000: episode: 3809, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002044, mae: 0.127033, mean_q: 0.183452\n",
      " 29095/50000: episode: 3810, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001852, mae: 0.121953, mean_q: 0.176649\n",
      " 29102/50000: episode: 3811, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001895, mae: 0.126946, mean_q: 0.180832\n",
      " 29113/50000: episode: 3812, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003458, mae: 0.129320, mean_q: 0.181630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29122/50000: episode: 3813, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [1.000, 3.000],  loss: 0.001849, mae: 0.119761, mean_q: 0.171918\n",
      " 29126/50000: episode: 3814, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.001610, mae: 0.121294, mean_q: 0.171547\n",
      " 29128/50000: episode: 3815, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005428, mae: 0.125476, mean_q: 0.173505\n",
      " 29137/50000: episode: 3816, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002639, mae: 0.125860, mean_q: 0.176856\n",
      " 29139/50000: episode: 3817, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001481, mae: 0.123095, mean_q: 0.175128\n",
      " 29150/50000: episode: 3818, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002377, mae: 0.128068, mean_q: 0.178717\n",
      " 29154/50000: episode: 3819, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001484, mae: 0.121829, mean_q: 0.172693\n",
      " 29166/50000: episode: 3820, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002656, mae: 0.125171, mean_q: 0.176291\n",
      " 29175/50000: episode: 3821, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [1.000, 3.000],  loss: 0.001991, mae: 0.124888, mean_q: 0.176046\n",
      " 29192/50000: episode: 3822, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002663, mae: 0.126187, mean_q: 0.177234\n",
      " 29203/50000: episode: 3823, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002573, mae: 0.124795, mean_q: 0.175151\n",
      " 29207/50000: episode: 3824, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003311, mae: 0.130493, mean_q: 0.183503\n",
      " 29219/50000: episode: 3825, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001949, mae: 0.123992, mean_q: 0.176540\n",
      " 29228/50000: episode: 3826, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002679, mae: 0.128593, mean_q: 0.177415\n",
      " 29248/50000: episode: 3827, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002816, mae: 0.127082, mean_q: 0.178532\n",
      " 29252/50000: episode: 3828, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.002456, mae: 0.130158, mean_q: 0.182277\n",
      " 29262/50000: episode: 3829, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002310, mae: 0.133876, mean_q: 0.188127\n",
      " 29268/50000: episode: 3830, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001963, mae: 0.125912, mean_q: 0.176934\n",
      " 29272/50000: episode: 3831, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002232, mae: 0.118359, mean_q: 0.165068\n",
      " 29289/50000: episode: 3832, duration: 0.242s, episode steps:  17, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002544, mae: 0.120593, mean_q: 0.172754\n",
      " 29294/50000: episode: 3833, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.001518, mae: 0.131806, mean_q: 0.189336\n",
      " 29307/50000: episode: 3834, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002615, mae: 0.127442, mean_q: 0.180228\n",
      " 29320/50000: episode: 3835, duration: 0.172s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002646, mae: 0.124874, mean_q: 0.178032\n",
      " 29324/50000: episode: 3836, duration: 0.065s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.001676, mae: 0.123334, mean_q: 0.172370\n",
      " 29331/50000: episode: 3837, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002064, mae: 0.123694, mean_q: 0.175453\n",
      " 29342/50000: episode: 3838, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002070, mae: 0.130059, mean_q: 0.182492\n",
      " 29355/50000: episode: 3839, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002895, mae: 0.123457, mean_q: 0.172912\n",
      " 29360/50000: episode: 3840, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002621, mae: 0.119623, mean_q: 0.168821\n",
      " 29365/50000: episode: 3841, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 2.000],  loss: 0.001693, mae: 0.123947, mean_q: 0.175769\n",
      " 29367/50000: episode: 3842, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003940, mae: 0.130692, mean_q: 0.184820\n",
      " 29378/50000: episode: 3843, duration: 0.155s, episode steps:  11, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002906, mae: 0.127258, mean_q: 0.178518\n",
      " 29390/50000: episode: 3844, duration: 0.176s, episode steps:  12, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [1.000, 2.000],  loss: 0.001671, mae: 0.123210, mean_q: 0.177370\n",
      " 29398/50000: episode: 3845, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002309, mae: 0.124705, mean_q: 0.176816\n",
      " 29400/50000: episode: 3846, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.008133, mae: 0.145227, mean_q: 0.205537\n",
      " 29410/50000: episode: 3847, duration: 0.144s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002423, mae: 0.128461, mean_q: 0.184335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29420/50000: episode: 3848, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003351, mae: 0.136836, mean_q: 0.192622\n",
      " 29447/50000: episode: 3849, duration: 0.409s, episode steps:  27, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.185 [0.000, 3.000],  loss: 0.002810, mae: 0.133370, mean_q: 0.189552\n",
      " 29453/50000: episode: 3850, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002638, mae: 0.137703, mean_q: 0.190856\n",
      " 29457/50000: episode: 3851, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003079, mae: 0.136549, mean_q: 0.187442\n",
      " 29461/50000: episode: 3852, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.004033, mae: 0.132613, mean_q: 0.184734\n",
      " 29467/50000: episode: 3853, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002921, mae: 0.130404, mean_q: 0.182366\n",
      " 29473/50000: episode: 3854, duration: 0.108s, episode steps:   6, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002915, mae: 0.129971, mean_q: 0.183017\n",
      " 29479/50000: episode: 3855, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002491, mae: 0.132036, mean_q: 0.183936\n",
      " 29486/50000: episode: 3856, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.004310, mae: 0.134650, mean_q: 0.184419\n",
      " 29489/50000: episode: 3857, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002542, mae: 0.131658, mean_q: 0.185340\n",
      " 29496/50000: episode: 3858, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 2.000],  loss: 0.002215, mae: 0.123065, mean_q: 0.176640\n",
      " 29506/50000: episode: 3859, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003179, mae: 0.136075, mean_q: 0.188857\n",
      " 29508/50000: episode: 3860, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003509, mae: 0.134461, mean_q: 0.188404\n",
      " 29512/50000: episode: 3861, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002120, mae: 0.138469, mean_q: 0.194381\n",
      " 29521/50000: episode: 3862, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002053, mae: 0.126125, mean_q: 0.178367\n",
      " 29525/50000: episode: 3863, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001454, mae: 0.129137, mean_q: 0.183348\n",
      " 29534/50000: episode: 3864, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.003695, mae: 0.134548, mean_q: 0.190020\n",
      " 29536/50000: episode: 3865, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001866, mae: 0.132088, mean_q: 0.188718\n",
      " 29552/50000: episode: 3866, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003092, mae: 0.132861, mean_q: 0.186353\n",
      " 29555/50000: episode: 3867, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.001540, mae: 0.127686, mean_q: 0.180960\n",
      " 29566/50000: episode: 3868, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002267, mae: 0.131065, mean_q: 0.179988\n",
      " 29572/50000: episode: 3869, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.003283, mae: 0.130700, mean_q: 0.179714\n",
      " 29574/50000: episode: 3870, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001472, mae: 0.122351, mean_q: 0.172414\n",
      " 29577/50000: episode: 3871, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003693, mae: 0.136448, mean_q: 0.190317\n",
      " 29591/50000: episode: 3872, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002832, mae: 0.129601, mean_q: 0.181846\n",
      " 29603/50000: episode: 3873, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002177, mae: 0.132487, mean_q: 0.188153\n",
      " 29611/50000: episode: 3874, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.001959, mae: 0.126182, mean_q: 0.180793\n",
      " 29615/50000: episode: 3875, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002736, mae: 0.132931, mean_q: 0.187660\n",
      " 29617/50000: episode: 3876, duration: 0.045s, episode steps:   2, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001916, mae: 0.132803, mean_q: 0.183084\n",
      " 29619/50000: episode: 3877, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002005, mae: 0.142953, mean_q: 0.199576\n",
      " 29626/50000: episode: 3878, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.003705, mae: 0.137451, mean_q: 0.190061\n",
      " 29630/50000: episode: 3879, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002341, mae: 0.126239, mean_q: 0.177930\n",
      " 29640/50000: episode: 3880, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002241, mae: 0.130797, mean_q: 0.184663\n",
      " 29649/50000: episode: 3881, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002696, mae: 0.128464, mean_q: 0.182438\n",
      " 29659/50000: episode: 3882, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003391, mae: 0.131737, mean_q: 0.185741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29665/50000: episode: 3883, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002024, mae: 0.137394, mean_q: 0.194412\n",
      " 29672/50000: episode: 3884, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002766, mae: 0.135214, mean_q: 0.188420\n",
      " 29675/50000: episode: 3885, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002468, mae: 0.133352, mean_q: 0.183508\n",
      " 29685/50000: episode: 3886, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002840, mae: 0.127013, mean_q: 0.180076\n",
      " 29698/50000: episode: 3887, duration: 0.183s, episode steps:  13, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002586, mae: 0.127667, mean_q: 0.180070\n",
      " 29709/50000: episode: 3888, duration: 0.152s, episode steps:  11, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.003183, mae: 0.126295, mean_q: 0.181098\n",
      " 29712/50000: episode: 3889, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001221, mae: 0.129437, mean_q: 0.188290\n",
      " 29717/50000: episode: 3890, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002777, mae: 0.137250, mean_q: 0.193310\n",
      " 29721/50000: episode: 3891, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003851, mae: 0.156266, mean_q: 0.213827\n",
      " 29730/50000: episode: 3892, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002711, mae: 0.131290, mean_q: 0.185869\n",
      " 29734/50000: episode: 3893, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003094, mae: 0.136649, mean_q: 0.191174\n",
      " 29742/50000: episode: 3894, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002761, mae: 0.131728, mean_q: 0.185564\n",
      " 29745/50000: episode: 3895, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002153, mae: 0.134794, mean_q: 0.187662\n",
      " 29751/50000: episode: 3896, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.003432, mae: 0.136712, mean_q: 0.191394\n",
      " 29756/50000: episode: 3897, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001369, mae: 0.129240, mean_q: 0.183225\n",
      " 29762/50000: episode: 3898, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.002045, mae: 0.130605, mean_q: 0.184146\n",
      " 29767/50000: episode: 3899, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003318, mae: 0.139350, mean_q: 0.194853\n",
      " 29771/50000: episode: 3900, duration: 0.056s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002259, mae: 0.132896, mean_q: 0.186449\n",
      " 29777/50000: episode: 3901, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002286, mae: 0.127646, mean_q: 0.180164\n",
      " 29787/50000: episode: 3902, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001948, mae: 0.129065, mean_q: 0.183521\n",
      " 29795/50000: episode: 3903, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002018, mae: 0.132565, mean_q: 0.186839\n",
      " 29800/50000: episode: 3904, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001871, mae: 0.127429, mean_q: 0.181661\n",
      " 29806/50000: episode: 3905, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003273, mae: 0.130404, mean_q: 0.183950\n",
      " 29818/50000: episode: 3906, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003245, mae: 0.131972, mean_q: 0.185970\n",
      " 29839/50000: episode: 3907, duration: 0.263s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003105, mae: 0.136235, mean_q: 0.190120\n",
      " 29861/50000: episode: 3908, duration: 0.268s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002543, mae: 0.128237, mean_q: 0.181344\n",
      " 29865/50000: episode: 3909, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002085, mae: 0.130991, mean_q: 0.185137\n",
      " 29868/50000: episode: 3910, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.007560, mae: 0.144815, mean_q: 0.198438\n",
      " 29871/50000: episode: 3911, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001952, mae: 0.136729, mean_q: 0.187986\n",
      " 29882/50000: episode: 3912, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002317, mae: 0.125644, mean_q: 0.176153\n",
      " 29888/50000: episode: 3913, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.001480, mae: 0.121902, mean_q: 0.172409\n",
      " 29895/50000: episode: 3914, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001884, mae: 0.124754, mean_q: 0.176937\n",
      " 29921/50000: episode: 3915, duration: 0.323s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.808 [0.000, 3.000],  loss: 0.002814, mae: 0.130389, mean_q: 0.183702\n",
      " 29926/50000: episode: 3916, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003022, mae: 0.127089, mean_q: 0.180029\n",
      " 29928/50000: episode: 3917, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001677, mae: 0.120345, mean_q: 0.171863\n",
      " 29931/50000: episode: 3918, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002621, mae: 0.127130, mean_q: 0.179816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29943/50000: episode: 3919, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002144, mae: 0.122496, mean_q: 0.175142\n",
      " 29952/50000: episode: 3920, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002764, mae: 0.131712, mean_q: 0.185893\n",
      " 29963/50000: episode: 3921, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002061, mae: 0.129063, mean_q: 0.183060\n",
      " 29969/50000: episode: 3922, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003348, mae: 0.132100, mean_q: 0.185713\n",
      " 29972/50000: episode: 3923, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002777, mae: 0.124756, mean_q: 0.179448\n",
      " 29978/50000: episode: 3924, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001740, mae: 0.121993, mean_q: 0.175858\n",
      " 29985/50000: episode: 3925, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002186, mae: 0.131168, mean_q: 0.187819\n",
      " 30004/50000: episode: 3926, duration: 0.233s, episode steps:  19, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.158 [0.000, 3.000],  loss: 0.001964, mae: 0.123505, mean_q: 0.176243\n",
      " 30010/50000: episode: 3927, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002014, mae: 0.136727, mean_q: 0.192041\n",
      " 30020/50000: episode: 3928, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001879, mae: 0.126895, mean_q: 0.180872\n",
      " 30026/50000: episode: 3929, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003649, mae: 0.135098, mean_q: 0.184626\n",
      " 30032/50000: episode: 3930, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002315, mae: 0.130144, mean_q: 0.180891\n",
      " 30038/50000: episode: 3931, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002782, mae: 0.133613, mean_q: 0.185334\n",
      " 30040/50000: episode: 3932, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004596, mae: 0.139291, mean_q: 0.193558\n",
      " 30043/50000: episode: 3933, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002281, mae: 0.133238, mean_q: 0.187825\n",
      " 30046/50000: episode: 3934, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002661, mae: 0.129749, mean_q: 0.182448\n",
      " 30049/50000: episode: 3935, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003127, mae: 0.136638, mean_q: 0.190355\n",
      " 30053/50000: episode: 3936, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003233, mae: 0.136549, mean_q: 0.193649\n",
      " 30065/50000: episode: 3937, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002649, mae: 0.130104, mean_q: 0.181987\n",
      " 30068/50000: episode: 3938, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.001811, mae: 0.125499, mean_q: 0.179650\n",
      " 30070/50000: episode: 3939, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003731, mae: 0.134226, mean_q: 0.189925\n",
      " 30074/50000: episode: 3940, duration: 0.077s, episode steps:   4, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002413, mae: 0.131286, mean_q: 0.186178\n",
      " 30078/50000: episode: 3941, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002038, mae: 0.142263, mean_q: 0.200637\n",
      " 30081/50000: episode: 3942, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001811, mae: 0.130103, mean_q: 0.186361\n",
      " 30084/50000: episode: 3943, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.004796, mae: 0.142795, mean_q: 0.198202\n",
      " 30089/50000: episode: 3944, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004301, mae: 0.136050, mean_q: 0.193759\n",
      " 30106/50000: episode: 3945, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001849, mae: 0.128570, mean_q: 0.184024\n",
      " 30117/50000: episode: 3946, duration: 0.186s, episode steps:  11, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002596, mae: 0.129656, mean_q: 0.182126\n",
      " 30119/50000: episode: 3947, duration: 0.052s, episode steps:   2, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001789, mae: 0.130104, mean_q: 0.182308\n",
      " 30143/50000: episode: 3948, duration: 0.341s, episode steps:  24, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002278, mae: 0.134202, mean_q: 0.188505\n",
      " 30158/50000: episode: 3949, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002411, mae: 0.127630, mean_q: 0.178903\n",
      " 30161/50000: episode: 3950, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001533, mae: 0.126488, mean_q: 0.179794\n",
      " 30166/50000: episode: 3951, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002020, mae: 0.132382, mean_q: 0.185051\n",
      " 30168/50000: episode: 3952, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002562, mae: 0.134634, mean_q: 0.185262\n",
      " 30178/50000: episode: 3953, duration: 0.138s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001998, mae: 0.130423, mean_q: 0.185287\n",
      " 30184/50000: episode: 3954, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002724, mae: 0.135433, mean_q: 0.190445\n",
      " 30188/50000: episode: 3955, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002040, mae: 0.122370, mean_q: 0.176337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30195/50000: episode: 3956, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003050, mae: 0.128409, mean_q: 0.179695\n",
      " 30201/50000: episode: 3957, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 3.000],  loss: 0.002157, mae: 0.129442, mean_q: 0.180762\n",
      " 30214/50000: episode: 3958, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002602, mae: 0.129531, mean_q: 0.182170\n",
      " 30220/50000: episode: 3959, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001746, mae: 0.122014, mean_q: 0.175825\n",
      " 30229/50000: episode: 3960, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002412, mae: 0.129720, mean_q: 0.182315\n",
      " 30234/50000: episode: 3961, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002865, mae: 0.128225, mean_q: 0.180302\n",
      " 30236/50000: episode: 3962, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001723, mae: 0.127379, mean_q: 0.184728\n",
      " 30250/50000: episode: 3963, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002999, mae: 0.131244, mean_q: 0.188714\n",
      " 30254/50000: episode: 3964, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002088, mae: 0.128141, mean_q: 0.188589\n",
      " 30263/50000: episode: 3965, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003660, mae: 0.132778, mean_q: 0.191141\n",
      " 30266/50000: episode: 3966, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002004, mae: 0.129948, mean_q: 0.190365\n",
      " 30278/50000: episode: 3967, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002591, mae: 0.132113, mean_q: 0.186820\n",
      " 30282/50000: episode: 3968, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.001720, mae: 0.130783, mean_q: 0.183795\n",
      " 30287/50000: episode: 3969, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002441, mae: 0.130709, mean_q: 0.186177\n",
      " 30297/50000: episode: 3970, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002389, mae: 0.133848, mean_q: 0.189313\n",
      " 30302/50000: episode: 3971, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [0.000, 3.000],  loss: 0.002215, mae: 0.137605, mean_q: 0.191118\n",
      " 30311/50000: episode: 3972, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001824, mae: 0.124998, mean_q: 0.178109\n",
      " 30334/50000: episode: 3973, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002454, mae: 0.128134, mean_q: 0.180331\n",
      " 30338/50000: episode: 3974, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.001782, mae: 0.131353, mean_q: 0.186783\n",
      " 30342/50000: episode: 3975, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002596, mae: 0.130070, mean_q: 0.182498\n",
      " 30348/50000: episode: 3976, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001631, mae: 0.130946, mean_q: 0.188848\n",
      " 30355/50000: episode: 3977, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.004076, mae: 0.139703, mean_q: 0.197268\n",
      " 30367/50000: episode: 3978, duration: 0.161s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002640, mae: 0.125655, mean_q: 0.185253\n",
      " 30375/50000: episode: 3979, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002525, mae: 0.130446, mean_q: 0.186154\n",
      " 30378/50000: episode: 3980, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002710, mae: 0.127935, mean_q: 0.183538\n",
      " 30400/50000: episode: 3981, duration: 0.280s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.864 [0.000, 3.000],  loss: 0.002434, mae: 0.127178, mean_q: 0.180548\n",
      " 30402/50000: episode: 3982, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002628, mae: 0.127621, mean_q: 0.181111\n",
      " 30405/50000: episode: 3983, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.004323, mae: 0.131771, mean_q: 0.187842\n",
      " 30417/50000: episode: 3984, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.083 [1.000, 3.000],  loss: 0.002599, mae: 0.134061, mean_q: 0.189309\n",
      " 30425/50000: episode: 3985, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002686, mae: 0.133130, mean_q: 0.190155\n",
      " 30429/50000: episode: 3986, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.005359, mae: 0.138934, mean_q: 0.194431\n",
      " 30432/50000: episode: 3987, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002761, mae: 0.137237, mean_q: 0.195752\n",
      " 30441/50000: episode: 3988, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002537, mae: 0.127854, mean_q: 0.182950\n",
      " 30450/50000: episode: 3989, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.003117, mae: 0.134686, mean_q: 0.188417\n",
      " 30453/50000: episode: 3990, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002768, mae: 0.128174, mean_q: 0.179481\n",
      " 30465/50000: episode: 3991, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001900, mae: 0.124003, mean_q: 0.178159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30468/50000: episode: 3992, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001479, mae: 0.128033, mean_q: 0.183119\n",
      " 30473/50000: episode: 3993, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002450, mae: 0.137902, mean_q: 0.191688\n",
      " 30480/50000: episode: 3994, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.001620, mae: 0.136740, mean_q: 0.189684\n",
      " 30489/50000: episode: 3995, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002219, mae: 0.134198, mean_q: 0.185918\n",
      " 30500/50000: episode: 3996, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001837, mae: 0.123589, mean_q: 0.175466\n",
      " 30507/50000: episode: 3997, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [1.000, 3.000],  loss: 0.003926, mae: 0.134959, mean_q: 0.187168\n",
      " 30511/50000: episode: 3998, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.004346, mae: 0.132347, mean_q: 0.184068\n",
      " 30516/50000: episode: 3999, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002275, mae: 0.132676, mean_q: 0.187970\n",
      " 30525/50000: episode: 4000, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003172, mae: 0.133959, mean_q: 0.186677\n",
      " 30543/50000: episode: 4001, duration: 0.240s, episode steps:  18, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002897, mae: 0.124904, mean_q: 0.175853\n",
      " 30545/50000: episode: 4002, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004002, mae: 0.130268, mean_q: 0.179637\n",
      " 30548/50000: episode: 4003, duration: 0.057s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003428, mae: 0.128036, mean_q: 0.182866\n",
      " 30553/50000: episode: 4004, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001918, mae: 0.125081, mean_q: 0.178713\n",
      " 30573/50000: episode: 4005, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.001976, mae: 0.125070, mean_q: 0.179419\n",
      " 30580/50000: episode: 4006, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 2.000],  loss: 0.003216, mae: 0.129681, mean_q: 0.183338\n",
      " 30588/50000: episode: 4007, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001686, mae: 0.126376, mean_q: 0.186256\n",
      " 30592/50000: episode: 4008, duration: 0.065s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002750, mae: 0.130308, mean_q: 0.185946\n",
      " 30599/50000: episode: 4009, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002803, mae: 0.131616, mean_q: 0.188269\n",
      " 30610/50000: episode: 4010, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001861, mae: 0.129639, mean_q: 0.186111\n",
      " 30614/50000: episode: 4011, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002343, mae: 0.131224, mean_q: 0.186352\n",
      " 30620/50000: episode: 4012, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001820, mae: 0.126698, mean_q: 0.180281\n",
      " 30627/50000: episode: 4013, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003795, mae: 0.128504, mean_q: 0.183774\n",
      " 30638/50000: episode: 4014, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.909 [0.000, 3.000],  loss: 0.002413, mae: 0.131615, mean_q: 0.185489\n",
      " 30644/50000: episode: 4015, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.004227, mae: 0.133726, mean_q: 0.185313\n",
      " 30653/50000: episode: 4016, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002137, mae: 0.132500, mean_q: 0.185849\n",
      " 30659/50000: episode: 4017, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001948, mae: 0.123602, mean_q: 0.177323\n",
      " 30661/50000: episode: 4018, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001602, mae: 0.120866, mean_q: 0.178721\n",
      " 30670/50000: episode: 4019, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002450, mae: 0.125680, mean_q: 0.180399\n",
      " 30675/50000: episode: 4020, duration: 0.098s, episode steps:   5, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.001682, mae: 0.125628, mean_q: 0.180280\n",
      " 30682/50000: episode: 4021, duration: 0.125s, episode steps:   7, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.002204, mae: 0.128020, mean_q: 0.184512\n",
      " 30689/50000: episode: 4022, duration: 0.176s, episode steps:   7, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.004479, mae: 0.133209, mean_q: 0.193727\n",
      " 30707/50000: episode: 4023, duration: 0.301s, episode steps:  18, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.278 [0.000, 3.000],  loss: 0.002188, mae: 0.127476, mean_q: 0.185244\n",
      " 30716/50000: episode: 4024, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002361, mae: 0.131487, mean_q: 0.186229\n",
      " 30724/50000: episode: 4025, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002475, mae: 0.126241, mean_q: 0.177868\n",
      " 30727/50000: episode: 4026, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001698, mae: 0.122510, mean_q: 0.175517\n",
      " 30732/50000: episode: 4027, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002700, mae: 0.132793, mean_q: 0.185032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30744/50000: episode: 4028, duration: 0.177s, episode steps:  12, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002292, mae: 0.129149, mean_q: 0.181421\n",
      " 30748/50000: episode: 4029, duration: 0.080s, episode steps:   4, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002185, mae: 0.124971, mean_q: 0.176597\n",
      " 30759/50000: episode: 4030, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002717, mae: 0.128933, mean_q: 0.182823\n",
      " 30767/50000: episode: 4031, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002925, mae: 0.136344, mean_q: 0.189402\n",
      " 30771/50000: episode: 4032, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002160, mae: 0.127937, mean_q: 0.178220\n",
      " 30775/50000: episode: 4033, duration: 0.085s, episode steps:   4, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003428, mae: 0.132020, mean_q: 0.182895\n",
      " 30778/50000: episode: 4034, duration: 0.065s, episode steps:   3, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001654, mae: 0.122014, mean_q: 0.176097\n",
      " 30789/50000: episode: 4035, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002819, mae: 0.128025, mean_q: 0.180612\n",
      " 30799/50000: episode: 4036, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002585, mae: 0.128183, mean_q: 0.179589\n",
      " 30802/50000: episode: 4037, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001836, mae: 0.126549, mean_q: 0.179380\n",
      " 30816/50000: episode: 4038, duration: 0.227s, episode steps:  14, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002494, mae: 0.123540, mean_q: 0.177808\n",
      " 30818/50000: episode: 4039, duration: 0.042s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001066, mae: 0.123741, mean_q: 0.181103\n",
      " 30820/50000: episode: 4040, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002242, mae: 0.133289, mean_q: 0.189418\n",
      " 30831/50000: episode: 4041, duration: 0.140s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [1.000, 3.000],  loss: 0.002733, mae: 0.131972, mean_q: 0.186824\n",
      " 30848/50000: episode: 4042, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002228, mae: 0.121022, mean_q: 0.176446\n",
      " 30860/50000: episode: 4043, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.002799, mae: 0.131466, mean_q: 0.187292\n",
      " 30865/50000: episode: 4044, duration: 0.098s, episode steps:   5, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002113, mae: 0.126620, mean_q: 0.185704\n",
      " 30869/50000: episode: 4045, duration: 0.072s, episode steps:   4, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001703, mae: 0.125648, mean_q: 0.182095\n",
      " 30871/50000: episode: 4046, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001834, mae: 0.125004, mean_q: 0.185102\n",
      " 30880/50000: episode: 4047, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.001942, mae: 0.128431, mean_q: 0.184391\n",
      " 30888/50000: episode: 4048, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002377, mae: 0.132094, mean_q: 0.188170\n",
      " 30891/50000: episode: 4049, duration: 0.068s, episode steps:   3, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002264, mae: 0.127018, mean_q: 0.182159\n",
      " 30897/50000: episode: 4050, duration: 0.106s, episode steps:   6, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003028, mae: 0.126307, mean_q: 0.181289\n",
      " 30906/50000: episode: 4051, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.778 [1.000, 3.000],  loss: 0.002153, mae: 0.128252, mean_q: 0.180667\n",
      " 30925/50000: episode: 4052, duration: 0.286s, episode steps:  19, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.003040, mae: 0.133580, mean_q: 0.187964\n",
      " 30944/50000: episode: 4053, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002072, mae: 0.126377, mean_q: 0.180681\n",
      " 30946/50000: episode: 4054, duration: 0.040s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002191, mae: 0.130751, mean_q: 0.182178\n",
      " 30949/50000: episode: 4055, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001814, mae: 0.140039, mean_q: 0.195339\n",
      " 30952/50000: episode: 4056, duration: 0.065s, episode steps:   3, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002065, mae: 0.132581, mean_q: 0.186497\n",
      " 30957/50000: episode: 4057, duration: 0.094s, episode steps:   5, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001853, mae: 0.128988, mean_q: 0.182111\n",
      " 30960/50000: episode: 4058, duration: 0.058s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001920, mae: 0.126621, mean_q: 0.180505\n",
      " 30967/50000: episode: 4059, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002183, mae: 0.129822, mean_q: 0.184052\n",
      " 30977/50000: episode: 4060, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002653, mae: 0.133084, mean_q: 0.187620\n",
      " 30980/50000: episode: 4061, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.003859, mae: 0.134921, mean_q: 0.191172\n",
      " 30993/50000: episode: 4062, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.003223, mae: 0.141549, mean_q: 0.200116\n",
      " 31003/50000: episode: 4063, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003277, mae: 0.141912, mean_q: 0.199932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31006/50000: episode: 4064, duration: 0.059s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002811, mae: 0.128854, mean_q: 0.180432\n",
      " 31009/50000: episode: 4065, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002920, mae: 0.124016, mean_q: 0.174777\n",
      " 31013/50000: episode: 4066, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002581, mae: 0.123272, mean_q: 0.174999\n",
      " 31029/50000: episode: 4067, duration: 0.220s, episode steps:  16, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002865, mae: 0.132982, mean_q: 0.186366\n",
      " 31040/50000: episode: 4068, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002780, mae: 0.131583, mean_q: 0.184528\n",
      " 31044/50000: episode: 4069, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002548, mae: 0.131206, mean_q: 0.183352\n",
      " 31069/50000: episode: 4070, duration: 0.316s, episode steps:  25, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002986, mae: 0.139944, mean_q: 0.196554\n",
      " 31071/50000: episode: 4071, duration: 0.044s, episode steps:   2, steps per second:  45, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001339, mae: 0.125283, mean_q: 0.183945\n",
      " 31084/50000: episode: 4072, duration: 0.217s, episode steps:  13, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002815, mae: 0.135917, mean_q: 0.195800\n",
      " 31099/50000: episode: 4073, duration: 0.208s, episode steps:  15, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.002823, mae: 0.134807, mean_q: 0.187525\n",
      " 31111/50000: episode: 4074, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002545, mae: 0.135682, mean_q: 0.190882\n",
      " 31115/50000: episode: 4075, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002379, mae: 0.135584, mean_q: 0.188581\n",
      " 31128/50000: episode: 4076, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003062, mae: 0.130881, mean_q: 0.180812\n",
      " 31134/50000: episode: 4077, duration: 0.098s, episode steps:   6, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002593, mae: 0.128700, mean_q: 0.183302\n",
      " 31144/50000: episode: 4078, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003523, mae: 0.136909, mean_q: 0.191337\n",
      " 31146/50000: episode: 4079, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002486, mae: 0.138348, mean_q: 0.191796\n",
      " 31154/50000: episode: 4080, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002462, mae: 0.137018, mean_q: 0.194222\n",
      " 31160/50000: episode: 4081, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002630, mae: 0.132000, mean_q: 0.188061\n",
      " 31164/50000: episode: 4082, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003461, mae: 0.131663, mean_q: 0.183484\n",
      " 31179/50000: episode: 4083, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002252, mae: 0.133178, mean_q: 0.189315\n",
      " 31188/50000: episode: 4084, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 2.000],  loss: 0.003700, mae: 0.138628, mean_q: 0.193436\n",
      " 31204/50000: episode: 4085, duration: 0.204s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002736, mae: 0.135310, mean_q: 0.190590\n",
      " 31208/50000: episode: 4086, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002586, mae: 0.131261, mean_q: 0.185553\n",
      " 31213/50000: episode: 4087, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003227, mae: 0.132846, mean_q: 0.184552\n",
      " 31220/50000: episode: 4088, duration: 0.128s, episode steps:   7, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002787, mae: 0.138867, mean_q: 0.195540\n",
      " 31226/50000: episode: 4089, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002222, mae: 0.132422, mean_q: 0.186055\n",
      " 31233/50000: episode: 4090, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003576, mae: 0.135129, mean_q: 0.189912\n",
      " 31242/50000: episode: 4091, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003068, mae: 0.132255, mean_q: 0.187355\n",
      " 31251/50000: episode: 4092, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [0.000, 3.000],  loss: 0.003770, mae: 0.141046, mean_q: 0.196630\n",
      " 31260/50000: episode: 4093, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003143, mae: 0.135701, mean_q: 0.191387\n",
      " 31264/50000: episode: 4094, duration: 0.071s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002448, mae: 0.131997, mean_q: 0.186529\n",
      " 31270/50000: episode: 4095, duration: 0.096s, episode steps:   6, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002675, mae: 0.130803, mean_q: 0.186712\n",
      " 31279/50000: episode: 4096, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.004277, mae: 0.137552, mean_q: 0.193074\n",
      " 31281/50000: episode: 4097, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002523, mae: 0.132806, mean_q: 0.187620\n",
      " 31283/50000: episode: 4098, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001408, mae: 0.132206, mean_q: 0.186226\n",
      " 31291/50000: episode: 4099, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002724, mae: 0.138772, mean_q: 0.194868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31294/50000: episode: 4100, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002839, mae: 0.131129, mean_q: 0.185050\n",
      " 31306/50000: episode: 4101, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.083 [0.000, 3.000],  loss: 0.003567, mae: 0.136222, mean_q: 0.193537\n",
      " 31319/50000: episode: 4102, duration: 0.185s, episode steps:  13, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002788, mae: 0.134450, mean_q: 0.196240\n",
      " 31321/50000: episode: 4103, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001744, mae: 0.131235, mean_q: 0.192942\n",
      " 31327/50000: episode: 4104, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003190, mae: 0.138881, mean_q: 0.202675\n",
      " 31333/50000: episode: 4105, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003151, mae: 0.137052, mean_q: 0.194945\n",
      " 31339/50000: episode: 4106, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003267, mae: 0.140210, mean_q: 0.200342\n",
      " 31361/50000: episode: 4107, duration: 0.308s, episode steps:  22, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.002782, mae: 0.133377, mean_q: 0.190405\n",
      " 31375/50000: episode: 4108, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.003192, mae: 0.143149, mean_q: 0.199879\n",
      " 31377/50000: episode: 4109, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002254, mae: 0.136960, mean_q: 0.192466\n",
      " 31381/50000: episode: 4110, duration: 0.066s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003003, mae: 0.137158, mean_q: 0.192079\n",
      " 31390/50000: episode: 4111, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [0.000, 3.000],  loss: 0.003114, mae: 0.134179, mean_q: 0.191280\n",
      " 31396/50000: episode: 4112, duration: 0.104s, episode steps:   6, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003198, mae: 0.142316, mean_q: 0.202299\n",
      " 31398/50000: episode: 4113, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002127, mae: 0.133478, mean_q: 0.188566\n",
      " 31401/50000: episode: 4114, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.004050, mae: 0.146399, mean_q: 0.203648\n",
      " 31410/50000: episode: 4115, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003675, mae: 0.138656, mean_q: 0.191158\n",
      " 31414/50000: episode: 4116, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003106, mae: 0.141710, mean_q: 0.195810\n",
      " 31425/50000: episode: 4117, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.003464, mae: 0.140369, mean_q: 0.194355\n",
      " 31445/50000: episode: 4118, duration: 0.273s, episode steps:  20, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.003378, mae: 0.136418, mean_q: 0.190269\n",
      " 31450/50000: episode: 4119, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.001604, mae: 0.134636, mean_q: 0.191152\n",
      " 31459/50000: episode: 4120, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002574, mae: 0.138666, mean_q: 0.195796\n",
      " 31465/50000: episode: 4121, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002991, mae: 0.134444, mean_q: 0.192816\n",
      " 31471/50000: episode: 4122, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003602, mae: 0.137780, mean_q: 0.194517\n",
      " 31473/50000: episode: 4123, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004553, mae: 0.140035, mean_q: 0.193987\n",
      " 31486/50000: episode: 4124, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 2.154 [1.000, 3.000],  loss: 0.002867, mae: 0.142715, mean_q: 0.200252\n",
      " 31492/50000: episode: 4125, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002802, mae: 0.142017, mean_q: 0.203233\n",
      " 31497/50000: episode: 4126, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002758, mae: 0.134069, mean_q: 0.195865\n",
      " 31501/50000: episode: 4127, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002007, mae: 0.134050, mean_q: 0.194224\n",
      " 31510/50000: episode: 4128, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003098, mae: 0.136794, mean_q: 0.194418\n",
      " 31524/50000: episode: 4129, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002910, mae: 0.144563, mean_q: 0.203995\n",
      " 31531/50000: episode: 4130, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002833, mae: 0.133291, mean_q: 0.187002\n",
      " 31536/50000: episode: 4131, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003045, mae: 0.136515, mean_q: 0.189560\n",
      " 31542/50000: episode: 4132, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003247, mae: 0.141052, mean_q: 0.196641\n",
      " 31555/50000: episode: 4133, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002860, mae: 0.135121, mean_q: 0.187893\n",
      " 31557/50000: episode: 4134, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002454, mae: 0.136053, mean_q: 0.193508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31570/50000: episode: 4135, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.077 [0.000, 3.000],  loss: 0.002802, mae: 0.135780, mean_q: 0.194096\n",
      " 31599/50000: episode: 4136, duration: 0.360s, episode steps:  29, steps per second:  80, episode reward:  1.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002513, mae: 0.136077, mean_q: 0.192473\n",
      " 31602/50000: episode: 4137, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002607, mae: 0.132440, mean_q: 0.188192\n",
      " 31618/50000: episode: 4138, duration: 0.198s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002489, mae: 0.140162, mean_q: 0.198008\n",
      " 31621/50000: episode: 4139, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002582, mae: 0.141319, mean_q: 0.195661\n",
      " 31627/50000: episode: 4140, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002563, mae: 0.143990, mean_q: 0.203229\n",
      " 31647/50000: episode: 4141, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.050 [0.000, 3.000],  loss: 0.003158, mae: 0.140979, mean_q: 0.202155\n",
      " 31653/50000: episode: 4142, duration: 0.101s, episode steps:   6, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003220, mae: 0.133908, mean_q: 0.191182\n",
      " 31657/50000: episode: 4143, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002870, mae: 0.128993, mean_q: 0.182265\n",
      " 31669/50000: episode: 4144, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002392, mae: 0.135593, mean_q: 0.192777\n",
      " 31675/50000: episode: 4145, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.002118, mae: 0.142687, mean_q: 0.201066\n",
      " 31680/50000: episode: 4146, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002217, mae: 0.137048, mean_q: 0.194160\n",
      " 31686/50000: episode: 4147, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003085, mae: 0.136335, mean_q: 0.190873\n",
      " 31698/50000: episode: 4148, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001464, mae: 0.135247, mean_q: 0.190913\n",
      " 31708/50000: episode: 4149, duration: 0.131s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002953, mae: 0.137502, mean_q: 0.196189\n",
      " 31715/50000: episode: 4150, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002464, mae: 0.139621, mean_q: 0.199984\n",
      " 31722/50000: episode: 4151, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003487, mae: 0.141027, mean_q: 0.200607\n",
      " 31738/50000: episode: 4152, duration: 0.210s, episode steps:  16, steps per second:  76, episode reward:  1.000, mean reward:  0.062 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002758, mae: 0.142533, mean_q: 0.201019\n",
      " 31744/50000: episode: 4153, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002466, mae: 0.137645, mean_q: 0.197860\n",
      " 31762/50000: episode: 4154, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002285, mae: 0.139004, mean_q: 0.197207\n",
      " 31765/50000: episode: 4155, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002642, mae: 0.142391, mean_q: 0.197748\n",
      " 31767/50000: episode: 4156, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004035, mae: 0.142782, mean_q: 0.195135\n",
      " 31769/50000: episode: 4157, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004572, mae: 0.152391, mean_q: 0.215252\n",
      " 31773/50000: episode: 4158, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001763, mae: 0.138025, mean_q: 0.195582\n",
      " 31778/50000: episode: 4159, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.200 [0.000, 1.000],  loss: 0.002618, mae: 0.138261, mean_q: 0.194396\n",
      " 31781/50000: episode: 4160, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.004153, mae: 0.137158, mean_q: 0.188740\n",
      " 31784/50000: episode: 4161, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003081, mae: 0.129728, mean_q: 0.188322\n",
      " 31791/50000: episode: 4162, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003884, mae: 0.134266, mean_q: 0.190398\n",
      " 31795/50000: episode: 4163, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002801, mae: 0.134500, mean_q: 0.195022\n",
      " 31810/50000: episode: 4164, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.003287, mae: 0.143646, mean_q: 0.203152\n",
      " 31812/50000: episode: 4165, duration: 0.040s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003363, mae: 0.144635, mean_q: 0.205394\n",
      " 31827/50000: episode: 4166, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.133 [0.000, 3.000],  loss: 0.002878, mae: 0.136977, mean_q: 0.197463\n",
      " 31835/50000: episode: 4167, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002771, mae: 0.141920, mean_q: 0.203905\n",
      " 31844/50000: episode: 4168, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003278, mae: 0.149440, mean_q: 0.208985\n",
      " 31851/50000: episode: 4169, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003428, mae: 0.137899, mean_q: 0.195678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31868/50000: episode: 4170, duration: 0.213s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.003327, mae: 0.140225, mean_q: 0.196122\n",
      " 31872/50000: episode: 4171, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001405, mae: 0.140232, mean_q: 0.200082\n",
      " 31880/50000: episode: 4172, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.002288, mae: 0.137789, mean_q: 0.196791\n",
      " 31887/50000: episode: 4173, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002158, mae: 0.138855, mean_q: 0.197303\n",
      " 31889/50000: episode: 4174, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003076, mae: 0.149666, mean_q: 0.206115\n",
      " 31894/50000: episode: 4175, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [1.000, 2.000],  loss: 0.002847, mae: 0.141267, mean_q: 0.200590\n",
      " 31906/50000: episode: 4176, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002632, mae: 0.143474, mean_q: 0.200101\n",
      " 31909/50000: episode: 4177, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001671, mae: 0.142684, mean_q: 0.205071\n",
      " 31919/50000: episode: 4178, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.002871, mae: 0.144196, mean_q: 0.203364\n",
      " 31924/50000: episode: 4179, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002641, mae: 0.137247, mean_q: 0.196107\n",
      " 31930/50000: episode: 4180, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002906, mae: 0.143268, mean_q: 0.201749\n",
      " 31932/50000: episode: 4181, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001907, mae: 0.132843, mean_q: 0.186768\n",
      " 31939/50000: episode: 4182, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003031, mae: 0.143713, mean_q: 0.198382\n",
      " 31945/50000: episode: 4183, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002639, mae: 0.139326, mean_q: 0.194968\n",
      " 31952/50000: episode: 4184, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 2.000],  loss: 0.004140, mae: 0.152424, mean_q: 0.214315\n",
      " 31960/50000: episode: 4185, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002346, mae: 0.143122, mean_q: 0.200654\n",
      " 31968/50000: episode: 4186, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002253, mae: 0.136981, mean_q: 0.195003\n",
      " 31984/50000: episode: 4187, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002819, mae: 0.143058, mean_q: 0.200635\n",
      " 31986/50000: episode: 4188, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003050, mae: 0.147349, mean_q: 0.204051\n",
      " 31991/50000: episode: 4189, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003313, mae: 0.139338, mean_q: 0.194386\n",
      " 32003/50000: episode: 4190, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003022, mae: 0.138990, mean_q: 0.199745\n",
      " 32021/50000: episode: 4191, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002564, mae: 0.138527, mean_q: 0.198176\n",
      " 32026/50000: episode: 4192, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003015, mae: 0.148556, mean_q: 0.209971\n",
      " 32038/50000: episode: 4193, duration: 0.151s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002941, mae: 0.144215, mean_q: 0.203884\n",
      " 32043/50000: episode: 4194, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003083, mae: 0.142444, mean_q: 0.202223\n",
      " 32049/50000: episode: 4195, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002172, mae: 0.137692, mean_q: 0.196470\n",
      " 32051/50000: episode: 4196, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004173, mae: 0.148871, mean_q: 0.206214\n",
      " 32053/50000: episode: 4197, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001259, mae: 0.128879, mean_q: 0.185721\n",
      " 32062/50000: episode: 4198, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002153, mae: 0.134744, mean_q: 0.195393\n",
      " 32082/50000: episode: 4199, duration: 0.243s, episode steps:  20, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.350 [0.000, 3.000],  loss: 0.002540, mae: 0.137660, mean_q: 0.197847\n",
      " 32085/50000: episode: 4200, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001980, mae: 0.145136, mean_q: 0.207053\n",
      " 32088/50000: episode: 4201, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003283, mae: 0.142365, mean_q: 0.203057\n",
      " 32091/50000: episode: 4202, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002630, mae: 0.143701, mean_q: 0.200616\n",
      " 32094/50000: episode: 4203, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002667, mae: 0.139792, mean_q: 0.195821\n",
      " 32106/50000: episode: 4204, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.583 [0.000, 2.000],  loss: 0.002707, mae: 0.141114, mean_q: 0.202585\n",
      " 32116/50000: episode: 4205, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002960, mae: 0.141038, mean_q: 0.201390\n",
      " 32118/50000: episode: 4206, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003324, mae: 0.143732, mean_q: 0.201767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32122/50000: episode: 4207, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002114, mae: 0.139520, mean_q: 0.202414\n",
      " 32129/50000: episode: 4208, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002730, mae: 0.135705, mean_q: 0.196316\n",
      " 32137/50000: episode: 4209, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003016, mae: 0.140380, mean_q: 0.201328\n",
      " 32144/50000: episode: 4210, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003913, mae: 0.147464, mean_q: 0.209865\n",
      " 32152/50000: episode: 4211, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002883, mae: 0.143162, mean_q: 0.203128\n",
      " 32165/50000: episode: 4212, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.003601, mae: 0.143429, mean_q: 0.200327\n",
      " 32172/50000: episode: 4213, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002204, mae: 0.137508, mean_q: 0.197304\n",
      " 32182/50000: episode: 4214, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003330, mae: 0.144529, mean_q: 0.205370\n",
      " 32190/50000: episode: 4215, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002758, mae: 0.141168, mean_q: 0.199018\n",
      " 32198/50000: episode: 4216, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002384, mae: 0.136865, mean_q: 0.195157\n",
      " 32200/50000: episode: 4217, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002769, mae: 0.140805, mean_q: 0.201132\n",
      " 32210/50000: episode: 4218, duration: 0.127s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002168, mae: 0.132727, mean_q: 0.189810\n",
      " 32215/50000: episode: 4219, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.600 [1.000, 3.000],  loss: 0.002832, mae: 0.135298, mean_q: 0.194324\n",
      " 32222/50000: episode: 4220, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001898, mae: 0.137250, mean_q: 0.191721\n",
      " 32231/50000: episode: 4221, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002111, mae: 0.136964, mean_q: 0.194464\n",
      " 32240/50000: episode: 4222, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002806, mae: 0.139636, mean_q: 0.196877\n",
      " 32247/50000: episode: 4223, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.429 [1.000, 3.000],  loss: 0.002126, mae: 0.134499, mean_q: 0.192866\n",
      " 32253/50000: episode: 4224, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003065, mae: 0.138030, mean_q: 0.197272\n",
      " 32262/50000: episode: 4225, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002815, mae: 0.137449, mean_q: 0.197441\n",
      " 32266/50000: episode: 4226, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002553, mae: 0.140014, mean_q: 0.200395\n",
      " 32272/50000: episode: 4227, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002630, mae: 0.142578, mean_q: 0.202777\n",
      " 32277/50000: episode: 4228, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.002952, mae: 0.144087, mean_q: 0.203050\n",
      " 32287/50000: episode: 4229, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002564, mae: 0.141043, mean_q: 0.198856\n",
      " 32291/50000: episode: 4230, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002866, mae: 0.141157, mean_q: 0.200440\n",
      " 32298/50000: episode: 4231, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002589, mae: 0.141222, mean_q: 0.203506\n",
      " 32323/50000: episode: 4232, duration: 0.334s, episode steps:  25, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003279, mae: 0.140516, mean_q: 0.199038\n",
      " 32328/50000: episode: 4233, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002640, mae: 0.139019, mean_q: 0.195932\n",
      " 32339/50000: episode: 4234, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003733, mae: 0.142138, mean_q: 0.200848\n",
      " 32357/50000: episode: 4235, duration: 0.224s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002561, mae: 0.135039, mean_q: 0.197462\n",
      " 32363/50000: episode: 4236, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002602, mae: 0.135264, mean_q: 0.197755\n",
      " 32386/50000: episode: 4237, duration: 0.282s, episode steps:  23, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.261 [0.000, 3.000],  loss: 0.003106, mae: 0.142789, mean_q: 0.205306\n",
      " 32389/50000: episode: 4238, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002468, mae: 0.144721, mean_q: 0.204559\n",
      " 32412/50000: episode: 4239, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.003295, mae: 0.142488, mean_q: 0.200693\n",
      " 32419/50000: episode: 4240, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.002060, mae: 0.145727, mean_q: 0.208902\n",
      " 32430/50000: episode: 4241, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003578, mae: 0.150253, mean_q: 0.213471\n",
      " 32435/50000: episode: 4242, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.002716, mae: 0.142881, mean_q: 0.203287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32450/50000: episode: 4243, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002513, mae: 0.142960, mean_q: 0.203147\n",
      " 32459/50000: episode: 4244, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002447, mae: 0.137515, mean_q: 0.197509\n",
      " 32465/50000: episode: 4245, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002977, mae: 0.144030, mean_q: 0.203208\n",
      " 32472/50000: episode: 4246, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002541, mae: 0.144963, mean_q: 0.204760\n",
      " 32478/50000: episode: 4247, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002425, mae: 0.142221, mean_q: 0.200109\n",
      " 32489/50000: episode: 4248, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002660, mae: 0.140094, mean_q: 0.197739\n",
      " 32491/50000: episode: 4249, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003056, mae: 0.145208, mean_q: 0.205636\n",
      " 32505/50000: episode: 4250, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002602, mae: 0.139592, mean_q: 0.199111\n",
      " 32526/50000: episode: 4251, duration: 0.296s, episode steps:  21, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.381 [0.000, 3.000],  loss: 0.003218, mae: 0.140795, mean_q: 0.199529\n",
      " 32528/50000: episode: 4252, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002297, mae: 0.137732, mean_q: 0.199815\n",
      " 32534/50000: episode: 4253, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002403, mae: 0.137437, mean_q: 0.197157\n",
      " 32545/50000: episode: 4254, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.364 [0.000, 2.000],  loss: 0.002065, mae: 0.139075, mean_q: 0.203662\n",
      " 32564/50000: episode: 4255, duration: 0.250s, episode steps:  19, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.002715, mae: 0.140239, mean_q: 0.201180\n",
      " 32580/50000: episode: 4256, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.003156, mae: 0.144630, mean_q: 0.205281\n",
      " 32585/50000: episode: 4257, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.004873, mae: 0.146326, mean_q: 0.211560\n",
      " 32587/50000: episode: 4258, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003054, mae: 0.148500, mean_q: 0.211033\n",
      " 32601/50000: episode: 4259, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002496, mae: 0.138641, mean_q: 0.198868\n",
      " 32608/50000: episode: 4260, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003260, mae: 0.144397, mean_q: 0.202795\n",
      " 32611/50000: episode: 4261, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002185, mae: 0.142703, mean_q: 0.205701\n",
      " 32620/50000: episode: 4262, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003060, mae: 0.141722, mean_q: 0.203323\n",
      " 32633/50000: episode: 4263, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.002941, mae: 0.140551, mean_q: 0.202067\n",
      " 32636/50000: episode: 4264, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002516, mae: 0.143714, mean_q: 0.201533\n",
      " 32641/50000: episode: 4265, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002605, mae: 0.141732, mean_q: 0.200669\n",
      " 32643/50000: episode: 4266, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003997, mae: 0.148812, mean_q: 0.208171\n",
      " 32650/50000: episode: 4267, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002644, mae: 0.140647, mean_q: 0.199576\n",
      " 32672/50000: episode: 4268, duration: 0.267s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002777, mae: 0.142580, mean_q: 0.200446\n",
      " 32674/50000: episode: 4269, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002769, mae: 0.142829, mean_q: 0.203063\n",
      " 32689/50000: episode: 4270, duration: 0.186s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002486, mae: 0.143718, mean_q: 0.201930\n",
      " 32691/50000: episode: 4271, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002981, mae: 0.155206, mean_q: 0.220838\n",
      " 32698/50000: episode: 4272, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 2.000],  loss: 0.002190, mae: 0.142189, mean_q: 0.201975\n",
      " 32702/50000: episode: 4273, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003967, mae: 0.146301, mean_q: 0.210092\n",
      " 32711/50000: episode: 4274, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.003068, mae: 0.136929, mean_q: 0.199250\n",
      " 32725/50000: episode: 4275, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003415, mae: 0.144402, mean_q: 0.205988\n",
      " 32730/50000: episode: 4276, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.002703, mae: 0.138789, mean_q: 0.196435\n",
      " 32734/50000: episode: 4277, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002555, mae: 0.135660, mean_q: 0.189042\n",
      " 32743/50000: episode: 4278, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003628, mae: 0.140953, mean_q: 0.197104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32745/50000: episode: 4279, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.005338, mae: 0.146857, mean_q: 0.201855\n",
      " 32755/50000: episode: 4280, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002161, mae: 0.143027, mean_q: 0.200413\n",
      " 32762/50000: episode: 4281, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002743, mae: 0.142193, mean_q: 0.197990\n",
      " 32772/50000: episode: 4282, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003631, mae: 0.141139, mean_q: 0.199127\n",
      " 32782/50000: episode: 4283, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002669, mae: 0.137562, mean_q: 0.194013\n",
      " 32785/50000: episode: 4284, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001748, mae: 0.133886, mean_q: 0.191772\n",
      " 32803/50000: episode: 4285, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003381, mae: 0.141937, mean_q: 0.200632\n",
      " 32808/50000: episode: 4286, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002073, mae: 0.140906, mean_q: 0.198814\n",
      " 32825/50000: episode: 4287, duration: 0.209s, episode steps:  17, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.003102, mae: 0.144639, mean_q: 0.200888\n",
      " 32830/50000: episode: 4288, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003400, mae: 0.134197, mean_q: 0.186352\n",
      " 32832/50000: episode: 4289, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003037, mae: 0.139728, mean_q: 0.195765\n",
      " 32836/50000: episode: 4290, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002915, mae: 0.140219, mean_q: 0.197661\n",
      " 32841/50000: episode: 4291, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004260, mae: 0.146797, mean_q: 0.205301\n",
      " 32859/50000: episode: 4292, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003024, mae: 0.141801, mean_q: 0.201533\n",
      " 32863/50000: episode: 4293, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002579, mae: 0.141943, mean_q: 0.198459\n",
      " 32867/50000: episode: 4294, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002142, mae: 0.140905, mean_q: 0.198335\n",
      " 32873/50000: episode: 4295, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004207, mae: 0.150442, mean_q: 0.207144\n",
      " 32879/50000: episode: 4296, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.003297, mae: 0.144202, mean_q: 0.202194\n",
      " 32882/50000: episode: 4297, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002790, mae: 0.150337, mean_q: 0.208387\n",
      " 32885/50000: episode: 4298, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002504, mae: 0.138328, mean_q: 0.195666\n",
      " 32890/50000: episode: 4299, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003262, mae: 0.145797, mean_q: 0.203923\n",
      " 32906/50000: episode: 4300, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002631, mae: 0.137422, mean_q: 0.193452\n",
      " 32910/50000: episode: 4301, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002213, mae: 0.141350, mean_q: 0.202001\n",
      " 32913/50000: episode: 4302, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003235, mae: 0.146410, mean_q: 0.206019\n",
      " 32919/50000: episode: 4303, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002080, mae: 0.141779, mean_q: 0.196934\n",
      " 32922/50000: episode: 4304, duration: 0.065s, episode steps:   3, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001775, mae: 0.136727, mean_q: 0.191649\n",
      " 32932/50000: episode: 4305, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001482, mae: 0.138920, mean_q: 0.195973\n",
      " 32943/50000: episode: 4306, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002553, mae: 0.149025, mean_q: 0.208037\n",
      " 32949/50000: episode: 4307, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.002564, mae: 0.140124, mean_q: 0.202070\n",
      " 32952/50000: episode: 4308, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003278, mae: 0.138975, mean_q: 0.201842\n",
      " 32954/50000: episode: 4309, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.006680, mae: 0.148035, mean_q: 0.212175\n",
      " 32962/50000: episode: 4310, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003288, mae: 0.142454, mean_q: 0.210258\n",
      " 32971/50000: episode: 4311, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.002380, mae: 0.138171, mean_q: 0.202949\n",
      " 33001/50000: episode: 4312, duration: 0.366s, episode steps:  30, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003281, mae: 0.145297, mean_q: 0.203369\n",
      " 33003/50000: episode: 4313, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002907, mae: 0.145922, mean_q: 0.204636\n",
      " 33012/50000: episode: 4314, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.002072, mae: 0.137857, mean_q: 0.197449\n",
      " 33015/50000: episode: 4315, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002347, mae: 0.140378, mean_q: 0.198089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33021/50000: episode: 4316, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004390, mae: 0.140279, mean_q: 0.201159\n",
      " 33024/50000: episode: 4317, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002462, mae: 0.133193, mean_q: 0.195015\n",
      " 33029/50000: episode: 4318, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001857, mae: 0.133530, mean_q: 0.193657\n",
      " 33032/50000: episode: 4319, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003000, mae: 0.154256, mean_q: 0.215726\n",
      " 33043/50000: episode: 4320, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 0.909 [0.000, 3.000],  loss: 0.002318, mae: 0.145587, mean_q: 0.203824\n",
      " 33052/50000: episode: 4321, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002524, mae: 0.145256, mean_q: 0.205563\n",
      " 33055/50000: episode: 4322, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001683, mae: 0.142133, mean_q: 0.197392\n",
      " 33069/50000: episode: 4323, duration: 0.196s, episode steps:  14, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002988, mae: 0.146336, mean_q: 0.205084\n",
      " 33073/50000: episode: 4324, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.004898, mae: 0.151012, mean_q: 0.211603\n",
      " 33083/50000: episode: 4325, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002841, mae: 0.145072, mean_q: 0.210578\n",
      " 33086/50000: episode: 4326, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002088, mae: 0.132972, mean_q: 0.195214\n",
      " 33112/50000: episode: 4327, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.002884, mae: 0.143437, mean_q: 0.205833\n",
      " 33122/50000: episode: 4328, duration: 0.126s, episode steps:  10, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003200, mae: 0.147199, mean_q: 0.205086\n",
      " 33124/50000: episode: 4329, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.006277, mae: 0.157817, mean_q: 0.218824\n",
      " 33129/50000: episode: 4330, duration: 0.104s, episode steps:   5, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003147, mae: 0.145902, mean_q: 0.205960\n",
      " 33133/50000: episode: 4331, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002445, mae: 0.144748, mean_q: 0.204839\n",
      " 33136/50000: episode: 4332, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.002732, mae: 0.138137, mean_q: 0.199905\n",
      " 33144/50000: episode: 4333, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003710, mae: 0.142530, mean_q: 0.204641\n",
      " 33152/50000: episode: 4334, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003920, mae: 0.149468, mean_q: 0.210616\n",
      " 33154/50000: episode: 4335, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002649, mae: 0.137247, mean_q: 0.193775\n",
      " 33158/50000: episode: 4336, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002087, mae: 0.137306, mean_q: 0.197363\n",
      " 33175/50000: episode: 4337, duration: 0.252s, episode steps:  17, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.003213, mae: 0.143987, mean_q: 0.204848\n",
      " 33180/50000: episode: 4338, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002657, mae: 0.144933, mean_q: 0.205549\n",
      " 33182/50000: episode: 4339, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005005, mae: 0.151040, mean_q: 0.213731\n",
      " 33186/50000: episode: 4340, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004136, mae: 0.147303, mean_q: 0.206134\n",
      " 33199/50000: episode: 4341, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.003673, mae: 0.151656, mean_q: 0.211499\n",
      " 33202/50000: episode: 4342, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003734, mae: 0.149245, mean_q: 0.199983\n",
      " 33211/50000: episode: 4343, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002674, mae: 0.146018, mean_q: 0.204272\n",
      " 33228/50000: episode: 4344, duration: 0.211s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002508, mae: 0.141350, mean_q: 0.201693\n",
      " 33243/50000: episode: 4345, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002679, mae: 0.142664, mean_q: 0.199433\n",
      " 33251/50000: episode: 4346, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 1.875 [1.000, 3.000],  loss: 0.002207, mae: 0.143095, mean_q: 0.200591\n",
      " 33254/50000: episode: 4347, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003050, mae: 0.139809, mean_q: 0.192080\n",
      " 33259/50000: episode: 4348, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002097, mae: 0.135900, mean_q: 0.193843\n",
      " 33279/50000: episode: 4349, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002495, mae: 0.145043, mean_q: 0.200406\n",
      " 33290/50000: episode: 4350, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.003854, mae: 0.145880, mean_q: 0.203712\n",
      " 33294/50000: episode: 4351, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.004024, mae: 0.139532, mean_q: 0.198354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33303/50000: episode: 4352, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.004159, mae: 0.147363, mean_q: 0.213113\n",
      " 33310/50000: episode: 4353, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.004378, mae: 0.151222, mean_q: 0.217880\n",
      " 33315/50000: episode: 4354, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003282, mae: 0.148436, mean_q: 0.212707\n",
      " 33328/50000: episode: 4355, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.003160, mae: 0.145951, mean_q: 0.207641\n",
      " 33334/50000: episode: 4356, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002208, mae: 0.137615, mean_q: 0.195655\n",
      " 33337/50000: episode: 4357, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002100, mae: 0.137507, mean_q: 0.198119\n",
      " 33342/50000: episode: 4358, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002196, mae: 0.141979, mean_q: 0.201649\n",
      " 33346/50000: episode: 4359, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003358, mae: 0.152915, mean_q: 0.214211\n",
      " 33349/50000: episode: 4360, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003515, mae: 0.153330, mean_q: 0.217566\n",
      " 33360/50000: episode: 4361, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002534, mae: 0.147302, mean_q: 0.205093\n",
      " 33371/50000: episode: 4362, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.002828, mae: 0.145273, mean_q: 0.204918\n",
      " 33382/50000: episode: 4363, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002767, mae: 0.139343, mean_q: 0.197079\n",
      " 33384/50000: episode: 4364, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003049, mae: 0.139257, mean_q: 0.196952\n",
      " 33389/50000: episode: 4365, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003025, mae: 0.142525, mean_q: 0.205586\n",
      " 33398/50000: episode: 4366, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003141, mae: 0.145180, mean_q: 0.210961\n",
      " 33412/50000: episode: 4367, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.002938, mae: 0.142966, mean_q: 0.201215\n",
      " 33415/50000: episode: 4368, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003030, mae: 0.141731, mean_q: 0.198427\n",
      " 33420/50000: episode: 4369, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002939, mae: 0.134837, mean_q: 0.195676\n",
      " 33424/50000: episode: 4370, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003525, mae: 0.139744, mean_q: 0.197104\n",
      " 33430/50000: episode: 4371, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003106, mae: 0.142069, mean_q: 0.204223\n",
      " 33438/50000: episode: 4372, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002505, mae: 0.140150, mean_q: 0.200360\n",
      " 33456/50000: episode: 4373, duration: 0.222s, episode steps:  18, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.003030, mae: 0.137644, mean_q: 0.196794\n",
      " 33470/50000: episode: 4374, duration: 0.170s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.929 [0.000, 3.000],  loss: 0.002841, mae: 0.144924, mean_q: 0.204974\n",
      " 33478/50000: episode: 4375, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002081, mae: 0.140048, mean_q: 0.200519\n",
      " 33481/50000: episode: 4376, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002328, mae: 0.141346, mean_q: 0.199460\n",
      " 33483/50000: episode: 4377, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002382, mae: 0.145403, mean_q: 0.205949\n",
      " 33488/50000: episode: 4378, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001771, mae: 0.142031, mean_q: 0.203617\n",
      " 33498/50000: episode: 4379, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002949, mae: 0.141926, mean_q: 0.198938\n",
      " 33503/50000: episode: 4380, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.004340, mae: 0.147707, mean_q: 0.205378\n",
      " 33514/50000: episode: 4381, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002817, mae: 0.139900, mean_q: 0.199429\n",
      " 33539/50000: episode: 4382, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.002667, mae: 0.140760, mean_q: 0.199708\n",
      " 33546/50000: episode: 4383, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002195, mae: 0.145881, mean_q: 0.208228\n",
      " 33550/50000: episode: 4384, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002803, mae: 0.145391, mean_q: 0.203912\n",
      " 33553/50000: episode: 4385, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.008288, mae: 0.154640, mean_q: 0.213573\n",
      " 33557/50000: episode: 4386, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002747, mae: 0.138755, mean_q: 0.195284\n",
      " 33561/50000: episode: 4387, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002116, mae: 0.139055, mean_q: 0.199485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33573/50000: episode: 4388, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002237, mae: 0.138109, mean_q: 0.196617\n",
      " 33576/50000: episode: 4389, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001794, mae: 0.136740, mean_q: 0.192553\n",
      " 33578/50000: episode: 4390, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004084, mae: 0.146312, mean_q: 0.202787\n",
      " 33583/50000: episode: 4391, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003211, mae: 0.142150, mean_q: 0.200000\n",
      " 33587/50000: episode: 4392, duration: 0.065s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.004275, mae: 0.144516, mean_q: 0.205151\n",
      " 33592/50000: episode: 4393, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003389, mae: 0.139868, mean_q: 0.197754\n",
      " 33598/50000: episode: 4394, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.003020, mae: 0.140035, mean_q: 0.196635\n",
      " 33601/50000: episode: 4395, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001879, mae: 0.142510, mean_q: 0.204599\n",
      " 33606/50000: episode: 4396, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002343, mae: 0.143306, mean_q: 0.203877\n",
      " 33620/50000: episode: 4397, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002986, mae: 0.140741, mean_q: 0.202835\n",
      " 33623/50000: episode: 4398, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002574, mae: 0.140453, mean_q: 0.200638\n",
      " 33639/50000: episode: 4399, duration: 0.196s, episode steps:  16, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002861, mae: 0.137866, mean_q: 0.196226\n",
      " 33650/50000: episode: 4400, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003017, mae: 0.144210, mean_q: 0.206395\n",
      " 33666/50000: episode: 4401, duration: 0.199s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002803, mae: 0.142526, mean_q: 0.206232\n",
      " 33670/50000: episode: 4402, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002520, mae: 0.132973, mean_q: 0.197462\n",
      " 33678/50000: episode: 4403, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003817, mae: 0.137380, mean_q: 0.195023\n",
      " 33691/50000: episode: 4404, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002664, mae: 0.139806, mean_q: 0.198324\n",
      " 33695/50000: episode: 4405, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002941, mae: 0.138877, mean_q: 0.193613\n",
      " 33699/50000: episode: 4406, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003427, mae: 0.145188, mean_q: 0.198883\n",
      " 33704/50000: episode: 4407, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002012, mae: 0.143116, mean_q: 0.196328\n",
      " 33710/50000: episode: 4408, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002954, mae: 0.143616, mean_q: 0.196792\n",
      " 33716/50000: episode: 4409, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.004134, mae: 0.144086, mean_q: 0.200853\n",
      " 33729/50000: episode: 4410, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003664, mae: 0.143689, mean_q: 0.204558\n",
      " 33731/50000: episode: 4411, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003668, mae: 0.144143, mean_q: 0.203087\n",
      " 33733/50000: episode: 4412, duration: 0.042s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001088, mae: 0.134573, mean_q: 0.193874\n",
      " 33738/50000: episode: 4413, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [1.000, 3.000],  loss: 0.003072, mae: 0.134759, mean_q: 0.190494\n",
      " 33741/50000: episode: 4414, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003229, mae: 0.136571, mean_q: 0.193594\n",
      " 33746/50000: episode: 4415, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002357, mae: 0.139806, mean_q: 0.198118\n",
      " 33764/50000: episode: 4416, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002135, mae: 0.139994, mean_q: 0.196021\n",
      " 33766/50000: episode: 4417, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002071, mae: 0.135973, mean_q: 0.195525\n",
      " 33771/50000: episode: 4418, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003326, mae: 0.144416, mean_q: 0.199357\n",
      " 33777/50000: episode: 4419, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002400, mae: 0.136274, mean_q: 0.194638\n",
      " 33783/50000: episode: 4420, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002156, mae: 0.134389, mean_q: 0.192722\n",
      " 33794/50000: episode: 4421, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002311, mae: 0.135970, mean_q: 0.193827\n",
      " 33800/50000: episode: 4422, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003513, mae: 0.146967, mean_q: 0.204371\n",
      " 33806/50000: episode: 4423, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001929, mae: 0.140599, mean_q: 0.200997\n",
      " 33813/50000: episode: 4424, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.429 [0.000, 1.000],  loss: 0.003130, mae: 0.141974, mean_q: 0.203975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33829/50000: episode: 4425, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003560, mae: 0.151054, mean_q: 0.211046\n",
      " 33841/50000: episode: 4426, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002533, mae: 0.140671, mean_q: 0.198245\n",
      " 33845/50000: episode: 4427, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001618, mae: 0.129525, mean_q: 0.191353\n",
      " 33847/50000: episode: 4428, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002899, mae: 0.132587, mean_q: 0.190200\n",
      " 33860/50000: episode: 4429, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002693, mae: 0.141640, mean_q: 0.202120\n",
      " 33862/50000: episode: 4430, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004628, mae: 0.146876, mean_q: 0.203160\n",
      " 33867/50000: episode: 4431, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003552, mae: 0.149081, mean_q: 0.207905\n",
      " 33874/50000: episode: 4432, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003320, mae: 0.143810, mean_q: 0.201343\n",
      " 33877/50000: episode: 4433, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003063, mae: 0.137880, mean_q: 0.196679\n",
      " 33879/50000: episode: 4434, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002277, mae: 0.140413, mean_q: 0.202549\n",
      " 33889/50000: episode: 4435, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003089, mae: 0.141532, mean_q: 0.200210\n",
      " 33896/50000: episode: 4436, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.002822, mae: 0.139297, mean_q: 0.194664\n",
      " 33899/50000: episode: 4437, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003061, mae: 0.140529, mean_q: 0.193501\n",
      " 33908/50000: episode: 4438, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002511, mae: 0.135409, mean_q: 0.190322\n",
      " 33912/50000: episode: 4439, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002603, mae: 0.138928, mean_q: 0.192199\n",
      " 33914/50000: episode: 4440, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002694, mae: 0.143866, mean_q: 0.206536\n",
      " 33916/50000: episode: 4441, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004225, mae: 0.137324, mean_q: 0.192644\n",
      " 33927/50000: episode: 4442, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003388, mae: 0.137017, mean_q: 0.193199\n",
      " 33936/50000: episode: 4443, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003479, mae: 0.133204, mean_q: 0.189676\n",
      " 33938/50000: episode: 4444, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005311, mae: 0.148687, mean_q: 0.202955\n",
      " 33940/50000: episode: 4445, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002071, mae: 0.140335, mean_q: 0.196401\n",
      " 33944/50000: episode: 4446, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002627, mae: 0.141534, mean_q: 0.196792\n",
      " 33950/50000: episode: 4447, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002204, mae: 0.139823, mean_q: 0.197387\n",
      " 33954/50000: episode: 4448, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002343, mae: 0.136977, mean_q: 0.195111\n",
      " 33957/50000: episode: 4449, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001638, mae: 0.135844, mean_q: 0.192829\n",
      " 33969/50000: episode: 4450, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002910, mae: 0.138850, mean_q: 0.197398\n",
      " 33980/50000: episode: 4451, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002662, mae: 0.138458, mean_q: 0.197898\n",
      " 33983/50000: episode: 4452, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004072, mae: 0.146005, mean_q: 0.204393\n",
      " 33989/50000: episode: 4453, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002859, mae: 0.143490, mean_q: 0.207460\n",
      " 33999/50000: episode: 4454, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003871, mae: 0.139805, mean_q: 0.200963\n",
      " 34003/50000: episode: 4455, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004928, mae: 0.143130, mean_q: 0.205216\n",
      " 34011/50000: episode: 4456, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002627, mae: 0.138928, mean_q: 0.202250\n",
      " 34014/50000: episode: 4457, duration: 0.047s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002282, mae: 0.134571, mean_q: 0.190345\n",
      " 34018/50000: episode: 4458, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003847, mae: 0.135952, mean_q: 0.195602\n",
      " 34026/50000: episode: 4459, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.003480, mae: 0.138735, mean_q: 0.200970\n",
      " 34036/50000: episode: 4460, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002282, mae: 0.139737, mean_q: 0.198344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34044/50000: episode: 4461, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003395, mae: 0.142036, mean_q: 0.197911\n",
      " 34051/50000: episode: 4462, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003084, mae: 0.138182, mean_q: 0.192813\n",
      " 34056/50000: episode: 4463, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001775, mae: 0.132005, mean_q: 0.189413\n",
      " 34066/50000: episode: 4464, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.002457, mae: 0.135588, mean_q: 0.190582\n",
      " 34068/50000: episode: 4465, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002908, mae: 0.127999, mean_q: 0.183776\n",
      " 34072/50000: episode: 4466, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002564, mae: 0.128570, mean_q: 0.187517\n",
      " 34093/50000: episode: 4467, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002661, mae: 0.140223, mean_q: 0.200629\n",
      " 34100/50000: episode: 4468, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003872, mae: 0.139551, mean_q: 0.198271\n",
      " 34111/50000: episode: 4469, duration: 0.144s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003682, mae: 0.137017, mean_q: 0.197322\n",
      " 34115/50000: episode: 4470, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002812, mae: 0.134881, mean_q: 0.193558\n",
      " 34126/50000: episode: 4471, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.727 [0.000, 2.000],  loss: 0.001715, mae: 0.136144, mean_q: 0.194553\n",
      " 34135/50000: episode: 4472, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001991, mae: 0.138939, mean_q: 0.199799\n",
      " 34137/50000: episode: 4473, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002714, mae: 0.143676, mean_q: 0.201367\n",
      " 34143/50000: episode: 4474, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002947, mae: 0.143810, mean_q: 0.201253\n",
      " 34149/50000: episode: 4475, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001956, mae: 0.138291, mean_q: 0.193587\n",
      " 34155/50000: episode: 4476, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002661, mae: 0.142343, mean_q: 0.198324\n",
      " 34173/50000: episode: 4477, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.002902, mae: 0.140135, mean_q: 0.198554\n",
      " 34176/50000: episode: 4478, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004895, mae: 0.143745, mean_q: 0.204725\n",
      " 34178/50000: episode: 4479, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001988, mae: 0.136403, mean_q: 0.201151\n",
      " 34187/50000: episode: 4480, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002246, mae: 0.136006, mean_q: 0.195248\n",
      " 34192/50000: episode: 4481, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004717, mae: 0.146248, mean_q: 0.207699\n",
      " 34195/50000: episode: 4482, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.004466, mae: 0.142196, mean_q: 0.202178\n",
      " 34199/50000: episode: 4483, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002057, mae: 0.138833, mean_q: 0.199975\n",
      " 34202/50000: episode: 4484, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001298, mae: 0.133802, mean_q: 0.193639\n",
      " 34207/50000: episode: 4485, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003082, mae: 0.139044, mean_q: 0.198133\n",
      " 34211/50000: episode: 4486, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003495, mae: 0.140429, mean_q: 0.197953\n",
      " 34219/50000: episode: 4487, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003441, mae: 0.140179, mean_q: 0.203882\n",
      " 34227/50000: episode: 4488, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002565, mae: 0.142262, mean_q: 0.207085\n",
      " 34236/50000: episode: 4489, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002976, mae: 0.138787, mean_q: 0.201409\n",
      " 34248/50000: episode: 4490, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003635, mae: 0.143723, mean_q: 0.204752\n",
      " 34254/50000: episode: 4491, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001612, mae: 0.130705, mean_q: 0.187291\n",
      " 34260/50000: episode: 4492, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  1.000, mean reward:  0.167 [ 0.000,  1.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003256, mae: 0.138324, mean_q: 0.195330\n",
      " 34268/50000: episode: 4493, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002455, mae: 0.139755, mean_q: 0.198995\n",
      " 34271/50000: episode: 4494, duration: 0.047s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002452, mae: 0.142774, mean_q: 0.206807\n",
      " 34283/50000: episode: 4495, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002494, mae: 0.141678, mean_q: 0.200007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34293/50000: episode: 4496, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002358, mae: 0.139193, mean_q: 0.196412\n",
      " 34308/50000: episode: 4497, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002720, mae: 0.138223, mean_q: 0.197920\n",
      " 34328/50000: episode: 4498, duration: 0.296s, episode steps:  20, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002418, mae: 0.139704, mean_q: 0.200619\n",
      " 34334/50000: episode: 4499, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002138, mae: 0.133697, mean_q: 0.192787\n",
      " 34342/50000: episode: 4500, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002508, mae: 0.137698, mean_q: 0.198758\n",
      " 34351/50000: episode: 4501, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003389, mae: 0.144260, mean_q: 0.205961\n",
      " 34357/50000: episode: 4502, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.003235, mae: 0.140732, mean_q: 0.200676\n",
      " 34359/50000: episode: 4503, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001831, mae: 0.135406, mean_q: 0.194108\n",
      " 34372/50000: episode: 4504, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002200, mae: 0.141553, mean_q: 0.201488\n",
      " 34385/50000: episode: 4505, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.077 [0.000, 3.000],  loss: 0.002387, mae: 0.137621, mean_q: 0.196465\n",
      " 34395/50000: episode: 4506, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002941, mae: 0.147056, mean_q: 0.207129\n",
      " 34401/50000: episode: 4507, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003721, mae: 0.154912, mean_q: 0.214017\n",
      " 34409/50000: episode: 4508, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002450, mae: 0.148413, mean_q: 0.210057\n",
      " 34412/50000: episode: 4509, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003233, mae: 0.142094, mean_q: 0.200585\n",
      " 34417/50000: episode: 4510, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 1.000],  loss: 0.004733, mae: 0.141070, mean_q: 0.199472\n",
      " 34424/50000: episode: 4511, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002690, mae: 0.134512, mean_q: 0.192997\n",
      " 34429/50000: episode: 4512, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003581, mae: 0.145448, mean_q: 0.204103\n",
      " 34431/50000: episode: 4513, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004586, mae: 0.150466, mean_q: 0.204224\n",
      " 34438/50000: episode: 4514, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.004194, mae: 0.152816, mean_q: 0.210663\n",
      " 34451/50000: episode: 4515, duration: 0.166s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.002934, mae: 0.144122, mean_q: 0.200976\n",
      " 34459/50000: episode: 4516, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002166, mae: 0.135590, mean_q: 0.192365\n",
      " 34465/50000: episode: 4517, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002356, mae: 0.142090, mean_q: 0.199122\n",
      " 34467/50000: episode: 4518, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001613, mae: 0.137564, mean_q: 0.194194\n",
      " 34476/50000: episode: 4519, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003740, mae: 0.141370, mean_q: 0.199376\n",
      " 34483/50000: episode: 4520, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002247, mae: 0.136434, mean_q: 0.197143\n",
      " 34508/50000: episode: 4521, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward:  1.000, mean reward:  0.040 [ 0.000,  1.000], mean action: 1.280 [0.000, 3.000],  loss: 0.002979, mae: 0.145385, mean_q: 0.207089\n",
      " 34510/50000: episode: 4522, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002426, mae: 0.145108, mean_q: 0.206798\n",
      " 34520/50000: episode: 4523, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002208, mae: 0.134119, mean_q: 0.191210\n",
      " 34527/50000: episode: 4524, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.286 [1.000, 3.000],  loss: 0.004108, mae: 0.149202, mean_q: 0.207443\n",
      " 34533/50000: episode: 4525, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003190, mae: 0.150302, mean_q: 0.214218\n",
      " 34539/50000: episode: 4526, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002844, mae: 0.144638, mean_q: 0.203394\n",
      " 34545/50000: episode: 4527, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002533, mae: 0.145655, mean_q: 0.205962\n",
      " 34547/50000: episode: 4528, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004618, mae: 0.142836, mean_q: 0.201487\n",
      " 34560/50000: episode: 4529, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002531, mae: 0.136968, mean_q: 0.194646\n",
      " 34566/50000: episode: 4530, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003008, mae: 0.142024, mean_q: 0.198894\n",
      " 34571/50000: episode: 4531, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002780, mae: 0.142602, mean_q: 0.200162\n",
      " 34575/50000: episode: 4532, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002861, mae: 0.141818, mean_q: 0.200205\n",
      " 34579/50000: episode: 4533, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004636, mae: 0.147427, mean_q: 0.203257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34587/50000: episode: 4534, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003869, mae: 0.142676, mean_q: 0.202009\n",
      " 34591/50000: episode: 4535, duration: 0.076s, episode steps:   4, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.003482, mae: 0.137434, mean_q: 0.201560\n",
      " 34610/50000: episode: 4536, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.737 [0.000, 3.000],  loss: 0.003086, mae: 0.144952, mean_q: 0.207500\n",
      " 34612/50000: episode: 4537, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001955, mae: 0.134739, mean_q: 0.189086\n",
      " 34614/50000: episode: 4538, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004146, mae: 0.143070, mean_q: 0.198044\n",
      " 34619/50000: episode: 4539, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002760, mae: 0.133731, mean_q: 0.192880\n",
      " 34629/50000: episode: 4540, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002758, mae: 0.137290, mean_q: 0.195865\n",
      " 34636/50000: episode: 4541, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001965, mae: 0.140430, mean_q: 0.197754\n",
      " 34642/50000: episode: 4542, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002788, mae: 0.147299, mean_q: 0.205750\n",
      " 34652/50000: episode: 4543, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003068, mae: 0.144343, mean_q: 0.204500\n",
      " 34658/50000: episode: 4544, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.002527, mae: 0.143205, mean_q: 0.207531\n",
      " 34662/50000: episode: 4545, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.250 [0.000, 1.000],  loss: 0.002576, mae: 0.147804, mean_q: 0.209089\n",
      " 34667/50000: episode: 4546, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004245, mae: 0.154014, mean_q: 0.212420\n",
      " 34669/50000: episode: 4547, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002812, mae: 0.151535, mean_q: 0.208494\n",
      " 34671/50000: episode: 4548, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002208, mae: 0.145443, mean_q: 0.199355\n",
      " 34695/50000: episode: 4549, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002959, mae: 0.141902, mean_q: 0.198333\n",
      " 34709/50000: episode: 4550, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002772, mae: 0.141457, mean_q: 0.200087\n",
      " 34716/50000: episode: 4551, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002035, mae: 0.139740, mean_q: 0.199596\n",
      " 34720/50000: episode: 4552, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003256, mae: 0.141858, mean_q: 0.197834\n",
      " 34743/50000: episode: 4553, duration: 0.280s, episode steps:  23, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.174 [0.000, 3.000],  loss: 0.003031, mae: 0.145925, mean_q: 0.205350\n",
      " 34751/50000: episode: 4554, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002398, mae: 0.139342, mean_q: 0.199390\n",
      " 34758/50000: episode: 4555, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002757, mae: 0.139804, mean_q: 0.204124\n",
      " 34769/50000: episode: 4556, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.727 [0.000, 3.000],  loss: 0.002839, mae: 0.143057, mean_q: 0.207739\n",
      " 34779/50000: episode: 4557, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002282, mae: 0.144258, mean_q: 0.205013\n",
      " 34781/50000: episode: 4558, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003383, mae: 0.143878, mean_q: 0.199449\n",
      " 34784/50000: episode: 4559, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001826, mae: 0.134119, mean_q: 0.193727\n",
      " 34797/50000: episode: 4560, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003347, mae: 0.136141, mean_q: 0.195015\n",
      " 34808/50000: episode: 4561, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002693, mae: 0.139020, mean_q: 0.200634\n",
      " 34824/50000: episode: 4562, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.938 [0.000, 3.000],  loss: 0.003171, mae: 0.147985, mean_q: 0.206555\n",
      " 34834/50000: episode: 4563, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002741, mae: 0.143827, mean_q: 0.202335\n",
      " 34846/50000: episode: 4564, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003437, mae: 0.143573, mean_q: 0.201315\n",
      " 34850/50000: episode: 4565, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002896, mae: 0.138177, mean_q: 0.195705\n",
      " 34859/50000: episode: 4566, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002688, mae: 0.142871, mean_q: 0.203704\n",
      " 34877/50000: episode: 4567, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002255, mae: 0.138504, mean_q: 0.201242\n",
      " 34900/50000: episode: 4568, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002473, mae: 0.138054, mean_q: 0.195418\n",
      " 34902/50000: episode: 4569, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001331, mae: 0.138913, mean_q: 0.199604\n",
      " 34912/50000: episode: 4570, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.003467, mae: 0.149366, mean_q: 0.210057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34919/50000: episode: 4571, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002912, mae: 0.148025, mean_q: 0.214321\n",
      " 34926/50000: episode: 4572, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.003193, mae: 0.147687, mean_q: 0.212678\n",
      " 34930/50000: episode: 4573, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004208, mae: 0.146733, mean_q: 0.207073\n",
      " 34932/50000: episode: 4574, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002576, mae: 0.150205, mean_q: 0.211116\n",
      " 34938/50000: episode: 4575, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.002903, mae: 0.142739, mean_q: 0.202529\n",
      " 34942/50000: episode: 4576, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002023, mae: 0.145513, mean_q: 0.207643\n",
      " 34951/50000: episode: 4577, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003505, mae: 0.141542, mean_q: 0.199590\n",
      " 34956/50000: episode: 4578, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003187, mae: 0.139707, mean_q: 0.201074\n",
      " 34958/50000: episode: 4579, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002962, mae: 0.139349, mean_q: 0.197518\n",
      " 34960/50000: episode: 4580, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.006249, mae: 0.153900, mean_q: 0.214356\n",
      " 34964/50000: episode: 4581, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002236, mae: 0.139682, mean_q: 0.202000\n",
      " 34975/50000: episode: 4582, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002983, mae: 0.141250, mean_q: 0.202589\n",
      " 34987/50000: episode: 4583, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002664, mae: 0.145561, mean_q: 0.207201\n",
      " 34989/50000: episode: 4584, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003770, mae: 0.147446, mean_q: 0.206800\n",
      " 35004/50000: episode: 4585, duration: 0.191s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002893, mae: 0.145667, mean_q: 0.206757\n",
      " 35017/50000: episode: 4586, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002947, mae: 0.145658, mean_q: 0.207088\n",
      " 35031/50000: episode: 4587, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003762, mae: 0.145876, mean_q: 0.204167\n",
      " 35039/50000: episode: 4588, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.002836, mae: 0.136472, mean_q: 0.195009\n",
      " 35054/50000: episode: 4589, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002829, mae: 0.137760, mean_q: 0.201382\n",
      " 35056/50000: episode: 4590, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002214, mae: 0.145583, mean_q: 0.203273\n",
      " 35067/50000: episode: 4591, duration: 0.144s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [1.000, 3.000],  loss: 0.002537, mae: 0.147984, mean_q: 0.209726\n",
      " 35078/50000: episode: 4592, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002595, mae: 0.144229, mean_q: 0.205110\n",
      " 35087/50000: episode: 4593, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003098, mae: 0.143239, mean_q: 0.202878\n",
      " 35089/50000: episode: 4594, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003495, mae: 0.147772, mean_q: 0.208902\n",
      " 35099/50000: episode: 4595, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.003703, mae: 0.145369, mean_q: 0.203055\n",
      " 35104/50000: episode: 4596, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001722, mae: 0.139763, mean_q: 0.204180\n",
      " 35106/50000: episode: 4597, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003965, mae: 0.144924, mean_q: 0.210052\n",
      " 35114/50000: episode: 4598, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003532, mae: 0.145542, mean_q: 0.209366\n",
      " 35117/50000: episode: 4599, duration: 0.050s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003324, mae: 0.143575, mean_q: 0.203286\n",
      " 35121/50000: episode: 4600, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002546, mae: 0.145510, mean_q: 0.202348\n",
      " 35126/50000: episode: 4601, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003435, mae: 0.148526, mean_q: 0.210781\n",
      " 35128/50000: episode: 4602, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002347, mae: 0.144161, mean_q: 0.203534\n",
      " 35131/50000: episode: 4603, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002991, mae: 0.142789, mean_q: 0.204177\n",
      " 35137/50000: episode: 4604, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002559, mae: 0.142247, mean_q: 0.204399\n",
      " 35154/50000: episode: 4605, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [0.000, 3.000],  loss: 0.001978, mae: 0.147755, mean_q: 0.215894\n",
      " 35166/50000: episode: 4606, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.917 [0.000, 3.000],  loss: 0.003836, mae: 0.149836, mean_q: 0.214569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35175/50000: episode: 4607, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.002723, mae: 0.151360, mean_q: 0.214485\n",
      " 35182/50000: episode: 4608, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003744, mae: 0.154138, mean_q: 0.215604\n",
      " 35184/50000: episode: 4609, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002344, mae: 0.144059, mean_q: 0.201099\n",
      " 35186/50000: episode: 4610, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002457, mae: 0.136491, mean_q: 0.195386\n",
      " 35201/50000: episode: 4611, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.004231, mae: 0.152779, mean_q: 0.214935\n",
      " 35204/50000: episode: 4612, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002033, mae: 0.153773, mean_q: 0.218117\n",
      " 35209/50000: episode: 4613, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002394, mae: 0.152105, mean_q: 0.213866\n",
      " 35223/50000: episode: 4614, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002628, mae: 0.142263, mean_q: 0.203871\n",
      " 35231/50000: episode: 4615, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002989, mae: 0.146047, mean_q: 0.209345\n",
      " 35250/50000: episode: 4616, duration: 0.233s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.002744, mae: 0.145101, mean_q: 0.208077\n",
      " 35258/50000: episode: 4617, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003574, mae: 0.148710, mean_q: 0.209901\n",
      " 35275/50000: episode: 4618, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.002573, mae: 0.149568, mean_q: 0.212802\n",
      " 35284/50000: episode: 4619, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003124, mae: 0.142893, mean_q: 0.204419\n",
      " 35289/50000: episode: 4620, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002697, mae: 0.144250, mean_q: 0.204264\n",
      " 35293/50000: episode: 4621, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004210, mae: 0.144805, mean_q: 0.205477\n",
      " 35299/50000: episode: 4622, duration: 0.081s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002283, mae: 0.144508, mean_q: 0.206310\n",
      " 35311/50000: episode: 4623, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003409, mae: 0.148382, mean_q: 0.214174\n",
      " 35320/50000: episode: 4624, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003228, mae: 0.146377, mean_q: 0.206164\n",
      " 35323/50000: episode: 4625, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002071, mae: 0.134329, mean_q: 0.194391\n",
      " 35332/50000: episode: 4626, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003446, mae: 0.141336, mean_q: 0.200427\n",
      " 35362/50000: episode: 4627, duration: 0.365s, episode steps:  30, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002775, mae: 0.147979, mean_q: 0.208711\n",
      " 35366/50000: episode: 4628, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002381, mae: 0.146844, mean_q: 0.205093\n",
      " 35376/50000: episode: 4629, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.004145, mae: 0.154577, mean_q: 0.214590\n",
      " 35379/50000: episode: 4630, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003641, mae: 0.152720, mean_q: 0.216243\n",
      " 35382/50000: episode: 4631, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.004982, mae: 0.156813, mean_q: 0.221207\n",
      " 35399/50000: episode: 4632, duration: 0.211s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002953, mae: 0.144524, mean_q: 0.206411\n",
      " 35403/50000: episode: 4633, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002766, mae: 0.144748, mean_q: 0.207705\n",
      " 35407/50000: episode: 4634, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002599, mae: 0.152991, mean_q: 0.213764\n",
      " 35416/50000: episode: 4635, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.002279, mae: 0.150057, mean_q: 0.211789\n",
      " 35445/50000: episode: 4636, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.759 [0.000, 3.000],  loss: 0.003566, mae: 0.150977, mean_q: 0.214235\n",
      " 35447/50000: episode: 4637, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003822, mae: 0.160660, mean_q: 0.221508\n",
      " 35451/50000: episode: 4638, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003219, mae: 0.147245, mean_q: 0.208671\n",
      " 35486/50000: episode: 4639, duration: 0.423s, episode steps:  35, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002984, mae: 0.151882, mean_q: 0.215358\n",
      " 35490/50000: episode: 4640, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002707, mae: 0.149103, mean_q: 0.212276\n",
      " 35502/50000: episode: 4641, duration: 0.151s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002236, mae: 0.142373, mean_q: 0.209180\n",
      " 35511/50000: episode: 4642, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003884, mae: 0.148056, mean_q: 0.211879\n",
      " 35516/50000: episode: 4643, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003851, mae: 0.145698, mean_q: 0.212035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35528/50000: episode: 4644, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 0.833 [0.000, 2.000],  loss: 0.002637, mae: 0.152262, mean_q: 0.217654\n",
      " 35531/50000: episode: 4645, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003722, mae: 0.153611, mean_q: 0.212269\n",
      " 35543/50000: episode: 4646, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003181, mae: 0.152114, mean_q: 0.214623\n",
      " 35550/50000: episode: 4647, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004682, mae: 0.155012, mean_q: 0.215964\n",
      " 35559/50000: episode: 4648, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003063, mae: 0.151788, mean_q: 0.215646\n",
      " 35568/50000: episode: 4649, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002930, mae: 0.156862, mean_q: 0.218790\n",
      " 35572/50000: episode: 4650, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002825, mae: 0.142900, mean_q: 0.205891\n",
      " 35574/50000: episode: 4651, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003567, mae: 0.145975, mean_q: 0.203660\n",
      " 35580/50000: episode: 4652, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.003771, mae: 0.147876, mean_q: 0.210785\n",
      " 35589/50000: episode: 4653, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004200, mae: 0.156776, mean_q: 0.218597\n",
      " 35604/50000: episode: 4654, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002562, mae: 0.154561, mean_q: 0.216946\n",
      " 35612/50000: episode: 4655, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003353, mae: 0.152975, mean_q: 0.211935\n",
      " 35625/50000: episode: 4656, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.002920, mae: 0.148040, mean_q: 0.210529\n",
      " 35629/50000: episode: 4657, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002365, mae: 0.148351, mean_q: 0.216409\n",
      " 35639/50000: episode: 4658, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002770, mae: 0.150097, mean_q: 0.216647\n",
      " 35644/50000: episode: 4659, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003041, mae: 0.153202, mean_q: 0.216657\n",
      " 35659/50000: episode: 4660, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002288, mae: 0.148236, mean_q: 0.208262\n",
      " 35668/50000: episode: 4661, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003630, mae: 0.157006, mean_q: 0.219900\n",
      " 35680/50000: episode: 4662, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.003694, mae: 0.154399, mean_q: 0.216430\n",
      " 35684/50000: episode: 4663, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.004838, mae: 0.155746, mean_q: 0.212558\n",
      " 35697/50000: episode: 4664, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002637, mae: 0.143043, mean_q: 0.202821\n",
      " 35703/50000: episode: 4665, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002681, mae: 0.150061, mean_q: 0.212163\n",
      " 35710/50000: episode: 4666, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.003564, mae: 0.156849, mean_q: 0.218725\n",
      " 35723/50000: episode: 4667, duration: 0.168s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002775, mae: 0.151212, mean_q: 0.213726\n",
      " 35730/50000: episode: 4668, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002790, mae: 0.148733, mean_q: 0.212902\n",
      " 35736/50000: episode: 4669, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002882, mae: 0.145756, mean_q: 0.203586\n",
      " 35753/50000: episode: 4670, duration: 0.259s, episode steps:  17, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002549, mae: 0.149317, mean_q: 0.210652\n",
      " 35755/50000: episode: 4671, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004902, mae: 0.164199, mean_q: 0.225525\n",
      " 35762/50000: episode: 4672, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003067, mae: 0.158992, mean_q: 0.223802\n",
      " 35774/50000: episode: 4673, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002559, mae: 0.149025, mean_q: 0.214101\n",
      " 35776/50000: episode: 4674, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001962, mae: 0.149355, mean_q: 0.210402\n",
      " 35786/50000: episode: 4675, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.003173, mae: 0.156150, mean_q: 0.222624\n",
      " 35802/50000: episode: 4676, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002807, mae: 0.154811, mean_q: 0.221602\n",
      " 35808/50000: episode: 4677, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002356, mae: 0.149656, mean_q: 0.217755\n",
      " 35821/50000: episode: 4678, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003414, mae: 0.150083, mean_q: 0.215116\n",
      " 35834/50000: episode: 4679, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.002817, mae: 0.152913, mean_q: 0.215972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35845/50000: episode: 4680, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003960, mae: 0.159809, mean_q: 0.224516\n",
      " 35850/50000: episode: 4681, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003881, mae: 0.151382, mean_q: 0.214601\n",
      " 35854/50000: episode: 4682, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002792, mae: 0.150866, mean_q: 0.214666\n",
      " 35856/50000: episode: 4683, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003796, mae: 0.144164, mean_q: 0.202037\n",
      " 35866/50000: episode: 4684, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002911, mae: 0.152229, mean_q: 0.214175\n",
      " 35879/50000: episode: 4685, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.003450, mae: 0.154547, mean_q: 0.217094\n",
      " 35887/50000: episode: 4686, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002291, mae: 0.148306, mean_q: 0.215910\n",
      " 35891/50000: episode: 4687, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001976, mae: 0.150148, mean_q: 0.215928\n",
      " 35894/50000: episode: 4688, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.004294, mae: 0.156469, mean_q: 0.216909\n",
      " 35905/50000: episode: 4689, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002388, mae: 0.153408, mean_q: 0.219318\n",
      " 35910/50000: episode: 4690, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003663, mae: 0.153178, mean_q: 0.216965\n",
      " 35917/50000: episode: 4691, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002709, mae: 0.152551, mean_q: 0.218574\n",
      " 35919/50000: episode: 4692, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003409, mae: 0.161329, mean_q: 0.224600\n",
      " 35942/50000: episode: 4693, duration: 0.282s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.261 [0.000, 3.000],  loss: 0.003629, mae: 0.157924, mean_q: 0.223015\n",
      " 35958/50000: episode: 4694, duration: 0.194s, episode steps:  16, steps per second:  82, episode reward:  1.000, mean reward:  0.062 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003087, mae: 0.151737, mean_q: 0.215744\n",
      " 35966/50000: episode: 4695, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002667, mae: 0.158054, mean_q: 0.223095\n",
      " 35968/50000: episode: 4696, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004365, mae: 0.166162, mean_q: 0.230944\n",
      " 35972/50000: episode: 4697, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002970, mae: 0.159085, mean_q: 0.225425\n",
      " 35986/50000: episode: 4698, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.004049, mae: 0.154301, mean_q: 0.218789\n",
      " 35999/50000: episode: 4699, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003796, mae: 0.149189, mean_q: 0.213405\n",
      " 36008/50000: episode: 4700, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002689, mae: 0.150583, mean_q: 0.217780\n",
      " 36024/50000: episode: 4701, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003018, mae: 0.150550, mean_q: 0.216996\n",
      " 36028/50000: episode: 4702, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.003282, mae: 0.160258, mean_q: 0.228591\n",
      " 36030/50000: episode: 4703, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002601, mae: 0.155915, mean_q: 0.221989\n",
      " 36044/50000: episode: 4704, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003214, mae: 0.157538, mean_q: 0.224777\n",
      " 36060/50000: episode: 4705, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.003759, mae: 0.153860, mean_q: 0.218260\n",
      " 36067/50000: episode: 4706, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.002796, mae: 0.149884, mean_q: 0.212440\n",
      " 36073/50000: episode: 4707, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 3.000],  loss: 0.002580, mae: 0.146610, mean_q: 0.210163\n",
      " 36095/50000: episode: 4708, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.227 [0.000, 3.000],  loss: 0.002748, mae: 0.151607, mean_q: 0.214198\n",
      " 36101/50000: episode: 4709, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004570, mae: 0.155483, mean_q: 0.215529\n",
      " 36119/50000: episode: 4710, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [1.000, 3.000],  loss: 0.003173, mae: 0.148113, mean_q: 0.211206\n",
      " 36140/50000: episode: 4711, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002719, mae: 0.154833, mean_q: 0.216844\n",
      " 36142/50000: episode: 4712, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003414, mae: 0.155112, mean_q: 0.213190\n",
      " 36149/50000: episode: 4713, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.002700, mae: 0.153973, mean_q: 0.215785\n",
      " 36159/50000: episode: 4714, duration: 0.131s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002869, mae: 0.144097, mean_q: 0.205144\n",
      " 36165/50000: episode: 4715, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.004189, mae: 0.157665, mean_q: 0.221013\n",
      " 36173/50000: episode: 4716, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003307, mae: 0.164337, mean_q: 0.233215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36175/50000: episode: 4717, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.005939, mae: 0.166967, mean_q: 0.226628\n",
      " 36183/50000: episode: 4718, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004054, mae: 0.158959, mean_q: 0.224818\n",
      " 36191/50000: episode: 4719, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002872, mae: 0.152812, mean_q: 0.221088\n",
      " 36194/50000: episode: 4720, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003606, mae: 0.159828, mean_q: 0.227451\n",
      " 36198/50000: episode: 4721, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002414, mae: 0.155063, mean_q: 0.217812\n",
      " 36211/50000: episode: 4722, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.003040, mae: 0.154245, mean_q: 0.217616\n",
      " 36218/50000: episode: 4723, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001884, mae: 0.144304, mean_q: 0.205726\n",
      " 36220/50000: episode: 4724, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003929, mae: 0.146737, mean_q: 0.209055\n",
      " 36222/50000: episode: 4725, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004181, mae: 0.154179, mean_q: 0.217821\n",
      " 36225/50000: episode: 4726, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002872, mae: 0.148612, mean_q: 0.212379\n",
      " 36243/50000: episode: 4727, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003274, mae: 0.149447, mean_q: 0.215284\n",
      " 36252/50000: episode: 4728, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.002990, mae: 0.145927, mean_q: 0.211751\n",
      " 36256/50000: episode: 4729, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003891, mae: 0.149256, mean_q: 0.211079\n",
      " 36258/50000: episode: 4730, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005990, mae: 0.156460, mean_q: 0.213428\n",
      " 36266/50000: episode: 4731, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001963, mae: 0.149643, mean_q: 0.213618\n",
      " 36269/50000: episode: 4732, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.003743, mae: 0.153274, mean_q: 0.216116\n",
      " 36287/50000: episode: 4733, duration: 0.224s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.003025, mae: 0.150636, mean_q: 0.211954\n",
      " 36291/50000: episode: 4734, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003656, mae: 0.144254, mean_q: 0.211545\n",
      " 36293/50000: episode: 4735, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002156, mae: 0.141755, mean_q: 0.204046\n",
      " 36300/50000: episode: 4736, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 2.000],  loss: 0.002358, mae: 0.146207, mean_q: 0.210084\n",
      " 36303/50000: episode: 4737, duration: 0.051s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003330, mae: 0.159406, mean_q: 0.223513\n",
      " 36307/50000: episode: 4738, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002665, mae: 0.153747, mean_q: 0.218496\n",
      " 36323/50000: episode: 4739, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.002769, mae: 0.157868, mean_q: 0.221844\n",
      " 36327/50000: episode: 4740, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002965, mae: 0.153740, mean_q: 0.221361\n",
      " 36329/50000: episode: 4741, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002190, mae: 0.147651, mean_q: 0.210954\n",
      " 36337/50000: episode: 4742, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002136, mae: 0.152893, mean_q: 0.216873\n",
      " 36342/50000: episode: 4743, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002824, mae: 0.151190, mean_q: 0.217598\n",
      " 36344/50000: episode: 4744, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002439, mae: 0.155615, mean_q: 0.218003\n",
      " 36366/50000: episode: 4745, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003074, mae: 0.150428, mean_q: 0.216328\n",
      " 36369/50000: episode: 4746, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003285, mae: 0.152549, mean_q: 0.212234\n",
      " 36371/50000: episode: 4747, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002744, mae: 0.149379, mean_q: 0.209325\n",
      " 36375/50000: episode: 4748, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.003060, mae: 0.152617, mean_q: 0.214834\n",
      " 36390/50000: episode: 4749, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003203, mae: 0.152265, mean_q: 0.215229\n",
      " 36402/50000: episode: 4750, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002714, mae: 0.151781, mean_q: 0.212216\n",
      " 36416/50000: episode: 4751, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.214 [0.000, 3.000],  loss: 0.003381, mae: 0.150436, mean_q: 0.209049\n",
      " 36424/50000: episode: 4752, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003233, mae: 0.150954, mean_q: 0.214840\n",
      " 36426/50000: episode: 4753, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003090, mae: 0.148552, mean_q: 0.209828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36432/50000: episode: 4754, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.003720, mae: 0.152050, mean_q: 0.217590\n",
      " 36435/50000: episode: 4755, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003890, mae: 0.160153, mean_q: 0.228820\n",
      " 36447/50000: episode: 4756, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.003082, mae: 0.147879, mean_q: 0.211900\n",
      " 36451/50000: episode: 4757, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.003945, mae: 0.156675, mean_q: 0.219945\n",
      " 36454/50000: episode: 4758, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002572, mae: 0.157256, mean_q: 0.223809\n",
      " 36464/50000: episode: 4759, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002214, mae: 0.146217, mean_q: 0.206835\n",
      " 36471/50000: episode: 4760, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [1.000, 3.000],  loss: 0.003863, mae: 0.146084, mean_q: 0.205167\n",
      " 36474/50000: episode: 4761, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.006053, mae: 0.152610, mean_q: 0.219437\n",
      " 36477/50000: episode: 4762, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002785, mae: 0.140845, mean_q: 0.203610\n",
      " 36487/50000: episode: 4763, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.002846, mae: 0.149207, mean_q: 0.211247\n",
      " 36500/50000: episode: 4764, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.003213, mae: 0.152769, mean_q: 0.214425\n",
      " 36503/50000: episode: 4765, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002617, mae: 0.144549, mean_q: 0.202349\n",
      " 36508/50000: episode: 4766, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 1.000],  loss: 0.003371, mae: 0.151266, mean_q: 0.212924\n",
      " 36517/50000: episode: 4767, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002680, mae: 0.152775, mean_q: 0.218925\n",
      " 36520/50000: episode: 4768, duration: 0.051s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002921, mae: 0.150524, mean_q: 0.215090\n",
      " 36524/50000: episode: 4769, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.002558, mae: 0.151747, mean_q: 0.215864\n",
      " 36527/50000: episode: 4770, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003096, mae: 0.155494, mean_q: 0.217392\n",
      " 36531/50000: episode: 4771, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002514, mae: 0.149014, mean_q: 0.210700\n",
      " 36542/50000: episode: 4772, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003212, mae: 0.151986, mean_q: 0.211233\n",
      " 36545/50000: episode: 4773, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002349, mae: 0.144778, mean_q: 0.211214\n",
      " 36548/50000: episode: 4774, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003454, mae: 0.157442, mean_q: 0.223791\n",
      " 36559/50000: episode: 4775, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [1.000, 3.000],  loss: 0.002972, mae: 0.151990, mean_q: 0.216166\n",
      " 36566/50000: episode: 4776, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002975, mae: 0.143360, mean_q: 0.206153\n",
      " 36569/50000: episode: 4777, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.004041, mae: 0.149112, mean_q: 0.207646\n",
      " 36573/50000: episode: 4778, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003681, mae: 0.148138, mean_q: 0.205321\n",
      " 36588/50000: episode: 4779, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.003523, mae: 0.145598, mean_q: 0.206298\n",
      " 36591/50000: episode: 4780, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002407, mae: 0.148675, mean_q: 0.207238\n",
      " 36597/50000: episode: 4781, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002968, mae: 0.152738, mean_q: 0.212712\n",
      " 36604/50000: episode: 4782, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002291, mae: 0.150652, mean_q: 0.214981\n",
      " 36606/50000: episode: 4783, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001736, mae: 0.150448, mean_q: 0.215449\n",
      " 36609/50000: episode: 4784, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001584, mae: 0.146692, mean_q: 0.210084\n",
      " 36627/50000: episode: 4785, duration: 0.240s, episode steps:  18, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002668, mae: 0.145601, mean_q: 0.208004\n",
      " 36629/50000: episode: 4786, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003495, mae: 0.151725, mean_q: 0.213909\n",
      " 36638/50000: episode: 4787, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002212, mae: 0.146828, mean_q: 0.208927\n",
      " 36645/50000: episode: 4788, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.003167, mae: 0.151884, mean_q: 0.215408\n",
      " 36648/50000: episode: 4789, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002605, mae: 0.149225, mean_q: 0.214379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36667/50000: episode: 4790, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002563, mae: 0.146315, mean_q: 0.213192\n",
      " 36671/50000: episode: 4791, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003256, mae: 0.144381, mean_q: 0.213255\n",
      " 36688/50000: episode: 4792, duration: 0.214s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.003507, mae: 0.150602, mean_q: 0.214853\n",
      " 36705/50000: episode: 4793, duration: 0.227s, episode steps:  17, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.003273, mae: 0.152398, mean_q: 0.212779\n",
      " 36727/50000: episode: 4794, duration: 0.276s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002824, mae: 0.151772, mean_q: 0.213131\n",
      " 36735/50000: episode: 4795, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003080, mae: 0.153767, mean_q: 0.214890\n",
      " 36744/50000: episode: 4796, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002692, mae: 0.145132, mean_q: 0.207309\n",
      " 36747/50000: episode: 4797, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003129, mae: 0.148027, mean_q: 0.208219\n",
      " 36758/50000: episode: 4798, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002654, mae: 0.147407, mean_q: 0.206881\n",
      " 36761/50000: episode: 4799, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003645, mae: 0.140760, mean_q: 0.203063\n",
      " 36772/50000: episode: 4800, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.003535, mae: 0.141139, mean_q: 0.206795\n",
      " 36774/50000: episode: 4801, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004102, mae: 0.152877, mean_q: 0.215873\n",
      " 36787/50000: episode: 4802, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003735, mae: 0.151597, mean_q: 0.211701\n",
      " 36797/50000: episode: 4803, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.003218, mae: 0.145700, mean_q: 0.206445\n",
      " 36802/50000: episode: 4804, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.001944, mae: 0.141612, mean_q: 0.203153\n",
      " 36805/50000: episode: 4805, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001727, mae: 0.149399, mean_q: 0.213065\n",
      " 36808/50000: episode: 4806, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002157, mae: 0.143902, mean_q: 0.206874\n",
      " 36820/50000: episode: 4807, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003051, mae: 0.147037, mean_q: 0.210047\n",
      " 36837/50000: episode: 4808, duration: 0.211s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002609, mae: 0.152542, mean_q: 0.213879\n",
      " 36841/50000: episode: 4809, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002663, mae: 0.144605, mean_q: 0.211413\n",
      " 36856/50000: episode: 4810, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.003084, mae: 0.147780, mean_q: 0.214704\n",
      " 36881/50000: episode: 4811, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002969, mae: 0.150423, mean_q: 0.215718\n",
      " 36883/50000: episode: 4812, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003592, mae: 0.150562, mean_q: 0.209689\n",
      " 36885/50000: episode: 4813, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004585, mae: 0.149374, mean_q: 0.209601\n",
      " 36892/50000: episode: 4814, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002543, mae: 0.149676, mean_q: 0.212281\n",
      " 36901/50000: episode: 4815, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002666, mae: 0.152406, mean_q: 0.217762\n",
      " 36910/50000: episode: 4816, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002424, mae: 0.152417, mean_q: 0.215156\n",
      " 36913/50000: episode: 4817, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003752, mae: 0.168598, mean_q: 0.235167\n",
      " 36917/50000: episode: 4818, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002702, mae: 0.149084, mean_q: 0.211447\n",
      " 36925/50000: episode: 4819, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003701, mae: 0.154946, mean_q: 0.216234\n",
      " 36929/50000: episode: 4820, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004031, mae: 0.150875, mean_q: 0.210838\n",
      " 36931/50000: episode: 4821, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005048, mae: 0.151822, mean_q: 0.219622\n",
      " 36937/50000: episode: 4822, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.003021, mae: 0.147435, mean_q: 0.212846\n",
      " 36939/50000: episode: 4823, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002127, mae: 0.151265, mean_q: 0.215727\n",
      " 36947/50000: episode: 4824, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002509, mae: 0.149430, mean_q: 0.217995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36966/50000: episode: 4825, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.003810, mae: 0.149616, mean_q: 0.213698\n",
      " 36972/50000: episode: 4826, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003744, mae: 0.158435, mean_q: 0.222855\n",
      " 36981/50000: episode: 4827, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.003047, mae: 0.156470, mean_q: 0.222380\n",
      " 36993/50000: episode: 4828, duration: 0.161s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003440, mae: 0.156819, mean_q: 0.220878\n",
      " 36996/50000: episode: 4829, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003537, mae: 0.155827, mean_q: 0.218771\n",
      " 37001/50000: episode: 4830, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002304, mae: 0.145062, mean_q: 0.213490\n",
      " 37005/50000: episode: 4831, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002195, mae: 0.148213, mean_q: 0.216198\n",
      " 37018/50000: episode: 4832, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.004252, mae: 0.155830, mean_q: 0.220715\n",
      " 37055/50000: episode: 4833, duration: 0.449s, episode steps:  37, steps per second:  82, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.003344, mae: 0.150291, mean_q: 0.215597\n",
      " 37065/50000: episode: 4834, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.003076, mae: 0.149318, mean_q: 0.209838\n",
      " 37068/50000: episode: 4835, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002645, mae: 0.149755, mean_q: 0.207818\n",
      " 37082/50000: episode: 4836, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002077, mae: 0.149551, mean_q: 0.209832\n",
      " 37084/50000: episode: 4837, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002294, mae: 0.155829, mean_q: 0.216860\n",
      " 37088/50000: episode: 4838, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.004499, mae: 0.154294, mean_q: 0.214767\n",
      " 37090/50000: episode: 4839, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002433, mae: 0.147388, mean_q: 0.204982\n",
      " 37093/50000: episode: 4840, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002109, mae: 0.137739, mean_q: 0.204675\n",
      " 37099/50000: episode: 4841, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003441, mae: 0.152301, mean_q: 0.217641\n",
      " 37104/50000: episode: 4842, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003164, mae: 0.153589, mean_q: 0.223536\n",
      " 37118/50000: episode: 4843, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.357 [0.000, 3.000],  loss: 0.003404, mae: 0.149254, mean_q: 0.212412\n",
      " 37128/50000: episode: 4844, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.002518, mae: 0.148314, mean_q: 0.211454\n",
      " 37133/50000: episode: 4845, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003004, mae: 0.149516, mean_q: 0.215022\n",
      " 37147/50000: episode: 4846, duration: 0.188s, episode steps:  14, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.002116, mae: 0.149069, mean_q: 0.212387\n",
      " 37154/50000: episode: 4847, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002445, mae: 0.150981, mean_q: 0.209680\n",
      " 37157/50000: episode: 4848, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001660, mae: 0.149496, mean_q: 0.210754\n",
      " 37169/50000: episode: 4849, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.002857, mae: 0.151304, mean_q: 0.214331\n",
      " 37176/50000: episode: 4850, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.003294, mae: 0.147565, mean_q: 0.212064\n",
      " 37181/50000: episode: 4851, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003665, mae: 0.149829, mean_q: 0.210514\n",
      " 37184/50000: episode: 4852, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003520, mae: 0.143976, mean_q: 0.211513\n",
      " 37191/50000: episode: 4853, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003543, mae: 0.157731, mean_q: 0.220120\n",
      " 37198/50000: episode: 4854, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002465, mae: 0.154097, mean_q: 0.216102\n",
      " 37207/50000: episode: 4855, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.002553, mae: 0.151688, mean_q: 0.213324\n",
      " 37223/50000: episode: 4856, duration: 0.219s, episode steps:  16, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.938 [0.000, 3.000],  loss: 0.002876, mae: 0.152764, mean_q: 0.214392\n",
      " 37228/50000: episode: 4857, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.004044, mae: 0.150399, mean_q: 0.214996\n",
      " 37230/50000: episode: 4858, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003183, mae: 0.156320, mean_q: 0.218730\n",
      " 37233/50000: episode: 4859, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003173, mae: 0.152224, mean_q: 0.214928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37238/50000: episode: 4860, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001689, mae: 0.149294, mean_q: 0.211595\n",
      " 37242/50000: episode: 4861, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002375, mae: 0.156387, mean_q: 0.220538\n",
      " 37245/50000: episode: 4862, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002826, mae: 0.153111, mean_q: 0.213291\n",
      " 37252/50000: episode: 4863, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002461, mae: 0.146049, mean_q: 0.208763\n",
      " 37256/50000: episode: 4864, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.003383, mae: 0.145726, mean_q: 0.209478\n",
      " 37259/50000: episode: 4865, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003205, mae: 0.152237, mean_q: 0.217199\n",
      " 37276/50000: episode: 4866, duration: 0.269s, episode steps:  17, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.003276, mae: 0.153119, mean_q: 0.217301\n",
      " 37290/50000: episode: 4867, duration: 0.194s, episode steps:  14, steps per second:  72, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002688, mae: 0.149580, mean_q: 0.209409\n",
      " 37304/50000: episode: 4868, duration: 0.207s, episode steps:  14, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002632, mae: 0.149963, mean_q: 0.212614\n",
      " 37323/50000: episode: 4869, duration: 0.244s, episode steps:  19, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.004388, mae: 0.156382, mean_q: 0.218063\n",
      " 37326/50000: episode: 4870, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002726, mae: 0.147445, mean_q: 0.209773\n",
      " 37346/50000: episode: 4871, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003042, mae: 0.152512, mean_q: 0.217203\n",
      " 37353/50000: episode: 4872, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002946, mae: 0.149467, mean_q: 0.213341\n",
      " 37365/50000: episode: 4873, duration: 0.172s, episode steps:  12, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002345, mae: 0.144195, mean_q: 0.206967\n",
      " 37379/50000: episode: 4874, duration: 0.183s, episode steps:  14, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003386, mae: 0.152451, mean_q: 0.217853\n",
      " 37389/50000: episode: 4875, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002502, mae: 0.148722, mean_q: 0.213611\n",
      " 37393/50000: episode: 4876, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002468, mae: 0.146791, mean_q: 0.211210\n",
      " 37395/50000: episode: 4877, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003125, mae: 0.150173, mean_q: 0.212185\n",
      " 37408/50000: episode: 4878, duration: 0.170s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002917, mae: 0.153178, mean_q: 0.216118\n",
      " 37416/50000: episode: 4879, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003423, mae: 0.158005, mean_q: 0.221690\n",
      " 37427/50000: episode: 4880, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003679, mae: 0.148909, mean_q: 0.213309\n",
      " 37439/50000: episode: 4881, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002369, mae: 0.152946, mean_q: 0.218031\n",
      " 37446/50000: episode: 4882, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002113, mae: 0.149876, mean_q: 0.214789\n",
      " 37461/50000: episode: 4883, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.067 [0.000, 3.000],  loss: 0.003307, mae: 0.151930, mean_q: 0.217436\n",
      " 37476/50000: episode: 4884, duration: 0.199s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.133 [0.000, 3.000],  loss: 0.002835, mae: 0.152453, mean_q: 0.215127\n",
      " 37484/50000: episode: 4885, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003686, mae: 0.148781, mean_q: 0.208643\n",
      " 37489/50000: episode: 4886, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 3.000],  loss: 0.002264, mae: 0.143768, mean_q: 0.205547\n",
      " 37499/50000: episode: 4887, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002922, mae: 0.153561, mean_q: 0.214144\n",
      " 37517/50000: episode: 4888, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.003264, mae: 0.150338, mean_q: 0.212140\n",
      " 37526/50000: episode: 4889, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002632, mae: 0.142078, mean_q: 0.203309\n",
      " 37537/50000: episode: 4890, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002795, mae: 0.149803, mean_q: 0.211799\n",
      " 37544/50000: episode: 4891, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002984, mae: 0.159352, mean_q: 0.222687\n",
      " 37550/50000: episode: 4892, duration: 0.090s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001854, mae: 0.156727, mean_q: 0.222974\n",
      " 37553/50000: episode: 4893, duration: 0.057s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002129, mae: 0.154519, mean_q: 0.218837\n",
      " 37559/50000: episode: 4894, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002094, mae: 0.149429, mean_q: 0.210886\n",
      " 37565/50000: episode: 4895, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003137, mae: 0.152730, mean_q: 0.215500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37567/50000: episode: 4896, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002941, mae: 0.145601, mean_q: 0.206970\n",
      " 37577/50000: episode: 4897, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003537, mae: 0.147469, mean_q: 0.210925\n",
      " 37582/50000: episode: 4898, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003644, mae: 0.155524, mean_q: 0.225932\n",
      " 37589/50000: episode: 4899, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002868, mae: 0.151728, mean_q: 0.216346\n",
      " 37593/50000: episode: 4900, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002179, mae: 0.143735, mean_q: 0.210681\n",
      " 37601/50000: episode: 4901, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003238, mae: 0.146739, mean_q: 0.212515\n",
      " 37611/50000: episode: 4902, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003396, mae: 0.145494, mean_q: 0.206919\n",
      " 37620/50000: episode: 4903, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002820, mae: 0.143279, mean_q: 0.209018\n",
      " 37633/50000: episode: 4904, duration: 0.178s, episode steps:  13, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002925, mae: 0.154016, mean_q: 0.216232\n",
      " 37648/50000: episode: 4905, duration: 0.240s, episode steps:  15, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002496, mae: 0.146445, mean_q: 0.211267\n",
      " 37655/50000: episode: 4906, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.003631, mae: 0.147128, mean_q: 0.207552\n",
      " 37657/50000: episode: 4907, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002954, mae: 0.152567, mean_q: 0.212505\n",
      " 37659/50000: episode: 4908, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001797, mae: 0.147258, mean_q: 0.207944\n",
      " 37664/50000: episode: 4909, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002499, mae: 0.154781, mean_q: 0.216957\n",
      " 37671/50000: episode: 4910, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002835, mae: 0.149383, mean_q: 0.208907\n",
      " 37677/50000: episode: 4911, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.833 [2.000, 3.000],  loss: 0.002852, mae: 0.148709, mean_q: 0.205934\n",
      " 37679/50000: episode: 4912, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002313, mae: 0.139972, mean_q: 0.200142\n",
      " 37692/50000: episode: 4913, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002730, mae: 0.142973, mean_q: 0.203615\n",
      " 37696/50000: episode: 4914, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003053, mae: 0.142960, mean_q: 0.204229\n",
      " 37699/50000: episode: 4915, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004058, mae: 0.149976, mean_q: 0.208971\n",
      " 37711/50000: episode: 4916, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002601, mae: 0.153037, mean_q: 0.211871\n",
      " 37719/50000: episode: 4917, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002556, mae: 0.147960, mean_q: 0.206460\n",
      " 37721/50000: episode: 4918, duration: 0.037s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.005102, mae: 0.146730, mean_q: 0.203986\n",
      " 37727/50000: episode: 4919, duration: 0.090s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002033, mae: 0.144278, mean_q: 0.204641\n",
      " 37731/50000: episode: 4920, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002549, mae: 0.149005, mean_q: 0.211450\n",
      " 37737/50000: episode: 4921, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001862, mae: 0.151583, mean_q: 0.213597\n",
      " 37740/50000: episode: 4922, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002024, mae: 0.153010, mean_q: 0.217772\n",
      " 37751/50000: episode: 4923, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.001744, mae: 0.145577, mean_q: 0.205068\n",
      " 37764/50000: episode: 4924, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002922, mae: 0.145244, mean_q: 0.205718\n",
      " 37784/50000: episode: 4925, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.003022, mae: 0.150927, mean_q: 0.209244\n",
      " 37788/50000: episode: 4926, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001882, mae: 0.143229, mean_q: 0.200356\n",
      " 37792/50000: episode: 4927, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.003142, mae: 0.146347, mean_q: 0.205908\n",
      " 37794/50000: episode: 4928, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003236, mae: 0.150126, mean_q: 0.205267\n",
      " 37796/50000: episode: 4929, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001344, mae: 0.143140, mean_q: 0.211201\n",
      " 37799/50000: episode: 4930, duration: 0.059s, episode steps:   3, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002816, mae: 0.148473, mean_q: 0.208435\n",
      " 37805/50000: episode: 4931, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003269, mae: 0.150199, mean_q: 0.209204\n",
      " 37811/50000: episode: 4932, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.001980, mae: 0.137769, mean_q: 0.197546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37818/50000: episode: 4933, duration: 0.107s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.002672, mae: 0.147797, mean_q: 0.207135\n",
      " 37826/50000: episode: 4934, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003046, mae: 0.147491, mean_q: 0.211992\n",
      " 37829/50000: episode: 4935, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003488, mae: 0.157811, mean_q: 0.221103\n",
      " 37849/50000: episode: 4936, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002343, mae: 0.149899, mean_q: 0.214070\n",
      " 37852/50000: episode: 4937, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002996, mae: 0.147548, mean_q: 0.209814\n",
      " 37860/50000: episode: 4938, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003508, mae: 0.152036, mean_q: 0.214923\n",
      " 37873/50000: episode: 4939, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.003199, mae: 0.151185, mean_q: 0.214040\n",
      " 37879/50000: episode: 4940, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003430, mae: 0.151264, mean_q: 0.213829\n",
      " 37887/50000: episode: 4941, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003819, mae: 0.152764, mean_q: 0.216237\n",
      " 37889/50000: episode: 4942, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002003, mae: 0.137920, mean_q: 0.200472\n",
      " 37891/50000: episode: 4943, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002540, mae: 0.150846, mean_q: 0.217795\n",
      " 37895/50000: episode: 4944, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002065, mae: 0.136817, mean_q: 0.196427\n",
      " 37903/50000: episode: 4945, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003055, mae: 0.141953, mean_q: 0.203438\n",
      " 37912/50000: episode: 4946, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002612, mae: 0.141490, mean_q: 0.205396\n",
      " 37926/50000: episode: 4947, duration: 0.190s, episode steps:  14, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002361, mae: 0.149630, mean_q: 0.211689\n",
      " 37929/50000: episode: 4948, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002620, mae: 0.138908, mean_q: 0.202899\n",
      " 37935/50000: episode: 4949, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002222, mae: 0.139581, mean_q: 0.201437\n",
      " 37950/50000: episode: 4950, duration: 0.196s, episode steps:  15, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.002779, mae: 0.146602, mean_q: 0.209602\n",
      " 37958/50000: episode: 4951, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002317, mae: 0.146047, mean_q: 0.211737\n",
      " 37960/50000: episode: 4952, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003621, mae: 0.146693, mean_q: 0.209250\n",
      " 37966/50000: episode: 4953, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002339, mae: 0.141969, mean_q: 0.200875\n",
      " 37971/50000: episode: 4954, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003731, mae: 0.146536, mean_q: 0.202763\n",
      " 37976/50000: episode: 4955, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.004246, mae: 0.147530, mean_q: 0.204616\n",
      " 37981/50000: episode: 4956, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002339, mae: 0.143966, mean_q: 0.203410\n",
      " 37991/50000: episode: 4957, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003865, mae: 0.145336, mean_q: 0.206005\n",
      " 37995/50000: episode: 4958, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.005581, mae: 0.144769, mean_q: 0.206686\n",
      " 38010/50000: episode: 4959, duration: 0.199s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002440, mae: 0.145674, mean_q: 0.207902\n",
      " 38019/50000: episode: 4960, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002322, mae: 0.147185, mean_q: 0.208464\n",
      " 38022/50000: episode: 4961, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.003287, mae: 0.151180, mean_q: 0.211649\n",
      " 38035/50000: episode: 4962, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002266, mae: 0.145041, mean_q: 0.208585\n",
      " 38042/50000: episode: 4963, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003117, mae: 0.143870, mean_q: 0.205995\n",
      " 38055/50000: episode: 4964, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002265, mae: 0.142998, mean_q: 0.205825\n",
      " 38062/50000: episode: 4965, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003883, mae: 0.148250, mean_q: 0.211964\n",
      " 38075/50000: episode: 4966, duration: 0.179s, episode steps:  13, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.003283, mae: 0.144335, mean_q: 0.210848\n",
      " 38082/50000: episode: 4967, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.003498, mae: 0.148871, mean_q: 0.216830\n",
      " 38088/50000: episode: 4968, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002243, mae: 0.152635, mean_q: 0.221196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38098/50000: episode: 4969, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003112, mae: 0.146773, mean_q: 0.207754\n",
      " 38101/50000: episode: 4970, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002541, mae: 0.142181, mean_q: 0.202370\n",
      " 38105/50000: episode: 4971, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002123, mae: 0.141571, mean_q: 0.203288\n",
      " 38110/50000: episode: 4972, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002576, mae: 0.140520, mean_q: 0.199070\n",
      " 38120/50000: episode: 4973, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002952, mae: 0.146458, mean_q: 0.207707\n",
      " 38122/50000: episode: 4974, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002340, mae: 0.147880, mean_q: 0.205564\n",
      " 38124/50000: episode: 4975, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.000975, mae: 0.138958, mean_q: 0.201371\n",
      " 38132/50000: episode: 4976, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003758, mae: 0.149367, mean_q: 0.210961\n",
      " 38135/50000: episode: 4977, duration: 0.056s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002956, mae: 0.143919, mean_q: 0.206029\n",
      " 38137/50000: episode: 4978, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001899, mae: 0.137055, mean_q: 0.195354\n",
      " 38140/50000: episode: 4979, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001791, mae: 0.141821, mean_q: 0.204643\n",
      " 38142/50000: episode: 4980, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001779, mae: 0.141406, mean_q: 0.204365\n",
      " 38150/50000: episode: 4981, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002292, mae: 0.145555, mean_q: 0.209252\n",
      " 38156/50000: episode: 4982, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001990, mae: 0.144986, mean_q: 0.206278\n",
      " 38160/50000: episode: 4983, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.004742, mae: 0.156422, mean_q: 0.215009\n",
      " 38165/50000: episode: 4984, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001747, mae: 0.144813, mean_q: 0.204901\n",
      " 38168/50000: episode: 4985, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002832, mae: 0.143675, mean_q: 0.206500\n",
      " 38177/50000: episode: 4986, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003313, mae: 0.148643, mean_q: 0.209401\n",
      " 38181/50000: episode: 4987, duration: 0.068s, episode steps:   4, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002414, mae: 0.148562, mean_q: 0.208878\n",
      " 38190/50000: episode: 4988, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002215, mae: 0.149476, mean_q: 0.210342\n",
      " 38192/50000: episode: 4989, duration: 0.037s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002082, mae: 0.142219, mean_q: 0.201553\n",
      " 38196/50000: episode: 4990, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003647, mae: 0.154972, mean_q: 0.215972\n",
      " 38205/50000: episode: 4991, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003363, mae: 0.146046, mean_q: 0.210867\n",
      " 38207/50000: episode: 4992, duration: 0.065s, episode steps:   2, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003628, mae: 0.146016, mean_q: 0.204466\n",
      " 38213/50000: episode: 4993, duration: 0.116s, episode steps:   6, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002459, mae: 0.145213, mean_q: 0.208583\n",
      " 38216/50000: episode: 4994, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002361, mae: 0.149177, mean_q: 0.212008\n",
      " 38224/50000: episode: 4995, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003403, mae: 0.146189, mean_q: 0.208832\n",
      " 38229/50000: episode: 4996, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002489, mae: 0.147195, mean_q: 0.208532\n",
      " 38235/50000: episode: 4997, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002910, mae: 0.141711, mean_q: 0.206545\n",
      " 38255/50000: episode: 4998, duration: 0.305s, episode steps:  20, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.002945, mae: 0.147573, mean_q: 0.210327\n",
      " 38257/50000: episode: 4999, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002706, mae: 0.151951, mean_q: 0.212689\n",
      " 38259/50000: episode: 5000, duration: 0.041s, episode steps:   2, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002031, mae: 0.147669, mean_q: 0.214279\n",
      " 38268/50000: episode: 5001, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002651, mae: 0.147822, mean_q: 0.210946\n",
      " 38278/50000: episode: 5002, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002341, mae: 0.147911, mean_q: 0.207014\n",
      " 38282/50000: episode: 5003, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003065, mae: 0.142878, mean_q: 0.197432\n",
      " 38284/50000: episode: 5004, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001711, mae: 0.144535, mean_q: 0.203341\n",
      " 38292/50000: episode: 5005, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002851, mae: 0.143033, mean_q: 0.197834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38301/50000: episode: 5006, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002841, mae: 0.146040, mean_q: 0.202756\n",
      " 38314/50000: episode: 5007, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003060, mae: 0.146517, mean_q: 0.207939\n",
      " 38317/50000: episode: 5008, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003647, mae: 0.144615, mean_q: 0.205344\n",
      " 38319/50000: episode: 5009, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003601, mae: 0.144858, mean_q: 0.204947\n",
      " 38333/50000: episode: 5010, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002449, mae: 0.136792, mean_q: 0.198051\n",
      " 38347/50000: episode: 5011, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002686, mae: 0.144695, mean_q: 0.204912\n",
      " 38353/50000: episode: 5012, duration: 0.090s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002317, mae: 0.141350, mean_q: 0.202292\n",
      " 38358/50000: episode: 5013, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [2.000, 3.000],  loss: 0.003107, mae: 0.144215, mean_q: 0.205638\n",
      " 38362/50000: episode: 5014, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003832, mae: 0.145872, mean_q: 0.205465\n",
      " 38371/50000: episode: 5015, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002287, mae: 0.144260, mean_q: 0.202972\n",
      " 38379/50000: episode: 5016, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002071, mae: 0.145034, mean_q: 0.205655\n",
      " 38385/50000: episode: 5017, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.002197, mae: 0.143015, mean_q: 0.203458\n",
      " 38388/50000: episode: 5018, duration: 0.058s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.002377, mae: 0.143181, mean_q: 0.207068\n",
      " 38404/50000: episode: 5019, duration: 0.212s, episode steps:  16, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003433, mae: 0.141860, mean_q: 0.204674\n",
      " 38412/50000: episode: 5020, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003215, mae: 0.145249, mean_q: 0.207888\n",
      " 38416/50000: episode: 5021, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002521, mae: 0.145881, mean_q: 0.205139\n",
      " 38421/50000: episode: 5022, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003491, mae: 0.149217, mean_q: 0.204965\n",
      " 38436/50000: episode: 5023, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002731, mae: 0.143544, mean_q: 0.201091\n",
      " 38443/50000: episode: 5024, duration: 0.110s, episode steps:   7, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.429 [1.000, 3.000],  loss: 0.004183, mae: 0.140631, mean_q: 0.201357\n",
      " 38448/50000: episode: 5025, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.200 [0.000, 1.000],  loss: 0.002479, mae: 0.137042, mean_q: 0.199648\n",
      " 38454/50000: episode: 5026, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.002223, mae: 0.142369, mean_q: 0.203950\n",
      " 38461/50000: episode: 5027, duration: 0.109s, episode steps:   7, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 2.000],  loss: 0.003144, mae: 0.143560, mean_q: 0.204711\n",
      " 38464/50000: episode: 5028, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002396, mae: 0.149628, mean_q: 0.211338\n",
      " 38475/50000: episode: 5029, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002161, mae: 0.149668, mean_q: 0.212173\n",
      " 38478/50000: episode: 5030, duration: 0.058s, episode steps:   3, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002152, mae: 0.142013, mean_q: 0.207406\n",
      " 38487/50000: episode: 5031, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003226, mae: 0.146863, mean_q: 0.213623\n",
      " 38494/50000: episode: 5032, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  1.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 0.857 [0.000, 2.000],  loss: 0.003487, mae: 0.144890, mean_q: 0.207638\n",
      " 38499/50000: episode: 5033, duration: 0.082s, episode steps:   5, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002621, mae: 0.142946, mean_q: 0.205370\n",
      " 38519/50000: episode: 5034, duration: 0.271s, episode steps:  20, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002329, mae: 0.147409, mean_q: 0.208529\n",
      " 38542/50000: episode: 5035, duration: 0.314s, episode steps:  23, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002667, mae: 0.144520, mean_q: 0.206219\n",
      " 38544/50000: episode: 5036, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001886, mae: 0.144339, mean_q: 0.209596\n",
      " 38553/50000: episode: 5037, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [1.000, 3.000],  loss: 0.002601, mae: 0.144493, mean_q: 0.202991\n",
      " 38565/50000: episode: 5038, duration: 0.168s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003540, mae: 0.144115, mean_q: 0.203392\n",
      " 38571/50000: episode: 5039, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002057, mae: 0.141506, mean_q: 0.201626\n",
      " 38576/50000: episode: 5040, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002039, mae: 0.140953, mean_q: 0.200213\n",
      " 38578/50000: episode: 5041, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002014, mae: 0.144518, mean_q: 0.203989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38588/50000: episode: 5042, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003572, mae: 0.146093, mean_q: 0.207403\n",
      " 38607/50000: episode: 5043, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.105 [0.000, 3.000],  loss: 0.003634, mae: 0.148323, mean_q: 0.208135\n",
      " 38620/50000: episode: 5044, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002629, mae: 0.147561, mean_q: 0.210901\n",
      " 38624/50000: episode: 5045, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003016, mae: 0.152255, mean_q: 0.213761\n",
      " 38634/50000: episode: 5046, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002241, mae: 0.143059, mean_q: 0.201965\n",
      " 38636/50000: episode: 5047, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003993, mae: 0.150429, mean_q: 0.208711\n",
      " 38650/50000: episode: 5048, duration: 0.185s, episode steps:  14, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.004119, mae: 0.147714, mean_q: 0.205290\n",
      " 38652/50000: episode: 5049, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005005, mae: 0.153373, mean_q: 0.214911\n",
      " 38669/50000: episode: 5050, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002849, mae: 0.142172, mean_q: 0.201377\n",
      " 38671/50000: episode: 5051, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001197, mae: 0.137995, mean_q: 0.195854\n",
      " 38677/50000: episode: 5052, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001833, mae: 0.135188, mean_q: 0.194825\n",
      " 38679/50000: episode: 5053, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003266, mae: 0.144537, mean_q: 0.203819\n",
      " 38684/50000: episode: 5054, duration: 0.092s, episode steps:   5, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002753, mae: 0.145636, mean_q: 0.206462\n",
      " 38693/50000: episode: 5055, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004398, mae: 0.154872, mean_q: 0.211858\n",
      " 38704/50000: episode: 5056, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001914, mae: 0.142661, mean_q: 0.199305\n",
      " 38707/50000: episode: 5057, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002358, mae: 0.140664, mean_q: 0.196579\n",
      " 38722/50000: episode: 5058, duration: 0.195s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002298, mae: 0.140004, mean_q: 0.200234\n",
      " 38732/50000: episode: 5059, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003227, mae: 0.147747, mean_q: 0.207317\n",
      " 38735/50000: episode: 5060, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002257, mae: 0.144876, mean_q: 0.202340\n",
      " 38748/50000: episode: 5061, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002850, mae: 0.143000, mean_q: 0.201816\n",
      " 38756/50000: episode: 5062, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002212, mae: 0.137823, mean_q: 0.197890\n",
      " 38759/50000: episode: 5063, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002047, mae: 0.141093, mean_q: 0.199670\n",
      " 38765/50000: episode: 5064, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002677, mae: 0.145669, mean_q: 0.204535\n",
      " 38775/50000: episode: 5065, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.003179, mae: 0.143652, mean_q: 0.200038\n",
      " 38779/50000: episode: 5066, duration: 0.073s, episode steps:   4, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002029, mae: 0.139676, mean_q: 0.195149\n",
      " 38782/50000: episode: 5067, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002249, mae: 0.140833, mean_q: 0.199322\n",
      " 38790/50000: episode: 5068, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002566, mae: 0.142105, mean_q: 0.200995\n",
      " 38798/50000: episode: 5069, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002961, mae: 0.142151, mean_q: 0.204193\n",
      " 38823/50000: episode: 5070, duration: 0.351s, episode steps:  25, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.002620, mae: 0.141867, mean_q: 0.204313\n",
      " 38829/50000: episode: 5071, duration: 0.111s, episode steps:   6, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002781, mae: 0.146594, mean_q: 0.208900\n",
      " 38837/50000: episode: 5072, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002861, mae: 0.144388, mean_q: 0.206874\n",
      " 38863/50000: episode: 5073, duration: 0.337s, episode steps:  26, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002827, mae: 0.144710, mean_q: 0.208058\n",
      " 38874/50000: episode: 5074, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.001989, mae: 0.141230, mean_q: 0.202614\n",
      " 38876/50000: episode: 5075, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001524, mae: 0.140310, mean_q: 0.202281\n",
      " 38882/50000: episode: 5076, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003301, mae: 0.145336, mean_q: 0.205957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38901/50000: episode: 5077, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002997, mae: 0.145648, mean_q: 0.205116\n",
      " 38903/50000: episode: 5078, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002979, mae: 0.139640, mean_q: 0.197608\n",
      " 38905/50000: episode: 5079, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004065, mae: 0.152316, mean_q: 0.214303\n",
      " 38916/50000: episode: 5080, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002688, mae: 0.143424, mean_q: 0.202681\n",
      " 38922/50000: episode: 5081, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002707, mae: 0.143128, mean_q: 0.205330\n",
      " 38924/50000: episode: 5082, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004535, mae: 0.149413, mean_q: 0.212164\n",
      " 38936/50000: episode: 5083, duration: 0.165s, episode steps:  12, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001811, mae: 0.143127, mean_q: 0.202572\n",
      " 38939/50000: episode: 5084, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004092, mae: 0.143837, mean_q: 0.199381\n",
      " 38944/50000: episode: 5085, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002013, mae: 0.140360, mean_q: 0.198335\n",
      " 38953/50000: episode: 5086, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002551, mae: 0.139583, mean_q: 0.198610\n",
      " 38960/50000: episode: 5087, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003398, mae: 0.143690, mean_q: 0.205704\n",
      " 38967/50000: episode: 5088, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.002168, mae: 0.148470, mean_q: 0.209350\n",
      " 38969/50000: episode: 5089, duration: 0.047s, episode steps:   2, steps per second:  43, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005661, mae: 0.162465, mean_q: 0.223769\n",
      " 38972/50000: episode: 5090, duration: 0.054s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001756, mae: 0.144955, mean_q: 0.202455\n",
      " 38975/50000: episode: 5091, duration: 0.056s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002128, mae: 0.146236, mean_q: 0.205443\n",
      " 38997/50000: episode: 5092, duration: 0.287s, episode steps:  22, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002817, mae: 0.142082, mean_q: 0.201096\n",
      " 39004/50000: episode: 5093, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002861, mae: 0.136937, mean_q: 0.203858\n",
      " 39017/50000: episode: 5094, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.003070, mae: 0.148212, mean_q: 0.208174\n",
      " 39032/50000: episode: 5095, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002571, mae: 0.143498, mean_q: 0.202851\n",
      " 39042/50000: episode: 5096, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.003312, mae: 0.148093, mean_q: 0.208939\n",
      " 39049/50000: episode: 5097, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002475, mae: 0.148636, mean_q: 0.213382\n",
      " 39052/50000: episode: 5098, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002693, mae: 0.140246, mean_q: 0.202242\n",
      " 39060/50000: episode: 5099, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003534, mae: 0.142535, mean_q: 0.202774\n",
      " 39072/50000: episode: 5100, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002262, mae: 0.137229, mean_q: 0.199179\n",
      " 39081/50000: episode: 5101, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002856, mae: 0.141775, mean_q: 0.202689\n",
      " 39090/50000: episode: 5102, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003661, mae: 0.144721, mean_q: 0.201443\n",
      " 39095/50000: episode: 5103, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003263, mae: 0.145337, mean_q: 0.203867\n",
      " 39107/50000: episode: 5104, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002842, mae: 0.143494, mean_q: 0.201794\n",
      " 39114/50000: episode: 5105, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.003349, mae: 0.142879, mean_q: 0.199843\n",
      " 39116/50000: episode: 5106, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001432, mae: 0.140689, mean_q: 0.202455\n",
      " 39123/50000: episode: 5107, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002632, mae: 0.144867, mean_q: 0.201731\n",
      " 39132/50000: episode: 5108, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002647, mae: 0.142055, mean_q: 0.201645\n",
      " 39136/50000: episode: 5109, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002314, mae: 0.142857, mean_q: 0.201261\n",
      " 39153/50000: episode: 5110, duration: 0.225s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.176 [1.000, 3.000],  loss: 0.002500, mae: 0.140247, mean_q: 0.199452\n",
      " 39160/50000: episode: 5111, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 2.000],  loss: 0.002450, mae: 0.141005, mean_q: 0.199906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39181/50000: episode: 5112, duration: 0.274s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.003264, mae: 0.145083, mean_q: 0.203535\n",
      " 39189/50000: episode: 5113, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003255, mae: 0.143351, mean_q: 0.201300\n",
      " 39194/50000: episode: 5114, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001784, mae: 0.144329, mean_q: 0.206026\n",
      " 39202/50000: episode: 5115, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002579, mae: 0.151226, mean_q: 0.211990\n",
      " 39207/50000: episode: 5116, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003630, mae: 0.142493, mean_q: 0.201423\n",
      " 39210/50000: episode: 5117, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002436, mae: 0.140818, mean_q: 0.206304\n",
      " 39213/50000: episode: 5118, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002135, mae: 0.140113, mean_q: 0.202747\n",
      " 39224/50000: episode: 5119, duration: 0.158s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003100, mae: 0.139844, mean_q: 0.202647\n",
      " 39251/50000: episode: 5120, duration: 0.354s, episode steps:  27, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.002515, mae: 0.138812, mean_q: 0.198548\n",
      " 39268/50000: episode: 5121, duration: 0.211s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002834, mae: 0.142965, mean_q: 0.203087\n",
      " 39281/50000: episode: 5122, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.003050, mae: 0.147478, mean_q: 0.208951\n",
      " 39288/50000: episode: 5123, duration: 0.110s, episode steps:   7, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002788, mae: 0.142619, mean_q: 0.201312\n",
      " 39299/50000: episode: 5124, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002680, mae: 0.145375, mean_q: 0.208267\n",
      " 39303/50000: episode: 5125, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002896, mae: 0.141675, mean_q: 0.203211\n",
      " 39310/50000: episode: 5126, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002795, mae: 0.152252, mean_q: 0.215327\n",
      " 39325/50000: episode: 5127, duration: 0.212s, episode steps:  15, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002376, mae: 0.147020, mean_q: 0.205708\n",
      " 39337/50000: episode: 5128, duration: 0.214s, episode steps:  12, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001840, mae: 0.143750, mean_q: 0.205008\n",
      " 39339/50000: episode: 5129, duration: 0.042s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.005160, mae: 0.165555, mean_q: 0.225637\n",
      " 39343/50000: episode: 5130, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002824, mae: 0.145586, mean_q: 0.204629\n",
      " 39345/50000: episode: 5131, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003638, mae: 0.141128, mean_q: 0.201225\n",
      " 39351/50000: episode: 5132, duration: 0.093s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003389, mae: 0.143330, mean_q: 0.202783\n",
      " 39353/50000: episode: 5133, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001848, mae: 0.137286, mean_q: 0.198485\n",
      " 39355/50000: episode: 5134, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002114, mae: 0.137102, mean_q: 0.191686\n",
      " 39361/50000: episode: 5135, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002063, mae: 0.141905, mean_q: 0.203699\n",
      " 39371/50000: episode: 5136, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.002961, mae: 0.151180, mean_q: 0.212798\n",
      " 39379/50000: episode: 5137, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002653, mae: 0.146982, mean_q: 0.209822\n",
      " 39392/50000: episode: 5138, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002827, mae: 0.141349, mean_q: 0.203489\n",
      " 39396/50000: episode: 5139, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002822, mae: 0.141843, mean_q: 0.206261\n",
      " 39403/50000: episode: 5140, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002387, mae: 0.136910, mean_q: 0.199089\n",
      " 39408/50000: episode: 5141, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002436, mae: 0.141029, mean_q: 0.204098\n",
      " 39418/50000: episode: 5142, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003508, mae: 0.142134, mean_q: 0.203354\n",
      " 39426/50000: episode: 5143, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002313, mae: 0.147336, mean_q: 0.210673\n",
      " 39430/50000: episode: 5144, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002338, mae: 0.144864, mean_q: 0.206312\n",
      " 39438/50000: episode: 5145, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002559, mae: 0.140271, mean_q: 0.200760\n",
      " 39448/50000: episode: 5146, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003416, mae: 0.143166, mean_q: 0.207335\n",
      " 39454/50000: episode: 5147, duration: 0.096s, episode steps:   6, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003048, mae: 0.142786, mean_q: 0.205202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39467/50000: episode: 5148, duration: 0.185s, episode steps:  13, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002991, mae: 0.147888, mean_q: 0.210117\n",
      " 39473/50000: episode: 5149, duration: 0.089s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [1.000, 3.000],  loss: 0.002342, mae: 0.139097, mean_q: 0.204346\n",
      " 39480/50000: episode: 5150, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003188, mae: 0.143720, mean_q: 0.206034\n",
      " 39486/50000: episode: 5151, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002361, mae: 0.141120, mean_q: 0.203671\n",
      " 39496/50000: episode: 5152, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.900 [0.000, 3.000],  loss: 0.002581, mae: 0.145784, mean_q: 0.206048\n",
      " 39502/50000: episode: 5153, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003310, mae: 0.149120, mean_q: 0.205973\n",
      " 39523/50000: episode: 5154, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002878, mae: 0.145421, mean_q: 0.204460\n",
      " 39530/50000: episode: 5155, duration: 0.107s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002910, mae: 0.145817, mean_q: 0.203121\n",
      " 39540/50000: episode: 5156, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.002397, mae: 0.148054, mean_q: 0.206756\n",
      " 39547/50000: episode: 5157, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002675, mae: 0.147863, mean_q: 0.208320\n",
      " 39551/50000: episode: 5158, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001334, mae: 0.144706, mean_q: 0.206991\n",
      " 39557/50000: episode: 5159, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.002302, mae: 0.145162, mean_q: 0.205609\n",
      " 39564/50000: episode: 5160, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.002681, mae: 0.147562, mean_q: 0.208915\n",
      " 39574/50000: episode: 5161, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002537, mae: 0.142618, mean_q: 0.202893\n",
      " 39579/50000: episode: 5162, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002049, mae: 0.142119, mean_q: 0.204381\n",
      " 39604/50000: episode: 5163, duration: 0.322s, episode steps:  25, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.960 [0.000, 3.000],  loss: 0.002796, mae: 0.142649, mean_q: 0.202569\n",
      " 39610/50000: episode: 5164, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002273, mae: 0.138642, mean_q: 0.198227\n",
      " 39616/50000: episode: 5165, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 2.000],  loss: 0.002344, mae: 0.136386, mean_q: 0.197573\n",
      " 39628/50000: episode: 5166, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002964, mae: 0.142928, mean_q: 0.205247\n",
      " 39631/50000: episode: 5167, duration: 0.047s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002954, mae: 0.146702, mean_q: 0.207046\n",
      " 39633/50000: episode: 5168, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003238, mae: 0.151233, mean_q: 0.207827\n",
      " 39637/50000: episode: 5169, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.004436, mae: 0.151255, mean_q: 0.206482\n",
      " 39643/50000: episode: 5170, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002227, mae: 0.140504, mean_q: 0.199677\n",
      " 39645/50000: episode: 5171, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001413, mae: 0.133264, mean_q: 0.194680\n",
      " 39653/50000: episode: 5172, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002836, mae: 0.146372, mean_q: 0.207603\n",
      " 39664/50000: episode: 5173, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002431, mae: 0.145108, mean_q: 0.205567\n",
      " 39668/50000: episode: 5174, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002628, mae: 0.147486, mean_q: 0.208457\n",
      " 39676/50000: episode: 5175, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003023, mae: 0.146733, mean_q: 0.208720\n",
      " 39692/50000: episode: 5176, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.003500, mae: 0.141725, mean_q: 0.205612\n",
      " 39698/50000: episode: 5177, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002421, mae: 0.146588, mean_q: 0.207798\n",
      " 39703/50000: episode: 5178, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002806, mae: 0.150379, mean_q: 0.209794\n",
      " 39715/50000: episode: 5179, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003447, mae: 0.147519, mean_q: 0.205958\n",
      " 39718/50000: episode: 5180, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002231, mae: 0.148461, mean_q: 0.206958\n",
      " 39728/50000: episode: 5181, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003098, mae: 0.149501, mean_q: 0.211800\n",
      " 39737/50000: episode: 5182, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003666, mae: 0.145719, mean_q: 0.209433\n",
      " 39742/50000: episode: 5183, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001990, mae: 0.139489, mean_q: 0.204242\n",
      " 39744/50000: episode: 5184, duration: 0.047s, episode steps:   2, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003117, mae: 0.142350, mean_q: 0.202745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39760/50000: episode: 5185, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002438, mae: 0.144990, mean_q: 0.208181\n",
      " 39765/50000: episode: 5186, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002493, mae: 0.143584, mean_q: 0.205718\n",
      " 39768/50000: episode: 5187, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001357, mae: 0.145010, mean_q: 0.208224\n",
      " 39772/50000: episode: 5188, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003285, mae: 0.144638, mean_q: 0.207760\n",
      " 39777/50000: episode: 5189, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002335, mae: 0.145153, mean_q: 0.210958\n",
      " 39780/50000: episode: 5190, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004088, mae: 0.140182, mean_q: 0.206846\n",
      " 39786/50000: episode: 5191, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003338, mae: 0.149763, mean_q: 0.217193\n",
      " 39790/50000: episode: 5192, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002044, mae: 0.144104, mean_q: 0.211947\n",
      " 39793/50000: episode: 5193, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002283, mae: 0.147410, mean_q: 0.212832\n",
      " 39798/50000: episode: 5194, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002733, mae: 0.153093, mean_q: 0.219792\n",
      " 39801/50000: episode: 5195, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003842, mae: 0.153922, mean_q: 0.215731\n",
      " 39813/50000: episode: 5196, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003025, mae: 0.147188, mean_q: 0.208277\n",
      " 39817/50000: episode: 5197, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.003974, mae: 0.145019, mean_q: 0.203844\n",
      " 39827/50000: episode: 5198, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003661, mae: 0.151573, mean_q: 0.212087\n",
      " 39831/50000: episode: 5199, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003123, mae: 0.151351, mean_q: 0.214350\n",
      " 39846/50000: episode: 5200, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002841, mae: 0.152794, mean_q: 0.217467\n",
      " 39848/50000: episode: 5201, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002813, mae: 0.145753, mean_q: 0.211232\n",
      " 39851/50000: episode: 5202, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002614, mae: 0.142642, mean_q: 0.202662\n",
      " 39856/50000: episode: 5203, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.004215, mae: 0.152939, mean_q: 0.212688\n",
      " 39860/50000: episode: 5204, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003489, mae: 0.151201, mean_q: 0.211641\n",
      " 39880/50000: episode: 5205, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.003083, mae: 0.148561, mean_q: 0.213327\n",
      " 39888/50000: episode: 5206, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002799, mae: 0.145525, mean_q: 0.207677\n",
      " 39890/50000: episode: 5207, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002639, mae: 0.141735, mean_q: 0.203925\n",
      " 39896/50000: episode: 5208, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003604, mae: 0.149141, mean_q: 0.206337\n",
      " 39911/50000: episode: 5209, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.933 [0.000, 3.000],  loss: 0.003743, mae: 0.157853, mean_q: 0.219577\n",
      " 39920/50000: episode: 5210, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003446, mae: 0.148202, mean_q: 0.209525\n",
      " 39934/50000: episode: 5211, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002967, mae: 0.141015, mean_q: 0.202279\n",
      " 39937/50000: episode: 5212, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002343, mae: 0.139676, mean_q: 0.202886\n",
      " 39957/50000: episode: 5213, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.150 [0.000, 3.000],  loss: 0.002886, mae: 0.149529, mean_q: 0.211873\n",
      " 39960/50000: episode: 5214, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002694, mae: 0.148005, mean_q: 0.217079\n",
      " 39966/50000: episode: 5215, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002746, mae: 0.146737, mean_q: 0.210694\n",
      " 39976/50000: episode: 5216, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.004475, mae: 0.150001, mean_q: 0.213192\n",
      " 39987/50000: episode: 5217, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002910, mae: 0.147618, mean_q: 0.210414\n",
      " 39997/50000: episode: 5218, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002752, mae: 0.153510, mean_q: 0.216900\n",
      " 40009/50000: episode: 5219, duration: 0.157s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002499, mae: 0.146785, mean_q: 0.209597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40013/50000: episode: 5220, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.003357, mae: 0.150871, mean_q: 0.213441\n",
      " 40026/50000: episode: 5221, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002699, mae: 0.153393, mean_q: 0.218161\n",
      " 40028/50000: episode: 5222, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002368, mae: 0.150303, mean_q: 0.212572\n",
      " 40042/50000: episode: 5223, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003185, mae: 0.154325, mean_q: 0.217681\n",
      " 40044/50000: episode: 5224, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003828, mae: 0.148515, mean_q: 0.215705\n",
      " 40048/50000: episode: 5225, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004218, mae: 0.148017, mean_q: 0.213849\n",
      " 40050/50000: episode: 5226, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.004732, mae: 0.152104, mean_q: 0.220378\n",
      " 40059/50000: episode: 5227, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003415, mae: 0.143972, mean_q: 0.209697\n",
      " 40067/50000: episode: 5228, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003304, mae: 0.148763, mean_q: 0.210426\n",
      " 40069/50000: episode: 5229, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002897, mae: 0.147330, mean_q: 0.204072\n",
      " 40083/50000: episode: 5230, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002187, mae: 0.149905, mean_q: 0.210382\n",
      " 40085/50000: episode: 5231, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003026, mae: 0.152294, mean_q: 0.210871\n",
      " 40095/50000: episode: 5232, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003034, mae: 0.149864, mean_q: 0.209694\n",
      " 40106/50000: episode: 5233, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.003322, mae: 0.154784, mean_q: 0.215983\n",
      " 40110/50000: episode: 5234, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 1.000],  loss: 0.003505, mae: 0.147047, mean_q: 0.207027\n",
      " 40121/50000: episode: 5235, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002423, mae: 0.143692, mean_q: 0.201735\n",
      " 40125/50000: episode: 5236, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001982, mae: 0.139961, mean_q: 0.198962\n",
      " 40136/50000: episode: 5237, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002939, mae: 0.146676, mean_q: 0.207635\n",
      " 40162/50000: episode: 5238, duration: 0.319s, episode steps:  26, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002798, mae: 0.146902, mean_q: 0.206705\n",
      " 40181/50000: episode: 5239, duration: 0.238s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002710, mae: 0.147202, mean_q: 0.209908\n",
      " 40189/50000: episode: 5240, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002588, mae: 0.148430, mean_q: 0.211467\n",
      " 40203/50000: episode: 5241, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002762, mae: 0.148605, mean_q: 0.209314\n",
      " 40218/50000: episode: 5242, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002729, mae: 0.148684, mean_q: 0.211524\n",
      " 40220/50000: episode: 5243, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004744, mae: 0.158033, mean_q: 0.224761\n",
      " 40251/50000: episode: 5244, duration: 0.377s, episode steps:  31, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.774 [0.000, 3.000],  loss: 0.003251, mae: 0.152923, mean_q: 0.216590\n",
      " 40257/50000: episode: 5245, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003098, mae: 0.149180, mean_q: 0.207270\n",
      " 40265/50000: episode: 5246, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.002676, mae: 0.142098, mean_q: 0.202169\n",
      " 40275/50000: episode: 5247, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.004282, mae: 0.149788, mean_q: 0.212219\n",
      " 40281/50000: episode: 5248, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002801, mae: 0.151236, mean_q: 0.218075\n",
      " 40295/50000: episode: 5249, duration: 0.178s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002903, mae: 0.151850, mean_q: 0.215534\n",
      " 40297/50000: episode: 5250, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002753, mae: 0.149650, mean_q: 0.210398\n",
      " 40311/50000: episode: 5251, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002718, mae: 0.144469, mean_q: 0.204436\n",
      " 40319/50000: episode: 5252, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002423, mae: 0.140799, mean_q: 0.202575\n",
      " 40333/50000: episode: 5253, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.003233, mae: 0.150002, mean_q: 0.212030\n",
      " 40337/50000: episode: 5254, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002034, mae: 0.145172, mean_q: 0.209238\n",
      " 40346/50000: episode: 5255, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002952, mae: 0.148793, mean_q: 0.215169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40353/50000: episode: 5256, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002016, mae: 0.144981, mean_q: 0.210333\n",
      " 40361/50000: episode: 5257, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.004137, mae: 0.151253, mean_q: 0.214663\n",
      " 40376/50000: episode: 5258, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002648, mae: 0.148279, mean_q: 0.208052\n",
      " 40401/50000: episode: 5259, duration: 0.307s, episode steps:  25, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.002909, mae: 0.151027, mean_q: 0.213881\n",
      " 40407/50000: episode: 5260, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.003083, mae: 0.147487, mean_q: 0.210988\n",
      " 40413/50000: episode: 5261, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002378, mae: 0.145096, mean_q: 0.207613\n",
      " 40416/50000: episode: 5262, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002909, mae: 0.150811, mean_q: 0.215338\n",
      " 40427/50000: episode: 5263, duration: 0.144s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002418, mae: 0.146359, mean_q: 0.212169\n",
      " 40433/50000: episode: 5264, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002373, mae: 0.148622, mean_q: 0.214291\n",
      " 40437/50000: episode: 5265, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001981, mae: 0.148599, mean_q: 0.206774\n",
      " 40464/50000: episode: 5266, duration: 0.334s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.002954, mae: 0.152124, mean_q: 0.215640\n",
      " 40472/50000: episode: 5267, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [0.000, 3.000],  loss: 0.003069, mae: 0.148777, mean_q: 0.211360\n",
      " 40485/50000: episode: 5268, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003207, mae: 0.152234, mean_q: 0.213088\n",
      " 40489/50000: episode: 5269, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.001654, mae: 0.143224, mean_q: 0.207146\n",
      " 40493/50000: episode: 5270, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.004170, mae: 0.148538, mean_q: 0.209234\n",
      " 40505/50000: episode: 5271, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002184, mae: 0.149052, mean_q: 0.214264\n",
      " 40509/50000: episode: 5272, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002676, mae: 0.156011, mean_q: 0.224121\n",
      " 40515/50000: episode: 5273, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002883, mae: 0.150783, mean_q: 0.213767\n",
      " 40518/50000: episode: 5274, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.004311, mae: 0.154341, mean_q: 0.214932\n",
      " 40522/50000: episode: 5275, duration: 0.067s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.001865, mae: 0.146650, mean_q: 0.210901\n",
      " 40524/50000: episode: 5276, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001667, mae: 0.153219, mean_q: 0.220792\n",
      " 40529/50000: episode: 5277, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002729, mae: 0.144377, mean_q: 0.206413\n",
      " 40555/50000: episode: 5278, duration: 0.321s, episode steps:  26, steps per second:  81, episode reward:  1.000, mean reward:  0.038 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002587, mae: 0.148918, mean_q: 0.211245\n",
      " 40559/50000: episode: 5279, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002376, mae: 0.151587, mean_q: 0.215848\n",
      " 40562/50000: episode: 5280, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003881, mae: 0.154483, mean_q: 0.216298\n",
      " 40570/50000: episode: 5281, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002901, mae: 0.150907, mean_q: 0.214399\n",
      " 40576/50000: episode: 5282, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.003476, mae: 0.151574, mean_q: 0.215146\n",
      " 40581/50000: episode: 5283, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002958, mae: 0.151239, mean_q: 0.215627\n",
      " 40595/50000: episode: 5284, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002939, mae: 0.150493, mean_q: 0.213484\n",
      " 40602/50000: episode: 5285, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002397, mae: 0.152610, mean_q: 0.214505\n",
      " 40607/50000: episode: 5286, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002265, mae: 0.158360, mean_q: 0.222149\n",
      " 40614/50000: episode: 5287, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002361, mae: 0.149326, mean_q: 0.211285\n",
      " 40635/50000: episode: 5288, duration: 0.261s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.002782, mae: 0.153089, mean_q: 0.216863\n",
      " 40639/50000: episode: 5289, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003158, mae: 0.149426, mean_q: 0.208877\n",
      " 40646/50000: episode: 5290, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002932, mae: 0.151422, mean_q: 0.211773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40649/50000: episode: 5291, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004525, mae: 0.154659, mean_q: 0.214730\n",
      " 40656/50000: episode: 5292, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002825, mae: 0.152207, mean_q: 0.214234\n",
      " 40670/50000: episode: 5293, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.002489, mae: 0.149659, mean_q: 0.209501\n",
      " 40677/50000: episode: 5294, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004569, mae: 0.156430, mean_q: 0.215982\n",
      " 40682/50000: episode: 5295, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002794, mae: 0.153045, mean_q: 0.212488\n",
      " 40684/50000: episode: 5296, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005640, mae: 0.155986, mean_q: 0.212411\n",
      " 40690/50000: episode: 5297, duration: 0.089s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003314, mae: 0.148631, mean_q: 0.205927\n",
      " 40705/50000: episode: 5298, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003178, mae: 0.152222, mean_q: 0.213300\n",
      " 40719/50000: episode: 5299, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.003091, mae: 0.154528, mean_q: 0.215595\n",
      " 40727/50000: episode: 5300, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [1.000, 3.000],  loss: 0.003785, mae: 0.151725, mean_q: 0.211995\n",
      " 40729/50000: episode: 5301, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001400, mae: 0.142892, mean_q: 0.204216\n",
      " 40745/50000: episode: 5302, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.003244, mae: 0.148016, mean_q: 0.209937\n",
      " 40759/50000: episode: 5303, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003139, mae: 0.148818, mean_q: 0.212964\n",
      " 40766/50000: episode: 5304, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002713, mae: 0.146925, mean_q: 0.207680\n",
      " 40770/50000: episode: 5305, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.001830, mae: 0.145987, mean_q: 0.212739\n",
      " 40773/50000: episode: 5306, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002041, mae: 0.145217, mean_q: 0.206784\n",
      " 40787/50000: episode: 5307, duration: 0.181s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.003526, mae: 0.149902, mean_q: 0.210656\n",
      " 40795/50000: episode: 5308, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.004176, mae: 0.153045, mean_q: 0.212219\n",
      " 40804/50000: episode: 5309, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003105, mae: 0.148251, mean_q: 0.210232\n",
      " 40809/50000: episode: 5310, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002672, mae: 0.144117, mean_q: 0.206966\n",
      " 40824/50000: episode: 5311, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002154, mae: 0.147485, mean_q: 0.209372\n",
      " 40827/50000: episode: 5312, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003922, mae: 0.145662, mean_q: 0.207419\n",
      " 40845/50000: episode: 5313, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.944 [0.000, 3.000],  loss: 0.002811, mae: 0.146939, mean_q: 0.208367\n",
      " 40855/50000: episode: 5314, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002618, mae: 0.145068, mean_q: 0.204789\n",
      " 40857/50000: episode: 5315, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002222, mae: 0.150214, mean_q: 0.211172\n",
      " 40865/50000: episode: 5316, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002197, mae: 0.146395, mean_q: 0.207101\n",
      " 40869/50000: episode: 5317, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.004143, mae: 0.153256, mean_q: 0.213117\n",
      " 40871/50000: episode: 5318, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002928, mae: 0.152080, mean_q: 0.212704\n",
      " 40878/50000: episode: 5319, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002201, mae: 0.150069, mean_q: 0.210397\n",
      " 40891/50000: episode: 5320, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002926, mae: 0.151161, mean_q: 0.212506\n",
      " 40901/50000: episode: 5321, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.003453, mae: 0.154086, mean_q: 0.217393\n",
      " 40915/50000: episode: 5322, duration: 0.181s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002861, mae: 0.150390, mean_q: 0.214472\n",
      " 40918/50000: episode: 5323, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005632, mae: 0.164381, mean_q: 0.224913\n",
      " 40927/50000: episode: 5324, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003415, mae: 0.154592, mean_q: 0.219060\n",
      " 40929/50000: episode: 5325, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002074, mae: 0.154239, mean_q: 0.220300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40931/50000: episode: 5326, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002116, mae: 0.147788, mean_q: 0.214764\n",
      " 40934/50000: episode: 5327, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003061, mae: 0.151680, mean_q: 0.217368\n",
      " 40943/50000: episode: 5328, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002631, mae: 0.148318, mean_q: 0.217729\n",
      " 40954/50000: episode: 5329, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002300, mae: 0.144913, mean_q: 0.212306\n",
      " 40956/50000: episode: 5330, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002652, mae: 0.150965, mean_q: 0.214855\n",
      " 40967/50000: episode: 5331, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.003773, mae: 0.155300, mean_q: 0.222158\n",
      " 40972/50000: episode: 5332, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.003566, mae: 0.151025, mean_q: 0.214636\n",
      " 40982/50000: episode: 5333, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003492, mae: 0.147900, mean_q: 0.212075\n",
      " 40985/50000: episode: 5334, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003508, mae: 0.152300, mean_q: 0.217417\n",
      " 40993/50000: episode: 5335, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003547, mae: 0.154978, mean_q: 0.217321\n",
      " 40997/50000: episode: 5336, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.005132, mae: 0.153796, mean_q: 0.215008\n",
      " 41005/50000: episode: 5337, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003420, mae: 0.156160, mean_q: 0.219041\n",
      " 41013/50000: episode: 5338, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002421, mae: 0.154838, mean_q: 0.219428\n",
      " 41015/50000: episode: 5339, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002186, mae: 0.153879, mean_q: 0.217782\n",
      " 41023/50000: episode: 5340, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002170, mae: 0.147662, mean_q: 0.210961\n",
      " 41030/50000: episode: 5341, duration: 0.102s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002374, mae: 0.152276, mean_q: 0.212627\n",
      " 41034/50000: episode: 5342, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003216, mae: 0.153200, mean_q: 0.215218\n",
      " 41041/50000: episode: 5343, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.001986, mae: 0.150180, mean_q: 0.211246\n",
      " 41050/50000: episode: 5344, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002522, mae: 0.147454, mean_q: 0.205740\n",
      " 41054/50000: episode: 5345, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003312, mae: 0.149695, mean_q: 0.211746\n",
      " 41071/50000: episode: 5346, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.059 [0.000, 3.000],  loss: 0.003649, mae: 0.154652, mean_q: 0.218321\n",
      " 41084/50000: episode: 5347, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.003070, mae: 0.145483, mean_q: 0.211452\n",
      " 41093/50000: episode: 5348, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003126, mae: 0.145193, mean_q: 0.210212\n",
      " 41097/50000: episode: 5349, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.005526, mae: 0.161480, mean_q: 0.221753\n",
      " 41101/50000: episode: 5350, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005394, mae: 0.155425, mean_q: 0.216211\n",
      " 41105/50000: episode: 5351, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003475, mae: 0.149486, mean_q: 0.212507\n",
      " 41108/50000: episode: 5352, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.003280, mae: 0.147705, mean_q: 0.211935\n",
      " 41117/50000: episode: 5353, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002640, mae: 0.147411, mean_q: 0.206578\n",
      " 41121/50000: episode: 5354, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.003709, mae: 0.161166, mean_q: 0.223474\n",
      " 41125/50000: episode: 5355, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003431, mae: 0.149562, mean_q: 0.209817\n",
      " 41127/50000: episode: 5356, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001750, mae: 0.158370, mean_q: 0.222683\n",
      " 41132/50000: episode: 5357, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.004704, mae: 0.155012, mean_q: 0.219109\n",
      " 41140/50000: episode: 5358, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003788, mae: 0.151110, mean_q: 0.212695\n",
      " 41142/50000: episode: 5359, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003734, mae: 0.158763, mean_q: 0.221945\n",
      " 41154/50000: episode: 5360, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002706, mae: 0.148663, mean_q: 0.209865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41162/50000: episode: 5361, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 2.000],  loss: 0.003263, mae: 0.150981, mean_q: 0.214308\n",
      " 41167/50000: episode: 5362, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002119, mae: 0.148584, mean_q: 0.210864\n",
      " 41178/50000: episode: 5363, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.003470, mae: 0.151393, mean_q: 0.214290\n",
      " 41183/50000: episode: 5364, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003760, mae: 0.153328, mean_q: 0.220094\n",
      " 41193/50000: episode: 5365, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002616, mae: 0.150822, mean_q: 0.213293\n",
      " 41203/50000: episode: 5366, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.003967, mae: 0.161301, mean_q: 0.226606\n",
      " 41210/50000: episode: 5367, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002721, mae: 0.152816, mean_q: 0.219741\n",
      " 41213/50000: episode: 5368, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002426, mae: 0.146870, mean_q: 0.214422\n",
      " 41219/50000: episode: 5369, duration: 0.121s, episode steps:   6, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003370, mae: 0.147948, mean_q: 0.214724\n",
      " 41232/50000: episode: 5370, duration: 0.179s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.004170, mae: 0.152341, mean_q: 0.215538\n",
      " 41248/50000: episode: 5371, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.003031, mae: 0.148014, mean_q: 0.208533\n",
      " 41252/50000: episode: 5372, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003321, mae: 0.145644, mean_q: 0.209001\n",
      " 41256/50000: episode: 5373, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.001950, mae: 0.143714, mean_q: 0.206086\n",
      " 41259/50000: episode: 5374, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.002228, mae: 0.144285, mean_q: 0.209886\n",
      " 41262/50000: episode: 5375, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005775, mae: 0.157803, mean_q: 0.219758\n",
      " 41272/50000: episode: 5376, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002364, mae: 0.150384, mean_q: 0.213442\n",
      " 41275/50000: episode: 5377, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.004424, mae: 0.161062, mean_q: 0.225234\n",
      " 41284/50000: episode: 5378, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.002505, mae: 0.152564, mean_q: 0.214255\n",
      " 41295/50000: episode: 5379, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.003032, mae: 0.151780, mean_q: 0.215791\n",
      " 41302/50000: episode: 5380, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003534, mae: 0.151402, mean_q: 0.213500\n",
      " 41304/50000: episode: 5381, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002543, mae: 0.155691, mean_q: 0.219880\n",
      " 41306/50000: episode: 5382, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002978, mae: 0.156046, mean_q: 0.216602\n",
      " 41308/50000: episode: 5383, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002583, mae: 0.155041, mean_q: 0.218473\n",
      " 41312/50000: episode: 5384, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002541, mae: 0.154346, mean_q: 0.219425\n",
      " 41321/50000: episode: 5385, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003488, mae: 0.156359, mean_q: 0.219051\n",
      " 41325/50000: episode: 5386, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003195, mae: 0.152575, mean_q: 0.212161\n",
      " 41329/50000: episode: 5387, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002843, mae: 0.150125, mean_q: 0.213958\n",
      " 41332/50000: episode: 5388, duration: 0.047s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002258, mae: 0.143023, mean_q: 0.206687\n",
      " 41339/50000: episode: 5389, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003398, mae: 0.151636, mean_q: 0.211637\n",
      " 41343/50000: episode: 5390, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001740, mae: 0.154065, mean_q: 0.216253\n",
      " 41348/50000: episode: 5391, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [0.000, 3.000],  loss: 0.002804, mae: 0.157831, mean_q: 0.221533\n",
      " 41358/50000: episode: 5392, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.003021, mae: 0.153218, mean_q: 0.216053\n",
      " 41381/50000: episode: 5393, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002493, mae: 0.152474, mean_q: 0.215022\n",
      " 41403/50000: episode: 5394, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003019, mae: 0.148427, mean_q: 0.211343\n",
      " 41411/50000: episode: 5395, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002985, mae: 0.150065, mean_q: 0.217981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41421/50000: episode: 5396, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003116, mae: 0.157895, mean_q: 0.219800\n",
      " 41432/50000: episode: 5397, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003078, mae: 0.155377, mean_q: 0.216076\n",
      " 41439/50000: episode: 5398, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002821, mae: 0.148444, mean_q: 0.208961\n",
      " 41441/50000: episode: 5399, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002092, mae: 0.149099, mean_q: 0.210783\n",
      " 41444/50000: episode: 5400, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.001515, mae: 0.152531, mean_q: 0.209749\n",
      " 41446/50000: episode: 5401, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001974, mae: 0.146643, mean_q: 0.209248\n",
      " 41451/50000: episode: 5402, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002774, mae: 0.150585, mean_q: 0.216188\n",
      " 41454/50000: episode: 5403, duration: 0.053s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002833, mae: 0.145101, mean_q: 0.210029\n",
      " 41468/50000: episode: 5404, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003353, mae: 0.155651, mean_q: 0.220653\n",
      " 41472/50000: episode: 5405, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002696, mae: 0.159742, mean_q: 0.222751\n",
      " 41502/50000: episode: 5406, duration: 0.367s, episode steps:  30, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002747, mae: 0.153229, mean_q: 0.215538\n",
      " 41513/50000: episode: 5407, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.727 [0.000, 3.000],  loss: 0.002573, mae: 0.149918, mean_q: 0.209271\n",
      " 41517/50000: episode: 5408, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003130, mae: 0.146737, mean_q: 0.212874\n",
      " 41520/50000: episode: 5409, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003324, mae: 0.156751, mean_q: 0.222403\n",
      " 41523/50000: episode: 5410, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001560, mae: 0.150431, mean_q: 0.219793\n",
      " 41528/50000: episode: 5411, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002527, mae: 0.154073, mean_q: 0.219586\n",
      " 41530/50000: episode: 5412, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001847, mae: 0.153223, mean_q: 0.220798\n",
      " 41533/50000: episode: 5413, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003368, mae: 0.160914, mean_q: 0.226113\n",
      " 41545/50000: episode: 5414, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.003333, mae: 0.158462, mean_q: 0.220616\n",
      " 41556/50000: episode: 5415, duration: 0.146s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003163, mae: 0.154800, mean_q: 0.218007\n",
      " 41563/50000: episode: 5416, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002378, mae: 0.154352, mean_q: 0.214929\n",
      " 41568/50000: episode: 5417, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002451, mae: 0.152797, mean_q: 0.216789\n",
      " 41570/50000: episode: 5418, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003099, mae: 0.157263, mean_q: 0.219147\n",
      " 41577/50000: episode: 5419, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [1.000, 3.000],  loss: 0.003677, mae: 0.152841, mean_q: 0.218313\n",
      " 41585/50000: episode: 5420, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003725, mae: 0.145526, mean_q: 0.215526\n",
      " 41587/50000: episode: 5421, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004239, mae: 0.151425, mean_q: 0.218656\n",
      " 41592/50000: episode: 5422, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.004506, mae: 0.151639, mean_q: 0.216245\n",
      " 41594/50000: episode: 5423, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002347, mae: 0.147950, mean_q: 0.214742\n",
      " 41596/50000: episode: 5424, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002620, mae: 0.149759, mean_q: 0.212299\n",
      " 41600/50000: episode: 5425, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002718, mae: 0.152795, mean_q: 0.214145\n",
      " 41606/50000: episode: 5426, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  1.000, mean reward:  0.167 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002515, mae: 0.153254, mean_q: 0.219478\n",
      " 41613/50000: episode: 5427, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 3.000],  loss: 0.002703, mae: 0.154828, mean_q: 0.216070\n",
      " 41630/50000: episode: 5428, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.003280, mae: 0.150345, mean_q: 0.213796\n",
      " 41636/50000: episode: 5429, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002763, mae: 0.156432, mean_q: 0.222656\n",
      " 41645/50000: episode: 5430, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002609, mae: 0.149132, mean_q: 0.212737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41664/50000: episode: 5431, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.105 [0.000, 3.000],  loss: 0.002865, mae: 0.153786, mean_q: 0.219970\n",
      " 41669/50000: episode: 5432, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002493, mae: 0.152976, mean_q: 0.220195\n",
      " 41704/50000: episode: 5433, duration: 0.424s, episode steps:  35, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.029 [0.000, 3.000],  loss: 0.002766, mae: 0.154253, mean_q: 0.220909\n",
      " 41713/50000: episode: 5434, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.002798, mae: 0.155730, mean_q: 0.214206\n",
      " 41718/50000: episode: 5435, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003989, mae: 0.167385, mean_q: 0.234109\n",
      " 41720/50000: episode: 5436, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002259, mae: 0.151385, mean_q: 0.211685\n",
      " 41722/50000: episode: 5437, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004654, mae: 0.161581, mean_q: 0.221262\n",
      " 41728/50000: episode: 5438, duration: 0.088s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002640, mae: 0.156628, mean_q: 0.223364\n",
      " 41736/50000: episode: 5439, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003081, mae: 0.160319, mean_q: 0.227849\n",
      " 41750/50000: episode: 5440, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.003181, mae: 0.158845, mean_q: 0.223646\n",
      " 41752/50000: episode: 5441, duration: 0.037s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.005604, mae: 0.169974, mean_q: 0.239616\n",
      " 41756/50000: episode: 5442, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004115, mae: 0.156214, mean_q: 0.222129\n",
      " 41760/50000: episode: 5443, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001905, mae: 0.156292, mean_q: 0.221069\n",
      " 41762/50000: episode: 5444, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.005524, mae: 0.164939, mean_q: 0.228786\n",
      " 41766/50000: episode: 5445, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003420, mae: 0.162588, mean_q: 0.228214\n",
      " 41773/50000: episode: 5446, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003104, mae: 0.158348, mean_q: 0.221754\n",
      " 41776/50000: episode: 5447, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002997, mae: 0.152944, mean_q: 0.215249\n",
      " 41781/50000: episode: 5448, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003126, mae: 0.150599, mean_q: 0.216730\n",
      " 41798/50000: episode: 5449, duration: 0.239s, episode steps:  17, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002665, mae: 0.153049, mean_q: 0.219171\n",
      " 41816/50000: episode: 5450, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002809, mae: 0.152086, mean_q: 0.218258\n",
      " 41820/50000: episode: 5451, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002610, mae: 0.152184, mean_q: 0.224747\n",
      " 41826/50000: episode: 5452, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002720, mae: 0.162443, mean_q: 0.230847\n",
      " 41844/50000: episode: 5453, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.003631, mae: 0.155905, mean_q: 0.217486\n",
      " 41851/50000: episode: 5454, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.005092, mae: 0.158102, mean_q: 0.221792\n",
      " 41856/50000: episode: 5455, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003706, mae: 0.161718, mean_q: 0.228823\n",
      " 41874/50000: episode: 5456, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.056 [0.000, 3.000],  loss: 0.002970, mae: 0.149252, mean_q: 0.215054\n",
      " 41882/50000: episode: 5457, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003098, mae: 0.152202, mean_q: 0.218182\n",
      " 41889/50000: episode: 5458, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003453, mae: 0.160447, mean_q: 0.225366\n",
      " 41897/50000: episode: 5459, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002617, mae: 0.152003, mean_q: 0.214648\n",
      " 41910/50000: episode: 5460, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.231 [0.000, 3.000],  loss: 0.003216, mae: 0.155862, mean_q: 0.218944\n",
      " 41915/50000: episode: 5461, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003003, mae: 0.158229, mean_q: 0.223058\n",
      " 41917/50000: episode: 5462, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003890, mae: 0.158565, mean_q: 0.225401\n",
      " 41933/50000: episode: 5463, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002585, mae: 0.153345, mean_q: 0.219039\n",
      " 41935/50000: episode: 5464, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001585, mae: 0.144694, mean_q: 0.211030\n",
      " 41947/50000: episode: 5465, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.003128, mae: 0.153185, mean_q: 0.217216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41957/50000: episode: 5466, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.003224, mae: 0.155468, mean_q: 0.215525\n",
      " 41960/50000: episode: 5467, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002662, mae: 0.154210, mean_q: 0.218754\n",
      " 41965/50000: episode: 5468, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002659, mae: 0.156345, mean_q: 0.221261\n",
      " 41967/50000: episode: 5469, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003747, mae: 0.157436, mean_q: 0.219801\n",
      " 41981/50000: episode: 5470, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003161, mae: 0.152370, mean_q: 0.214232\n",
      " 41987/50000: episode: 5471, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003621, mae: 0.155932, mean_q: 0.219402\n",
      " 41992/50000: episode: 5472, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003087, mae: 0.155438, mean_q: 0.218358\n",
      " 41998/50000: episode: 5473, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003277, mae: 0.157779, mean_q: 0.221049\n",
      " 42002/50000: episode: 5474, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.003544, mae: 0.159384, mean_q: 0.220125\n",
      " 42004/50000: episode: 5475, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003568, mae: 0.154433, mean_q: 0.215452\n",
      " 42010/50000: episode: 5476, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003027, mae: 0.151249, mean_q: 0.214841\n",
      " 42012/50000: episode: 5477, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004908, mae: 0.154065, mean_q: 0.217319\n",
      " 42028/50000: episode: 5478, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.003420, mae: 0.150577, mean_q: 0.213811\n",
      " 42043/50000: episode: 5479, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.002644, mae: 0.154021, mean_q: 0.222240\n",
      " 42045/50000: episode: 5480, duration: 0.043s, episode steps:   2, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001703, mae: 0.144885, mean_q: 0.214549\n",
      " 42050/50000: episode: 5481, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003289, mae: 0.156899, mean_q: 0.223337\n",
      " 42056/50000: episode: 5482, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003984, mae: 0.154919, mean_q: 0.219209\n",
      " 42073/50000: episode: 5483, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [0.000, 3.000],  loss: 0.002870, mae: 0.150112, mean_q: 0.214144\n",
      " 42077/50000: episode: 5484, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001992, mae: 0.149718, mean_q: 0.215426\n",
      " 42079/50000: episode: 5485, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003761, mae: 0.156321, mean_q: 0.224407\n",
      " 42085/50000: episode: 5486, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003110, mae: 0.150296, mean_q: 0.213293\n",
      " 42095/50000: episode: 5487, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003195, mae: 0.149523, mean_q: 0.213866\n",
      " 42101/50000: episode: 5488, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002879, mae: 0.142986, mean_q: 0.204623\n",
      " 42110/50000: episode: 5489, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.003296, mae: 0.148838, mean_q: 0.211427\n",
      " 42136/50000: episode: 5490, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.003065, mae: 0.151543, mean_q: 0.213570\n",
      " 42151/50000: episode: 5491, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003452, mae: 0.151937, mean_q: 0.213890\n",
      " 42155/50000: episode: 5492, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.001932, mae: 0.145281, mean_q: 0.208593\n",
      " 42161/50000: episode: 5493, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002303, mae: 0.158189, mean_q: 0.226263\n",
      " 42170/50000: episode: 5494, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002301, mae: 0.155930, mean_q: 0.224562\n",
      " 42175/50000: episode: 5495, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.004240, mae: 0.155463, mean_q: 0.224480\n",
      " 42184/50000: episode: 5496, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002592, mae: 0.153422, mean_q: 0.218410\n",
      " 42186/50000: episode: 5497, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002325, mae: 0.156570, mean_q: 0.221198\n",
      " 42196/50000: episode: 5498, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.700 [0.000, 3.000],  loss: 0.003243, mae: 0.156914, mean_q: 0.220148\n",
      " 42204/50000: episode: 5499, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002034, mae: 0.151398, mean_q: 0.215575\n",
      " 42213/50000: episode: 5500, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002680, mae: 0.155859, mean_q: 0.219934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42219/50000: episode: 5501, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.003435, mae: 0.154747, mean_q: 0.216614\n",
      " 42221/50000: episode: 5502, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001150, mae: 0.150368, mean_q: 0.215180\n",
      " 42225/50000: episode: 5503, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003686, mae: 0.159684, mean_q: 0.222671\n",
      " 42230/50000: episode: 5504, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003279, mae: 0.153851, mean_q: 0.212139\n",
      " 42249/50000: episode: 5505, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002699, mae: 0.151671, mean_q: 0.216819\n",
      " 42252/50000: episode: 5506, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002720, mae: 0.154533, mean_q: 0.220051\n",
      " 42264/50000: episode: 5507, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002868, mae: 0.151603, mean_q: 0.213415\n",
      " 42274/50000: episode: 5508, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002049, mae: 0.148856, mean_q: 0.210673\n",
      " 42280/50000: episode: 5509, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [1.000, 3.000],  loss: 0.004294, mae: 0.158264, mean_q: 0.222067\n",
      " 42286/50000: episode: 5510, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002570, mae: 0.153609, mean_q: 0.219340\n",
      " 42294/50000: episode: 5511, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002362, mae: 0.149872, mean_q: 0.215449\n",
      " 42296/50000: episode: 5512, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002238, mae: 0.144207, mean_q: 0.213796\n",
      " 42299/50000: episode: 5513, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001736, mae: 0.142977, mean_q: 0.209556\n",
      " 42328/50000: episode: 5514, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.003269, mae: 0.153153, mean_q: 0.215524\n",
      " 42344/50000: episode: 5515, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.002571, mae: 0.154431, mean_q: 0.217688\n",
      " 42352/50000: episode: 5516, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002220, mae: 0.149662, mean_q: 0.213267\n",
      " 42357/50000: episode: 5517, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002885, mae: 0.149653, mean_q: 0.212900\n",
      " 42368/50000: episode: 5518, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002574, mae: 0.149902, mean_q: 0.215107\n",
      " 42375/50000: episode: 5519, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003278, mae: 0.149289, mean_q: 0.214775\n",
      " 42381/50000: episode: 5520, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003628, mae: 0.153371, mean_q: 0.215224\n",
      " 42389/50000: episode: 5521, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003063, mae: 0.150350, mean_q: 0.213793\n",
      " 42394/50000: episode: 5522, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002776, mae: 0.150305, mean_q: 0.212617\n",
      " 42404/50000: episode: 5523, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.003038, mae: 0.151011, mean_q: 0.214923\n",
      " 42417/50000: episode: 5524, duration: 0.170s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003151, mae: 0.153824, mean_q: 0.218169\n",
      " 42425/50000: episode: 5525, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001944, mae: 0.152397, mean_q: 0.218044\n",
      " 42430/50000: episode: 5526, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.003269, mae: 0.152048, mean_q: 0.214899\n",
      " 42434/50000: episode: 5527, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002106, mae: 0.146186, mean_q: 0.211004\n",
      " 42436/50000: episode: 5528, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003504, mae: 0.154775, mean_q: 0.224812\n",
      " 42441/50000: episode: 5529, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003310, mae: 0.151018, mean_q: 0.216242\n",
      " 42447/50000: episode: 5530, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002824, mae: 0.154833, mean_q: 0.218395\n",
      " 42455/50000: episode: 5531, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002564, mae: 0.154675, mean_q: 0.216387\n",
      " 42465/50000: episode: 5532, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003427, mae: 0.153733, mean_q: 0.215256\n",
      " 42483/50000: episode: 5533, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward:  1.000, mean reward:  0.056 [ 0.000,  1.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002135, mae: 0.147697, mean_q: 0.211606\n",
      " 42487/50000: episode: 5534, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.003770, mae: 0.159223, mean_q: 0.222692\n",
      " 42492/50000: episode: 5535, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.003509, mae: 0.158392, mean_q: 0.221591\n",
      " 42496/50000: episode: 5536, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002564, mae: 0.153812, mean_q: 0.214948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42501/50000: episode: 5537, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.003069, mae: 0.153660, mean_q: 0.216483\n",
      " 42503/50000: episode: 5538, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003302, mae: 0.151749, mean_q: 0.213620\n",
      " 42511/50000: episode: 5539, duration: 0.105s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003146, mae: 0.152994, mean_q: 0.215925\n",
      " 42516/50000: episode: 5540, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003181, mae: 0.150243, mean_q: 0.214572\n",
      " 42525/50000: episode: 5541, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [1.000, 3.000],  loss: 0.002979, mae: 0.160112, mean_q: 0.225347\n",
      " 42528/50000: episode: 5542, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003334, mae: 0.154549, mean_q: 0.221793\n",
      " 42532/50000: episode: 5543, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002386, mae: 0.159353, mean_q: 0.225609\n",
      " 42540/50000: episode: 5544, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002028, mae: 0.157611, mean_q: 0.223004\n",
      " 42544/50000: episode: 5545, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.003424, mae: 0.157472, mean_q: 0.219213\n",
      " 42549/50000: episode: 5546, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003485, mae: 0.154930, mean_q: 0.217178\n",
      " 42553/50000: episode: 5547, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.002262, mae: 0.153498, mean_q: 0.212480\n",
      " 42557/50000: episode: 5548, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001957, mae: 0.147618, mean_q: 0.207798\n",
      " 42560/50000: episode: 5549, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003145, mae: 0.148664, mean_q: 0.211878\n",
      " 42569/50000: episode: 5550, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002574, mae: 0.151668, mean_q: 0.212892\n",
      " 42572/50000: episode: 5551, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004225, mae: 0.160818, mean_q: 0.220097\n",
      " 42586/50000: episode: 5552, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003329, mae: 0.151749, mean_q: 0.215710\n",
      " 42588/50000: episode: 5553, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002509, mae: 0.149086, mean_q: 0.215528\n",
      " 42596/50000: episode: 5554, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [0.000, 3.000],  loss: 0.003375, mae: 0.153216, mean_q: 0.215825\n",
      " 42604/50000: episode: 5555, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002945, mae: 0.156684, mean_q: 0.218099\n",
      " 42615/50000: episode: 5556, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002457, mae: 0.159648, mean_q: 0.221870\n",
      " 42620/50000: episode: 5557, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002454, mae: 0.156155, mean_q: 0.219168\n",
      " 42623/50000: episode: 5558, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003452, mae: 0.159394, mean_q: 0.221604\n",
      " 42658/50000: episode: 5559, duration: 0.427s, episode steps:  35, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.171 [0.000, 3.000],  loss: 0.002922, mae: 0.154277, mean_q: 0.218039\n",
      " 42661/50000: episode: 5560, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.004354, mae: 0.152557, mean_q: 0.220861\n",
      " 42663/50000: episode: 5561, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.007543, mae: 0.159197, mean_q: 0.224191\n",
      " 42668/50000: episode: 5562, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.005163, mae: 0.153778, mean_q: 0.218479\n",
      " 42678/50000: episode: 5563, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003643, mae: 0.158411, mean_q: 0.221965\n",
      " 42687/50000: episode: 5564, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003183, mae: 0.152742, mean_q: 0.214843\n",
      " 42696/50000: episode: 5565, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002703, mae: 0.153722, mean_q: 0.211865\n",
      " 42700/50000: episode: 5566, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002497, mae: 0.151062, mean_q: 0.214178\n",
      " 42703/50000: episode: 5567, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002007, mae: 0.143887, mean_q: 0.210889\n",
      " 42709/50000: episode: 5568, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004444, mae: 0.159335, mean_q: 0.220007\n",
      " 42715/50000: episode: 5569, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003852, mae: 0.155106, mean_q: 0.219343\n",
      " 42722/50000: episode: 5570, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003593, mae: 0.151200, mean_q: 0.212830\n",
      " 42730/50000: episode: 5571, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003965, mae: 0.157120, mean_q: 0.218624\n",
      " 42735/50000: episode: 5572, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.002508, mae: 0.150468, mean_q: 0.210418\n",
      " 42739/50000: episode: 5573, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.003681, mae: 0.151631, mean_q: 0.213071\n",
      " 42748/50000: episode: 5574, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.004074, mae: 0.153376, mean_q: 0.214595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42753/50000: episode: 5575, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [0.000, 3.000],  loss: 0.002968, mae: 0.150627, mean_q: 0.213774\n",
      " 42773/50000: episode: 5576, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002712, mae: 0.151011, mean_q: 0.212496\n",
      " 42781/50000: episode: 5577, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003390, mae: 0.150831, mean_q: 0.211397\n",
      " 42794/50000: episode: 5578, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.003745, mae: 0.141908, mean_q: 0.206662\n",
      " 42797/50000: episode: 5579, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003477, mae: 0.156602, mean_q: 0.217593\n",
      " 42803/50000: episode: 5580, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002713, mae: 0.151041, mean_q: 0.214619\n",
      " 42808/50000: episode: 5581, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003578, mae: 0.157156, mean_q: 0.222420\n",
      " 42821/50000: episode: 5582, duration: 0.172s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002671, mae: 0.155032, mean_q: 0.218722\n",
      " 42828/50000: episode: 5583, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002111, mae: 0.148132, mean_q: 0.208329\n",
      " 42834/50000: episode: 5584, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002553, mae: 0.148638, mean_q: 0.208181\n",
      " 42837/50000: episode: 5585, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002616, mae: 0.149985, mean_q: 0.210086\n",
      " 42839/50000: episode: 5586, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002044, mae: 0.145935, mean_q: 0.209690\n",
      " 42844/50000: episode: 5587, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002273, mae: 0.143954, mean_q: 0.209108\n",
      " 42851/50000: episode: 5588, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002591, mae: 0.148713, mean_q: 0.210568\n",
      " 42859/50000: episode: 5589, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002696, mae: 0.150029, mean_q: 0.212044\n",
      " 42863/50000: episode: 5590, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003247, mae: 0.144458, mean_q: 0.209754\n",
      " 42865/50000: episode: 5591, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003840, mae: 0.146667, mean_q: 0.208945\n",
      " 42867/50000: episode: 5592, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002702, mae: 0.149996, mean_q: 0.209765\n",
      " 42873/50000: episode: 5593, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003159, mae: 0.156246, mean_q: 0.215684\n",
      " 42883/50000: episode: 5594, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003522, mae: 0.152797, mean_q: 0.214011\n",
      " 42893/50000: episode: 5595, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002258, mae: 0.149828, mean_q: 0.214179\n",
      " 42896/50000: episode: 5596, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003878, mae: 0.150162, mean_q: 0.217822\n",
      " 42899/50000: episode: 5597, duration: 0.052s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002013, mae: 0.151209, mean_q: 0.218561\n",
      " 42901/50000: episode: 5598, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003909, mae: 0.152695, mean_q: 0.225390\n",
      " 42905/50000: episode: 5599, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.001673, mae: 0.155898, mean_q: 0.225223\n",
      " 42907/50000: episode: 5600, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002571, mae: 0.151595, mean_q: 0.215206\n",
      " 42922/50000: episode: 5601, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002994, mae: 0.152826, mean_q: 0.215223\n",
      " 42924/50000: episode: 5602, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002306, mae: 0.149259, mean_q: 0.208639\n",
      " 42926/50000: episode: 5603, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004117, mae: 0.153407, mean_q: 0.212327\n",
      " 42936/50000: episode: 5604, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003265, mae: 0.157685, mean_q: 0.221696\n",
      " 42938/50000: episode: 5605, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001759, mae: 0.145020, mean_q: 0.210232\n",
      " 42943/50000: episode: 5606, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002931, mae: 0.149273, mean_q: 0.218910\n",
      " 42953/50000: episode: 5607, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002959, mae: 0.152442, mean_q: 0.221061\n",
      " 42960/50000: episode: 5608, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.003785, mae: 0.159715, mean_q: 0.225402\n",
      " 42964/50000: episode: 5609, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002931, mae: 0.160173, mean_q: 0.227356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42974/50000: episode: 5610, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003074, mae: 0.159361, mean_q: 0.222625\n",
      " 42980/50000: episode: 5611, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002565, mae: 0.152098, mean_q: 0.214649\n",
      " 43003/50000: episode: 5612, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002741, mae: 0.150291, mean_q: 0.213760\n",
      " 43014/50000: episode: 5613, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.002876, mae: 0.150234, mean_q: 0.216160\n",
      " 43023/50000: episode: 5614, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003033, mae: 0.151029, mean_q: 0.215179\n",
      " 43026/50000: episode: 5615, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.004009, mae: 0.160086, mean_q: 0.222931\n",
      " 43051/50000: episode: 5616, duration: 0.317s, episode steps:  25, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.003534, mae: 0.151989, mean_q: 0.218024\n",
      " 43066/50000: episode: 5617, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003249, mae: 0.152886, mean_q: 0.221909\n",
      " 43075/50000: episode: 5618, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.004391, mae: 0.159271, mean_q: 0.228881\n",
      " 43079/50000: episode: 5619, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002285, mae: 0.155440, mean_q: 0.223051\n",
      " 43088/50000: episode: 5620, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002834, mae: 0.160310, mean_q: 0.229217\n",
      " 43095/50000: episode: 5621, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002923, mae: 0.158811, mean_q: 0.226516\n",
      " 43106/50000: episode: 5622, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003109, mae: 0.153581, mean_q: 0.221131\n",
      " 43112/50000: episode: 5623, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002391, mae: 0.151949, mean_q: 0.218414\n",
      " 43119/50000: episode: 5624, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002851, mae: 0.153471, mean_q: 0.221407\n",
      " 43122/50000: episode: 5625, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002409, mae: 0.147192, mean_q: 0.214374\n",
      " 43129/50000: episode: 5626, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003076, mae: 0.151328, mean_q: 0.213532\n",
      " 43135/50000: episode: 5627, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002355, mae: 0.153765, mean_q: 0.218683\n",
      " 43137/50000: episode: 5628, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001964, mae: 0.151065, mean_q: 0.216257\n",
      " 43143/50000: episode: 5629, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.004865, mae: 0.160941, mean_q: 0.226690\n",
      " 43150/50000: episode: 5630, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002806, mae: 0.152978, mean_q: 0.216952\n",
      " 43152/50000: episode: 5631, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.005131, mae: 0.158246, mean_q: 0.221677\n",
      " 43158/50000: episode: 5632, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003671, mae: 0.157466, mean_q: 0.219626\n",
      " 43160/50000: episode: 5633, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001803, mae: 0.148678, mean_q: 0.214062\n",
      " 43174/50000: episode: 5634, duration: 0.184s, episode steps:  14, steps per second:  76, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.003180, mae: 0.157342, mean_q: 0.220061\n",
      " 43183/50000: episode: 5635, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002608, mae: 0.156361, mean_q: 0.221932\n",
      " 43185/50000: episode: 5636, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003496, mae: 0.157958, mean_q: 0.222766\n",
      " 43194/50000: episode: 5637, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002881, mae: 0.151996, mean_q: 0.214310\n",
      " 43196/50000: episode: 5638, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003922, mae: 0.150681, mean_q: 0.212821\n",
      " 43198/50000: episode: 5639, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001879, mae: 0.149571, mean_q: 0.211068\n",
      " 43204/50000: episode: 5640, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003791, mae: 0.155031, mean_q: 0.217271\n",
      " 43221/50000: episode: 5641, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.003251, mae: 0.149574, mean_q: 0.214647\n",
      " 43224/50000: episode: 5642, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002325, mae: 0.147796, mean_q: 0.213199\n",
      " 43240/50000: episode: 5643, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002875, mae: 0.152628, mean_q: 0.215364\n",
      " 43248/50000: episode: 5644, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002167, mae: 0.152667, mean_q: 0.214350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43264/50000: episode: 5645, duration: 0.212s, episode steps:  16, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002967, mae: 0.152979, mean_q: 0.217308\n",
      " 43281/50000: episode: 5646, duration: 0.211s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.003666, mae: 0.156012, mean_q: 0.219294\n",
      " 43283/50000: episode: 5647, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002624, mae: 0.149375, mean_q: 0.214292\n",
      " 43303/50000: episode: 5648, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002776, mae: 0.151805, mean_q: 0.218851\n",
      " 43306/50000: episode: 5649, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003625, mae: 0.153961, mean_q: 0.217888\n",
      " 43308/50000: episode: 5650, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002824, mae: 0.150871, mean_q: 0.210916\n",
      " 43318/50000: episode: 5651, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003191, mae: 0.149730, mean_q: 0.214845\n",
      " 43320/50000: episode: 5652, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001468, mae: 0.149329, mean_q: 0.223750\n",
      " 43324/50000: episode: 5653, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003661, mae: 0.165979, mean_q: 0.232370\n",
      " 43332/50000: episode: 5654, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003916, mae: 0.160541, mean_q: 0.226241\n",
      " 43339/50000: episode: 5655, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003407, mae: 0.152614, mean_q: 0.218420\n",
      " 43346/50000: episode: 5656, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002719, mae: 0.148687, mean_q: 0.211701\n",
      " 43350/50000: episode: 5657, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002946, mae: 0.155489, mean_q: 0.217250\n",
      " 43355/50000: episode: 5658, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002982, mae: 0.149142, mean_q: 0.211249\n",
      " 43360/50000: episode: 5659, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002821, mae: 0.154604, mean_q: 0.219176\n",
      " 43368/50000: episode: 5660, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [0.000, 3.000],  loss: 0.002888, mae: 0.148840, mean_q: 0.210779\n",
      " 43371/50000: episode: 5661, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002712, mae: 0.149428, mean_q: 0.215108\n",
      " 43374/50000: episode: 5662, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002347, mae: 0.146335, mean_q: 0.217277\n",
      " 43389/50000: episode: 5663, duration: 0.195s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003406, mae: 0.153945, mean_q: 0.219170\n",
      " 43393/50000: episode: 5664, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003059, mae: 0.162887, mean_q: 0.229135\n",
      " 43405/50000: episode: 5665, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002352, mae: 0.152961, mean_q: 0.217674\n",
      " 43409/50000: episode: 5666, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.002743, mae: 0.152764, mean_q: 0.219167\n",
      " 43412/50000: episode: 5667, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.001880, mae: 0.144294, mean_q: 0.210418\n",
      " 43414/50000: episode: 5668, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001513, mae: 0.156250, mean_q: 0.224883\n",
      " 43417/50000: episode: 5669, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003472, mae: 0.155693, mean_q: 0.222330\n",
      " 43423/50000: episode: 5670, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.004019, mae: 0.154022, mean_q: 0.220128\n",
      " 43435/50000: episode: 5671, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [0.000, 3.000],  loss: 0.003880, mae: 0.153375, mean_q: 0.215640\n",
      " 43442/50000: episode: 5672, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003580, mae: 0.154578, mean_q: 0.218507\n",
      " 43448/50000: episode: 5673, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002083, mae: 0.154193, mean_q: 0.216113\n",
      " 43452/50000: episode: 5674, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002444, mae: 0.152902, mean_q: 0.216810\n",
      " 43480/50000: episode: 5675, duration: 0.406s, episode steps:  28, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.003751, mae: 0.155557, mean_q: 0.219464\n",
      " 43483/50000: episode: 5676, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002686, mae: 0.152139, mean_q: 0.217538\n",
      " 43489/50000: episode: 5677, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001591, mae: 0.148649, mean_q: 0.213304\n",
      " 43505/50000: episode: 5678, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.003484, mae: 0.151370, mean_q: 0.214234\n",
      " 43507/50000: episode: 5679, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002237, mae: 0.139401, mean_q: 0.200479\n",
      " 43517/50000: episode: 5680, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 0.700 [0.000, 3.000],  loss: 0.002898, mae: 0.147270, mean_q: 0.211603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43538/50000: episode: 5681, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002759, mae: 0.153775, mean_q: 0.219619\n",
      " 43540/50000: episode: 5682, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002642, mae: 0.148775, mean_q: 0.206083\n",
      " 43558/50000: episode: 5683, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003291, mae: 0.152511, mean_q: 0.214327\n",
      " 43568/50000: episode: 5684, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003044, mae: 0.151261, mean_q: 0.218486\n",
      " 43579/50000: episode: 5685, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003344, mae: 0.151377, mean_q: 0.213141\n",
      " 43589/50000: episode: 5686, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002301, mae: 0.144964, mean_q: 0.206508\n",
      " 43597/50000: episode: 5687, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002550, mae: 0.149517, mean_q: 0.212171\n",
      " 43602/50000: episode: 5688, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003548, mae: 0.153327, mean_q: 0.216468\n",
      " 43607/50000: episode: 5689, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004094, mae: 0.157219, mean_q: 0.218715\n",
      " 43612/50000: episode: 5690, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002760, mae: 0.147148, mean_q: 0.209961\n",
      " 43616/50000: episode: 5691, duration: 0.067s, episode steps:   4, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002246, mae: 0.144578, mean_q: 0.206689\n",
      " 43623/50000: episode: 5692, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002106, mae: 0.146386, mean_q: 0.209715\n",
      " 43638/50000: episode: 5693, duration: 0.197s, episode steps:  15, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002548, mae: 0.154068, mean_q: 0.217308\n",
      " 43646/50000: episode: 5694, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002849, mae: 0.151901, mean_q: 0.214911\n",
      " 43651/50000: episode: 5695, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002383, mae: 0.145898, mean_q: 0.203627\n",
      " 43656/50000: episode: 5696, duration: 0.080s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003975, mae: 0.155788, mean_q: 0.214033\n",
      " 43663/50000: episode: 5697, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002954, mae: 0.149046, mean_q: 0.212158\n",
      " 43669/50000: episode: 5698, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003522, mae: 0.147783, mean_q: 0.211296\n",
      " 43673/50000: episode: 5699, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004299, mae: 0.152256, mean_q: 0.215184\n",
      " 43675/50000: episode: 5700, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.008720, mae: 0.169615, mean_q: 0.230941\n",
      " 43683/50000: episode: 5701, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002908, mae: 0.151434, mean_q: 0.213079\n",
      " 43701/50000: episode: 5702, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002869, mae: 0.150661, mean_q: 0.212877\n",
      " 43708/50000: episode: 5703, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.002445, mae: 0.147526, mean_q: 0.207938\n",
      " 43722/50000: episode: 5704, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002778, mae: 0.147011, mean_q: 0.208822\n",
      " 43728/50000: episode: 5705, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002851, mae: 0.145839, mean_q: 0.210776\n",
      " 43732/50000: episode: 5706, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [1.000, 3.000],  loss: 0.003638, mae: 0.145615, mean_q: 0.207746\n",
      " 43735/50000: episode: 5707, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004667, mae: 0.148274, mean_q: 0.212291\n",
      " 43745/50000: episode: 5708, duration: 0.138s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002373, mae: 0.152718, mean_q: 0.213896\n",
      " 43750/50000: episode: 5709, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003247, mae: 0.154573, mean_q: 0.215177\n",
      " 43759/50000: episode: 5710, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002780, mae: 0.145043, mean_q: 0.206654\n",
      " 43764/50000: episode: 5711, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002319, mae: 0.147736, mean_q: 0.211698\n",
      " 43766/50000: episode: 5712, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003119, mae: 0.150666, mean_q: 0.214355\n",
      " 43775/50000: episode: 5713, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002729, mae: 0.152053, mean_q: 0.214876\n",
      " 43783/50000: episode: 5714, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003811, mae: 0.147203, mean_q: 0.209927\n",
      " 43796/50000: episode: 5715, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.846 [0.000, 3.000],  loss: 0.002569, mae: 0.152525, mean_q: 0.215079\n",
      " 43798/50000: episode: 5716, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003357, mae: 0.153116, mean_q: 0.213365\n",
      " 43801/50000: episode: 5717, duration: 0.064s, episode steps:   3, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002293, mae: 0.150206, mean_q: 0.212496\n",
      " 43805/50000: episode: 5718, duration: 0.084s, episode steps:   4, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002114, mae: 0.143860, mean_q: 0.208828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43807/50000: episode: 5719, duration: 0.052s, episode steps:   2, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003082, mae: 0.154418, mean_q: 0.224570\n",
      " 43814/50000: episode: 5720, duration: 0.113s, episode steps:   7, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003017, mae: 0.148380, mean_q: 0.213502\n",
      " 43820/50000: episode: 5721, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002090, mae: 0.148990, mean_q: 0.212908\n",
      " 43828/50000: episode: 5722, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.375 [1.000, 3.000],  loss: 0.003370, mae: 0.155510, mean_q: 0.218049\n",
      " 43830/50000: episode: 5723, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004661, mae: 0.147102, mean_q: 0.210687\n",
      " 43832/50000: episode: 5724, duration: 0.049s, episode steps:   2, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004156, mae: 0.149039, mean_q: 0.209497\n",
      " 43837/50000: episode: 5725, duration: 0.105s, episode steps:   5, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002886, mae: 0.145433, mean_q: 0.207481\n",
      " 43846/50000: episode: 5726, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002676, mae: 0.137356, mean_q: 0.203925\n",
      " 43854/50000: episode: 5727, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.003050, mae: 0.143619, mean_q: 0.210414\n",
      " 43862/50000: episode: 5728, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002201, mae: 0.147804, mean_q: 0.210996\n",
      " 43878/50000: episode: 5729, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.003332, mae: 0.148877, mean_q: 0.210980\n",
      " 43880/50000: episode: 5730, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001675, mae: 0.146930, mean_q: 0.208449\n",
      " 43885/50000: episode: 5731, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.005149, mae: 0.153504, mean_q: 0.217228\n",
      " 43900/50000: episode: 5732, duration: 0.201s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003242, mae: 0.155230, mean_q: 0.219254\n",
      " 43905/50000: episode: 5733, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002578, mae: 0.151291, mean_q: 0.214537\n",
      " 43921/50000: episode: 5734, duration: 0.229s, episode steps:  16, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002847, mae: 0.146807, mean_q: 0.211093\n",
      " 43940/50000: episode: 5735, duration: 0.255s, episode steps:  19, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.003165, mae: 0.152531, mean_q: 0.214559\n",
      " 43957/50000: episode: 5736, duration: 0.239s, episode steps:  17, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002335, mae: 0.145954, mean_q: 0.210110\n",
      " 43969/50000: episode: 5737, duration: 0.175s, episode steps:  12, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002977, mae: 0.152728, mean_q: 0.218687\n",
      " 43971/50000: episode: 5738, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001965, mae: 0.145126, mean_q: 0.211378\n",
      " 43981/50000: episode: 5739, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002602, mae: 0.147072, mean_q: 0.209709\n",
      " 43992/50000: episode: 5740, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003504, mae: 0.152144, mean_q: 0.217654\n",
      " 44004/50000: episode: 5741, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002837, mae: 0.152028, mean_q: 0.220425\n",
      " 44006/50000: episode: 5742, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003555, mae: 0.154080, mean_q: 0.221583\n",
      " 44020/50000: episode: 5743, duration: 0.223s, episode steps:  14, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002849, mae: 0.151205, mean_q: 0.213723\n",
      " 44024/50000: episode: 5744, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002820, mae: 0.149132, mean_q: 0.207362\n",
      " 44029/50000: episode: 5745, duration: 0.084s, episode steps:   5, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002238, mae: 0.144742, mean_q: 0.205169\n",
      " 44033/50000: episode: 5746, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.003055, mae: 0.148432, mean_q: 0.211186\n",
      " 44043/50000: episode: 5747, duration: 0.192s, episode steps:  10, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002852, mae: 0.149609, mean_q: 0.212325\n",
      " 44052/50000: episode: 5748, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003231, mae: 0.153447, mean_q: 0.213682\n",
      " 44063/50000: episode: 5749, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.003004, mae: 0.150541, mean_q: 0.213784\n",
      " 44066/50000: episode: 5750, duration: 0.051s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002509, mae: 0.144608, mean_q: 0.204949\n",
      " 44076/50000: episode: 5751, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002486, mae: 0.147049, mean_q: 0.210300\n",
      " 44082/50000: episode: 5752, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003265, mae: 0.148962, mean_q: 0.213488\n",
      " 44086/50000: episode: 5753, duration: 0.095s, episode steps:   4, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002792, mae: 0.154304, mean_q: 0.217484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44094/50000: episode: 5754, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003204, mae: 0.148793, mean_q: 0.209801\n",
      " 44099/50000: episode: 5755, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.002915, mae: 0.152302, mean_q: 0.210900\n",
      " 44106/50000: episode: 5756, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002620, mae: 0.151471, mean_q: 0.214126\n",
      " 44108/50000: episode: 5757, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005251, mae: 0.152443, mean_q: 0.212760\n",
      " 44112/50000: episode: 5758, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004203, mae: 0.152299, mean_q: 0.217137\n",
      " 44114/50000: episode: 5759, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004217, mae: 0.161393, mean_q: 0.225233\n",
      " 44121/50000: episode: 5760, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003819, mae: 0.162209, mean_q: 0.226717\n",
      " 44126/50000: episode: 5761, duration: 0.076s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002340, mae: 0.155014, mean_q: 0.217279\n",
      " 44130/50000: episode: 5762, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002435, mae: 0.152990, mean_q: 0.216637\n",
      " 44145/50000: episode: 5763, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003071, mae: 0.148733, mean_q: 0.210301\n",
      " 44152/50000: episode: 5764, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002605, mae: 0.151926, mean_q: 0.214693\n",
      " 44158/50000: episode: 5765, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003184, mae: 0.149320, mean_q: 0.215050\n",
      " 44170/50000: episode: 5766, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002537, mae: 0.146651, mean_q: 0.210607\n",
      " 44181/50000: episode: 5767, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002991, mae: 0.152155, mean_q: 0.213854\n",
      " 44184/50000: episode: 5768, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002067, mae: 0.146503, mean_q: 0.208344\n",
      " 44195/50000: episode: 5769, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002984, mae: 0.151177, mean_q: 0.213372\n",
      " 44202/50000: episode: 5770, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003495, mae: 0.153909, mean_q: 0.215061\n",
      " 44223/50000: episode: 5771, duration: 0.264s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003050, mae: 0.148602, mean_q: 0.213404\n",
      " 44226/50000: episode: 5772, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002291, mae: 0.148819, mean_q: 0.210942\n",
      " 44234/50000: episode: 5773, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004552, mae: 0.158989, mean_q: 0.222501\n",
      " 44237/50000: episode: 5774, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003085, mae: 0.163972, mean_q: 0.227669\n",
      " 44242/50000: episode: 5775, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 1.000],  loss: 0.003503, mae: 0.151406, mean_q: 0.212551\n",
      " 44247/50000: episode: 5776, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003286, mae: 0.145665, mean_q: 0.204864\n",
      " 44251/50000: episode: 5777, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 3.000],  loss: 0.003926, mae: 0.151015, mean_q: 0.213400\n",
      " 44261/50000: episode: 5778, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003527, mae: 0.155308, mean_q: 0.216960\n",
      " 44270/50000: episode: 5779, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002300, mae: 0.149150, mean_q: 0.213022\n",
      " 44274/50000: episode: 5780, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003865, mae: 0.158083, mean_q: 0.224108\n",
      " 44283/50000: episode: 5781, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002719, mae: 0.146886, mean_q: 0.208100\n",
      " 44289/50000: episode: 5782, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.001962, mae: 0.152843, mean_q: 0.217050\n",
      " 44304/50000: episode: 5783, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002669, mae: 0.156796, mean_q: 0.220773\n",
      " 44306/50000: episode: 5784, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001393, mae: 0.145921, mean_q: 0.210701\n",
      " 44314/50000: episode: 5785, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.003747, mae: 0.153160, mean_q: 0.217215\n",
      " 44317/50000: episode: 5786, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.003093, mae: 0.156542, mean_q: 0.225389\n",
      " 44321/50000: episode: 5787, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.004324, mae: 0.154933, mean_q: 0.218030\n",
      " 44324/50000: episode: 5788, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001788, mae: 0.149634, mean_q: 0.214189\n",
      " 44330/50000: episode: 5789, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003320, mae: 0.146691, mean_q: 0.212423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44335/50000: episode: 5790, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003709, mae: 0.152622, mean_q: 0.216451\n",
      " 44342/50000: episode: 5791, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003277, mae: 0.147844, mean_q: 0.208305\n",
      " 44346/50000: episode: 5792, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002274, mae: 0.154045, mean_q: 0.217137\n",
      " 44352/50000: episode: 5793, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 2.000],  loss: 0.003780, mae: 0.152788, mean_q: 0.215295\n",
      " 44366/50000: episode: 5794, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002833, mae: 0.149462, mean_q: 0.211929\n",
      " 44368/50000: episode: 5795, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003679, mae: 0.149207, mean_q: 0.208143\n",
      " 44375/50000: episode: 5796, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.714 [0.000, 3.000],  loss: 0.003258, mae: 0.148317, mean_q: 0.212070\n",
      " 44388/50000: episode: 5797, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.077 [0.000, 3.000],  loss: 0.003448, mae: 0.147582, mean_q: 0.214277\n",
      " 44398/50000: episode: 5798, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002409, mae: 0.146878, mean_q: 0.213829\n",
      " 44407/50000: episode: 5799, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.003334, mae: 0.153524, mean_q: 0.217641\n",
      " 44420/50000: episode: 5800, duration: 0.168s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003308, mae: 0.155741, mean_q: 0.222708\n",
      " 44422/50000: episode: 5801, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004719, mae: 0.150110, mean_q: 0.212851\n",
      " 44431/50000: episode: 5802, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003545, mae: 0.149469, mean_q: 0.213357\n",
      " 44438/50000: episode: 5803, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003637, mae: 0.151338, mean_q: 0.217054\n",
      " 44449/50000: episode: 5804, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.003246, mae: 0.149413, mean_q: 0.213542\n",
      " 44465/50000: episode: 5805, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003150, mae: 0.155839, mean_q: 0.218592\n",
      " 44470/50000: episode: 5806, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 1.000],  loss: 0.005038, mae: 0.153501, mean_q: 0.217036\n",
      " 44472/50000: episode: 5807, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001939, mae: 0.150850, mean_q: 0.219600\n",
      " 44483/50000: episode: 5808, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002135, mae: 0.147196, mean_q: 0.210698\n",
      " 44489/50000: episode: 5809, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.002281, mae: 0.148830, mean_q: 0.210187\n",
      " 44492/50000: episode: 5810, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.004107, mae: 0.149807, mean_q: 0.212988\n",
      " 44498/50000: episode: 5811, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.004127, mae: 0.154002, mean_q: 0.217397\n",
      " 44505/50000: episode: 5812, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002874, mae: 0.150125, mean_q: 0.215545\n",
      " 44507/50000: episode: 5813, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004069, mae: 0.155784, mean_q: 0.218158\n",
      " 44512/50000: episode: 5814, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002601, mae: 0.152407, mean_q: 0.217300\n",
      " 44519/50000: episode: 5815, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002438, mae: 0.154440, mean_q: 0.221062\n",
      " 44524/50000: episode: 5816, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002985, mae: 0.152360, mean_q: 0.218038\n",
      " 44534/50000: episode: 5817, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002757, mae: 0.147311, mean_q: 0.211728\n",
      " 44538/50000: episode: 5818, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 2.000],  loss: 0.004478, mae: 0.158043, mean_q: 0.220282\n",
      " 44546/50000: episode: 5819, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.004780, mae: 0.159785, mean_q: 0.224172\n",
      " 44559/50000: episode: 5820, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003290, mae: 0.150311, mean_q: 0.214446\n",
      " 44566/50000: episode: 5821, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003473, mae: 0.149328, mean_q: 0.214946\n",
      " 44569/50000: episode: 5822, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003097, mae: 0.148787, mean_q: 0.214855\n",
      " 44582/50000: episode: 5823, duration: 0.172s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002933, mae: 0.149483, mean_q: 0.212511\n",
      " 44584/50000: episode: 5824, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002328, mae: 0.143864, mean_q: 0.207990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44607/50000: episode: 5825, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.002849, mae: 0.153380, mean_q: 0.214508\n",
      " 44612/50000: episode: 5826, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002788, mae: 0.150425, mean_q: 0.212651\n",
      " 44614/50000: episode: 5827, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.004313, mae: 0.159031, mean_q: 0.221573\n",
      " 44619/50000: episode: 5828, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004143, mae: 0.157292, mean_q: 0.222945\n",
      " 44630/50000: episode: 5829, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.003185, mae: 0.152206, mean_q: 0.214308\n",
      " 44635/50000: episode: 5830, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 3.000],  loss: 0.003803, mae: 0.149277, mean_q: 0.209945\n",
      " 44647/50000: episode: 5831, duration: 0.157s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.002811, mae: 0.150829, mean_q: 0.212733\n",
      " 44649/50000: episode: 5832, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004265, mae: 0.157284, mean_q: 0.219862\n",
      " 44671/50000: episode: 5833, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.045 [0.000, 3.000],  loss: 0.002800, mae: 0.145901, mean_q: 0.209882\n",
      " 44688/50000: episode: 5834, duration: 0.211s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002721, mae: 0.153737, mean_q: 0.217312\n",
      " 44691/50000: episode: 5835, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001237, mae: 0.144959, mean_q: 0.207055\n",
      " 44697/50000: episode: 5836, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.002728, mae: 0.155521, mean_q: 0.217160\n",
      " 44711/50000: episode: 5837, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.003222, mae: 0.156566, mean_q: 0.218230\n",
      " 44717/50000: episode: 5838, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002428, mae: 0.146868, mean_q: 0.210527\n",
      " 44720/50000: episode: 5839, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004143, mae: 0.154815, mean_q: 0.222265\n",
      " 44722/50000: episode: 5840, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001796, mae: 0.149288, mean_q: 0.212036\n",
      " 44727/50000: episode: 5841, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.003296, mae: 0.159550, mean_q: 0.221623\n",
      " 44737/50000: episode: 5842, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004354, mae: 0.159765, mean_q: 0.223111\n",
      " 44742/50000: episode: 5843, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.400 [0.000, 1.000],  loss: 0.005388, mae: 0.159788, mean_q: 0.222061\n",
      " 44752/50000: episode: 5844, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003533, mae: 0.150770, mean_q: 0.214028\n",
      " 44761/50000: episode: 5845, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002689, mae: 0.147356, mean_q: 0.212810\n",
      " 44768/50000: episode: 5846, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002938, mae: 0.152545, mean_q: 0.219555\n",
      " 44771/50000: episode: 5847, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.004082, mae: 0.156999, mean_q: 0.221445\n",
      " 44778/50000: episode: 5848, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.004048, mae: 0.156751, mean_q: 0.223542\n",
      " 44781/50000: episode: 5849, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002114, mae: 0.145099, mean_q: 0.209134\n",
      " 44785/50000: episode: 5850, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.004717, mae: 0.155205, mean_q: 0.219744\n",
      " 44796/50000: episode: 5851, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002379, mae: 0.151858, mean_q: 0.214755\n",
      " 44799/50000: episode: 5852, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003153, mae: 0.147396, mean_q: 0.209549\n",
      " 44804/50000: episode: 5853, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003014, mae: 0.151680, mean_q: 0.216160\n",
      " 44808/50000: episode: 5854, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002489, mae: 0.148537, mean_q: 0.212955\n",
      " 44817/50000: episode: 5855, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002187, mae: 0.149538, mean_q: 0.215700\n",
      " 44819/50000: episode: 5856, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002078, mae: 0.153031, mean_q: 0.220823\n",
      " 44827/50000: episode: 5857, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003426, mae: 0.151162, mean_q: 0.217631\n",
      " 44833/50000: episode: 5858, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002459, mae: 0.149959, mean_q: 0.211802\n",
      " 44841/50000: episode: 5859, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003043, mae: 0.149892, mean_q: 0.211229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44848/50000: episode: 5860, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002917, mae: 0.148906, mean_q: 0.210542\n",
      " 44857/50000: episode: 5861, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002305, mae: 0.145858, mean_q: 0.204813\n",
      " 44867/50000: episode: 5862, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.700 [0.000, 3.000],  loss: 0.003532, mae: 0.153728, mean_q: 0.215714\n",
      " 44870/50000: episode: 5863, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003131, mae: 0.152617, mean_q: 0.213603\n",
      " 44872/50000: episode: 5864, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003122, mae: 0.147450, mean_q: 0.206049\n",
      " 44879/50000: episode: 5865, duration: 0.099s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002719, mae: 0.144488, mean_q: 0.209221\n",
      " 44883/50000: episode: 5866, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [2.000, 3.000],  loss: 0.003747, mae: 0.147225, mean_q: 0.212011\n",
      " 44902/50000: episode: 5867, duration: 0.243s, episode steps:  19, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002833, mae: 0.149958, mean_q: 0.212829\n",
      " 44913/50000: episode: 5868, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002210, mae: 0.149169, mean_q: 0.210298\n",
      " 44921/50000: episode: 5869, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.625 [0.000, 2.000],  loss: 0.002748, mae: 0.154075, mean_q: 0.216087\n",
      " 44927/50000: episode: 5870, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002956, mae: 0.159153, mean_q: 0.223341\n",
      " 44952/50000: episode: 5871, duration: 0.311s, episode steps:  25, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002737, mae: 0.153524, mean_q: 0.219534\n",
      " 44954/50000: episode: 5872, duration: 0.044s, episode steps:   2, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001402, mae: 0.149699, mean_q: 0.217558\n",
      " 44967/50000: episode: 5873, duration: 0.168s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002831, mae: 0.154120, mean_q: 0.222244\n",
      " 44996/50000: episode: 5874, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.003072, mae: 0.156801, mean_q: 0.219695\n",
      " 44999/50000: episode: 5875, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002459, mae: 0.151112, mean_q: 0.213155\n",
      " 45006/50000: episode: 5876, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003288, mae: 0.153228, mean_q: 0.217341\n",
      " 45013/50000: episode: 5877, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.004025, mae: 0.159525, mean_q: 0.226735\n",
      " 45019/50000: episode: 5878, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002338, mae: 0.151444, mean_q: 0.213477\n",
      " 45025/50000: episode: 5879, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.002906, mae: 0.151042, mean_q: 0.217259\n",
      " 45031/50000: episode: 5880, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.833 [0.000, 3.000],  loss: 0.003424, mae: 0.154861, mean_q: 0.222338\n",
      " 45036/50000: episode: 5881, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003958, mae: 0.158780, mean_q: 0.225116\n",
      " 45047/50000: episode: 5882, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.003082, mae: 0.158431, mean_q: 0.222918\n",
      " 45052/50000: episode: 5883, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002669, mae: 0.157452, mean_q: 0.223047\n",
      " 45056/50000: episode: 5884, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 0.003741, mae: 0.159020, mean_q: 0.220263\n",
      " 45063/50000: episode: 5885, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.571 [0.000, 3.000],  loss: 0.003851, mae: 0.153018, mean_q: 0.217625\n",
      " 45071/50000: episode: 5886, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [1.000, 3.000],  loss: 0.002646, mae: 0.147854, mean_q: 0.213043\n",
      " 45090/50000: episode: 5887, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.053 [0.000, 3.000],  loss: 0.003376, mae: 0.157659, mean_q: 0.223164\n",
      " 45092/50000: episode: 5888, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002567, mae: 0.157641, mean_q: 0.226499\n",
      " 45097/50000: episode: 5889, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002543, mae: 0.155355, mean_q: 0.220602\n",
      " 45106/50000: episode: 5890, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003870, mae: 0.158099, mean_q: 0.225278\n",
      " 45112/50000: episode: 5891, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002732, mae: 0.152311, mean_q: 0.217890\n",
      " 45126/50000: episode: 5892, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002817, mae: 0.150012, mean_q: 0.216760\n",
      " 45144/50000: episode: 5893, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002347, mae: 0.153786, mean_q: 0.218959\n",
      " 45148/50000: episode: 5894, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002526, mae: 0.145561, mean_q: 0.213832\n",
      " 45154/50000: episode: 5895, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003069, mae: 0.148938, mean_q: 0.213380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45160/50000: episode: 5896, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003327, mae: 0.155041, mean_q: 0.220925\n",
      " 45178/50000: episode: 5897, duration: 0.238s, episode steps:  18, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002528, mae: 0.155729, mean_q: 0.221077\n",
      " 45186/50000: episode: 5898, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.002269, mae: 0.149713, mean_q: 0.218135\n",
      " 45188/50000: episode: 5899, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002095, mae: 0.147291, mean_q: 0.209827\n",
      " 45204/50000: episode: 5900, duration: 0.204s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003430, mae: 0.155960, mean_q: 0.219376\n",
      " 45206/50000: episode: 5901, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003241, mae: 0.152197, mean_q: 0.215944\n",
      " 45209/50000: episode: 5902, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002049, mae: 0.150879, mean_q: 0.218692\n",
      " 45213/50000: episode: 5903, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002970, mae: 0.154957, mean_q: 0.218697\n",
      " 45216/50000: episode: 5904, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.002190, mae: 0.148780, mean_q: 0.212082\n",
      " 45221/50000: episode: 5905, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003131, mae: 0.151932, mean_q: 0.217286\n",
      " 45234/50000: episode: 5906, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002408, mae: 0.148736, mean_q: 0.211556\n",
      " 45248/50000: episode: 5907, duration: 0.183s, episode steps:  14, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003855, mae: 0.157449, mean_q: 0.221987\n",
      " 45252/50000: episode: 5908, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002410, mae: 0.151723, mean_q: 0.214270\n",
      " 45264/50000: episode: 5909, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003386, mae: 0.153999, mean_q: 0.219496\n",
      " 45289/50000: episode: 5910, duration: 0.318s, episode steps:  25, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003105, mae: 0.154733, mean_q: 0.219965\n",
      " 45294/50000: episode: 5911, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.003688, mae: 0.159271, mean_q: 0.225672\n",
      " 45300/50000: episode: 5912, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.004088, mae: 0.156094, mean_q: 0.223436\n",
      " 45311/50000: episode: 5913, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003545, mae: 0.149176, mean_q: 0.214829\n",
      " 45322/50000: episode: 5914, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003079, mae: 0.152886, mean_q: 0.216763\n",
      " 45326/50000: episode: 5915, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003241, mae: 0.152840, mean_q: 0.220014\n",
      " 45333/50000: episode: 5916, duration: 0.099s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [1.000, 3.000],  loss: 0.003896, mae: 0.155310, mean_q: 0.218216\n",
      " 45341/50000: episode: 5917, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002722, mae: 0.153959, mean_q: 0.218435\n",
      " 45368/50000: episode: 5918, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.002994, mae: 0.156988, mean_q: 0.223542\n",
      " 45371/50000: episode: 5919, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003612, mae: 0.153831, mean_q: 0.223040\n",
      " 45374/50000: episode: 5920, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002465, mae: 0.148507, mean_q: 0.214641\n",
      " 45385/50000: episode: 5921, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002883, mae: 0.157723, mean_q: 0.221869\n",
      " 45399/50000: episode: 5922, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward:  1.000, mean reward:  0.071 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002568, mae: 0.151436, mean_q: 0.212438\n",
      " 45401/50000: episode: 5923, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003620, mae: 0.149661, mean_q: 0.214125\n",
      " 45403/50000: episode: 5924, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002803, mae: 0.149638, mean_q: 0.218413\n",
      " 45412/50000: episode: 5925, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003813, mae: 0.155175, mean_q: 0.219000\n",
      " 45414/50000: episode: 5926, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003476, mae: 0.157831, mean_q: 0.225547\n",
      " 45421/50000: episode: 5927, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.003122, mae: 0.160084, mean_q: 0.223749\n",
      " 45435/50000: episode: 5928, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.071 [1.000, 3.000],  loss: 0.002340, mae: 0.149913, mean_q: 0.214258\n",
      " 45438/50000: episode: 5929, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003228, mae: 0.155792, mean_q: 0.225243\n",
      " 45440/50000: episode: 5930, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002245, mae: 0.144525, mean_q: 0.211275\n",
      " 45444/50000: episode: 5931, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003564, mae: 0.159697, mean_q: 0.223770\n",
      " 45447/50000: episode: 5932, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003316, mae: 0.150589, mean_q: 0.213836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45456/50000: episode: 5933, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.004008, mae: 0.155060, mean_q: 0.218241\n",
      " 45473/50000: episode: 5934, duration: 0.214s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.118 [0.000, 3.000],  loss: 0.002485, mae: 0.146556, mean_q: 0.207601\n",
      " 45475/50000: episode: 5935, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003205, mae: 0.145315, mean_q: 0.207057\n",
      " 45480/50000: episode: 5936, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003050, mae: 0.145338, mean_q: 0.207140\n",
      " 45485/50000: episode: 5937, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003757, mae: 0.157069, mean_q: 0.224168\n",
      " 45487/50000: episode: 5938, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002315, mae: 0.152378, mean_q: 0.214949\n",
      " 45496/50000: episode: 5939, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002306, mae: 0.154623, mean_q: 0.216685\n",
      " 45504/50000: episode: 5940, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002506, mae: 0.155911, mean_q: 0.217467\n",
      " 45506/50000: episode: 5941, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002585, mae: 0.152642, mean_q: 0.210442\n",
      " 45527/50000: episode: 5942, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003957, mae: 0.158614, mean_q: 0.219997\n",
      " 45529/50000: episode: 5943, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002481, mae: 0.155056, mean_q: 0.227079\n",
      " 45542/50000: episode: 5944, duration: 0.219s, episode steps:  13, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002384, mae: 0.147904, mean_q: 0.216489\n",
      " 45547/50000: episode: 5945, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003350, mae: 0.153222, mean_q: 0.221188\n",
      " 45555/50000: episode: 5946, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003465, mae: 0.150313, mean_q: 0.216718\n",
      " 45562/50000: episode: 5947, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003124, mae: 0.154765, mean_q: 0.221698\n",
      " 45567/50000: episode: 5948, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002652, mae: 0.155983, mean_q: 0.222307\n",
      " 45590/50000: episode: 5949, duration: 0.285s, episode steps:  23, steps per second:  81, episode reward:  1.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 1.217 [0.000, 3.000],  loss: 0.003588, mae: 0.153592, mean_q: 0.217499\n",
      " 45598/50000: episode: 5950, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002128, mae: 0.151224, mean_q: 0.220489\n",
      " 45607/50000: episode: 5951, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.003807, mae: 0.161624, mean_q: 0.225170\n",
      " 45614/50000: episode: 5952, duration: 0.114s, episode steps:   7, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002605, mae: 0.149650, mean_q: 0.215431\n",
      " 45626/50000: episode: 5953, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002659, mae: 0.146318, mean_q: 0.210089\n",
      " 45639/50000: episode: 5954, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.003046, mae: 0.152890, mean_q: 0.218680\n",
      " 45643/50000: episode: 5955, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002477, mae: 0.148851, mean_q: 0.213117\n",
      " 45649/50000: episode: 5956, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002918, mae: 0.153790, mean_q: 0.221048\n",
      " 45653/50000: episode: 5957, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002353, mae: 0.153926, mean_q: 0.213079\n",
      " 45656/50000: episode: 5958, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001542, mae: 0.148658, mean_q: 0.213328\n",
      " 45660/50000: episode: 5959, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.004021, mae: 0.156585, mean_q: 0.222069\n",
      " 45663/50000: episode: 5960, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.333 [0.000, 1.000],  loss: 0.003299, mae: 0.148885, mean_q: 0.214414\n",
      " 45694/50000: episode: 5961, duration: 0.388s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.645 [0.000, 3.000],  loss: 0.002841, mae: 0.152458, mean_q: 0.219879\n",
      " 45698/50000: episode: 5962, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002237, mae: 0.148057, mean_q: 0.217142\n",
      " 45700/50000: episode: 5963, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004228, mae: 0.156482, mean_q: 0.225194\n",
      " 45703/50000: episode: 5964, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003461, mae: 0.151943, mean_q: 0.221048\n",
      " 45705/50000: episode: 5965, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001777, mae: 0.147540, mean_q: 0.215333\n",
      " 45717/50000: episode: 5966, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.003719, mae: 0.157295, mean_q: 0.219888\n",
      " 45730/50000: episode: 5967, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002696, mae: 0.148134, mean_q: 0.211159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45733/50000: episode: 5968, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002668, mae: 0.151927, mean_q: 0.219412\n",
      " 45744/50000: episode: 5969, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002294, mae: 0.152876, mean_q: 0.218656\n",
      " 45749/50000: episode: 5970, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003047, mae: 0.155478, mean_q: 0.219881\n",
      " 45753/50000: episode: 5971, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.005233, mae: 0.156299, mean_q: 0.219716\n",
      " 45759/50000: episode: 5972, duration: 0.095s, episode steps:   6, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002685, mae: 0.154468, mean_q: 0.218613\n",
      " 45765/50000: episode: 5973, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002410, mae: 0.152486, mean_q: 0.215877\n",
      " 45767/50000: episode: 5974, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002734, mae: 0.150021, mean_q: 0.217655\n",
      " 45772/50000: episode: 5975, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002864, mae: 0.152762, mean_q: 0.219322\n",
      " 45799/50000: episode: 5976, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002829, mae: 0.152314, mean_q: 0.214722\n",
      " 45801/50000: episode: 5977, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003559, mae: 0.155001, mean_q: 0.217346\n",
      " 45816/50000: episode: 5978, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002720, mae: 0.151994, mean_q: 0.214222\n",
      " 45818/50000: episode: 5979, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001944, mae: 0.146740, mean_q: 0.212381\n",
      " 45822/50000: episode: 5980, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001793, mae: 0.149420, mean_q: 0.215024\n",
      " 45824/50000: episode: 5981, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004571, mae: 0.156729, mean_q: 0.218669\n",
      " 45835/50000: episode: 5982, duration: 0.163s, episode steps:  11, steps per second:  67, episode reward:  1.000, mean reward:  0.091 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002714, mae: 0.153622, mean_q: 0.217482\n",
      " 45840/50000: episode: 5983, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.005218, mae: 0.153278, mean_q: 0.218597\n",
      " 45845/50000: episode: 5984, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.003603, mae: 0.156318, mean_q: 0.226749\n",
      " 45847/50000: episode: 5985, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003372, mae: 0.152026, mean_q: 0.222278\n",
      " 45850/50000: episode: 5986, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.002255, mae: 0.147220, mean_q: 0.213011\n",
      " 45855/50000: episode: 5987, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.002670, mae: 0.148813, mean_q: 0.214759\n",
      " 45861/50000: episode: 5988, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004354, mae: 0.158565, mean_q: 0.221967\n",
      " 45866/50000: episode: 5989, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003084, mae: 0.154623, mean_q: 0.219563\n",
      " 45871/50000: episode: 5990, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002123, mae: 0.150745, mean_q: 0.217389\n",
      " 45874/50000: episode: 5991, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004919, mae: 0.157065, mean_q: 0.228506\n",
      " 45878/50000: episode: 5992, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002452, mae: 0.146262, mean_q: 0.214301\n",
      " 45885/50000: episode: 5993, duration: 0.104s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003548, mae: 0.156393, mean_q: 0.222959\n",
      " 45888/50000: episode: 5994, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002339, mae: 0.154764, mean_q: 0.221662\n",
      " 45891/50000: episode: 5995, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002078, mae: 0.151978, mean_q: 0.221202\n",
      " 45896/50000: episode: 5996, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001834, mae: 0.152288, mean_q: 0.221578\n",
      " 45898/50000: episode: 5997, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003089, mae: 0.156680, mean_q: 0.225787\n",
      " 45907/50000: episode: 5998, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 3.000],  loss: 0.002483, mae: 0.153767, mean_q: 0.217822\n",
      " 45920/50000: episode: 5999, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003361, mae: 0.152096, mean_q: 0.213966\n",
      " 45935/50000: episode: 6000, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003074, mae: 0.152994, mean_q: 0.217040\n",
      " 45946/50000: episode: 6001, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003208, mae: 0.153937, mean_q: 0.218001\n",
      " 45956/50000: episode: 6002, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002708, mae: 0.152292, mean_q: 0.213353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45964/50000: episode: 6003, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002506, mae: 0.148030, mean_q: 0.210120\n",
      " 45975/50000: episode: 6004, duration: 0.146s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002866, mae: 0.149632, mean_q: 0.212714\n",
      " 45978/50000: episode: 6005, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002570, mae: 0.148266, mean_q: 0.216212\n",
      " 45980/50000: episode: 6006, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002853, mae: 0.159339, mean_q: 0.225098\n",
      " 45985/50000: episode: 6007, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003116, mae: 0.154442, mean_q: 0.218217\n",
      " 45988/50000: episode: 6008, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.003325, mae: 0.151520, mean_q: 0.212927\n",
      " 45995/50000: episode: 6009, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002722, mae: 0.153813, mean_q: 0.218606\n",
      " 45997/50000: episode: 6010, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002239, mae: 0.155057, mean_q: 0.218043\n",
      " 46003/50000: episode: 6011, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002548, mae: 0.152680, mean_q: 0.213528\n",
      " 46012/50000: episode: 6012, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003748, mae: 0.152854, mean_q: 0.217673\n",
      " 46026/50000: episode: 6013, duration: 0.176s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.071 [0.000, 3.000],  loss: 0.002590, mae: 0.145393, mean_q: 0.211957\n",
      " 46030/50000: episode: 6014, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002051, mae: 0.144219, mean_q: 0.213402\n",
      " 46056/50000: episode: 6015, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002771, mae: 0.150581, mean_q: 0.213894\n",
      " 46063/50000: episode: 6016, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002500, mae: 0.150414, mean_q: 0.210065\n",
      " 46070/50000: episode: 6017, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003145, mae: 0.153540, mean_q: 0.216906\n",
      " 46072/50000: episode: 6018, duration: 0.045s, episode steps:   2, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.003174, mae: 0.150729, mean_q: 0.212331\n",
      " 46087/50000: episode: 6019, duration: 0.191s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.733 [0.000, 3.000],  loss: 0.003221, mae: 0.153031, mean_q: 0.217642\n",
      " 46107/50000: episode: 6020, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003079, mae: 0.153451, mean_q: 0.215038\n",
      " 46114/50000: episode: 6021, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003706, mae: 0.155583, mean_q: 0.219696\n",
      " 46122/50000: episode: 6022, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.002483, mae: 0.147831, mean_q: 0.213095\n",
      " 46139/50000: episode: 6023, duration: 0.222s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002385, mae: 0.148149, mean_q: 0.214433\n",
      " 46157/50000: episode: 6024, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002477, mae: 0.152179, mean_q: 0.218733\n",
      " 46169/50000: episode: 6025, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002629, mae: 0.151743, mean_q: 0.216975\n",
      " 46176/50000: episode: 6026, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002812, mae: 0.148691, mean_q: 0.213857\n",
      " 46181/50000: episode: 6027, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002896, mae: 0.161214, mean_q: 0.231117\n",
      " 46183/50000: episode: 6028, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.005066, mae: 0.168852, mean_q: 0.237103\n",
      " 46194/50000: episode: 6029, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.003652, mae: 0.157296, mean_q: 0.220499\n",
      " 46201/50000: episode: 6030, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001983, mae: 0.147504, mean_q: 0.214270\n",
      " 46207/50000: episode: 6031, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002386, mae: 0.146247, mean_q: 0.215190\n",
      " 46216/50000: episode: 6032, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004452, mae: 0.153486, mean_q: 0.224774\n",
      " 46223/50000: episode: 6033, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.002915, mae: 0.152467, mean_q: 0.221914\n",
      " 46226/50000: episode: 6034, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002448, mae: 0.152668, mean_q: 0.221254\n",
      " 46238/50000: episode: 6035, duration: 0.157s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002666, mae: 0.153290, mean_q: 0.217624\n",
      " 46249/50000: episode: 6036, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003792, mae: 0.156212, mean_q: 0.216255\n",
      " 46258/50000: episode: 6037, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003211, mae: 0.151710, mean_q: 0.211098\n",
      " 46261/50000: episode: 6038, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003012, mae: 0.150648, mean_q: 0.217100\n",
      " 46266/50000: episode: 6039, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.004652, mae: 0.149685, mean_q: 0.215226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46272/50000: episode: 6040, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004209, mae: 0.155022, mean_q: 0.221545\n",
      " 46303/50000: episode: 6041, duration: 0.394s, episode steps:  31, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.645 [0.000, 3.000],  loss: 0.002929, mae: 0.149366, mean_q: 0.211389\n",
      " 46314/50000: episode: 6042, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002138, mae: 0.143508, mean_q: 0.206240\n",
      " 46316/50000: episode: 6043, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003557, mae: 0.154170, mean_q: 0.222200\n",
      " 46320/50000: episode: 6044, duration: 0.066s, episode steps:   4, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002325, mae: 0.151899, mean_q: 0.220287\n",
      " 46328/50000: episode: 6045, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002710, mae: 0.151750, mean_q: 0.218148\n",
      " 46339/50000: episode: 6046, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002947, mae: 0.150863, mean_q: 0.216640\n",
      " 46359/50000: episode: 6047, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.003046, mae: 0.149700, mean_q: 0.215478\n",
      " 46370/50000: episode: 6048, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002238, mae: 0.150190, mean_q: 0.214458\n",
      " 46378/50000: episode: 6049, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003381, mae: 0.152024, mean_q: 0.216755\n",
      " 46382/50000: episode: 6050, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002239, mae: 0.146106, mean_q: 0.209794\n",
      " 46386/50000: episode: 6051, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003244, mae: 0.151871, mean_q: 0.215418\n",
      " 46400/50000: episode: 6052, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002932, mae: 0.152208, mean_q: 0.214675\n",
      " 46403/50000: episode: 6053, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002767, mae: 0.153717, mean_q: 0.214608\n",
      " 46409/50000: episode: 6054, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002742, mae: 0.146040, mean_q: 0.210681\n",
      " 46417/50000: episode: 6055, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002452, mae: 0.151588, mean_q: 0.216117\n",
      " 46421/50000: episode: 6056, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002448, mae: 0.151966, mean_q: 0.214190\n",
      " 46438/50000: episode: 6057, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.003343, mae: 0.157672, mean_q: 0.222432\n",
      " 46453/50000: episode: 6058, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003395, mae: 0.147714, mean_q: 0.210314\n",
      " 46476/50000: episode: 6059, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.174 [0.000, 3.000],  loss: 0.002679, mae: 0.154831, mean_q: 0.221361\n",
      " 46478/50000: episode: 6060, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002545, mae: 0.147943, mean_q: 0.210680\n",
      " 46482/50000: episode: 6061, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002100, mae: 0.151258, mean_q: 0.218192\n",
      " 46485/50000: episode: 6062, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002863, mae: 0.157076, mean_q: 0.224177\n",
      " 46492/50000: episode: 6063, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002568, mae: 0.149819, mean_q: 0.213483\n",
      " 46507/50000: episode: 6064, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002799, mae: 0.152351, mean_q: 0.217553\n",
      " 46511/50000: episode: 6065, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.003447, mae: 0.155409, mean_q: 0.220517\n",
      " 46519/50000: episode: 6066, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002746, mae: 0.152700, mean_q: 0.215335\n",
      " 46524/50000: episode: 6067, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002474, mae: 0.148247, mean_q: 0.210682\n",
      " 46533/50000: episode: 6068, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.003494, mae: 0.154853, mean_q: 0.218520\n",
      " 46543/50000: episode: 6069, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002785, mae: 0.153870, mean_q: 0.215543\n",
      " 46556/50000: episode: 6070, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002708, mae: 0.149746, mean_q: 0.211267\n",
      " 46566/50000: episode: 6071, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.100 [0.000, 3.000],  loss: 0.002149, mae: 0.146823, mean_q: 0.211664\n",
      " 46570/50000: episode: 6072, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.003158, mae: 0.152010, mean_q: 0.211837\n",
      " 46573/50000: episode: 6073, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002483, mae: 0.151439, mean_q: 0.216110\n",
      " 46578/50000: episode: 6074, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003732, mae: 0.155132, mean_q: 0.219437\n",
      " 46585/50000: episode: 6075, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.002805, mae: 0.154734, mean_q: 0.220683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46602/50000: episode: 6076, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002385, mae: 0.152517, mean_q: 0.217459\n",
      " 46607/50000: episode: 6077, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.003043, mae: 0.148144, mean_q: 0.211912\n",
      " 46620/50000: episode: 6078, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [1.000, 3.000],  loss: 0.003192, mae: 0.152406, mean_q: 0.213540\n",
      " 46625/50000: episode: 6079, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.002917, mae: 0.152357, mean_q: 0.212758\n",
      " 46628/50000: episode: 6080, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.001012, mae: 0.143213, mean_q: 0.204424\n",
      " 46652/50000: episode: 6081, duration: 0.312s, episode steps:  24, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.003161, mae: 0.150935, mean_q: 0.214325\n",
      " 46661/50000: episode: 6082, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002070, mae: 0.148233, mean_q: 0.214365\n",
      " 46672/50000: episode: 6083, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002229, mae: 0.146490, mean_q: 0.210379\n",
      " 46678/50000: episode: 6084, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002940, mae: 0.150483, mean_q: 0.212170\n",
      " 46681/50000: episode: 6085, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001785, mae: 0.143148, mean_q: 0.203784\n",
      " 46690/50000: episode: 6086, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003435, mae: 0.149396, mean_q: 0.209794\n",
      " 46696/50000: episode: 6087, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002214, mae: 0.143373, mean_q: 0.205693\n",
      " 46699/50000: episode: 6088, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001899, mae: 0.148561, mean_q: 0.211520\n",
      " 46713/50000: episode: 6089, duration: 0.249s, episode steps:  14, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002847, mae: 0.152979, mean_q: 0.217058\n",
      " 46716/50000: episode: 6090, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002099, mae: 0.142168, mean_q: 0.205024\n",
      " 46730/50000: episode: 6091, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002561, mae: 0.145263, mean_q: 0.206873\n",
      " 46740/50000: episode: 6092, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002081, mae: 0.147724, mean_q: 0.211279\n",
      " 46753/50000: episode: 6093, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.003229, mae: 0.153401, mean_q: 0.214179\n",
      " 46760/50000: episode: 6094, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001725, mae: 0.151462, mean_q: 0.215241\n",
      " 46764/50000: episode: 6095, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003416, mae: 0.152697, mean_q: 0.214044\n",
      " 46782/50000: episode: 6096, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002678, mae: 0.145171, mean_q: 0.204949\n",
      " 46789/50000: episode: 6097, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.002839, mae: 0.150908, mean_q: 0.215101\n",
      " 46794/50000: episode: 6098, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002861, mae: 0.148324, mean_q: 0.211393\n",
      " 46816/50000: episode: 6099, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002728, mae: 0.144810, mean_q: 0.206550\n",
      " 46833/50000: episode: 6100, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.002026, mae: 0.145831, mean_q: 0.205940\n",
      " 46837/50000: episode: 6101, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002572, mae: 0.145132, mean_q: 0.206674\n",
      " 46843/50000: episode: 6102, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002264, mae: 0.147683, mean_q: 0.206918\n",
      " 46848/50000: episode: 6103, duration: 0.082s, episode steps:   5, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [1.000, 2.000],  loss: 0.002957, mae: 0.150328, mean_q: 0.211825\n",
      " 46850/50000: episode: 6104, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004714, mae: 0.151816, mean_q: 0.212359\n",
      " 46852/50000: episode: 6105, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004740, mae: 0.156983, mean_q: 0.215754\n",
      " 46855/50000: episode: 6106, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002718, mae: 0.146432, mean_q: 0.205276\n",
      " 46871/50000: episode: 6107, duration: 0.210s, episode steps:  16, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003327, mae: 0.147270, mean_q: 0.210524\n",
      " 46874/50000: episode: 6108, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002745, mae: 0.150437, mean_q: 0.215046\n",
      " 46907/50000: episode: 6109, duration: 0.403s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.002923, mae: 0.146297, mean_q: 0.205744\n",
      " 46919/50000: episode: 6110, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003642, mae: 0.151906, mean_q: 0.214343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46925/50000: episode: 6111, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003248, mae: 0.148037, mean_q: 0.209752\n",
      " 46927/50000: episode: 6112, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.005356, mae: 0.155569, mean_q: 0.217093\n",
      " 46932/50000: episode: 6113, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.200 [0.000, 1.000],  loss: 0.002474, mae: 0.150859, mean_q: 0.211125\n",
      " 46934/50000: episode: 6114, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002676, mae: 0.140155, mean_q: 0.202424\n",
      " 46937/50000: episode: 6115, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002334, mae: 0.144405, mean_q: 0.202900\n",
      " 46942/50000: episode: 6116, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002797, mae: 0.145386, mean_q: 0.204957\n",
      " 46944/50000: episode: 6117, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003969, mae: 0.153248, mean_q: 0.218917\n",
      " 46950/50000: episode: 6118, duration: 0.082s, episode steps:   6, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002278, mae: 0.146034, mean_q: 0.206094\n",
      " 46956/50000: episode: 6119, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002526, mae: 0.151030, mean_q: 0.210728\n",
      " 46961/50000: episode: 6120, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002447, mae: 0.148325, mean_q: 0.209487\n",
      " 46965/50000: episode: 6121, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002206, mae: 0.143606, mean_q: 0.202786\n",
      " 46969/50000: episode: 6122, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001946, mae: 0.140386, mean_q: 0.206686\n",
      " 46972/50000: episode: 6123, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003275, mae: 0.142331, mean_q: 0.202680\n",
      " 46976/50000: episode: 6124, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002629, mae: 0.142326, mean_q: 0.204423\n",
      " 46985/50000: episode: 6125, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 2.000],  loss: 0.002720, mae: 0.146252, mean_q: 0.208215\n",
      " 46988/50000: episode: 6126, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003311, mae: 0.147588, mean_q: 0.209038\n",
      " 46994/50000: episode: 6127, duration: 0.098s, episode steps:   6, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003162, mae: 0.147416, mean_q: 0.207023\n",
      " 47008/50000: episode: 6128, duration: 0.187s, episode steps:  14, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002773, mae: 0.148349, mean_q: 0.208508\n",
      " 47011/50000: episode: 6129, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.001814, mae: 0.141985, mean_q: 0.203003\n",
      " 47016/50000: episode: 6130, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002615, mae: 0.148947, mean_q: 0.212052\n",
      " 47021/50000: episode: 6131, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.200 [1.000, 3.000],  loss: 0.003272, mae: 0.148725, mean_q: 0.212499\n",
      " 47029/50000: episode: 6132, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003370, mae: 0.151968, mean_q: 0.215201\n",
      " 47033/50000: episode: 6133, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002561, mae: 0.144209, mean_q: 0.205114\n",
      " 47040/50000: episode: 6134, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002963, mae: 0.146711, mean_q: 0.206680\n",
      " 47047/50000: episode: 6135, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.429 [0.000, 2.000],  loss: 0.003436, mae: 0.146829, mean_q: 0.205949\n",
      " 47051/50000: episode: 6136, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004021, mae: 0.150861, mean_q: 0.209950\n",
      " 47058/50000: episode: 6137, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002650, mae: 0.141457, mean_q: 0.203028\n",
      " 47062/50000: episode: 6138, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.003652, mae: 0.148904, mean_q: 0.210435\n",
      " 47064/50000: episode: 6139, duration: 0.037s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004120, mae: 0.148173, mean_q: 0.210511\n",
      " 47068/50000: episode: 6140, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.002505, mae: 0.147526, mean_q: 0.210064\n",
      " 47093/50000: episode: 6141, duration: 0.311s, episode steps:  25, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.002619, mae: 0.149851, mean_q: 0.213483\n",
      " 47110/50000: episode: 6142, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002942, mae: 0.144336, mean_q: 0.206482\n",
      " 47121/50000: episode: 6143, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002707, mae: 0.147445, mean_q: 0.210608\n",
      " 47142/50000: episode: 6144, duration: 0.274s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002701, mae: 0.150299, mean_q: 0.210487\n",
      " 47150/50000: episode: 6145, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.003409, mae: 0.149981, mean_q: 0.210049\n",
      " 47152/50000: episode: 6146, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002867, mae: 0.146156, mean_q: 0.203026\n",
      " 47154/50000: episode: 6147, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001941, mae: 0.147097, mean_q: 0.211903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47157/50000: episode: 6148, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001830, mae: 0.145808, mean_q: 0.211477\n",
      " 47160/50000: episode: 6149, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002932, mae: 0.142919, mean_q: 0.203563\n",
      " 47162/50000: episode: 6150, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001241, mae: 0.142732, mean_q: 0.207966\n",
      " 47171/50000: episode: 6151, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003294, mae: 0.149079, mean_q: 0.208748\n",
      " 47186/50000: episode: 6152, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.002864, mae: 0.143497, mean_q: 0.203701\n",
      " 47208/50000: episode: 6153, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.955 [0.000, 3.000],  loss: 0.002660, mae: 0.147159, mean_q: 0.207942\n",
      " 47215/50000: episode: 6154, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002680, mae: 0.141281, mean_q: 0.206028\n",
      " 47223/50000: episode: 6155, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.003596, mae: 0.147983, mean_q: 0.210436\n",
      " 47225/50000: episode: 6156, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004718, mae: 0.152374, mean_q: 0.222134\n",
      " 47235/50000: episode: 6157, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.002248, mae: 0.150645, mean_q: 0.210968\n",
      " 47241/50000: episode: 6158, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003154, mae: 0.146704, mean_q: 0.204592\n",
      " 47246/50000: episode: 6159, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.200 [0.000, 1.000],  loss: 0.003618, mae: 0.146659, mean_q: 0.207297\n",
      " 47257/50000: episode: 6160, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002260, mae: 0.140258, mean_q: 0.200502\n",
      " 47261/50000: episode: 6161, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003259, mae: 0.146046, mean_q: 0.204479\n",
      " 47263/50000: episode: 6162, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002379, mae: 0.150399, mean_q: 0.207307\n",
      " 47267/50000: episode: 6163, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.004586, mae: 0.156481, mean_q: 0.219046\n",
      " 47271/50000: episode: 6164, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001585, mae: 0.149230, mean_q: 0.215204\n",
      " 47280/50000: episode: 6165, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003143, mae: 0.145784, mean_q: 0.208814\n",
      " 47289/50000: episode: 6166, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [1.000, 3.000],  loss: 0.002919, mae: 0.148503, mean_q: 0.210472\n",
      " 47291/50000: episode: 6167, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.004378, mae: 0.149753, mean_q: 0.211856\n",
      " 47293/50000: episode: 6168, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002910, mae: 0.145395, mean_q: 0.210648\n",
      " 47301/50000: episode: 6169, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002781, mae: 0.148933, mean_q: 0.211780\n",
      " 47315/50000: episode: 6170, duration: 0.181s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002921, mae: 0.144741, mean_q: 0.207948\n",
      " 47325/50000: episode: 6171, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002992, mae: 0.146180, mean_q: 0.207415\n",
      " 47327/50000: episode: 6172, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.005182, mae: 0.151230, mean_q: 0.212132\n",
      " 47360/50000: episode: 6173, duration: 0.406s, episode steps:  33, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.002790, mae: 0.147656, mean_q: 0.209557\n",
      " 47366/50000: episode: 6174, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002779, mae: 0.147240, mean_q: 0.212122\n",
      " 47379/50000: episode: 6175, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002408, mae: 0.146441, mean_q: 0.207011\n",
      " 47383/50000: episode: 6176, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.004121, mae: 0.144145, mean_q: 0.204772\n",
      " 47386/50000: episode: 6177, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.003122, mae: 0.143808, mean_q: 0.201712\n",
      " 47400/50000: episode: 6178, duration: 0.199s, episode steps:  14, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 2.000],  loss: 0.002980, mae: 0.140579, mean_q: 0.201413\n",
      " 47402/50000: episode: 6179, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003381, mae: 0.143489, mean_q: 0.198794\n",
      " 47406/50000: episode: 6180, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.003100, mae: 0.146163, mean_q: 0.207677\n",
      " 47415/50000: episode: 6181, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.111 [0.000, 3.000],  loss: 0.002347, mae: 0.148250, mean_q: 0.211163\n",
      " 47429/50000: episode: 6182, duration: 0.193s, episode steps:  14, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002396, mae: 0.146075, mean_q: 0.206826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47444/50000: episode: 6183, duration: 0.205s, episode steps:  15, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.002933, mae: 0.146256, mean_q: 0.207753\n",
      " 47455/50000: episode: 6184, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002412, mae: 0.146282, mean_q: 0.206845\n",
      " 47470/50000: episode: 6185, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.003959, mae: 0.150448, mean_q: 0.208088\n",
      " 47475/50000: episode: 6186, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001875, mae: 0.142448, mean_q: 0.206400\n",
      " 47479/50000: episode: 6187, duration: 0.065s, episode steps:   4, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003775, mae: 0.149419, mean_q: 0.213508\n",
      " 47495/50000: episode: 6188, duration: 0.229s, episode steps:  16, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.062 [0.000, 3.000],  loss: 0.002702, mae: 0.145533, mean_q: 0.204916\n",
      " 47497/50000: episode: 6189, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005624, mae: 0.159363, mean_q: 0.217733\n",
      " 47503/50000: episode: 6190, duration: 0.099s, episode steps:   6, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.002189, mae: 0.138872, mean_q: 0.200147\n",
      " 47517/50000: episode: 6191, duration: 0.216s, episode steps:  14, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.002002, mae: 0.141046, mean_q: 0.203088\n",
      " 47523/50000: episode: 6192, duration: 0.093s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003017, mae: 0.146282, mean_q: 0.206456\n",
      " 47530/50000: episode: 6193, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.003567, mae: 0.148704, mean_q: 0.209361\n",
      " 47536/50000: episode: 6194, duration: 0.093s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002973, mae: 0.143142, mean_q: 0.201985\n",
      " 47543/50000: episode: 6195, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001977, mae: 0.136782, mean_q: 0.197897\n",
      " 47546/50000: episode: 6196, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002292, mae: 0.134560, mean_q: 0.193435\n",
      " 47553/50000: episode: 6197, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002807, mae: 0.140300, mean_q: 0.201837\n",
      " 47560/50000: episode: 6198, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.002137, mae: 0.136455, mean_q: 0.198443\n",
      " 47562/50000: episode: 6199, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.005823, mae: 0.143903, mean_q: 0.210291\n",
      " 47565/50000: episode: 6200, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002196, mae: 0.134260, mean_q: 0.196445\n",
      " 47592/50000: episode: 6201, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002615, mae: 0.143703, mean_q: 0.202383\n",
      " 47603/50000: episode: 6202, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.003038, mae: 0.145062, mean_q: 0.201536\n",
      " 47606/50000: episode: 6203, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.002609, mae: 0.144060, mean_q: 0.198390\n",
      " 47617/50000: episode: 6204, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002166, mae: 0.138852, mean_q: 0.200004\n",
      " 47621/50000: episode: 6205, duration: 0.058s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002066, mae: 0.136392, mean_q: 0.194676\n",
      " 47630/50000: episode: 6206, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002192, mae: 0.139763, mean_q: 0.199322\n",
      " 47637/50000: episode: 6207, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003438, mae: 0.144825, mean_q: 0.202220\n",
      " 47656/50000: episode: 6208, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.002446, mae: 0.142409, mean_q: 0.201314\n",
      " 47659/50000: episode: 6209, duration: 0.055s, episode steps:   3, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.001663, mae: 0.138442, mean_q: 0.197215\n",
      " 47663/50000: episode: 6210, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002688, mae: 0.143097, mean_q: 0.200576\n",
      " 47676/50000: episode: 6211, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  1.000, mean reward:  0.077 [ 0.000,  1.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002182, mae: 0.137797, mean_q: 0.196345\n",
      " 47688/50000: episode: 6212, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002095, mae: 0.140128, mean_q: 0.198810\n",
      " 47692/50000: episode: 6213, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001979, mae: 0.141161, mean_q: 0.201514\n",
      " 47701/50000: episode: 6214, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002810, mae: 0.144210, mean_q: 0.207231\n",
      " 47725/50000: episode: 6215, duration: 0.305s, episode steps:  24, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002498, mae: 0.142351, mean_q: 0.201162\n",
      " 47727/50000: episode: 6216, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002179, mae: 0.134662, mean_q: 0.194109\n",
      " 47730/50000: episode: 6217, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001407, mae: 0.140640, mean_q: 0.199552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47742/50000: episode: 6218, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.003192, mae: 0.145752, mean_q: 0.204435\n",
      " 47763/50000: episode: 6219, duration: 0.264s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002352, mae: 0.143023, mean_q: 0.199723\n",
      " 47774/50000: episode: 6220, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 3.000],  loss: 0.002649, mae: 0.137406, mean_q: 0.194529\n",
      " 47793/50000: episode: 6221, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.002445, mae: 0.141560, mean_q: 0.201634\n",
      " 47798/50000: episode: 6222, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [2.000, 3.000],  loss: 0.001642, mae: 0.135507, mean_q: 0.194274\n",
      " 47801/50000: episode: 6223, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.002855, mae: 0.143639, mean_q: 0.204644\n",
      " 47809/50000: episode: 6224, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002729, mae: 0.136369, mean_q: 0.195970\n",
      " 47811/50000: episode: 6225, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003551, mae: 0.148480, mean_q: 0.207320\n",
      " 47816/50000: episode: 6226, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.002714, mae: 0.133441, mean_q: 0.194630\n",
      " 47822/50000: episode: 6227, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002362, mae: 0.133410, mean_q: 0.196587\n",
      " 47825/50000: episode: 6228, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.004243, mae: 0.139442, mean_q: 0.204505\n",
      " 47837/50000: episode: 6229, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002138, mae: 0.140164, mean_q: 0.202731\n",
      " 47840/50000: episode: 6230, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003543, mae: 0.149022, mean_q: 0.208193\n",
      " 47842/50000: episode: 6231, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002209, mae: 0.143356, mean_q: 0.202601\n",
      " 47846/50000: episode: 6232, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001988, mae: 0.142387, mean_q: 0.201784\n",
      " 47852/50000: episode: 6233, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003019, mae: 0.142294, mean_q: 0.198854\n",
      " 47874/50000: episode: 6234, duration: 0.276s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002118, mae: 0.140435, mean_q: 0.198106\n",
      " 47887/50000: episode: 6235, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002301, mae: 0.137784, mean_q: 0.194922\n",
      " 47898/50000: episode: 6236, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002569, mae: 0.141713, mean_q: 0.200744\n",
      " 47912/50000: episode: 6237, duration: 0.176s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.002519, mae: 0.139835, mean_q: 0.196258\n",
      " 47915/50000: episode: 6238, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002258, mae: 0.149206, mean_q: 0.209383\n",
      " 47920/50000: episode: 6239, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002709, mae: 0.139309, mean_q: 0.193172\n",
      " 47927/50000: episode: 6240, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002324, mae: 0.133655, mean_q: 0.190614\n",
      " 47938/50000: episode: 6241, duration: 0.156s, episode steps:  11, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.003004, mae: 0.136729, mean_q: 0.193272\n",
      " 47941/50000: episode: 6242, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003189, mae: 0.127683, mean_q: 0.187602\n",
      " 47947/50000: episode: 6243, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001835, mae: 0.132954, mean_q: 0.195018\n",
      " 47949/50000: episode: 6244, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.001891, mae: 0.137970, mean_q: 0.200992\n",
      " 47954/50000: episode: 6245, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002146, mae: 0.134375, mean_q: 0.196239\n",
      " 47962/50000: episode: 6246, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002687, mae: 0.137562, mean_q: 0.198856\n",
      " 47971/50000: episode: 6247, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001856, mae: 0.139133, mean_q: 0.199278\n",
      " 47983/50000: episode: 6248, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002519, mae: 0.139723, mean_q: 0.196746\n",
      " 47989/50000: episode: 6249, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 1.000],  loss: 0.002231, mae: 0.138859, mean_q: 0.198451\n",
      " 47997/50000: episode: 6250, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 2.000],  loss: 0.001921, mae: 0.135971, mean_q: 0.197127\n",
      " 48000/50000: episode: 6251, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001911, mae: 0.138631, mean_q: 0.198653\n",
      " 48008/50000: episode: 6252, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.001371, mae: 0.141390, mean_q: 0.201355\n",
      " 48015/50000: episode: 6253, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003251, mae: 0.141709, mean_q: 0.200105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48032/50000: episode: 6254, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [0.000, 3.000],  loss: 0.002839, mae: 0.140868, mean_q: 0.198988\n",
      " 48040/50000: episode: 6255, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002896, mae: 0.141982, mean_q: 0.201736\n",
      " 48055/50000: episode: 6256, duration: 0.195s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.002429, mae: 0.137513, mean_q: 0.196062\n",
      " 48065/50000: episode: 6257, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002769, mae: 0.140325, mean_q: 0.199389\n",
      " 48096/50000: episode: 6258, duration: 0.388s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.323 [0.000, 3.000],  loss: 0.002594, mae: 0.139009, mean_q: 0.199542\n",
      " 48104/50000: episode: 6259, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.003333, mae: 0.140868, mean_q: 0.195908\n",
      " 48106/50000: episode: 6260, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001992, mae: 0.133795, mean_q: 0.187701\n",
      " 48114/50000: episode: 6261, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.003304, mae: 0.145059, mean_q: 0.203853\n",
      " 48117/50000: episode: 6262, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.004958, mae: 0.149511, mean_q: 0.202490\n",
      " 48131/50000: episode: 6263, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.002283, mae: 0.136658, mean_q: 0.194507\n",
      " 48148/50000: episode: 6264, duration: 0.215s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.002695, mae: 0.141202, mean_q: 0.199521\n",
      " 48154/50000: episode: 6265, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003228, mae: 0.142691, mean_q: 0.200643\n",
      " 48166/50000: episode: 6266, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002406, mae: 0.138985, mean_q: 0.194932\n",
      " 48173/50000: episode: 6267, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002389, mae: 0.137316, mean_q: 0.196336\n",
      " 48195/50000: episode: 6268, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.227 [0.000, 3.000],  loss: 0.002795, mae: 0.140281, mean_q: 0.198813\n",
      " 48208/50000: episode: 6269, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002317, mae: 0.138646, mean_q: 0.194768\n",
      " 48215/50000: episode: 6270, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.003144, mae: 0.147436, mean_q: 0.207190\n",
      " 48220/50000: episode: 6271, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002225, mae: 0.138038, mean_q: 0.196927\n",
      " 48227/50000: episode: 6272, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 2.000],  loss: 0.003126, mae: 0.138776, mean_q: 0.198879\n",
      " 48238/50000: episode: 6273, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002894, mae: 0.139341, mean_q: 0.201202\n",
      " 48248/50000: episode: 6274, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002799, mae: 0.141204, mean_q: 0.199011\n",
      " 48254/50000: episode: 6275, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.003101, mae: 0.141114, mean_q: 0.199429\n",
      " 48257/50000: episode: 6276, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001381, mae: 0.135921, mean_q: 0.194057\n",
      " 48266/50000: episode: 6277, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002536, mae: 0.140452, mean_q: 0.199560\n",
      " 48268/50000: episode: 6278, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001679, mae: 0.130823, mean_q: 0.189523\n",
      " 48282/50000: episode: 6279, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002241, mae: 0.139742, mean_q: 0.200800\n",
      " 48287/50000: episode: 6280, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001690, mae: 0.139821, mean_q: 0.197291\n",
      " 48294/50000: episode: 6281, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002369, mae: 0.143383, mean_q: 0.198987\n",
      " 48296/50000: episode: 6282, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001566, mae: 0.134588, mean_q: 0.186823\n",
      " 48305/50000: episode: 6283, duration: 0.192s, episode steps:   9, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002421, mae: 0.131843, mean_q: 0.189665\n",
      " 48324/50000: episode: 6284, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002633, mae: 0.132134, mean_q: 0.191844\n",
      " 48333/50000: episode: 6285, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.222 [1.000, 3.000],  loss: 0.002092, mae: 0.140890, mean_q: 0.203907\n",
      " 48338/50000: episode: 6286, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002867, mae: 0.140354, mean_q: 0.196618\n",
      " 48340/50000: episode: 6287, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001749, mae: 0.135409, mean_q: 0.193237\n",
      " 48352/50000: episode: 6288, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002841, mae: 0.138158, mean_q: 0.196488\n",
      " 48355/50000: episode: 6289, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002559, mae: 0.138165, mean_q: 0.194316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48358/50000: episode: 6290, duration: 0.054s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001690, mae: 0.142614, mean_q: 0.204845\n",
      " 48368/50000: episode: 6291, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.300 [0.000, 3.000],  loss: 0.002327, mae: 0.138463, mean_q: 0.196317\n",
      " 48382/50000: episode: 6292, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002922, mae: 0.138430, mean_q: 0.198405\n",
      " 48399/50000: episode: 6293, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002545, mae: 0.134597, mean_q: 0.192203\n",
      " 48404/50000: episode: 6294, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 3.000],  loss: 0.001845, mae: 0.130770, mean_q: 0.189929\n",
      " 48406/50000: episode: 6295, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.000942, mae: 0.132877, mean_q: 0.188890\n",
      " 48408/50000: episode: 6296, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002508, mae: 0.137867, mean_q: 0.193898\n",
      " 48411/50000: episode: 6297, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002234, mae: 0.137064, mean_q: 0.192387\n",
      " 48414/50000: episode: 6298, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002409, mae: 0.143286, mean_q: 0.198547\n",
      " 48430/50000: episode: 6299, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.938 [0.000, 3.000],  loss: 0.001790, mae: 0.136285, mean_q: 0.196613\n",
      " 48433/50000: episode: 6300, duration: 0.051s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.003166, mae: 0.143166, mean_q: 0.199987\n",
      " 48454/50000: episode: 6301, duration: 0.270s, episode steps:  21, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.002950, mae: 0.140213, mean_q: 0.200452\n",
      " 48472/50000: episode: 6302, duration: 0.231s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [0.000, 3.000],  loss: 0.003052, mae: 0.137493, mean_q: 0.195776\n",
      " 48478/50000: episode: 6303, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003990, mae: 0.147348, mean_q: 0.209604\n",
      " 48480/50000: episode: 6304, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001796, mae: 0.143428, mean_q: 0.203280\n",
      " 48484/50000: episode: 6305, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.005356, mae: 0.145856, mean_q: 0.204690\n",
      " 48488/50000: episode: 6306, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001887, mae: 0.141397, mean_q: 0.198920\n",
      " 48491/50000: episode: 6307, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003995, mae: 0.142583, mean_q: 0.199613\n",
      " 48502/50000: episode: 6308, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002280, mae: 0.138939, mean_q: 0.197546\n",
      " 48506/50000: episode: 6309, duration: 0.063s, episode steps:   4, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002754, mae: 0.136414, mean_q: 0.193481\n",
      " 48509/50000: episode: 6310, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002065, mae: 0.134502, mean_q: 0.191828\n",
      " 48512/50000: episode: 6311, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.005315, mae: 0.143806, mean_q: 0.196452\n",
      " 48515/50000: episode: 6312, duration: 0.053s, episode steps:   3, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.004867, mae: 0.146566, mean_q: 0.207179\n",
      " 48521/50000: episode: 6313, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [0.000, 3.000],  loss: 0.001668, mae: 0.136044, mean_q: 0.199146\n",
      " 48533/50000: episode: 6314, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002165, mae: 0.140156, mean_q: 0.202208\n",
      " 48535/50000: episode: 6315, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002322, mae: 0.140114, mean_q: 0.200170\n",
      " 48542/50000: episode: 6316, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001576, mae: 0.132103, mean_q: 0.187438\n",
      " 48549/50000: episode: 6317, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003108, mae: 0.137685, mean_q: 0.193048\n",
      " 48562/50000: episode: 6318, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001945, mae: 0.135559, mean_q: 0.191216\n",
      " 48574/50000: episode: 6319, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002589, mae: 0.139146, mean_q: 0.195039\n",
      " 48580/50000: episode: 6320, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.003215, mae: 0.143137, mean_q: 0.199062\n",
      " 48586/50000: episode: 6321, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.001606, mae: 0.136648, mean_q: 0.192423\n",
      " 48599/50000: episode: 6322, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002659, mae: 0.142288, mean_q: 0.198944\n",
      " 48606/50000: episode: 6323, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002313, mae: 0.138530, mean_q: 0.193383\n",
      " 48624/50000: episode: 6324, duration: 0.248s, episode steps:  18, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002519, mae: 0.137955, mean_q: 0.197301\n",
      " 48633/50000: episode: 6325, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003222, mae: 0.146206, mean_q: 0.207668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48640/50000: episode: 6326, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.002875, mae: 0.138907, mean_q: 0.202836\n",
      " 48646/50000: episode: 6327, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002559, mae: 0.133933, mean_q: 0.197088\n",
      " 48651/50000: episode: 6328, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002130, mae: 0.133809, mean_q: 0.191901\n",
      " 48666/50000: episode: 6329, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002272, mae: 0.139434, mean_q: 0.196502\n",
      " 48681/50000: episode: 6330, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002169, mae: 0.138156, mean_q: 0.196090\n",
      " 48693/50000: episode: 6331, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002762, mae: 0.139423, mean_q: 0.196936\n",
      " 48696/50000: episode: 6332, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.002358, mae: 0.142052, mean_q: 0.200057\n",
      " 48707/50000: episode: 6333, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003023, mae: 0.136591, mean_q: 0.196463\n",
      " 48710/50000: episode: 6334, duration: 0.056s, episode steps:   3, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.002147, mae: 0.140938, mean_q: 0.196865\n",
      " 48712/50000: episode: 6335, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002330, mae: 0.142160, mean_q: 0.199645\n",
      " 48720/50000: episode: 6336, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  1.000, mean reward:  0.125 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003401, mae: 0.142827, mean_q: 0.201290\n",
      " 48727/50000: episode: 6337, duration: 0.110s, episode steps:   7, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002980, mae: 0.135387, mean_q: 0.196141\n",
      " 48744/50000: episode: 6338, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.059 [0.000, 3.000],  loss: 0.003050, mae: 0.135392, mean_q: 0.191708\n",
      " 48749/50000: episode: 6339, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.003842, mae: 0.146391, mean_q: 0.203896\n",
      " 48761/50000: episode: 6340, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  1.000, mean reward:  0.083 [ 0.000,  1.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002367, mae: 0.138454, mean_q: 0.196115\n",
      " 48772/50000: episode: 6341, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002058, mae: 0.137799, mean_q: 0.194339\n",
      " 48786/50000: episode: 6342, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.002165, mae: 0.139249, mean_q: 0.195490\n",
      " 48788/50000: episode: 6343, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001997, mae: 0.139604, mean_q: 0.200196\n",
      " 48790/50000: episode: 6344, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.001424, mae: 0.133872, mean_q: 0.192494\n",
      " 48797/50000: episode: 6345, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.003455, mae: 0.137973, mean_q: 0.195452\n",
      " 48813/50000: episode: 6346, duration: 0.221s, episode steps:  16, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002665, mae: 0.133857, mean_q: 0.193856\n",
      " 48822/50000: episode: 6347, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002233, mae: 0.140012, mean_q: 0.199534\n",
      " 48827/50000: episode: 6348, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002261, mae: 0.136087, mean_q: 0.191689\n",
      " 48851/50000: episode: 6349, duration: 0.326s, episode steps:  24, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.003050, mae: 0.139005, mean_q: 0.196556\n",
      " 48863/50000: episode: 6350, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002698, mae: 0.135715, mean_q: 0.193962\n",
      " 48865/50000: episode: 6351, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001841, mae: 0.132695, mean_q: 0.190726\n",
      " 48871/50000: episode: 6352, duration: 0.096s, episode steps:   6, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002802, mae: 0.137967, mean_q: 0.197726\n",
      " 48880/50000: episode: 6353, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002788, mae: 0.135795, mean_q: 0.191985\n",
      " 48893/50000: episode: 6354, duration: 0.223s, episode steps:  13, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002893, mae: 0.140181, mean_q: 0.197543\n",
      " 48901/50000: episode: 6355, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002378, mae: 0.138563, mean_q: 0.197365\n",
      " 48904/50000: episode: 6356, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001921, mae: 0.137641, mean_q: 0.196912\n",
      " 48910/50000: episode: 6357, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001828, mae: 0.138385, mean_q: 0.193568\n",
      " 48913/50000: episode: 6358, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002273, mae: 0.143290, mean_q: 0.198600\n",
      " 48923/50000: episode: 6359, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.002194, mae: 0.134898, mean_q: 0.189566\n",
      " 48932/50000: episode: 6360, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002348, mae: 0.137297, mean_q: 0.191801\n",
      " 48938/50000: episode: 6361, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003683, mae: 0.147785, mean_q: 0.202443\n",
      " 48940/50000: episode: 6362, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.003044, mae: 0.154419, mean_q: 0.212751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48960/50000: episode: 6363, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.002494, mae: 0.139698, mean_q: 0.198626\n",
      " 48962/50000: episode: 6364, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.004773, mae: 0.142489, mean_q: 0.199244\n",
      " 48964/50000: episode: 6365, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001967, mae: 0.134447, mean_q: 0.194696\n",
      " 48969/50000: episode: 6366, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002793, mae: 0.134669, mean_q: 0.196838\n",
      " 48974/50000: episode: 6367, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 2.000],  loss: 0.002672, mae: 0.140319, mean_q: 0.201106\n",
      " 48984/50000: episode: 6368, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002554, mae: 0.141394, mean_q: 0.201406\n",
      " 49004/50000: episode: 6369, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.350 [0.000, 3.000],  loss: 0.002554, mae: 0.138882, mean_q: 0.199262\n",
      " 49006/50000: episode: 6370, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.001779, mae: 0.132347, mean_q: 0.188633\n",
      " 49012/50000: episode: 6371, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.002854, mae: 0.137476, mean_q: 0.196824\n",
      " 49032/50000: episode: 6372, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002791, mae: 0.143160, mean_q: 0.205497\n",
      " 49041/50000: episode: 6373, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.002217, mae: 0.142171, mean_q: 0.202456\n",
      " 49051/50000: episode: 6374, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.002573, mae: 0.142768, mean_q: 0.202079\n",
      " 49054/50000: episode: 6375, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.003385, mae: 0.149904, mean_q: 0.211871\n",
      " 49057/50000: episode: 6376, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [1.000, 3.000],  loss: 0.003303, mae: 0.143341, mean_q: 0.200655\n",
      " 49060/50000: episode: 6377, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002014, mae: 0.144910, mean_q: 0.198710\n",
      " 49069/50000: episode: 6378, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003375, mae: 0.147936, mean_q: 0.213071\n",
      " 49079/50000: episode: 6379, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.003419, mae: 0.139286, mean_q: 0.202159\n",
      " 49087/50000: episode: 6380, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002181, mae: 0.137189, mean_q: 0.195139\n",
      " 49092/50000: episode: 6381, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.002299, mae: 0.141194, mean_q: 0.201352\n",
      " 49103/50000: episode: 6382, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.003254, mae: 0.144686, mean_q: 0.201818\n",
      " 49107/50000: episode: 6383, duration: 0.066s, episode steps:   4, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002816, mae: 0.139403, mean_q: 0.194568\n",
      " 49110/50000: episode: 6384, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003529, mae: 0.143770, mean_q: 0.200175\n",
      " 49112/50000: episode: 6385, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.003669, mae: 0.137167, mean_q: 0.194732\n",
      " 49128/50000: episode: 6386, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003439, mae: 0.140776, mean_q: 0.197347\n",
      " 49132/50000: episode: 6387, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002587, mae: 0.134009, mean_q: 0.188581\n",
      " 49140/50000: episode: 6388, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003676, mae: 0.135446, mean_q: 0.191555\n",
      " 49143/50000: episode: 6389, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.002409, mae: 0.137098, mean_q: 0.192502\n",
      " 49161/50000: episode: 6390, duration: 0.240s, episode steps:  18, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002354, mae: 0.140780, mean_q: 0.201117\n",
      " 49163/50000: episode: 6391, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.005137, mae: 0.150048, mean_q: 0.211332\n",
      " 49169/50000: episode: 6392, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002956, mae: 0.149621, mean_q: 0.210668\n",
      " 49174/50000: episode: 6393, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.004080, mae: 0.153440, mean_q: 0.213871\n",
      " 49184/50000: episode: 6394, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002549, mae: 0.139987, mean_q: 0.202465\n",
      " 49200/50000: episode: 6395, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003071, mae: 0.141836, mean_q: 0.203071\n",
      " 49208/50000: episode: 6396, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.875 [0.000, 3.000],  loss: 0.002458, mae: 0.140161, mean_q: 0.204027\n",
      " 49212/50000: episode: 6397, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.005206, mae: 0.149388, mean_q: 0.212013\n",
      " 49216/50000: episode: 6398, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003545, mae: 0.145785, mean_q: 0.206507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49224/50000: episode: 6399, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003158, mae: 0.140370, mean_q: 0.200008\n",
      " 49245/50000: episode: 6400, duration: 0.265s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003340, mae: 0.139456, mean_q: 0.200825\n",
      " 49254/50000: episode: 6401, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.002488, mae: 0.146651, mean_q: 0.207941\n",
      " 49271/50000: episode: 6402, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward:  1.000, mean reward:  0.059 [ 0.000,  1.000], mean action: 1.235 [0.000, 3.000],  loss: 0.002638, mae: 0.141221, mean_q: 0.199343\n",
      " 49273/50000: episode: 6403, duration: 0.035s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002572, mae: 0.140041, mean_q: 0.194852\n",
      " 49277/50000: episode: 6404, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002453, mae: 0.139453, mean_q: 0.198291\n",
      " 49281/50000: episode: 6405, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.003385, mae: 0.140963, mean_q: 0.198322\n",
      " 49296/50000: episode: 6406, duration: 0.199s, episode steps:  15, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002732, mae: 0.144602, mean_q: 0.201067\n",
      " 49302/50000: episode: 6407, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.002638, mae: 0.144369, mean_q: 0.202180\n",
      " 49320/50000: episode: 6408, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002415, mae: 0.138657, mean_q: 0.197811\n",
      " 49328/50000: episode: 6409, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [0.000, 3.000],  loss: 0.002395, mae: 0.136748, mean_q: 0.196686\n",
      " 49341/50000: episode: 6410, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002680, mae: 0.137320, mean_q: 0.193932\n",
      " 49347/50000: episode: 6411, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: 0.002797, mae: 0.140358, mean_q: 0.202375\n",
      " 49352/50000: episode: 6412, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002148, mae: 0.140214, mean_q: 0.197262\n",
      " 49356/50000: episode: 6413, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002991, mae: 0.143322, mean_q: 0.198772\n",
      " 49369/50000: episode: 6414, duration: 0.182s, episode steps:  13, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.002905, mae: 0.137105, mean_q: 0.194766\n",
      " 49385/50000: episode: 6415, duration: 0.204s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002432, mae: 0.137256, mean_q: 0.195417\n",
      " 49387/50000: episode: 6416, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.002419, mae: 0.140028, mean_q: 0.192541\n",
      " 49391/50000: episode: 6417, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002843, mae: 0.141843, mean_q: 0.198528\n",
      " 49400/50000: episode: 6418, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.778 [0.000, 3.000],  loss: 0.002662, mae: 0.138049, mean_q: 0.191584\n",
      " 49423/50000: episode: 6419, duration: 0.292s, episode steps:  23, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.003225, mae: 0.137813, mean_q: 0.194402\n",
      " 49426/50000: episode: 6420, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.003229, mae: 0.151263, mean_q: 0.217489\n",
      " 49442/50000: episode: 6421, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.002579, mae: 0.134787, mean_q: 0.193024\n",
      " 49445/50000: episode: 6422, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001630, mae: 0.132224, mean_q: 0.194554\n",
      " 49452/50000: episode: 6423, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.429 [1.000, 3.000],  loss: 0.002303, mae: 0.138316, mean_q: 0.200421\n",
      " 49461/50000: episode: 6424, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.003821, mae: 0.144006, mean_q: 0.206724\n",
      " 49470/50000: episode: 6425, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003222, mae: 0.141266, mean_q: 0.203919\n",
      " 49478/50000: episode: 6426, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002539, mae: 0.132430, mean_q: 0.193452\n",
      " 49483/50000: episode: 6427, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001942, mae: 0.137229, mean_q: 0.197256\n",
      " 49497/50000: episode: 6428, duration: 0.181s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002544, mae: 0.141417, mean_q: 0.200834\n",
      " 49505/50000: episode: 6429, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002733, mae: 0.141520, mean_q: 0.201622\n",
      " 49507/50000: episode: 6430, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.003566, mae: 0.139925, mean_q: 0.200639\n",
      " 49512/50000: episode: 6431, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002902, mae: 0.140777, mean_q: 0.199137\n",
      " 49519/50000: episode: 6432, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002692, mae: 0.142263, mean_q: 0.201026\n",
      " 49522/50000: episode: 6433, duration: 0.048s, episode steps:   3, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002336, mae: 0.130925, mean_q: 0.187017\n",
      " 49524/50000: episode: 6434, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003052, mae: 0.136367, mean_q: 0.195119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49537/50000: episode: 6435, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002975, mae: 0.140963, mean_q: 0.204818\n",
      " 49560/50000: episode: 6436, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002614, mae: 0.139885, mean_q: 0.201459\n",
      " 49567/50000: episode: 6437, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.429 [1.000, 3.000],  loss: 0.003237, mae: 0.144988, mean_q: 0.204783\n",
      " 49571/50000: episode: 6438, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.750 [2.000, 3.000],  loss: 0.002824, mae: 0.138548, mean_q: 0.194838\n",
      " 49573/50000: episode: 6439, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.006301, mae: 0.147818, mean_q: 0.206385\n",
      " 49575/50000: episode: 6440, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002825, mae: 0.134070, mean_q: 0.187451\n",
      " 49577/50000: episode: 6441, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.004054, mae: 0.138708, mean_q: 0.194182\n",
      " 49582/50000: episode: 6442, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002040, mae: 0.135759, mean_q: 0.193719\n",
      " 49586/50000: episode: 6443, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.002027, mae: 0.138773, mean_q: 0.197050\n",
      " 49595/50000: episode: 6444, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [1.000, 3.000],  loss: 0.002527, mae: 0.145130, mean_q: 0.205281\n",
      " 49601/50000: episode: 6445, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001790, mae: 0.140628, mean_q: 0.197224\n",
      " 49621/50000: episode: 6446, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002949, mae: 0.141314, mean_q: 0.198032\n",
      " 49632/50000: episode: 6447, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.002458, mae: 0.140032, mean_q: 0.196204\n",
      " 49645/50000: episode: 6448, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.231 [1.000, 3.000],  loss: 0.002524, mae: 0.136292, mean_q: 0.192456\n",
      " 49650/50000: episode: 6449, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001646, mae: 0.135238, mean_q: 0.192768\n",
      " 49657/50000: episode: 6450, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.004585, mae: 0.143041, mean_q: 0.202275\n",
      " 49663/50000: episode: 6451, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.167 [0.000, 3.000],  loss: 0.002705, mae: 0.141674, mean_q: 0.204298\n",
      " 49679/50000: episode: 6452, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002637, mae: 0.142133, mean_q: 0.200313\n",
      " 49689/50000: episode: 6453, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.003121, mae: 0.137040, mean_q: 0.191969\n",
      " 49694/50000: episode: 6454, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.003154, mae: 0.137526, mean_q: 0.193416\n",
      " 49696/50000: episode: 6455, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.002780, mae: 0.137134, mean_q: 0.196810\n",
      " 49700/50000: episode: 6456, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002981, mae: 0.140665, mean_q: 0.200351\n",
      " 49704/50000: episode: 6457, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.002935, mae: 0.141789, mean_q: 0.205595\n",
      " 49709/50000: episode: 6458, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002231, mae: 0.144105, mean_q: 0.209507\n",
      " 49715/50000: episode: 6459, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [1.000, 3.000],  loss: 0.002626, mae: 0.145961, mean_q: 0.210857\n",
      " 49723/50000: episode: 6460, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002393, mae: 0.142703, mean_q: 0.204383\n",
      " 49726/50000: episode: 6461, duration: 0.053s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.001955, mae: 0.142090, mean_q: 0.205486\n",
      " 49729/50000: episode: 6462, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.001775, mae: 0.140504, mean_q: 0.201012\n",
      " 49732/50000: episode: 6463, duration: 0.055s, episode steps:   3, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.004040, mae: 0.147609, mean_q: 0.208529\n",
      " 49744/50000: episode: 6464, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002813, mae: 0.144642, mean_q: 0.202233\n",
      " 49753/50000: episode: 6465, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002891, mae: 0.144095, mean_q: 0.201875\n",
      " 49778/50000: episode: 6466, duration: 0.341s, episode steps:  25, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.003426, mae: 0.140184, mean_q: 0.198713\n",
      " 49791/50000: episode: 6467, duration: 0.181s, episode steps:  13, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.002786, mae: 0.140640, mean_q: 0.203792\n",
      " 49793/50000: episode: 6468, duration: 0.039s, episode steps:   2, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 0.001889, mae: 0.128666, mean_q: 0.193016\n",
      " 49799/50000: episode: 6469, duration: 0.099s, episode steps:   6, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002770, mae: 0.134083, mean_q: 0.199557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49816/50000: episode: 6470, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002776, mae: 0.139178, mean_q: 0.202290\n",
      " 49820/50000: episode: 6471, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [1.000, 3.000],  loss: 0.002411, mae: 0.137949, mean_q: 0.199777\n",
      " 49823/50000: episode: 6472, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.001932, mae: 0.140442, mean_q: 0.204658\n",
      " 49843/50000: episode: 6473, duration: 0.258s, episode steps:  20, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003423, mae: 0.142258, mean_q: 0.202110\n",
      " 49857/50000: episode: 6474, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.002972, mae: 0.143629, mean_q: 0.203796\n",
      " 49869/50000: episode: 6475, duration: 0.168s, episode steps:  12, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002475, mae: 0.141652, mean_q: 0.199924\n",
      " 49878/50000: episode: 6476, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002636, mae: 0.144280, mean_q: 0.205064\n",
      " 49881/50000: episode: 6477, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003686, mae: 0.147741, mean_q: 0.207124\n",
      " 49883/50000: episode: 6478, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.500 [2.000, 3.000],  loss: 0.002714, mae: 0.141038, mean_q: 0.203601\n",
      " 49892/50000: episode: 6479, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.002676, mae: 0.143786, mean_q: 0.205920\n",
      " 49897/50000: episode: 6480, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.003543, mae: 0.143228, mean_q: 0.204667\n",
      " 49899/50000: episode: 6481, duration: 0.036s, episode steps:   2, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004336, mae: 0.151928, mean_q: 0.211848\n",
      " 49914/50000: episode: 6482, duration: 0.218s, episode steps:  15, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003330, mae: 0.144265, mean_q: 0.204371\n",
      " 49925/50000: episode: 6483, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [1.000, 3.000],  loss: 0.004374, mae: 0.146720, mean_q: 0.207948\n",
      " 49936/50000: episode: 6484, duration: 0.160s, episode steps:  11, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002970, mae: 0.141781, mean_q: 0.201000\n",
      " 49947/50000: episode: 6485, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.003376, mae: 0.146896, mean_q: 0.205690\n",
      " 49964/50000: episode: 6486, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.941 [0.000, 3.000],  loss: 0.003888, mae: 0.147528, mean_q: 0.208322\n",
      " 49967/50000: episode: 6487, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.003509, mae: 0.142678, mean_q: 0.201153\n",
      " 49974/50000: episode: 6488, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.002447, mae: 0.142373, mean_q: 0.200571\n",
      " 49977/50000: episode: 6489, duration: 0.052s, episode steps:   3, steps per second:  57, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.002948, mae: 0.146299, mean_q: 0.204796\n",
      " 49982/50000: episode: 6490, duration: 0.084s, episode steps:   5, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002631, mae: 0.139703, mean_q: 0.198409\n",
      " 49993/50000: episode: 6491, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002594, mae: 0.145656, mean_q: 0.205536\n",
      " 49999/50000: episode: 6492, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002610, mae: 0.143324, mean_q: 0.199851\n",
      "done, took 688.694 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe75394c1f0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af89d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode 1: reward: 0.000, steps: 5\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode 2: reward: 1.000, steps: 27\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode 3: reward: 0.000, steps: 19\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFF\u001b[41mH\u001b[0m\n",
      "HFFG\n",
      "Episode 4: reward: 0.000, steps: 53\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFF\u001b[41mH\u001b[0m\n",
      "HFFG\n",
      "Episode 5: reward: 0.000, steps: 14\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode 6: reward: 0.000, steps: 21\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode 7: reward: 1.000, steps: 71\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode 8: reward: 0.000, steps: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode 9: reward: 0.000, steps: 18\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode 10: reward: 1.000, steps: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6a00effa0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e7283",
   "metadata": {},
   "source": [
    "## Frozen Lake (8 by 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f9b1c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bd1ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    21/50000: episode: 1, duration: 0.477s, episode steps:  21, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    71/50000: episode: 2, duration: 0.037s, episode steps:  50, steps per second: 1347, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    83/50000: episode: 3, duration: 0.009s, episode steps:  12, steps per second: 1272, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   123/50000: episode: 4, duration: 1.736s, episode steps:  40, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.700 [0.000, 3.000],  loss: 0.139666, mae: 1.053857, mean_q: 0.792187\n",
      "   146/50000: episode: 5, duration: 0.127s, episode steps:  23, steps per second: 181, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.261 [0.000, 3.000],  loss: 0.126538, mae: 0.783607, mean_q: 0.843383\n",
      "   158/50000: episode: 6, duration: 0.068s, episode steps:  12, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.103001, mae: 0.573195, mean_q: 0.814182\n",
      "   176/50000: episode: 7, duration: 0.108s, episode steps:  18, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.086655, mae: 0.476064, mean_q: 0.772428\n",
      "   189/50000: episode: 8, duration: 0.073s, episode steps:  13, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.116363, mae: 0.383349, mean_q: 0.769211\n",
      "   250/50000: episode: 9, duration: 0.356s, episode steps:  61, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.071850, mae: 0.351923, mean_q: 0.702062\n",
      "   283/50000: episode: 10, duration: 0.182s, episode steps:  33, steps per second: 181, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.212 [0.000, 3.000],  loss: 0.048453, mae: 0.423352, mean_q: 0.657153\n",
      "   314/50000: episode: 11, duration: 0.181s, episode steps:  31, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.044508, mae: 0.476011, mean_q: 0.659680\n",
      "   338/50000: episode: 12, duration: 0.134s, episode steps:  24, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: 0.049371, mae: 0.569113, mean_q: 0.777912\n",
      "   358/50000: episode: 13, duration: 0.119s, episode steps:  20, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.037715, mae: 0.537170, mean_q: 0.733705\n",
      "   379/50000: episode: 14, duration: 0.120s, episode steps:  21, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.044540, mae: 0.528314, mean_q: 0.703338\n",
      "   395/50000: episode: 15, duration: 0.094s, episode steps:  16, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.036977, mae: 0.529713, mean_q: 0.729539\n",
      "   427/50000: episode: 16, duration: 0.179s, episode steps:  32, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.037764, mae: 0.498723, mean_q: 0.697199\n",
      "   485/50000: episode: 17, duration: 0.333s, episode steps:  58, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.030848, mae: 0.492494, mean_q: 0.692426\n",
      "   510/50000: episode: 18, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.029306, mae: 0.470039, mean_q: 0.658763\n",
      "   538/50000: episode: 19, duration: 0.210s, episode steps:  28, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.031782, mae: 0.488179, mean_q: 0.669119\n",
      "   553/50000: episode: 20, duration: 0.112s, episode steps:  15, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.027120, mae: 0.507167, mean_q: 0.692836\n",
      "   566/50000: episode: 21, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.385 [1.000, 3.000],  loss: 0.010629, mae: 0.468368, mean_q: 0.655225\n",
      "   577/50000: episode: 22, duration: 0.078s, episode steps:  11, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.024511, mae: 0.488537, mean_q: 0.660060\n",
      "   588/50000: episode: 23, duration: 0.073s, episode steps:  11, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.022512, mae: 0.502805, mean_q: 0.673606\n",
      "   624/50000: episode: 24, duration: 0.239s, episode steps:  36, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.027443, mae: 0.475630, mean_q: 0.645523\n",
      "   661/50000: episode: 25, duration: 0.242s, episode steps:  37, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.024090, mae: 0.481616, mean_q: 0.669896\n",
      "   676/50000: episode: 26, duration: 0.099s, episode steps:  15, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.022361, mae: 0.455743, mean_q: 0.647833\n",
      "   686/50000: episode: 27, duration: 0.070s, episode steps:  10, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.011710, mae: 0.439518, mean_q: 0.609693\n",
      "   719/50000: episode: 28, duration: 0.217s, episode steps:  33, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.020555, mae: 0.449954, mean_q: 0.614525\n",
      "   730/50000: episode: 29, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.018373, mae: 0.455175, mean_q: 0.621614\n",
      "   773/50000: episode: 30, duration: 0.312s, episode steps:  43, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.014948, mae: 0.434488, mean_q: 0.592287\n",
      "   792/50000: episode: 31, duration: 0.137s, episode steps:  19, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.013238, mae: 0.445581, mean_q: 0.601682\n",
      "   808/50000: episode: 32, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.018305, mae: 0.430162, mean_q: 0.573182\n",
      "   816/50000: episode: 33, duration: 0.055s, episode steps:   8, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.011899, mae: 0.418686, mean_q: 0.570325\n",
      "   835/50000: episode: 34, duration: 0.120s, episode steps:  19, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.019106, mae: 0.440712, mean_q: 0.597626\n",
      "   844/50000: episode: 35, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.009596, mae: 0.411534, mean_q: 0.558542\n",
      "   869/50000: episode: 36, duration: 0.142s, episode steps:  25, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.011612, mae: 0.432852, mean_q: 0.590654\n",
      "   892/50000: episode: 37, duration: 0.142s, episode steps:  23, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.013104, mae: 0.427806, mean_q: 0.592373\n",
      "   912/50000: episode: 38, duration: 0.123s, episode steps:  20, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.017837, mae: 0.423161, mean_q: 0.572910\n",
      "   918/50000: episode: 39, duration: 0.042s, episode steps:   6, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.011951, mae: 0.446144, mean_q: 0.603013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   951/50000: episode: 40, duration: 0.189s, episode steps:  33, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.013872, mae: 0.434144, mean_q: 0.585709\n",
      "   976/50000: episode: 41, duration: 0.139s, episode steps:  25, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.080 [0.000, 3.000],  loss: 0.008228, mae: 0.426205, mean_q: 0.573209\n",
      "  1010/50000: episode: 42, duration: 0.192s, episode steps:  34, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.009253, mae: 0.420450, mean_q: 0.573911\n",
      "  1026/50000: episode: 43, duration: 0.092s, episode steps:  16, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.006718, mae: 0.411687, mean_q: 0.555737\n",
      "  1088/50000: episode: 44, duration: 0.369s, episode steps:  62, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.177 [0.000, 3.000],  loss: 0.009597, mae: 0.417631, mean_q: 0.582247\n",
      "  1095/50000: episode: 45, duration: 0.047s, episode steps:   7, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.008896, mae: 0.426555, mean_q: 0.580042\n",
      "  1133/50000: episode: 46, duration: 0.235s, episode steps:  38, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.011117, mae: 0.400566, mean_q: 0.553699\n",
      "  1151/50000: episode: 47, duration: 0.111s, episode steps:  18, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.006422, mae: 0.410766, mean_q: 0.570154\n",
      "  1179/50000: episode: 48, duration: 0.187s, episode steps:  28, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.007448, mae: 0.401592, mean_q: 0.548822\n",
      "  1228/50000: episode: 49, duration: 0.325s, episode steps:  49, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 0.007821, mae: 0.390258, mean_q: 0.532988\n",
      "  1239/50000: episode: 50, duration: 0.078s, episode steps:  11, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.007873, mae: 0.381695, mean_q: 0.521341\n",
      "  1249/50000: episode: 51, duration: 0.070s, episode steps:  10, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.009221, mae: 0.381017, mean_q: 0.508279\n",
      "  1263/50000: episode: 52, duration: 0.099s, episode steps:  14, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.006186, mae: 0.387961, mean_q: 0.524612\n",
      "  1293/50000: episode: 53, duration: 0.201s, episode steps:  30, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.005829, mae: 0.379222, mean_q: 0.519484\n",
      "  1384/50000: episode: 54, duration: 0.592s, episode steps:  91, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.005788, mae: 0.373233, mean_q: 0.505340\n",
      "  1430/50000: episode: 55, duration: 0.305s, episode steps:  46, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.004504, mae: 0.363609, mean_q: 0.498922\n",
      "  1455/50000: episode: 56, duration: 0.166s, episode steps:  25, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.003917, mae: 0.358866, mean_q: 0.485486\n",
      "  1531/50000: episode: 57, duration: 0.500s, episode steps:  76, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.487 [0.000, 3.000],  loss: 0.004708, mae: 0.359488, mean_q: 0.490808\n",
      "  1546/50000: episode: 58, duration: 0.104s, episode steps:  15, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.003487, mae: 0.355915, mean_q: 0.494697\n",
      "  1583/50000: episode: 59, duration: 0.250s, episode steps:  37, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 0.003734, mae: 0.355322, mean_q: 0.484744\n",
      "  1657/50000: episode: 60, duration: 0.481s, episode steps:  74, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.003621, mae: 0.350455, mean_q: 0.476851\n",
      "  1683/50000: episode: 61, duration: 0.145s, episode steps:  26, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002734, mae: 0.351743, mean_q: 0.480102\n",
      "  1702/50000: episode: 62, duration: 0.112s, episode steps:  19, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.002776, mae: 0.351886, mean_q: 0.483755\n",
      "  1736/50000: episode: 63, duration: 0.189s, episode steps:  34, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.147 [0.000, 3.000],  loss: 0.003150, mae: 0.350297, mean_q: 0.477480\n",
      "  1744/50000: episode: 64, duration: 0.052s, episode steps:   8, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.004377, mae: 0.337753, mean_q: 0.454762\n",
      "  1763/50000: episode: 65, duration: 0.108s, episode steps:  19, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.003451, mae: 0.342623, mean_q: 0.477670\n",
      "  1864/50000: episode: 66, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.003322, mae: 0.340991, mean_q: 0.463601\n",
      "  1896/50000: episode: 67, duration: 0.179s, episode steps:  32, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002456, mae: 0.338831, mean_q: 0.460118\n",
      "  1919/50000: episode: 68, duration: 0.132s, episode steps:  23, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002657, mae: 0.336067, mean_q: 0.459789\n",
      "  1934/50000: episode: 69, duration: 0.085s, episode steps:  15, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002615, mae: 0.344741, mean_q: 0.470612\n",
      "  1976/50000: episode: 70, duration: 0.235s, episode steps:  42, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002367, mae: 0.338623, mean_q: 0.459318\n",
      "  1996/50000: episode: 71, duration: 0.113s, episode steps:  20, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.002752, mae: 0.328118, mean_q: 0.445281\n",
      "  2025/50000: episode: 72, duration: 0.165s, episode steps:  29, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.003146, mae: 0.333375, mean_q: 0.450818\n",
      "  2036/50000: episode: 73, duration: 0.065s, episode steps:  11, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002760, mae: 0.328332, mean_q: 0.457045\n",
      "  2048/50000: episode: 74, duration: 0.076s, episode steps:  12, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002390, mae: 0.330086, mean_q: 0.459127\n",
      "  2134/50000: episode: 75, duration: 0.545s, episode steps:  86, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.002914, mae: 0.331769, mean_q: 0.450848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2226/50000: episode: 76, duration: 0.596s, episode steps:  92, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.380 [0.000, 3.000],  loss: 0.002433, mae: 0.326069, mean_q: 0.445476\n",
      "  2246/50000: episode: 77, duration: 0.114s, episode steps:  20, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.003683, mae: 0.324133, mean_q: 0.440504\n",
      "  2272/50000: episode: 78, duration: 0.151s, episode steps:  26, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002414, mae: 0.326873, mean_q: 0.443588\n",
      "  2302/50000: episode: 79, duration: 0.167s, episode steps:  30, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.002788, mae: 0.327236, mean_q: 0.447221\n",
      "  2346/50000: episode: 80, duration: 0.245s, episode steps:  44, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.136 [0.000, 3.000],  loss: 0.002810, mae: 0.319176, mean_q: 0.436547\n",
      "  2391/50000: episode: 81, duration: 0.252s, episode steps:  45, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001996, mae: 0.323842, mean_q: 0.444006\n",
      "  2442/50000: episode: 82, duration: 0.284s, episode steps:  51, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002323, mae: 0.319066, mean_q: 0.434604\n",
      "  2460/50000: episode: 83, duration: 0.103s, episode steps:  18, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002584, mae: 0.318529, mean_q: 0.432152\n",
      "  2482/50000: episode: 84, duration: 0.127s, episode steps:  22, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002832, mae: 0.321193, mean_q: 0.439062\n",
      "  2523/50000: episode: 85, duration: 0.249s, episode steps:  41, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.171 [0.000, 3.000],  loss: 0.002518, mae: 0.318664, mean_q: 0.436562\n",
      "  2554/50000: episode: 86, duration: 0.188s, episode steps:  31, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.710 [0.000, 3.000],  loss: 0.002512, mae: 0.322294, mean_q: 0.438623\n",
      "  2579/50000: episode: 87, duration: 0.156s, episode steps:  25, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002270, mae: 0.321827, mean_q: 0.439615\n",
      "  2595/50000: episode: 88, duration: 0.098s, episode steps:  16, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002760, mae: 0.315678, mean_q: 0.420790\n",
      "  2601/50000: episode: 89, duration: 0.037s, episode steps:   6, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001339, mae: 0.321388, mean_q: 0.434845\n",
      "  2632/50000: episode: 90, duration: 0.177s, episode steps:  31, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.002176, mae: 0.310238, mean_q: 0.424739\n",
      "  2648/50000: episode: 91, duration: 0.092s, episode steps:  16, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.001472, mae: 0.315270, mean_q: 0.439014\n",
      "  2669/50000: episode: 92, duration: 0.121s, episode steps:  21, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.048 [0.000, 3.000],  loss: 0.002438, mae: 0.313379, mean_q: 0.428096\n",
      "  2691/50000: episode: 93, duration: 0.126s, episode steps:  22, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002428, mae: 0.318084, mean_q: 0.435054\n",
      "  2722/50000: episode: 94, duration: 0.177s, episode steps:  31, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002612, mae: 0.316068, mean_q: 0.431864\n",
      "  2744/50000: episode: 95, duration: 0.129s, episode steps:  22, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002495, mae: 0.311350, mean_q: 0.424368\n",
      "  2775/50000: episode: 96, duration: 0.208s, episode steps:  31, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.226 [0.000, 3.000],  loss: 0.002641, mae: 0.308848, mean_q: 0.419325\n",
      "  2807/50000: episode: 97, duration: 0.215s, episode steps:  32, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.002276, mae: 0.306075, mean_q: 0.417786\n",
      "  2821/50000: episode: 98, duration: 0.098s, episode steps:  14, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.001976, mae: 0.313832, mean_q: 0.424923\n",
      "  2835/50000: episode: 99, duration: 0.096s, episode steps:  14, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.001874, mae: 0.311667, mean_q: 0.432733\n",
      "  2869/50000: episode: 100, duration: 0.195s, episode steps:  34, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002383, mae: 0.311278, mean_q: 0.428735\n",
      "  2874/50000: episode: 101, duration: 0.035s, episode steps:   5, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002391, mae: 0.327595, mean_q: 0.441086\n",
      "  2897/50000: episode: 102, duration: 0.145s, episode steps:  23, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.261 [0.000, 3.000],  loss: 0.002327, mae: 0.316286, mean_q: 0.431681\n",
      "  2910/50000: episode: 103, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002935, mae: 0.306674, mean_q: 0.410936\n",
      "  2934/50000: episode: 104, duration: 0.143s, episode steps:  24, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002624, mae: 0.315345, mean_q: 0.437743\n",
      "  2968/50000: episode: 105, duration: 0.194s, episode steps:  34, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002413, mae: 0.315937, mean_q: 0.427121\n",
      "  2983/50000: episode: 106, duration: 0.091s, episode steps:  15, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002807, mae: 0.315318, mean_q: 0.425256\n",
      "  2999/50000: episode: 107, duration: 0.099s, episode steps:  16, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001941, mae: 0.314397, mean_q: 0.427281\n",
      "  3020/50000: episode: 108, duration: 0.133s, episode steps:  21, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.190 [0.000, 3.000],  loss: 0.003289, mae: 0.305430, mean_q: 0.415901\n",
      "  3055/50000: episode: 109, duration: 0.214s, episode steps:  35, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002324, mae: 0.311927, mean_q: 0.423672\n",
      "  3072/50000: episode: 110, duration: 0.107s, episode steps:  17, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002397, mae: 0.308177, mean_q: 0.426619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3111/50000: episode: 111, duration: 0.248s, episode steps:  39, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002685, mae: 0.310244, mean_q: 0.426282\n",
      "  3153/50000: episode: 112, duration: 0.274s, episode steps:  42, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002688, mae: 0.313888, mean_q: 0.431531\n",
      "  3228/50000: episode: 113, duration: 0.423s, episode steps:  75, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.747 [0.000, 3.000],  loss: 0.002643, mae: 0.311758, mean_q: 0.430436\n",
      "  3237/50000: episode: 114, duration: 0.054s, episode steps:   9, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002230, mae: 0.301886, mean_q: 0.400677\n",
      "  3249/50000: episode: 115, duration: 0.069s, episode steps:  12, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002322, mae: 0.320566, mean_q: 0.435471\n",
      "  3307/50000: episode: 116, duration: 0.322s, episode steps:  58, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002313, mae: 0.303941, mean_q: 0.417854\n",
      "  3331/50000: episode: 117, duration: 0.134s, episode steps:  24, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002375, mae: 0.307137, mean_q: 0.419279\n",
      "  3350/50000: episode: 118, duration: 0.115s, episode steps:  19, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002561, mae: 0.305549, mean_q: 0.419405\n",
      "  3364/50000: episode: 119, duration: 0.089s, episode steps:  14, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002421, mae: 0.309311, mean_q: 0.425197\n",
      "  3404/50000: episode: 120, duration: 0.241s, episode steps:  40, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.002122, mae: 0.309896, mean_q: 0.428356\n",
      "  3410/50000: episode: 121, duration: 0.038s, episode steps:   6, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.002157, mae: 0.302367, mean_q: 0.415526\n",
      "  3458/50000: episode: 122, duration: 0.267s, episode steps:  48, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.002276, mae: 0.306670, mean_q: 0.416146\n",
      "  3476/50000: episode: 123, duration: 0.102s, episode steps:  18, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002527, mae: 0.306498, mean_q: 0.416898\n",
      "  3488/50000: episode: 124, duration: 0.069s, episode steps:  12, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003391, mae: 0.314780, mean_q: 0.428936\n",
      "  3507/50000: episode: 125, duration: 0.111s, episode steps:  19, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002469, mae: 0.307991, mean_q: 0.422713\n",
      "  3553/50000: episode: 126, duration: 0.259s, episode steps:  46, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.002909, mae: 0.306166, mean_q: 0.422987\n",
      "  3639/50000: episode: 127, duration: 0.475s, episode steps:  86, steps per second: 181, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002185, mae: 0.307227, mean_q: 0.419159\n",
      "  3647/50000: episode: 128, duration: 0.049s, episode steps:   8, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001736, mae: 0.305427, mean_q: 0.419597\n",
      "  3683/50000: episode: 129, duration: 0.204s, episode steps:  36, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002120, mae: 0.306245, mean_q: 0.420685\n",
      "  3719/50000: episode: 130, duration: 0.200s, episode steps:  36, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002543, mae: 0.304677, mean_q: 0.417078\n",
      "  3766/50000: episode: 131, duration: 0.280s, episode steps:  47, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002360, mae: 0.304033, mean_q: 0.417387\n",
      "  3805/50000: episode: 132, duration: 0.259s, episode steps:  39, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.256 [0.000, 3.000],  loss: 0.002361, mae: 0.308370, mean_q: 0.425976\n",
      "  3841/50000: episode: 133, duration: 0.206s, episode steps:  36, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.002369, mae: 0.304939, mean_q: 0.416968\n",
      "  3927/50000: episode: 134, duration: 0.515s, episode steps:  86, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.002486, mae: 0.307153, mean_q: 0.420927\n",
      "  3956/50000: episode: 135, duration: 0.162s, episode steps:  29, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.002396, mae: 0.306350, mean_q: 0.419371\n",
      "  3969/50000: episode: 136, duration: 0.086s, episode steps:  13, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.002773, mae: 0.306213, mean_q: 0.413953\n",
      "  3995/50000: episode: 137, duration: 0.160s, episode steps:  26, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002347, mae: 0.306589, mean_q: 0.425582\n",
      "  4092/50000: episode: 138, duration: 0.576s, episode steps:  97, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002255, mae: 0.306565, mean_q: 0.419699\n",
      "  4118/50000: episode: 139, duration: 0.148s, episode steps:  26, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002657, mae: 0.303873, mean_q: 0.417218\n",
      "  4236/50000: episode: 140, duration: 0.654s, episode steps: 118, steps per second: 181, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.002338, mae: 0.305191, mean_q: 0.415476\n",
      "  4245/50000: episode: 141, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 2.000],  loss: 0.002187, mae: 0.298960, mean_q: 0.413694\n",
      "  4297/50000: episode: 142, duration: 0.307s, episode steps:  52, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: 0.002332, mae: 0.307146, mean_q: 0.419964\n",
      "  4358/50000: episode: 143, duration: 0.338s, episode steps:  61, steps per second: 181, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.377 [0.000, 3.000],  loss: 0.002094, mae: 0.303784, mean_q: 0.416910\n",
      "  4401/50000: episode: 144, duration: 0.242s, episode steps:  43, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.349 [0.000, 3.000],  loss: 0.002306, mae: 0.300081, mean_q: 0.408320\n",
      "  4412/50000: episode: 145, duration: 0.067s, episode steps:  11, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001586, mae: 0.300939, mean_q: 0.410687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4459/50000: episode: 146, duration: 0.272s, episode steps:  47, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002242, mae: 0.299869, mean_q: 0.411426\n",
      "  4516/50000: episode: 147, duration: 0.318s, episode steps:  57, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.002564, mae: 0.300068, mean_q: 0.407652\n",
      "  4553/50000: episode: 148, duration: 0.208s, episode steps:  37, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002502, mae: 0.294741, mean_q: 0.405147\n",
      "  4593/50000: episode: 149, duration: 0.238s, episode steps:  40, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002422, mae: 0.300879, mean_q: 0.414724\n",
      "  4611/50000: episode: 150, duration: 0.114s, episode steps:  18, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.002219, mae: 0.302811, mean_q: 0.408975\n",
      "  4658/50000: episode: 151, duration: 0.286s, episode steps:  47, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.002046, mae: 0.298421, mean_q: 0.409930\n",
      "  4689/50000: episode: 152, duration: 0.190s, episode steps:  31, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.226 [0.000, 3.000],  loss: 0.001802, mae: 0.300433, mean_q: 0.413423\n",
      "  4712/50000: episode: 153, duration: 0.144s, episode steps:  23, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.348 [0.000, 3.000],  loss: 0.002438, mae: 0.298235, mean_q: 0.402604\n",
      "  4747/50000: episode: 154, duration: 0.223s, episode steps:  35, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002129, mae: 0.296185, mean_q: 0.408656\n",
      "  4755/50000: episode: 155, duration: 0.058s, episode steps:   8, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001938, mae: 0.299644, mean_q: 0.401705\n",
      "  4810/50000: episode: 156, duration: 0.353s, episode steps:  55, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002083, mae: 0.299143, mean_q: 0.409914\n",
      "  4818/50000: episode: 157, duration: 0.057s, episode steps:   8, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001714, mae: 0.298807, mean_q: 0.422107\n",
      "  4863/50000: episode: 158, duration: 0.291s, episode steps:  45, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002134, mae: 0.300848, mean_q: 0.412861\n",
      "  4890/50000: episode: 159, duration: 0.176s, episode steps:  27, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002221, mae: 0.294579, mean_q: 0.400039\n",
      "  4907/50000: episode: 160, duration: 0.117s, episode steps:  17, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002396, mae: 0.295955, mean_q: 0.409959\n",
      "  4915/50000: episode: 161, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002075, mae: 0.293316, mean_q: 0.411685\n",
      "  4943/50000: episode: 162, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.964 [0.000, 3.000],  loss: 0.002347, mae: 0.297656, mean_q: 0.410167\n",
      "  5026/50000: episode: 163, duration: 0.531s, episode steps:  83, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.687 [0.000, 3.000],  loss: 0.001884, mae: 0.302417, mean_q: 0.412037\n",
      "  5034/50000: episode: 164, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.002180, mae: 0.283645, mean_q: 0.392092\n",
      "  5064/50000: episode: 165, duration: 0.201s, episode steps:  30, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.001863, mae: 0.303476, mean_q: 0.415508\n",
      "  5113/50000: episode: 166, duration: 0.319s, episode steps:  49, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002283, mae: 0.297782, mean_q: 0.407032\n",
      "  5137/50000: episode: 167, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001666, mae: 0.304775, mean_q: 0.414015\n",
      "  5200/50000: episode: 168, duration: 0.407s, episode steps:  63, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.540 [0.000, 3.000],  loss: 0.002006, mae: 0.294555, mean_q: 0.399772\n",
      "  5216/50000: episode: 169, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.001696, mae: 0.301064, mean_q: 0.415614\n",
      "  5238/50000: episode: 170, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.864 [0.000, 3.000],  loss: 0.002127, mae: 0.292651, mean_q: 0.399570\n",
      "  5243/50000: episode: 171, duration: 0.038s, episode steps:   5, steps per second: 130, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001556, mae: 0.289527, mean_q: 0.399872\n",
      "  5309/50000: episode: 172, duration: 0.461s, episode steps:  66, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002044, mae: 0.296851, mean_q: 0.405802\n",
      "  5342/50000: episode: 173, duration: 0.238s, episode steps:  33, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.121 [0.000, 3.000],  loss: 0.002038, mae: 0.296116, mean_q: 0.406545\n",
      "  5411/50000: episode: 174, duration: 0.465s, episode steps:  69, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001775, mae: 0.296997, mean_q: 0.403848\n",
      "  5432/50000: episode: 175, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002405, mae: 0.292532, mean_q: 0.403166\n",
      "  5450/50000: episode: 176, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002146, mae: 0.297118, mean_q: 0.408474\n",
      "  5514/50000: episode: 177, duration: 0.482s, episode steps:  64, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001896, mae: 0.295086, mean_q: 0.403347\n",
      "  5527/50000: episode: 178, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.002111, mae: 0.292406, mean_q: 0.398778\n",
      "  5599/50000: episode: 179, duration: 0.533s, episode steps:  72, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002015, mae: 0.287799, mean_q: 0.395172\n",
      "  5628/50000: episode: 180, duration: 0.198s, episode steps:  29, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.001655, mae: 0.292660, mean_q: 0.398990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5677/50000: episode: 181, duration: 0.329s, episode steps:  49, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001782, mae: 0.286810, mean_q: 0.390395\n",
      "  5706/50000: episode: 182, duration: 0.193s, episode steps:  29, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.690 [0.000, 3.000],  loss: 0.001828, mae: 0.292993, mean_q: 0.401533\n",
      "  5721/50000: episode: 183, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001302, mae: 0.286674, mean_q: 0.385183\n",
      "  5739/50000: episode: 184, duration: 0.123s, episode steps:  18, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.001460, mae: 0.291649, mean_q: 0.400517\n",
      "  5783/50000: episode: 185, duration: 0.295s, episode steps:  44, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.001922, mae: 0.294529, mean_q: 0.402105\n",
      "  5816/50000: episode: 186, duration: 0.221s, episode steps:  33, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001521, mae: 0.294781, mean_q: 0.402259\n",
      "  5848/50000: episode: 187, duration: 0.219s, episode steps:  32, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.156 [0.000, 3.000],  loss: 0.002315, mae: 0.292399, mean_q: 0.401235\n",
      "  5863/50000: episode: 188, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001704, mae: 0.293090, mean_q: 0.403862\n",
      "  5898/50000: episode: 189, duration: 0.238s, episode steps:  35, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001693, mae: 0.292250, mean_q: 0.395891\n",
      "  5903/50000: episode: 190, duration: 0.039s, episode steps:   5, steps per second: 128, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.001380, mae: 0.296497, mean_q: 0.402648\n",
      "  5920/50000: episode: 191, duration: 0.117s, episode steps:  17, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.882 [0.000, 3.000],  loss: 0.001603, mae: 0.289958, mean_q: 0.393199\n",
      "  5944/50000: episode: 192, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001822, mae: 0.291391, mean_q: 0.398459\n",
      "  5981/50000: episode: 193, duration: 0.268s, episode steps:  37, steps per second: 138, episode reward:  1.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.001756, mae: 0.284132, mean_q: 0.387264\n",
      "  6037/50000: episode: 194, duration: 0.451s, episode steps:  56, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.946 [0.000, 3.000],  loss: 0.001982, mae: 0.287192, mean_q: 0.392854\n",
      "  6053/50000: episode: 195, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.001939, mae: 0.289933, mean_q: 0.396779\n",
      "  6068/50000: episode: 196, duration: 0.129s, episode steps:  15, steps per second: 116, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002071, mae: 0.290322, mean_q: 0.396070\n",
      "  6080/50000: episode: 197, duration: 0.104s, episode steps:  12, steps per second: 116, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.917 [0.000, 3.000],  loss: 0.002352, mae: 0.282054, mean_q: 0.387385\n",
      "  6101/50000: episode: 198, duration: 0.175s, episode steps:  21, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.190 [0.000, 3.000],  loss: 0.002071, mae: 0.287709, mean_q: 0.389940\n",
      "  6125/50000: episode: 199, duration: 0.196s, episode steps:  24, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001688, mae: 0.288244, mean_q: 0.398552\n",
      "  6139/50000: episode: 200, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.003293, mae: 0.286977, mean_q: 0.389001\n",
      "  6168/50000: episode: 201, duration: 0.201s, episode steps:  29, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.002559, mae: 0.283547, mean_q: 0.390759\n",
      "  6216/50000: episode: 202, duration: 0.332s, episode steps:  48, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.001912, mae: 0.281842, mean_q: 0.383761\n",
      "  6255/50000: episode: 203, duration: 0.273s, episode steps:  39, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.002588, mae: 0.280953, mean_q: 0.379569\n",
      "  6285/50000: episode: 204, duration: 0.210s, episode steps:  30, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001746, mae: 0.283425, mean_q: 0.390357\n",
      "  6304/50000: episode: 205, duration: 0.137s, episode steps:  19, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.001902, mae: 0.282249, mean_q: 0.380662\n",
      "  6319/50000: episode: 206, duration: 0.111s, episode steps:  15, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.002030, mae: 0.281426, mean_q: 0.391488\n",
      "  6366/50000: episode: 207, duration: 0.327s, episode steps:  47, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001877, mae: 0.281607, mean_q: 0.384181\n",
      "  6435/50000: episode: 208, duration: 0.474s, episode steps:  69, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001871, mae: 0.280840, mean_q: 0.381266\n",
      "  6507/50000: episode: 209, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.002018, mae: 0.281319, mean_q: 0.384881\n",
      "  6553/50000: episode: 210, duration: 0.317s, episode steps:  46, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001791, mae: 0.280625, mean_q: 0.385871\n",
      "  6574/50000: episode: 211, duration: 0.147s, episode steps:  21, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.381 [0.000, 3.000],  loss: 0.002406, mae: 0.282352, mean_q: 0.386125\n",
      "  6614/50000: episode: 212, duration: 0.282s, episode steps:  40, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.825 [0.000, 3.000],  loss: 0.001687, mae: 0.280005, mean_q: 0.382997\n",
      "  6640/50000: episode: 213, duration: 0.181s, episode steps:  26, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001826, mae: 0.277353, mean_q: 0.379450\n",
      "  6657/50000: episode: 214, duration: 0.115s, episode steps:  17, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001743, mae: 0.284560, mean_q: 0.398731\n",
      "  6666/50000: episode: 215, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001644, mae: 0.280510, mean_q: 0.378201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6689/50000: episode: 216, duration: 0.144s, episode steps:  23, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001849, mae: 0.278806, mean_q: 0.385038\n",
      "  6708/50000: episode: 217, duration: 0.135s, episode steps:  19, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.001366, mae: 0.283895, mean_q: 0.388395\n",
      "  6713/50000: episode: 218, duration: 0.039s, episode steps:   5, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.002320, mae: 0.278797, mean_q: 0.372404\n",
      "  6731/50000: episode: 219, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001967, mae: 0.276786, mean_q: 0.381597\n",
      "  6826/50000: episode: 220, duration: 0.584s, episode steps:  95, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.001953, mae: 0.278610, mean_q: 0.377085\n",
      "  6844/50000: episode: 221, duration: 0.104s, episode steps:  18, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.002026, mae: 0.274444, mean_q: 0.375998\n",
      "  6860/50000: episode: 222, duration: 0.092s, episode steps:  16, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.001692, mae: 0.281543, mean_q: 0.387320\n",
      "  6884/50000: episode: 223, duration: 0.140s, episode steps:  24, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001619, mae: 0.277846, mean_q: 0.377550\n",
      "  6896/50000: episode: 224, duration: 0.070s, episode steps:  12, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001537, mae: 0.288742, mean_q: 0.392368\n",
      "  6916/50000: episode: 225, duration: 0.115s, episode steps:  20, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.001865, mae: 0.278965, mean_q: 0.381337\n",
      "  6943/50000: episode: 226, duration: 0.158s, episode steps:  27, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.002674, mae: 0.278210, mean_q: 0.381066\n",
      "  6972/50000: episode: 227, duration: 0.162s, episode steps:  29, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.862 [0.000, 3.000],  loss: 0.002213, mae: 0.277012, mean_q: 0.381864\n",
      "  6994/50000: episode: 228, duration: 0.129s, episode steps:  22, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.002318, mae: 0.280510, mean_q: 0.378799\n",
      "  7030/50000: episode: 229, duration: 0.202s, episode steps:  36, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.194 [0.000, 3.000],  loss: 0.002202, mae: 0.276595, mean_q: 0.379185\n",
      "  7081/50000: episode: 230, duration: 0.290s, episode steps:  51, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.002043, mae: 0.275782, mean_q: 0.375826\n",
      "  7089/50000: episode: 231, duration: 0.050s, episode steps:   8, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.001529, mae: 0.272505, mean_q: 0.370821\n",
      "  7163/50000: episode: 232, duration: 0.411s, episode steps:  74, steps per second: 180, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002097, mae: 0.274098, mean_q: 0.378130\n",
      "  7172/50000: episode: 233, duration: 0.054s, episode steps:   9, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001590, mae: 0.281449, mean_q: 0.377885\n",
      "  7179/50000: episode: 234, duration: 0.043s, episode steps:   7, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001691, mae: 0.282181, mean_q: 0.386039\n",
      "  7201/50000: episode: 235, duration: 0.127s, episode steps:  22, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.955 [0.000, 3.000],  loss: 0.001798, mae: 0.274299, mean_q: 0.373234\n",
      "  7240/50000: episode: 236, duration: 0.220s, episode steps:  39, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.205 [0.000, 3.000],  loss: 0.002395, mae: 0.271958, mean_q: 0.371660\n",
      "  7249/50000: episode: 237, duration: 0.055s, episode steps:   9, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001988, mae: 0.277886, mean_q: 0.387110\n",
      "  7291/50000: episode: 238, duration: 0.255s, episode steps:  42, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001905, mae: 0.270677, mean_q: 0.373387\n",
      "  7299/50000: episode: 239, duration: 0.049s, episode steps:   8, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.001431, mae: 0.274860, mean_q: 0.374181\n",
      "  7312/50000: episode: 240, duration: 0.076s, episode steps:  13, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002186, mae: 0.269853, mean_q: 0.364380\n",
      "  7328/50000: episode: 241, duration: 0.096s, episode steps:  16, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002260, mae: 0.271083, mean_q: 0.372935\n",
      "  7375/50000: episode: 242, duration: 0.267s, episode steps:  47, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.001966, mae: 0.274475, mean_q: 0.378350\n",
      "  7382/50000: episode: 243, duration: 0.044s, episode steps:   7, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.001407, mae: 0.276160, mean_q: 0.388947\n",
      "  7440/50000: episode: 244, duration: 0.328s, episode steps:  58, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001890, mae: 0.271523, mean_q: 0.372930\n",
      "  7458/50000: episode: 245, duration: 0.103s, episode steps:  18, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001675, mae: 0.264369, mean_q: 0.363961\n",
      "  7483/50000: episode: 246, duration: 0.145s, episode steps:  25, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001636, mae: 0.266281, mean_q: 0.366359\n",
      "  7496/50000: episode: 247, duration: 0.077s, episode steps:  13, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001525, mae: 0.274445, mean_q: 0.376717\n",
      "  7514/50000: episode: 248, duration: 0.104s, episode steps:  18, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.002332, mae: 0.272588, mean_q: 0.372503\n",
      "  7568/50000: episode: 249, duration: 0.319s, episode steps:  54, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.002057, mae: 0.269001, mean_q: 0.369934\n",
      "  7588/50000: episode: 250, duration: 0.121s, episode steps:  20, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001848, mae: 0.273390, mean_q: 0.371258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7607/50000: episode: 251, duration: 0.137s, episode steps:  19, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.158 [0.000, 3.000],  loss: 0.001765, mae: 0.271435, mean_q: 0.370980\n",
      "  7643/50000: episode: 252, duration: 0.246s, episode steps:  36, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001962, mae: 0.267758, mean_q: 0.365271\n",
      "  7653/50000: episode: 253, duration: 0.061s, episode steps:  10, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001545, mae: 0.264450, mean_q: 0.368131\n",
      "  7690/50000: episode: 254, duration: 0.210s, episode steps:  37, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.001689, mae: 0.268098, mean_q: 0.364539\n",
      "  7729/50000: episode: 255, duration: 0.221s, episode steps:  39, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002661, mae: 0.270700, mean_q: 0.367005\n",
      "  7742/50000: episode: 256, duration: 0.076s, episode steps:  13, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.001347, mae: 0.269228, mean_q: 0.369932\n",
      "  7755/50000: episode: 257, duration: 0.077s, episode steps:  13, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.002277, mae: 0.266005, mean_q: 0.364428\n",
      "  7830/50000: episode: 258, duration: 0.468s, episode steps:  75, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.001800, mae: 0.266721, mean_q: 0.363208\n",
      "  7854/50000: episode: 259, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001268, mae: 0.261284, mean_q: 0.359895\n",
      "  7859/50000: episode: 260, duration: 0.034s, episode steps:   5, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002517, mae: 0.270971, mean_q: 0.364475\n",
      "  7879/50000: episode: 261, duration: 0.122s, episode steps:  20, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001581, mae: 0.266161, mean_q: 0.365359\n",
      "  7890/50000: episode: 262, duration: 0.067s, episode steps:  11, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.002020, mae: 0.266480, mean_q: 0.361745\n",
      "  7958/50000: episode: 263, duration: 0.395s, episode steps:  68, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001515, mae: 0.264907, mean_q: 0.358309\n",
      "  7983/50000: episode: 264, duration: 0.164s, episode steps:  25, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.002236, mae: 0.265714, mean_q: 0.358574\n",
      "  7998/50000: episode: 265, duration: 0.091s, episode steps:  15, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.867 [0.000, 3.000],  loss: 0.001963, mae: 0.264981, mean_q: 0.354854\n",
      "  8064/50000: episode: 266, duration: 0.396s, episode steps:  66, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001644, mae: 0.261745, mean_q: 0.357604\n",
      "  8089/50000: episode: 267, duration: 0.164s, episode steps:  25, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.001986, mae: 0.257136, mean_q: 0.351722\n",
      "  8109/50000: episode: 268, duration: 0.118s, episode steps:  20, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001713, mae: 0.256946, mean_q: 0.357891\n",
      "  8125/50000: episode: 269, duration: 0.094s, episode steps:  16, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001601, mae: 0.257376, mean_q: 0.354423\n",
      "  8142/50000: episode: 270, duration: 0.098s, episode steps:  17, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.059 [0.000, 3.000],  loss: 0.002617, mae: 0.265789, mean_q: 0.364466\n",
      "  8157/50000: episode: 271, duration: 0.090s, episode steps:  15, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.133 [0.000, 3.000],  loss: 0.001541, mae: 0.257354, mean_q: 0.357063\n",
      "  8165/50000: episode: 272, duration: 0.049s, episode steps:   8, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 2.000],  loss: 0.001654, mae: 0.266246, mean_q: 0.360554\n",
      "  8177/50000: episode: 273, duration: 0.071s, episode steps:  12, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001912, mae: 0.256586, mean_q: 0.350176\n",
      "  8205/50000: episode: 274, duration: 0.163s, episode steps:  28, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001784, mae: 0.260349, mean_q: 0.357641\n",
      "  8237/50000: episode: 275, duration: 0.181s, episode steps:  32, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001305, mae: 0.258709, mean_q: 0.348766\n",
      "  8253/50000: episode: 276, duration: 0.098s, episode steps:  16, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.001155, mae: 0.258175, mean_q: 0.350385\n",
      "  8264/50000: episode: 277, duration: 0.066s, episode steps:  11, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.001574, mae: 0.266166, mean_q: 0.364268\n",
      "  8271/50000: episode: 278, duration: 0.044s, episode steps:   7, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001418, mae: 0.253135, mean_q: 0.339466\n",
      "  8285/50000: episode: 279, duration: 0.090s, episode steps:  14, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001814, mae: 0.257148, mean_q: 0.354463\n",
      "  8313/50000: episode: 280, duration: 0.171s, episode steps:  28, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.001577, mae: 0.261745, mean_q: 0.352790\n",
      "  8368/50000: episode: 281, duration: 0.309s, episode steps:  55, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001533, mae: 0.256063, mean_q: 0.348063\n",
      "  8416/50000: episode: 282, duration: 0.271s, episode steps:  48, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001463, mae: 0.258187, mean_q: 0.350552\n",
      "  8425/50000: episode: 283, duration: 0.055s, episode steps:   9, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001136, mae: 0.254491, mean_q: 0.355436\n",
      "  8456/50000: episode: 284, duration: 0.178s, episode steps:  31, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001644, mae: 0.250490, mean_q: 0.337688\n",
      "  8470/50000: episode: 285, duration: 0.086s, episode steps:  14, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001351, mae: 0.251751, mean_q: 0.341380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8502/50000: episode: 286, duration: 0.200s, episode steps:  32, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001346, mae: 0.251873, mean_q: 0.343612\n",
      "  8573/50000: episode: 287, duration: 0.452s, episode steps:  71, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.001479, mae: 0.255259, mean_q: 0.344422\n",
      "  8581/50000: episode: 288, duration: 0.054s, episode steps:   8, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.001391, mae: 0.248120, mean_q: 0.341694\n",
      "  8619/50000: episode: 289, duration: 0.227s, episode steps:  38, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001391, mae: 0.254783, mean_q: 0.347460\n",
      "  8651/50000: episode: 290, duration: 0.179s, episode steps:  32, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001129, mae: 0.252115, mean_q: 0.344210\n",
      "  8696/50000: episode: 291, duration: 0.255s, episode steps:  45, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.001504, mae: 0.252867, mean_q: 0.343035\n",
      "  8715/50000: episode: 292, duration: 0.108s, episode steps:  19, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.001660, mae: 0.250300, mean_q: 0.341152\n",
      "  8743/50000: episode: 293, duration: 0.187s, episode steps:  28, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001324, mae: 0.258067, mean_q: 0.352621\n",
      "  8784/50000: episode: 294, duration: 0.234s, episode steps:  41, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.001825, mae: 0.249471, mean_q: 0.339632\n",
      "  8805/50000: episode: 295, duration: 0.121s, episode steps:  21, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001224, mae: 0.245405, mean_q: 0.336608\n",
      "  8830/50000: episode: 296, duration: 0.145s, episode steps:  25, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001202, mae: 0.242911, mean_q: 0.330277\n",
      "  8865/50000: episode: 297, duration: 0.195s, episode steps:  35, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001194, mae: 0.242320, mean_q: 0.331443\n",
      "  8897/50000: episode: 298, duration: 0.185s, episode steps:  32, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001567, mae: 0.244062, mean_q: 0.334586\n",
      "  9031/50000: episode: 299, duration: 0.771s, episode steps: 134, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 0.001480, mae: 0.244862, mean_q: 0.334198\n",
      "  9058/50000: episode: 300, duration: 0.160s, episode steps:  27, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.815 [0.000, 3.000],  loss: 0.001188, mae: 0.244462, mean_q: 0.328388\n",
      "  9083/50000: episode: 301, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.001297, mae: 0.244036, mean_q: 0.329054\n",
      "  9113/50000: episode: 302, duration: 0.178s, episode steps:  30, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.367 [0.000, 3.000],  loss: 0.001193, mae: 0.240807, mean_q: 0.328069\n",
      "  9164/50000: episode: 303, duration: 0.305s, episode steps:  51, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.001293, mae: 0.240381, mean_q: 0.327012\n",
      "  9171/50000: episode: 304, duration: 0.047s, episode steps:   7, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001038, mae: 0.233268, mean_q: 0.321527\n",
      "  9228/50000: episode: 305, duration: 0.338s, episode steps:  57, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001205, mae: 0.238486, mean_q: 0.324585\n",
      "  9241/50000: episode: 306, duration: 0.081s, episode steps:  13, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.001359, mae: 0.235999, mean_q: 0.318624\n",
      "  9261/50000: episode: 307, duration: 0.122s, episode steps:  20, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.000944, mae: 0.237062, mean_q: 0.325916\n",
      "  9274/50000: episode: 308, duration: 0.086s, episode steps:  13, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.000988, mae: 0.235550, mean_q: 0.319227\n",
      "  9310/50000: episode: 309, duration: 0.218s, episode steps:  36, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001065, mae: 0.238168, mean_q: 0.324908\n",
      "  9333/50000: episode: 310, duration: 0.138s, episode steps:  23, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.001293, mae: 0.238188, mean_q: 0.324675\n",
      "  9379/50000: episode: 311, duration: 0.284s, episode steps:  46, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.348 [0.000, 3.000],  loss: 0.001133, mae: 0.236527, mean_q: 0.320262\n",
      "  9428/50000: episode: 312, duration: 0.319s, episode steps:  49, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001226, mae: 0.234760, mean_q: 0.317753\n",
      "  9435/50000: episode: 313, duration: 0.053s, episode steps:   7, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001579, mae: 0.241490, mean_q: 0.324468\n",
      "  9462/50000: episode: 314, duration: 0.198s, episode steps:  27, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.001263, mae: 0.234152, mean_q: 0.321781\n",
      "  9519/50000: episode: 315, duration: 0.387s, episode steps:  57, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.175 [0.000, 3.000],  loss: 0.001791, mae: 0.235287, mean_q: 0.327216\n",
      "  9532/50000: episode: 316, duration: 0.085s, episode steps:  13, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.001242, mae: 0.234295, mean_q: 0.317522\n",
      "  9587/50000: episode: 317, duration: 0.347s, episode steps:  55, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.001069, mae: 0.232816, mean_q: 0.313998\n",
      "  9608/50000: episode: 318, duration: 0.134s, episode steps:  21, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001211, mae: 0.231867, mean_q: 0.317364\n",
      "  9666/50000: episode: 319, duration: 0.370s, episode steps:  58, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.001040, mae: 0.231641, mean_q: 0.316170\n",
      "  9675/50000: episode: 320, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001167, mae: 0.229227, mean_q: 0.311277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9717/50000: episode: 321, duration: 0.295s, episode steps:  42, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001125, mae: 0.235301, mean_q: 0.322443\n",
      "  9754/50000: episode: 322, duration: 0.262s, episode steps:  37, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.784 [0.000, 3.000],  loss: 0.001141, mae: 0.233364, mean_q: 0.317643\n",
      "  9772/50000: episode: 323, duration: 0.128s, episode steps:  18, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001057, mae: 0.231770, mean_q: 0.312908\n",
      "  9795/50000: episode: 324, duration: 0.167s, episode steps:  23, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.783 [0.000, 3.000],  loss: 0.001099, mae: 0.228288, mean_q: 0.307713\n",
      "  9823/50000: episode: 325, duration: 0.213s, episode steps:  28, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000818, mae: 0.232028, mean_q: 0.318289\n",
      "  9844/50000: episode: 326, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.810 [0.000, 3.000],  loss: 0.001262, mae: 0.227913, mean_q: 0.305707\n",
      "  9886/50000: episode: 327, duration: 0.239s, episode steps:  42, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.738 [0.000, 3.000],  loss: 0.001145, mae: 0.230118, mean_q: 0.310643\n",
      "  9926/50000: episode: 328, duration: 0.233s, episode steps:  40, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001155, mae: 0.227094, mean_q: 0.307019\n",
      "  9961/50000: episode: 329, duration: 0.218s, episode steps:  35, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001563, mae: 0.227085, mean_q: 0.307780\n",
      "  9984/50000: episode: 330, duration: 0.144s, episode steps:  23, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.000882, mae: 0.226137, mean_q: 0.308934\n",
      " 10042/50000: episode: 331, duration: 0.354s, episode steps:  58, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.001205, mae: 0.225713, mean_q: 0.307812\n",
      " 10080/50000: episode: 332, duration: 0.237s, episode steps:  38, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000999, mae: 0.227463, mean_q: 0.311548\n",
      " 10092/50000: episode: 333, duration: 0.078s, episode steps:  12, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.001165, mae: 0.226450, mean_q: 0.311421\n",
      " 10220/50000: episode: 334, duration: 0.747s, episode steps: 128, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.352 [0.000, 3.000],  loss: 0.001085, mae: 0.224504, mean_q: 0.305053\n",
      " 10242/50000: episode: 335, duration: 0.136s, episode steps:  22, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001068, mae: 0.226380, mean_q: 0.303656\n",
      " 10304/50000: episode: 336, duration: 0.377s, episode steps:  62, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.001227, mae: 0.221550, mean_q: 0.300608\n",
      " 10345/50000: episode: 337, duration: 0.253s, episode steps:  41, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001040, mae: 0.220714, mean_q: 0.301232\n",
      " 10354/50000: episode: 338, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 2.000],  loss: 0.000797, mae: 0.216536, mean_q: 0.298871\n",
      " 10393/50000: episode: 339, duration: 0.244s, episode steps:  39, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.001623, mae: 0.220090, mean_q: 0.298673\n",
      " 10431/50000: episode: 340, duration: 0.230s, episode steps:  38, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.001205, mae: 0.219753, mean_q: 0.300929\n",
      " 10445/50000: episode: 341, duration: 0.082s, episode steps:  14, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001052, mae: 0.220453, mean_q: 0.294672\n",
      " 10463/50000: episode: 342, duration: 0.104s, episode steps:  18, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001290, mae: 0.215644, mean_q: 0.293954\n",
      " 10500/50000: episode: 343, duration: 0.213s, episode steps:  37, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.001103, mae: 0.217883, mean_q: 0.294079\n",
      " 10516/50000: episode: 344, duration: 0.093s, episode steps:  16, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.002105, mae: 0.217222, mean_q: 0.288070\n",
      " 10549/50000: episode: 345, duration: 0.196s, episode steps:  33, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000987, mae: 0.215156, mean_q: 0.292303\n",
      " 10605/50000: episode: 346, duration: 0.317s, episode steps:  56, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: 0.000983, mae: 0.214457, mean_q: 0.291471\n",
      " 10632/50000: episode: 347, duration: 0.151s, episode steps:  27, steps per second: 179, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.296 [0.000, 3.000],  loss: 0.001090, mae: 0.214334, mean_q: 0.287172\n",
      " 10657/50000: episode: 348, duration: 0.146s, episode steps:  25, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001235, mae: 0.213034, mean_q: 0.287950\n",
      " 10681/50000: episode: 349, duration: 0.136s, episode steps:  24, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001036, mae: 0.214034, mean_q: 0.288148\n",
      " 10726/50000: episode: 350, duration: 0.259s, episode steps:  45, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001129, mae: 0.210916, mean_q: 0.284530\n",
      " 10751/50000: episode: 351, duration: 0.146s, episode steps:  25, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000907, mae: 0.213291, mean_q: 0.289258\n",
      " 10759/50000: episode: 352, duration: 0.053s, episode steps:   8, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.001190, mae: 0.214543, mean_q: 0.287032\n",
      " 10842/50000: episode: 353, duration: 0.502s, episode steps:  83, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.001027, mae: 0.211823, mean_q: 0.288726\n",
      " 10888/50000: episode: 354, duration: 0.285s, episode steps:  46, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.739 [0.000, 3.000],  loss: 0.000917, mae: 0.212692, mean_q: 0.292931\n",
      " 10937/50000: episode: 355, duration: 0.300s, episode steps:  49, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.001012, mae: 0.209385, mean_q: 0.285551\n",
      " 10959/50000: episode: 356, duration: 0.136s, episode steps:  22, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.001287, mae: 0.208745, mean_q: 0.282034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10975/50000: episode: 357, duration: 0.108s, episode steps:  16, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.001054, mae: 0.211500, mean_q: 0.285006\n",
      " 11004/50000: episode: 358, duration: 0.212s, episode steps:  29, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001029, mae: 0.209926, mean_q: 0.286362\n",
      " 11053/50000: episode: 359, duration: 0.358s, episode steps:  49, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.755 [0.000, 3.000],  loss: 0.001523, mae: 0.208436, mean_q: 0.285610\n",
      " 11066/50000: episode: 360, duration: 0.076s, episode steps:  13, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.001722, mae: 0.210173, mean_q: 0.288817\n",
      " 11092/50000: episode: 361, duration: 0.177s, episode steps:  26, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.001209, mae: 0.207946, mean_q: 0.289602\n",
      " 11128/50000: episode: 362, duration: 0.327s, episode steps:  36, steps per second: 110, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001091, mae: 0.208760, mean_q: 0.282660\n",
      " 11157/50000: episode: 363, duration: 0.180s, episode steps:  29, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.759 [0.000, 3.000],  loss: 0.000911, mae: 0.208237, mean_q: 0.280563\n",
      " 11168/50000: episode: 364, duration: 0.070s, episode steps:  11, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.000827, mae: 0.206773, mean_q: 0.278770\n",
      " 11179/50000: episode: 365, duration: 0.066s, episode steps:  11, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001055, mae: 0.206034, mean_q: 0.279493\n",
      " 11235/50000: episode: 366, duration: 0.314s, episode steps:  56, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.001115, mae: 0.208969, mean_q: 0.284755\n",
      " 11284/50000: episode: 367, duration: 0.275s, episode steps:  49, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000777, mae: 0.206883, mean_q: 0.280979\n",
      " 11396/50000: episode: 368, duration: 0.645s, episode steps: 112, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.001065, mae: 0.208513, mean_q: 0.285516\n",
      " 11419/50000: episode: 369, duration: 0.156s, episode steps:  23, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000607, mae: 0.208019, mean_q: 0.283111\n",
      " 11476/50000: episode: 370, duration: 0.386s, episode steps:  57, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.228 [0.000, 3.000],  loss: 0.000898, mae: 0.207495, mean_q: 0.279799\n",
      " 11491/50000: episode: 371, duration: 0.102s, episode steps:  15, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.001058, mae: 0.203815, mean_q: 0.278081\n",
      " 11512/50000: episode: 372, duration: 0.143s, episode steps:  21, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001070, mae: 0.200970, mean_q: 0.272133\n",
      " 11537/50000: episode: 373, duration: 0.179s, episode steps:  25, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.000843, mae: 0.204026, mean_q: 0.273175\n",
      " 11555/50000: episode: 374, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000771, mae: 0.205504, mean_q: 0.277590\n",
      " 11569/50000: episode: 375, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001015, mae: 0.202818, mean_q: 0.272271\n",
      " 11616/50000: episode: 376, duration: 0.340s, episode steps:  47, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: 0.000682, mae: 0.204998, mean_q: 0.279948\n",
      " 11627/50000: episode: 377, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000796, mae: 0.206007, mean_q: 0.282245\n",
      " 11691/50000: episode: 378, duration: 0.461s, episode steps:  64, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001049, mae: 0.202004, mean_q: 0.275137\n",
      " 11706/50000: episode: 379, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000741, mae: 0.207226, mean_q: 0.283728\n",
      " 11713/50000: episode: 380, duration: 0.056s, episode steps:   7, steps per second: 126, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000915, mae: 0.200274, mean_q: 0.272909\n",
      " 11738/50000: episode: 381, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.001077, mae: 0.202276, mean_q: 0.273948\n",
      " 11795/50000: episode: 382, duration: 0.361s, episode steps:  57, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.000788, mae: 0.201696, mean_q: 0.276624\n",
      " 11805/50000: episode: 383, duration: 0.063s, episode steps:  10, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000646, mae: 0.201303, mean_q: 0.277347\n",
      " 11840/50000: episode: 384, duration: 0.202s, episode steps:  35, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.229 [0.000, 3.000],  loss: 0.001115, mae: 0.199787, mean_q: 0.274473\n",
      " 11881/50000: episode: 385, duration: 0.234s, episode steps:  41, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001217, mae: 0.204127, mean_q: 0.279726\n",
      " 11886/50000: episode: 386, duration: 0.034s, episode steps:   5, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.400 [2.000, 3.000],  loss: 0.000731, mae: 0.198467, mean_q: 0.272092\n",
      " 11904/50000: episode: 387, duration: 0.103s, episode steps:  18, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.001017, mae: 0.200482, mean_q: 0.272949\n",
      " 11946/50000: episode: 388, duration: 0.243s, episode steps:  42, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001154, mae: 0.201302, mean_q: 0.275159\n",
      " 11966/50000: episode: 389, duration: 0.116s, episode steps:  20, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.000995, mae: 0.201536, mean_q: 0.276855\n",
      " 11973/50000: episode: 390, duration: 0.046s, episode steps:   7, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000925, mae: 0.196441, mean_q: 0.261744\n",
      " 11997/50000: episode: 391, duration: 0.153s, episode steps:  24, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000626, mae: 0.204114, mean_q: 0.276899\n",
      " 12012/50000: episode: 392, duration: 0.096s, episode steps:  15, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.867 [0.000, 3.000],  loss: 0.000765, mae: 0.199938, mean_q: 0.272145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12050/50000: episode: 393, duration: 0.240s, episode steps:  38, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.342 [0.000, 3.000],  loss: 0.000818, mae: 0.201219, mean_q: 0.272956\n",
      " 12096/50000: episode: 394, duration: 0.312s, episode steps:  46, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000879, mae: 0.200220, mean_q: 0.268510\n",
      " 12108/50000: episode: 395, duration: 0.084s, episode steps:  12, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000738, mae: 0.201711, mean_q: 0.274634\n",
      " 12117/50000: episode: 396, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 2.000],  loss: 0.000849, mae: 0.198352, mean_q: 0.268277\n",
      " 12155/50000: episode: 397, duration: 0.246s, episode steps:  38, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.000842, mae: 0.198540, mean_q: 0.268673\n",
      " 12168/50000: episode: 398, duration: 0.077s, episode steps:  13, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001028, mae: 0.195997, mean_q: 0.263925\n",
      " 12183/50000: episode: 399, duration: 0.095s, episode steps:  15, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.867 [0.000, 3.000],  loss: 0.001706, mae: 0.199109, mean_q: 0.266830\n",
      " 12198/50000: episode: 400, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.000729, mae: 0.198121, mean_q: 0.270585\n",
      " 12272/50000: episode: 401, duration: 0.491s, episode steps:  74, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001003, mae: 0.196719, mean_q: 0.266386\n",
      " 12283/50000: episode: 402, duration: 0.068s, episode steps:  11, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.000790, mae: 0.197339, mean_q: 0.265593\n",
      " 12314/50000: episode: 403, duration: 0.183s, episode steps:  31, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.000789, mae: 0.196968, mean_q: 0.268221\n",
      " 12331/50000: episode: 404, duration: 0.103s, episode steps:  17, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.000863, mae: 0.194442, mean_q: 0.264921\n",
      " 12352/50000: episode: 405, duration: 0.126s, episode steps:  21, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.000617, mae: 0.197777, mean_q: 0.267008\n",
      " 12366/50000: episode: 406, duration: 0.084s, episode steps:  14, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001094, mae: 0.188544, mean_q: 0.256010\n",
      " 12383/50000: episode: 407, duration: 0.099s, episode steps:  17, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000770, mae: 0.192782, mean_q: 0.265821\n",
      " 12395/50000: episode: 408, duration: 0.074s, episode steps:  12, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000675, mae: 0.196601, mean_q: 0.262920\n",
      " 12404/50000: episode: 409, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.000780, mae: 0.192032, mean_q: 0.265293\n",
      " 12425/50000: episode: 410, duration: 0.122s, episode steps:  21, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000896, mae: 0.194649, mean_q: 0.266324\n",
      " 12457/50000: episode: 411, duration: 0.187s, episode steps:  32, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.000948, mae: 0.190846, mean_q: 0.261551\n",
      " 12496/50000: episode: 412, duration: 0.222s, episode steps:  39, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001133, mae: 0.193038, mean_q: 0.261631\n",
      " 12517/50000: episode: 413, duration: 0.121s, episode steps:  21, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000919, mae: 0.191109, mean_q: 0.260479\n",
      " 12557/50000: episode: 414, duration: 0.230s, episode steps:  40, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000873, mae: 0.191050, mean_q: 0.257381\n",
      " 12606/50000: episode: 415, duration: 0.296s, episode steps:  49, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 0.000847, mae: 0.189251, mean_q: 0.256515\n",
      " 12626/50000: episode: 416, duration: 0.126s, episode steps:  20, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000710, mae: 0.190282, mean_q: 0.258472\n",
      " 12656/50000: episode: 417, duration: 0.188s, episode steps:  30, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000710, mae: 0.189598, mean_q: 0.255816\n",
      " 12685/50000: episode: 418, duration: 0.165s, episode steps:  29, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.000659, mae: 0.185572, mean_q: 0.253690\n",
      " 12746/50000: episode: 419, duration: 0.346s, episode steps:  61, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.623 [0.000, 3.000],  loss: 0.000749, mae: 0.187848, mean_q: 0.254385\n",
      " 12775/50000: episode: 420, duration: 0.164s, episode steps:  29, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001369, mae: 0.187016, mean_q: 0.252821\n",
      " 12782/50000: episode: 421, duration: 0.048s, episode steps:   7, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.429 [0.000, 3.000],  loss: 0.000742, mae: 0.186632, mean_q: 0.254322\n",
      " 12807/50000: episode: 422, duration: 0.157s, episode steps:  25, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000702, mae: 0.186586, mean_q: 0.253106\n",
      " 12847/50000: episode: 423, duration: 0.247s, episode steps:  40, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.000767, mae: 0.187034, mean_q: 0.254348\n",
      " 12864/50000: episode: 424, duration: 0.106s, episode steps:  17, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.000820, mae: 0.185762, mean_q: 0.251902\n",
      " 12879/50000: episode: 425, duration: 0.097s, episode steps:  15, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000716, mae: 0.186197, mean_q: 0.253385\n",
      " 12909/50000: episode: 426, duration: 0.222s, episode steps:  30, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000715, mae: 0.184374, mean_q: 0.252912\n",
      " 12921/50000: episode: 427, duration: 0.075s, episode steps:  12, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000860, mae: 0.184093, mean_q: 0.249801\n",
      " 12933/50000: episode: 428, duration: 0.071s, episode steps:  12, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000772, mae: 0.190369, mean_q: 0.257732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12959/50000: episode: 429, duration: 0.152s, episode steps:  26, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.000683, mae: 0.185938, mean_q: 0.251779\n",
      " 12969/50000: episode: 430, duration: 0.061s, episode steps:  10, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.000712, mae: 0.186565, mean_q: 0.257029\n",
      " 12999/50000: episode: 431, duration: 0.171s, episode steps:  30, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000665, mae: 0.183827, mean_q: 0.249237\n",
      " 13005/50000: episode: 432, duration: 0.039s, episode steps:   6, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.000715, mae: 0.179814, mean_q: 0.242502\n",
      " 13028/50000: episode: 433, duration: 0.130s, episode steps:  23, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000711, mae: 0.184488, mean_q: 0.251851\n",
      " 13053/50000: episode: 434, duration: 0.146s, episode steps:  25, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.000683, mae: 0.183610, mean_q: 0.252097\n",
      " 13077/50000: episode: 435, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000588, mae: 0.184814, mean_q: 0.248370\n",
      " 13111/50000: episode: 436, duration: 0.230s, episode steps:  34, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000815, mae: 0.179850, mean_q: 0.244240\n",
      " 13149/50000: episode: 437, duration: 0.258s, episode steps:  38, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.658 [0.000, 3.000],  loss: 0.000681, mae: 0.181010, mean_q: 0.245449\n",
      " 13163/50000: episode: 438, duration: 0.099s, episode steps:  14, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.000790, mae: 0.178009, mean_q: 0.242851\n",
      " 13189/50000: episode: 439, duration: 0.181s, episode steps:  26, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001179, mae: 0.181218, mean_q: 0.245780\n",
      " 13216/50000: episode: 440, duration: 0.187s, episode steps:  27, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000808, mae: 0.181799, mean_q: 0.246717\n",
      " 13227/50000: episode: 441, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000773, mae: 0.179200, mean_q: 0.248938\n",
      " 13232/50000: episode: 442, duration: 0.040s, episode steps:   5, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 2.000],  loss: 0.000503, mae: 0.177895, mean_q: 0.247389\n",
      " 13245/50000: episode: 443, duration: 0.094s, episode steps:  13, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.001158, mae: 0.179118, mean_q: 0.245057\n",
      " 13306/50000: episode: 444, duration: 0.456s, episode steps:  61, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.689 [0.000, 3.000],  loss: 0.000672, mae: 0.181258, mean_q: 0.246906\n",
      " 13339/50000: episode: 445, duration: 0.234s, episode steps:  33, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.758 [0.000, 3.000],  loss: 0.000711, mae: 0.177970, mean_q: 0.240994\n",
      " 13350/50000: episode: 446, duration: 0.069s, episode steps:  11, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000523, mae: 0.179072, mean_q: 0.243458\n",
      " 13357/50000: episode: 447, duration: 0.044s, episode steps:   7, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000510, mae: 0.179929, mean_q: 0.247092\n",
      " 13384/50000: episode: 448, duration: 0.156s, episode steps:  27, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000767, mae: 0.180061, mean_q: 0.244755\n",
      " 13403/50000: episode: 449, duration: 0.110s, episode steps:  19, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.000745, mae: 0.177172, mean_q: 0.238997\n",
      " 13460/50000: episode: 450, duration: 0.323s, episode steps:  57, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000836, mae: 0.177078, mean_q: 0.241694\n",
      " 13472/50000: episode: 451, duration: 0.072s, episode steps:  12, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000995, mae: 0.175355, mean_q: 0.235354\n",
      " 13507/50000: episode: 452, duration: 0.209s, episode steps:  35, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.943 [0.000, 3.000],  loss: 0.000631, mae: 0.177221, mean_q: 0.240071\n",
      " 13569/50000: episode: 453, duration: 0.382s, episode steps:  62, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.645 [0.000, 3.000],  loss: 0.000745, mae: 0.176097, mean_q: 0.241575\n",
      " 13585/50000: episode: 454, duration: 0.100s, episode steps:  16, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000633, mae: 0.178182, mean_q: 0.240089\n",
      " 13608/50000: episode: 455, duration: 0.150s, episode steps:  23, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000655, mae: 0.177150, mean_q: 0.244227\n",
      " 13624/50000: episode: 456, duration: 0.102s, episode steps:  16, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.938 [0.000, 3.000],  loss: 0.000722, mae: 0.173646, mean_q: 0.233808\n",
      " 13724/50000: episode: 457, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.340 [0.000, 3.000],  loss: 0.000760, mae: 0.175929, mean_q: 0.238841\n",
      " 13752/50000: episode: 458, duration: 0.193s, episode steps:  28, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.821 [0.000, 3.000],  loss: 0.000711, mae: 0.173278, mean_q: 0.235806\n",
      " 13759/50000: episode: 459, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.001034, mae: 0.181956, mean_q: 0.243153\n",
      " 13793/50000: episode: 460, duration: 0.241s, episode steps:  34, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.000555, mae: 0.172983, mean_q: 0.233686\n",
      " 13805/50000: episode: 461, duration: 0.084s, episode steps:  12, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000633, mae: 0.176578, mean_q: 0.240651\n",
      " 13880/50000: episode: 462, duration: 0.471s, episode steps:  75, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000658, mae: 0.173158, mean_q: 0.236123\n",
      " 13909/50000: episode: 463, duration: 0.164s, episode steps:  29, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.276 [0.000, 3.000],  loss: 0.000656, mae: 0.173630, mean_q: 0.238277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13936/50000: episode: 464, duration: 0.157s, episode steps:  27, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.000680, mae: 0.171080, mean_q: 0.230002\n",
      " 13953/50000: episode: 465, duration: 0.099s, episode steps:  17, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000408, mae: 0.176794, mean_q: 0.238152\n",
      " 13984/50000: episode: 466, duration: 0.182s, episode steps:  31, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000633, mae: 0.172727, mean_q: 0.237130\n",
      " 14039/50000: episode: 467, duration: 0.314s, episode steps:  55, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000683, mae: 0.171128, mean_q: 0.230968\n",
      " 14081/50000: episode: 468, duration: 0.240s, episode steps:  42, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.119 [0.000, 3.000],  loss: 0.000912, mae: 0.172499, mean_q: 0.232030\n",
      " 14101/50000: episode: 469, duration: 0.125s, episode steps:  20, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.350 [0.000, 3.000],  loss: 0.000577, mae: 0.166969, mean_q: 0.227806\n",
      " 14162/50000: episode: 470, duration: 0.375s, episode steps:  61, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000821, mae: 0.169780, mean_q: 0.232330\n",
      " 14217/50000: episode: 471, duration: 0.368s, episode steps:  55, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000633, mae: 0.169565, mean_q: 0.230232\n",
      " 14251/50000: episode: 472, duration: 0.229s, episode steps:  34, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.265 [0.000, 3.000],  loss: 0.000549, mae: 0.166129, mean_q: 0.226360\n",
      " 14266/50000: episode: 473, duration: 0.092s, episode steps:  15, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000532, mae: 0.171024, mean_q: 0.230507\n",
      " 14304/50000: episode: 474, duration: 0.223s, episode steps:  38, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.763 [0.000, 3.000],  loss: 0.000701, mae: 0.164956, mean_q: 0.223473\n",
      " 14353/50000: episode: 475, duration: 0.298s, episode steps:  49, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.306 [0.000, 3.000],  loss: 0.000943, mae: 0.167108, mean_q: 0.226657\n",
      " 14377/50000: episode: 476, duration: 0.154s, episode steps:  24, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000826, mae: 0.165007, mean_q: 0.228216\n",
      " 14406/50000: episode: 477, duration: 0.189s, episode steps:  29, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000479, mae: 0.166494, mean_q: 0.228411\n",
      " 14560/50000: episode: 478, duration: 0.958s, episode steps: 154, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.578 [0.000, 3.000],  loss: 0.000679, mae: 0.167232, mean_q: 0.227069\n",
      " 14589/50000: episode: 479, duration: 0.199s, episode steps:  29, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.655 [0.000, 3.000],  loss: 0.000580, mae: 0.167664, mean_q: 0.227700\n",
      " 14633/50000: episode: 480, duration: 0.316s, episode steps:  44, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000540, mae: 0.165937, mean_q: 0.221758\n",
      " 14654/50000: episode: 481, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000624, mae: 0.166158, mean_q: 0.224972\n",
      " 14665/50000: episode: 482, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.000727, mae: 0.163452, mean_q: 0.223636\n",
      " 14681/50000: episode: 483, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000588, mae: 0.164373, mean_q: 0.224211\n",
      " 14695/50000: episode: 484, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000688, mae: 0.165482, mean_q: 0.224435\n",
      " 14704/50000: episode: 485, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000366, mae: 0.162426, mean_q: 0.217300\n",
      " 14719/50000: episode: 486, duration: 0.089s, episode steps:  15, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000319, mae: 0.168810, mean_q: 0.229255\n",
      " 14737/50000: episode: 487, duration: 0.107s, episode steps:  18, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000596, mae: 0.162894, mean_q: 0.218529\n",
      " 14760/50000: episode: 488, duration: 0.132s, episode steps:  23, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000618, mae: 0.161250, mean_q: 0.218385\n",
      " 14820/50000: episode: 489, duration: 0.338s, episode steps:  60, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000559, mae: 0.163389, mean_q: 0.221647\n",
      " 14890/50000: episode: 490, duration: 0.394s, episode steps:  70, steps per second: 178, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000490, mae: 0.161215, mean_q: 0.217886\n",
      " 14903/50000: episode: 491, duration: 0.077s, episode steps:  13, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.000662, mae: 0.157096, mean_q: 0.212693\n",
      " 14911/50000: episode: 492, duration: 0.050s, episode steps:   8, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000340, mae: 0.157122, mean_q: 0.219571\n",
      " 14961/50000: episode: 493, duration: 0.286s, episode steps:  50, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000868, mae: 0.161752, mean_q: 0.218773\n",
      " 14978/50000: episode: 494, duration: 0.098s, episode steps:  17, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000507, mae: 0.156092, mean_q: 0.210567\n",
      " 15137/50000: episode: 495, duration: 0.911s, episode steps: 159, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.642 [0.000, 3.000],  loss: 0.000463, mae: 0.158948, mean_q: 0.215211\n",
      " 15156/50000: episode: 496, duration: 0.120s, episode steps:  19, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000459, mae: 0.158837, mean_q: 0.214024\n",
      " 15186/50000: episode: 497, duration: 0.192s, episode steps:  30, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000426, mae: 0.157030, mean_q: 0.211575\n",
      " 15204/50000: episode: 498, duration: 0.115s, episode steps:  18, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.000434, mae: 0.154553, mean_q: 0.206945\n",
      " 15215/50000: episode: 499, duration: 0.073s, episode steps:  11, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.000681, mae: 0.154820, mean_q: 0.206690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15268/50000: episode: 500, duration: 0.359s, episode steps:  53, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000562, mae: 0.152880, mean_q: 0.205726\n",
      " 15316/50000: episode: 501, duration: 0.301s, episode steps:  48, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.854 [0.000, 3.000],  loss: 0.000570, mae: 0.153521, mean_q: 0.208591\n",
      " 15340/50000: episode: 502, duration: 0.140s, episode steps:  24, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.208 [0.000, 3.000],  loss: 0.000465, mae: 0.152705, mean_q: 0.205735\n",
      " 15350/50000: episode: 503, duration: 0.061s, episode steps:  10, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000407, mae: 0.158021, mean_q: 0.212487\n",
      " 15374/50000: episode: 504, duration: 0.142s, episode steps:  24, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000426, mae: 0.151236, mean_q: 0.204939\n",
      " 15382/50000: episode: 505, duration: 0.050s, episode steps:   8, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000294, mae: 0.155748, mean_q: 0.211428\n",
      " 15404/50000: episode: 506, duration: 0.126s, episode steps:  22, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.136 [0.000, 3.000],  loss: 0.000458, mae: 0.152589, mean_q: 0.209200\n",
      " 15414/50000: episode: 507, duration: 0.064s, episode steps:  10, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000649, mae: 0.149858, mean_q: 0.199288\n",
      " 15449/50000: episode: 508, duration: 0.199s, episode steps:  35, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000577, mae: 0.149908, mean_q: 0.201361\n",
      " 15534/50000: episode: 509, duration: 0.488s, episode steps:  85, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000531, mae: 0.149946, mean_q: 0.203986\n",
      " 15570/50000: episode: 510, duration: 0.223s, episode steps:  36, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [0.000, 3.000],  loss: 0.000998, mae: 0.148317, mean_q: 0.199297\n",
      " 15589/50000: episode: 511, duration: 0.121s, episode steps:  19, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.000676, mae: 0.147173, mean_q: 0.201656\n",
      " 15596/50000: episode: 512, duration: 0.049s, episode steps:   7, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.000297, mae: 0.153425, mean_q: 0.209775\n",
      " 15620/50000: episode: 513, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000436, mae: 0.148588, mean_q: 0.203198\n",
      " 15708/50000: episode: 514, duration: 0.576s, episode steps:  88, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000499, mae: 0.148171, mean_q: 0.202551\n",
      " 15735/50000: episode: 515, duration: 0.204s, episode steps:  27, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000560, mae: 0.147092, mean_q: 0.200098\n",
      " 15747/50000: episode: 516, duration: 0.078s, episode steps:  12, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000339, mae: 0.149423, mean_q: 0.202053\n",
      " 15801/50000: episode: 517, duration: 0.323s, episode steps:  54, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.352 [0.000, 3.000],  loss: 0.000477, mae: 0.147838, mean_q: 0.200406\n",
      " 15824/50000: episode: 518, duration: 0.152s, episode steps:  23, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000422, mae: 0.146506, mean_q: 0.197379\n",
      " 15868/50000: episode: 519, duration: 0.313s, episode steps:  44, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000482, mae: 0.146667, mean_q: 0.199831\n",
      " 15892/50000: episode: 520, duration: 0.170s, episode steps:  24, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000505, mae: 0.145852, mean_q: 0.195437\n",
      " 15914/50000: episode: 521, duration: 0.162s, episode steps:  22, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000493, mae: 0.144961, mean_q: 0.195523\n",
      " 15995/50000: episode: 522, duration: 0.499s, episode steps:  81, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.000602, mae: 0.144595, mean_q: 0.196918\n",
      " 16045/50000: episode: 523, duration: 0.310s, episode steps:  50, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000437, mae: 0.144326, mean_q: 0.196702\n",
      " 16066/50000: episode: 524, duration: 0.131s, episode steps:  21, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000407, mae: 0.143395, mean_q: 0.194491\n",
      " 16094/50000: episode: 525, duration: 0.180s, episode steps:  28, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.000563, mae: 0.140760, mean_q: 0.190395\n",
      " 16181/50000: episode: 526, duration: 0.561s, episode steps:  87, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.000420, mae: 0.143597, mean_q: 0.196245\n",
      " 16293/50000: episode: 527, duration: 0.699s, episode steps: 112, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000513, mae: 0.142252, mean_q: 0.193556\n",
      " 16318/50000: episode: 528, duration: 0.144s, episode steps:  25, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.760 [0.000, 3.000],  loss: 0.000366, mae: 0.140748, mean_q: 0.190801\n",
      " 16338/50000: episode: 529, duration: 0.118s, episode steps:  20, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.850 [0.000, 3.000],  loss: 0.000388, mae: 0.142247, mean_q: 0.193474\n",
      " 16386/50000: episode: 530, duration: 0.287s, episode steps:  48, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000737, mae: 0.142795, mean_q: 0.194063\n",
      " 16396/50000: episode: 531, duration: 0.066s, episode steps:  10, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.000435, mae: 0.143273, mean_q: 0.193050\n",
      " 16407/50000: episode: 532, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [1.000, 3.000],  loss: 0.000539, mae: 0.144421, mean_q: 0.194531\n",
      " 16438/50000: episode: 533, duration: 0.196s, episode steps:  31, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.677 [0.000, 3.000],  loss: 0.000411, mae: 0.140620, mean_q: 0.190645\n",
      " 16449/50000: episode: 534, duration: 0.072s, episode steps:  11, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.000551, mae: 0.139610, mean_q: 0.191460\n",
      " 16464/50000: episode: 535, duration: 0.104s, episode steps:  15, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.067 [0.000, 3.000],  loss: 0.000400, mae: 0.142551, mean_q: 0.193817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16469/50000: episode: 536, duration: 0.045s, episode steps:   5, steps per second: 112, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000332, mae: 0.142391, mean_q: 0.193630\n",
      " 16558/50000: episode: 537, duration: 0.524s, episode steps:  89, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.742 [0.000, 3.000],  loss: 0.000460, mae: 0.140419, mean_q: 0.191478\n",
      " 16606/50000: episode: 538, duration: 0.307s, episode steps:  48, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000398, mae: 0.139604, mean_q: 0.190401\n",
      " 16666/50000: episode: 539, duration: 0.383s, episode steps:  60, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000442, mae: 0.140320, mean_q: 0.192630\n",
      " 16699/50000: episode: 540, duration: 0.213s, episode steps:  33, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.000397, mae: 0.140938, mean_q: 0.191919\n",
      " 16738/50000: episode: 541, duration: 0.237s, episode steps:  39, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.718 [0.000, 3.000],  loss: 0.000449, mae: 0.141017, mean_q: 0.189424\n",
      " 16769/50000: episode: 542, duration: 0.177s, episode steps:  31, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.290 [0.000, 3.000],  loss: 0.000412, mae: 0.139421, mean_q: 0.189598\n",
      " 16828/50000: episode: 543, duration: 0.337s, episode steps:  59, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.712 [0.000, 3.000],  loss: 0.000427, mae: 0.138134, mean_q: 0.187088\n",
      " 16843/50000: episode: 544, duration: 0.089s, episode steps:  15, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000319, mae: 0.141602, mean_q: 0.193199\n",
      " 16897/50000: episode: 545, duration: 0.335s, episode steps:  54, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000451, mae: 0.137631, mean_q: 0.186566\n",
      " 16916/50000: episode: 546, duration: 0.120s, episode steps:  19, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.053 [0.000, 3.000],  loss: 0.000492, mae: 0.138229, mean_q: 0.187633\n",
      " 16927/50000: episode: 547, duration: 0.072s, episode steps:  11, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000342, mae: 0.139579, mean_q: 0.190949\n",
      " 16967/50000: episode: 548, duration: 0.250s, episode steps:  40, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.675 [0.000, 3.000],  loss: 0.000384, mae: 0.138072, mean_q: 0.189037\n",
      " 16982/50000: episode: 549, duration: 0.102s, episode steps:  15, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000481, mae: 0.137432, mean_q: 0.184942\n",
      " 16992/50000: episode: 550, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000335, mae: 0.138507, mean_q: 0.189086\n",
      " 17024/50000: episode: 551, duration: 0.224s, episode steps:  32, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000470, mae: 0.137708, mean_q: 0.186141\n",
      " 17038/50000: episode: 552, duration: 0.100s, episode steps:  14, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.000292, mae: 0.135647, mean_q: 0.186080\n",
      " 17052/50000: episode: 553, duration: 0.099s, episode steps:  14, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000202, mae: 0.141048, mean_q: 0.190890\n",
      " 17069/50000: episode: 554, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.000386, mae: 0.137704, mean_q: 0.185831\n",
      " 17131/50000: episode: 555, duration: 0.407s, episode steps:  62, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.000463, mae: 0.135157, mean_q: 0.182657\n",
      " 17178/50000: episode: 556, duration: 0.306s, episode steps:  47, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.617 [0.000, 3.000],  loss: 0.000476, mae: 0.135147, mean_q: 0.183674\n",
      " 17187/50000: episode: 557, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000444, mae: 0.134483, mean_q: 0.182402\n",
      " 17218/50000: episode: 558, duration: 0.207s, episode steps:  31, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.000343, mae: 0.135174, mean_q: 0.186598\n",
      " 17286/50000: episode: 559, duration: 0.391s, episode steps:  68, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000443, mae: 0.134399, mean_q: 0.184467\n",
      " 17340/50000: episode: 560, duration: 0.305s, episode steps:  54, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: 0.000350, mae: 0.134878, mean_q: 0.184001\n",
      " 17404/50000: episode: 561, duration: 0.361s, episode steps:  64, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.000354, mae: 0.135397, mean_q: 0.183960\n",
      " 17436/50000: episode: 562, duration: 0.182s, episode steps:  32, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.000318, mae: 0.135996, mean_q: 0.185513\n",
      " 17499/50000: episode: 563, duration: 0.358s, episode steps:  63, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.000388, mae: 0.134076, mean_q: 0.182675\n",
      " 17534/50000: episode: 564, duration: 0.198s, episode steps:  35, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.000443, mae: 0.134045, mean_q: 0.181079\n",
      " 17549/50000: episode: 565, duration: 0.093s, episode steps:  15, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000329, mae: 0.133366, mean_q: 0.180813\n",
      " 17572/50000: episode: 566, duration: 0.133s, episode steps:  23, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.348 [0.000, 3.000],  loss: 0.000392, mae: 0.131896, mean_q: 0.179952\n",
      " 17617/50000: episode: 567, duration: 0.260s, episode steps:  45, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.089 [0.000, 3.000],  loss: 0.000388, mae: 0.132589, mean_q: 0.179773\n",
      " 17633/50000: episode: 568, duration: 0.093s, episode steps:  16, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000319, mae: 0.134999, mean_q: 0.182276\n",
      " 17639/50000: episode: 569, duration: 0.038s, episode steps:   6, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.000257, mae: 0.132964, mean_q: 0.182692\n",
      " 17668/50000: episode: 570, duration: 0.170s, episode steps:  29, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.000399, mae: 0.133750, mean_q: 0.182353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17708/50000: episode: 571, duration: 0.233s, episode steps:  40, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000377, mae: 0.133570, mean_q: 0.182135\n",
      " 17748/50000: episode: 572, duration: 0.235s, episode steps:  40, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.150 [0.000, 3.000],  loss: 0.000409, mae: 0.133433, mean_q: 0.180181\n",
      " 17792/50000: episode: 573, duration: 0.276s, episode steps:  44, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.000401, mae: 0.132066, mean_q: 0.178990\n",
      " 17854/50000: episode: 574, duration: 0.413s, episode steps:  62, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.919 [0.000, 3.000],  loss: 0.000327, mae: 0.130612, mean_q: 0.178564\n",
      " 17869/50000: episode: 575, duration: 0.117s, episode steps:  15, steps per second: 128, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000437, mae: 0.131216, mean_q: 0.177608\n",
      " 17928/50000: episode: 576, duration: 0.336s, episode steps:  59, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.356 [0.000, 3.000],  loss: 0.000426, mae: 0.129714, mean_q: 0.175999\n",
      " 17986/50000: episode: 577, duration: 0.329s, episode steps:  58, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.000635, mae: 0.130396, mean_q: 0.178464\n",
      " 18024/50000: episode: 578, duration: 0.240s, episode steps:  38, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.289 [0.000, 3.000],  loss: 0.000343, mae: 0.131314, mean_q: 0.179793\n",
      " 18043/50000: episode: 579, duration: 0.122s, episode steps:  19, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.000327, mae: 0.130630, mean_q: 0.177845\n",
      " 18062/50000: episode: 580, duration: 0.123s, episode steps:  19, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.895 [0.000, 3.000],  loss: 0.000314, mae: 0.130314, mean_q: 0.176123\n",
      " 18171/50000: episode: 581, duration: 0.666s, episode steps: 109, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000349, mae: 0.129465, mean_q: 0.174891\n",
      " 18195/50000: episode: 582, duration: 0.150s, episode steps:  24, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000276, mae: 0.128164, mean_q: 0.175131\n",
      " 18231/50000: episode: 583, duration: 0.227s, episode steps:  36, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.361 [0.000, 3.000],  loss: 0.000323, mae: 0.129511, mean_q: 0.175611\n",
      " 18266/50000: episode: 584, duration: 0.225s, episode steps:  35, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000301, mae: 0.128169, mean_q: 0.173950\n",
      " 18273/50000: episode: 585, duration: 0.053s, episode steps:   7, steps per second: 131, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000402, mae: 0.126328, mean_q: 0.170742\n",
      " 18392/50000: episode: 586, duration: 0.853s, episode steps: 119, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000358, mae: 0.127847, mean_q: 0.174114\n",
      " 18430/50000: episode: 587, duration: 0.281s, episode steps:  38, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.711 [0.000, 3.000],  loss: 0.000328, mae: 0.126861, mean_q: 0.171087\n",
      " 18460/50000: episode: 588, duration: 0.174s, episode steps:  30, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000361, mae: 0.128145, mean_q: 0.172656\n",
      " 18484/50000: episode: 589, duration: 0.156s, episode steps:  24, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000284, mae: 0.126269, mean_q: 0.170829\n",
      " 18506/50000: episode: 590, duration: 0.139s, episode steps:  22, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000329, mae: 0.127360, mean_q: 0.172373\n",
      " 18575/50000: episode: 591, duration: 0.398s, episode steps:  69, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.377 [0.000, 3.000],  loss: 0.000296, mae: 0.124968, mean_q: 0.170098\n",
      " 18628/50000: episode: 592, duration: 0.301s, episode steps:  53, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: 0.000345, mae: 0.125979, mean_q: 0.170352\n",
      " 18664/50000: episode: 593, duration: 0.206s, episode steps:  36, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.306 [0.000, 3.000],  loss: 0.000278, mae: 0.126173, mean_q: 0.171133\n",
      " 18684/50000: episode: 594, duration: 0.120s, episode steps:  20, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.650 [0.000, 3.000],  loss: 0.000269, mae: 0.125016, mean_q: 0.169827\n",
      " 18718/50000: episode: 595, duration: 0.198s, episode steps:  34, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.324 [0.000, 3.000],  loss: 0.000371, mae: 0.127168, mean_q: 0.174205\n",
      " 18742/50000: episode: 596, duration: 0.138s, episode steps:  24, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000313, mae: 0.124999, mean_q: 0.171677\n",
      " 18771/50000: episode: 597, duration: 0.170s, episode steps:  29, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.172 [0.000, 3.000],  loss: 0.000281, mae: 0.124063, mean_q: 0.168677\n",
      " 18797/50000: episode: 598, duration: 0.149s, episode steps:  26, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000336, mae: 0.124267, mean_q: 0.170292\n",
      " 18814/50000: episode: 599, duration: 0.105s, episode steps:  17, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000418, mae: 0.124401, mean_q: 0.169313\n",
      " 18847/50000: episode: 600, duration: 0.192s, episode steps:  33, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.485 [0.000, 3.000],  loss: 0.000307, mae: 0.123797, mean_q: 0.170601\n",
      " 18855/50000: episode: 601, duration: 0.054s, episode steps:   8, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.000330, mae: 0.124592, mean_q: 0.170792\n",
      " 18913/50000: episode: 602, duration: 0.338s, episode steps:  58, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: 0.000294, mae: 0.124356, mean_q: 0.169168\n",
      " 18966/50000: episode: 603, duration: 0.302s, episode steps:  53, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000289, mae: 0.123840, mean_q: 0.168311\n",
      " 18988/50000: episode: 604, duration: 0.129s, episode steps:  22, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000373, mae: 0.124222, mean_q: 0.168487\n",
      " 19010/50000: episode: 605, duration: 0.131s, episode steps:  22, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.864 [0.000, 3.000],  loss: 0.000224, mae: 0.125509, mean_q: 0.171600\n",
      " 19028/50000: episode: 606, duration: 0.109s, episode steps:  18, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.056 [0.000, 3.000],  loss: 0.000322, mae: 0.123176, mean_q: 0.168021\n",
      " 19040/50000: episode: 607, duration: 0.086s, episode steps:  12, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000366, mae: 0.122745, mean_q: 0.165887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19067/50000: episode: 608, duration: 0.163s, episode steps:  27, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.000398, mae: 0.123248, mean_q: 0.166608\n",
      " 19162/50000: episode: 609, duration: 0.537s, episode steps:  95, steps per second: 177, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000304, mae: 0.123329, mean_q: 0.167800\n",
      " 19201/50000: episode: 610, duration: 0.222s, episode steps:  39, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000301, mae: 0.122639, mean_q: 0.167555\n",
      " 19209/50000: episode: 611, duration: 0.051s, episode steps:   8, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000148, mae: 0.123827, mean_q: 0.168597\n",
      " 19229/50000: episode: 612, duration: 0.124s, episode steps:  20, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.350 [0.000, 3.000],  loss: 0.000336, mae: 0.123240, mean_q: 0.167995\n",
      " 19251/50000: episode: 613, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000194, mae: 0.123506, mean_q: 0.169013\n",
      " 19284/50000: episode: 614, duration: 0.228s, episode steps:  33, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.303 [0.000, 3.000],  loss: 0.000310, mae: 0.122863, mean_q: 0.166976\n",
      " 19350/50000: episode: 615, duration: 0.395s, episode steps:  66, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.000269, mae: 0.122938, mean_q: 0.167477\n",
      " 19398/50000: episode: 616, duration: 0.274s, episode steps:  48, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000316, mae: 0.122217, mean_q: 0.165999\n",
      " 19421/50000: episode: 617, duration: 0.134s, episode steps:  23, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.000302, mae: 0.122759, mean_q: 0.166869\n",
      " 19475/50000: episode: 618, duration: 0.309s, episode steps:  54, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.759 [0.000, 3.000],  loss: 0.000304, mae: 0.121962, mean_q: 0.165815\n",
      " 19519/50000: episode: 619, duration: 0.266s, episode steps:  44, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.000317, mae: 0.121727, mean_q: 0.165561\n",
      " 19531/50000: episode: 620, duration: 0.079s, episode steps:  12, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000344, mae: 0.121176, mean_q: 0.163645\n",
      " 19584/50000: episode: 621, duration: 0.328s, episode steps:  53, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000334, mae: 0.121743, mean_q: 0.165242\n",
      " 19665/50000: episode: 622, duration: 0.500s, episode steps:  81, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000365, mae: 0.121166, mean_q: 0.164972\n",
      " 19737/50000: episode: 623, duration: 0.480s, episode steps:  72, steps per second: 150, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000309, mae: 0.119649, mean_q: 0.163110\n",
      " 19756/50000: episode: 624, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000400, mae: 0.118591, mean_q: 0.162430\n",
      " 19765/50000: episode: 625, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.000273, mae: 0.120225, mean_q: 0.163714\n",
      " 19782/50000: episode: 626, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000317, mae: 0.117318, mean_q: 0.158899\n",
      " 19789/50000: episode: 627, duration: 0.054s, episode steps:   7, steps per second: 129, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000246, mae: 0.121436, mean_q: 0.164617\n",
      " 19794/50000: episode: 628, duration: 0.041s, episode steps:   5, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000256, mae: 0.121829, mean_q: 0.164925\n",
      " 19834/50000: episode: 629, duration: 0.233s, episode steps:  40, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.675 [0.000, 3.000],  loss: 0.000297, mae: 0.119777, mean_q: 0.163149\n",
      " 19849/50000: episode: 630, duration: 0.094s, episode steps:  15, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000255, mae: 0.118630, mean_q: 0.160142\n",
      " 19863/50000: episode: 631, duration: 0.093s, episode steps:  14, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000284, mae: 0.120003, mean_q: 0.162044\n",
      " 19886/50000: episode: 632, duration: 0.165s, episode steps:  23, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.000373, mae: 0.117830, mean_q: 0.160647\n",
      " 19903/50000: episode: 633, duration: 0.119s, episode steps:  17, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000338, mae: 0.116708, mean_q: 0.160968\n",
      " 19966/50000: episode: 634, duration: 0.368s, episode steps:  63, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.540 [0.000, 3.000],  loss: 0.000345, mae: 0.118111, mean_q: 0.161725\n",
      " 20033/50000: episode: 635, duration: 0.396s, episode steps:  67, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: 0.000254, mae: 0.119054, mean_q: 0.162063\n",
      " 20044/50000: episode: 636, duration: 0.072s, episode steps:  11, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000123, mae: 0.117609, mean_q: 0.160695\n",
      " 20055/50000: episode: 637, duration: 0.075s, episode steps:  11, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.818 [0.000, 3.000],  loss: 0.000425, mae: 0.118447, mean_q: 0.160227\n",
      " 20090/50000: episode: 638, duration: 0.241s, episode steps:  35, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000275, mae: 0.117326, mean_q: 0.159021\n",
      " 20132/50000: episode: 639, duration: 0.253s, episode steps:  42, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.000306, mae: 0.116448, mean_q: 0.158606\n",
      " 20211/50000: episode: 640, duration: 0.489s, episode steps:  79, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.367 [0.000, 3.000],  loss: 0.000226, mae: 0.117485, mean_q: 0.159475\n",
      " 20276/50000: episode: 641, duration: 0.441s, episode steps:  65, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.631 [0.000, 3.000],  loss: 0.000297, mae: 0.117053, mean_q: 0.159566\n",
      " 20302/50000: episode: 642, duration: 0.179s, episode steps:  26, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000299, mae: 0.116789, mean_q: 0.159328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20327/50000: episode: 643, duration: 0.173s, episode steps:  25, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000243, mae: 0.116366, mean_q: 0.158026\n",
      " 20403/50000: episode: 644, duration: 0.462s, episode steps:  76, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: 0.000296, mae: 0.116058, mean_q: 0.157633\n",
      " 20411/50000: episode: 645, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.125 [0.000, 3.000],  loss: 0.000266, mae: 0.115705, mean_q: 0.157308\n",
      " 20434/50000: episode: 646, duration: 0.168s, episode steps:  23, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000247, mae: 0.114274, mean_q: 0.155374\n",
      " 20455/50000: episode: 647, duration: 0.153s, episode steps:  21, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000254, mae: 0.114918, mean_q: 0.156524\n",
      " 20466/50000: episode: 648, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.000207, mae: 0.116761, mean_q: 0.158429\n",
      " 20516/50000: episode: 649, duration: 0.358s, episode steps:  50, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.000244, mae: 0.115080, mean_q: 0.156285\n",
      " 20543/50000: episode: 650, duration: 0.191s, episode steps:  27, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000234, mae: 0.114780, mean_q: 0.155944\n",
      " 20554/50000: episode: 651, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000277, mae: 0.113400, mean_q: 0.155224\n",
      " 20565/50000: episode: 652, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000139, mae: 0.115457, mean_q: 0.157054\n",
      " 20597/50000: episode: 653, duration: 0.230s, episode steps:  32, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.000266, mae: 0.114104, mean_q: 0.154499\n",
      " 20628/50000: episode: 654, duration: 0.223s, episode steps:  31, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000259, mae: 0.113234, mean_q: 0.154354\n",
      " 20696/50000: episode: 655, duration: 0.474s, episode steps:  68, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.000277, mae: 0.113717, mean_q: 0.154579\n",
      " 20706/50000: episode: 656, duration: 0.076s, episode steps:  10, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.100 [0.000, 3.000],  loss: 0.000270, mae: 0.113636, mean_q: 0.154533\n",
      " 20743/50000: episode: 657, duration: 0.266s, episode steps:  37, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.216 [0.000, 3.000],  loss: 0.000230, mae: 0.112958, mean_q: 0.154065\n",
      " 20748/50000: episode: 658, duration: 0.042s, episode steps:   5, steps per second: 120, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000376, mae: 0.111989, mean_q: 0.150929\n",
      " 20766/50000: episode: 659, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000262, mae: 0.113769, mean_q: 0.154390\n",
      " 20789/50000: episode: 660, duration: 0.146s, episode steps:  23, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.261 [0.000, 3.000],  loss: 0.000269, mae: 0.111644, mean_q: 0.153010\n",
      " 20800/50000: episode: 661, duration: 0.066s, episode steps:  11, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000277, mae: 0.111517, mean_q: 0.152356\n",
      " 20826/50000: episode: 662, duration: 0.152s, episode steps:  26, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000226, mae: 0.112122, mean_q: 0.153037\n",
      " 20833/50000: episode: 663, duration: 0.046s, episode steps:   7, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [1.000, 3.000],  loss: 0.000256, mae: 0.113436, mean_q: 0.155483\n",
      " 20859/50000: episode: 664, duration: 0.151s, episode steps:  26, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.808 [0.000, 3.000],  loss: 0.000341, mae: 0.112510, mean_q: 0.153641\n",
      " 20869/50000: episode: 665, duration: 0.065s, episode steps:  10, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000120, mae: 0.111647, mean_q: 0.152801\n",
      " 20891/50000: episode: 666, duration: 0.129s, episode steps:  22, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.000905, mae: 0.114664, mean_q: 0.155425\n",
      " 20935/50000: episode: 667, duration: 0.254s, episode steps:  44, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.705 [0.000, 3.000],  loss: 0.000259, mae: 0.112389, mean_q: 0.153043\n",
      " 20967/50000: episode: 668, duration: 0.185s, episode steps:  32, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000265, mae: 0.111370, mean_q: 0.152087\n",
      " 20981/50000: episode: 669, duration: 0.087s, episode steps:  14, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000277, mae: 0.113483, mean_q: 0.154942\n",
      " 21018/50000: episode: 670, duration: 0.222s, episode steps:  37, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000251, mae: 0.111352, mean_q: 0.151538\n",
      " 21034/50000: episode: 671, duration: 0.105s, episode steps:  16, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000351, mae: 0.110320, mean_q: 0.149751\n",
      " 21048/50000: episode: 672, duration: 0.091s, episode steps:  14, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000292, mae: 0.111563, mean_q: 0.152769\n",
      " 21064/50000: episode: 673, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.000255, mae: 0.111113, mean_q: 0.151470\n",
      " 21078/50000: episode: 674, duration: 0.091s, episode steps:  14, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.000223, mae: 0.112068, mean_q: 0.152021\n",
      " 21091/50000: episode: 675, duration: 0.084s, episode steps:  13, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000224, mae: 0.110967, mean_q: 0.150402\n",
      " 21114/50000: episode: 676, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.000280, mae: 0.113022, mean_q: 0.153847\n",
      " 21159/50000: episode: 677, duration: 0.281s, episode steps:  45, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000220, mae: 0.110989, mean_q: 0.151331\n",
      " 21177/50000: episode: 678, duration: 0.115s, episode steps:  18, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.944 [0.000, 3.000],  loss: 0.000300, mae: 0.111591, mean_q: 0.151511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21240/50000: episode: 679, duration: 0.359s, episode steps:  63, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.381 [0.000, 3.000],  loss: 0.000223, mae: 0.110314, mean_q: 0.149316\n",
      " 21272/50000: episode: 680, duration: 0.196s, episode steps:  32, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.000235, mae: 0.110515, mean_q: 0.149425\n",
      " 21285/50000: episode: 681, duration: 0.087s, episode steps:  13, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000195, mae: 0.110915, mean_q: 0.149483\n",
      " 21301/50000: episode: 682, duration: 0.100s, episode steps:  16, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000236, mae: 0.110742, mean_q: 0.150063\n",
      " 21319/50000: episode: 683, duration: 0.115s, episode steps:  18, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.000233, mae: 0.107333, mean_q: 0.147381\n",
      " 21369/50000: episode: 684, duration: 0.297s, episode steps:  50, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000255, mae: 0.109198, mean_q: 0.148726\n",
      " 21406/50000: episode: 685, duration: 0.214s, episode steps:  37, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.243 [0.000, 3.000],  loss: 0.000243, mae: 0.108910, mean_q: 0.147822\n",
      " 21454/50000: episode: 686, duration: 0.278s, episode steps:  48, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000291, mae: 0.107909, mean_q: 0.146863\n",
      " 21474/50000: episode: 687, duration: 0.116s, episode steps:  20, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000247, mae: 0.108184, mean_q: 0.148175\n",
      " 21493/50000: episode: 688, duration: 0.114s, episode steps:  19, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.053 [0.000, 3.000],  loss: 0.000280, mae: 0.107198, mean_q: 0.145058\n",
      " 21511/50000: episode: 689, duration: 0.111s, episode steps:  18, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.111 [0.000, 3.000],  loss: 0.001007, mae: 0.109480, mean_q: 0.148314\n",
      " 21528/50000: episode: 690, duration: 0.103s, episode steps:  17, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.000343, mae: 0.107232, mean_q: 0.146527\n",
      " 21542/50000: episode: 691, duration: 0.084s, episode steps:  14, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000252, mae: 0.105943, mean_q: 0.144536\n",
      " 21626/50000: episode: 692, duration: 0.478s, episode steps:  84, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000391, mae: 0.107447, mean_q: 0.145868\n",
      " 21644/50000: episode: 693, duration: 0.106s, episode steps:  18, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.000226, mae: 0.106645, mean_q: 0.145680\n",
      " 21669/50000: episode: 694, duration: 0.147s, episode steps:  25, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000276, mae: 0.106793, mean_q: 0.146750\n",
      " 21718/50000: episode: 695, duration: 0.285s, episode steps:  49, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000261, mae: 0.105889, mean_q: 0.143327\n",
      " 21737/50000: episode: 696, duration: 0.113s, episode steps:  19, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.947 [0.000, 3.000],  loss: 0.000216, mae: 0.105640, mean_q: 0.143143\n",
      " 21783/50000: episode: 697, duration: 0.264s, episode steps:  46, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.239 [0.000, 3.000],  loss: 0.000539, mae: 0.106332, mean_q: 0.144685\n",
      " 21809/50000: episode: 698, duration: 0.153s, episode steps:  26, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.000244, mae: 0.105552, mean_q: 0.144562\n",
      " 21820/50000: episode: 699, duration: 0.077s, episode steps:  11, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000140, mae: 0.106172, mean_q: 0.144269\n",
      " 21942/50000: episode: 700, duration: 0.729s, episode steps: 122, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.746 [0.000, 3.000],  loss: 0.000224, mae: 0.105434, mean_q: 0.143315\n",
      " 21960/50000: episode: 701, duration: 0.115s, episode steps:  18, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000238, mae: 0.104369, mean_q: 0.142239\n",
      " 21984/50000: episode: 702, duration: 0.156s, episode steps:  24, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000220, mae: 0.104804, mean_q: 0.142776\n",
      " 22117/50000: episode: 703, duration: 0.818s, episode steps: 133, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000223, mae: 0.104321, mean_q: 0.141133\n",
      " 22159/50000: episode: 704, duration: 0.251s, episode steps:  42, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.000229, mae: 0.103130, mean_q: 0.139710\n",
      " 22190/50000: episode: 705, duration: 0.196s, episode steps:  31, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.000192, mae: 0.101886, mean_q: 0.137990\n",
      " 22236/50000: episode: 706, duration: 0.291s, episode steps:  46, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.804 [0.000, 3.000],  loss: 0.000186, mae: 0.102444, mean_q: 0.138662\n",
      " 22272/50000: episode: 707, duration: 0.263s, episode steps:  36, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000239, mae: 0.102094, mean_q: 0.138479\n",
      " 22284/50000: episode: 708, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000157, mae: 0.103321, mean_q: 0.140444\n",
      " 22298/50000: episode: 709, duration: 0.111s, episode steps:  14, steps per second: 126, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000171, mae: 0.100961, mean_q: 0.136464\n",
      " 22325/50000: episode: 710, duration: 0.167s, episode steps:  27, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000184, mae: 0.102023, mean_q: 0.138551\n",
      " 22398/50000: episode: 711, duration: 0.429s, episode steps:  73, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: 0.000226, mae: 0.101223, mean_q: 0.138208\n",
      " 22408/50000: episode: 712, duration: 0.060s, episode steps:  10, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000294, mae: 0.101835, mean_q: 0.137235\n",
      " 22478/50000: episode: 713, duration: 0.401s, episode steps:  70, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.329 [0.000, 3.000],  loss: 0.000206, mae: 0.100829, mean_q: 0.137048\n",
      " 22494/50000: episode: 714, duration: 0.098s, episode steps:  16, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000279, mae: 0.099579, mean_q: 0.136611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22534/50000: episode: 715, duration: 0.232s, episode steps:  40, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.000211, mae: 0.101209, mean_q: 0.138725\n",
      " 22576/50000: episode: 716, duration: 0.252s, episode steps:  42, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000183, mae: 0.101635, mean_q: 0.138171\n",
      " 22586/50000: episode: 717, duration: 0.068s, episode steps:  10, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [1.000, 3.000],  loss: 0.000228, mae: 0.101161, mean_q: 0.137100\n",
      " 22606/50000: episode: 718, duration: 0.126s, episode steps:  20, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.000121, mae: 0.099008, mean_q: 0.134531\n",
      " 22621/50000: episode: 719, duration: 0.101s, episode steps:  15, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000152, mae: 0.101049, mean_q: 0.136520\n",
      " 22664/50000: episode: 720, duration: 0.272s, episode steps:  43, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.442 [0.000, 3.000],  loss: 0.000191, mae: 0.098761, mean_q: 0.134126\n",
      " 22720/50000: episode: 721, duration: 0.349s, episode steps:  56, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000192, mae: 0.099496, mean_q: 0.135325\n",
      " 22747/50000: episode: 722, duration: 0.170s, episode steps:  27, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000235, mae: 0.098978, mean_q: 0.135013\n",
      " 22771/50000: episode: 723, duration: 0.155s, episode steps:  24, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000114, mae: 0.099801, mean_q: 0.136095\n",
      " 22845/50000: episode: 724, duration: 0.506s, episode steps:  74, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.000191, mae: 0.098573, mean_q: 0.133488\n",
      " 22939/50000: episode: 725, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000226, mae: 0.098455, mean_q: 0.133967\n",
      " 22948/50000: episode: 726, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.889 [0.000, 3.000],  loss: 0.000176, mae: 0.099243, mean_q: 0.134479\n",
      " 23016/50000: episode: 727, duration: 0.462s, episode steps:  68, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000215, mae: 0.097727, mean_q: 0.133152\n",
      " 23054/50000: episode: 728, duration: 0.310s, episode steps:  38, steps per second: 123, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000192, mae: 0.097640, mean_q: 0.133546\n",
      " 23066/50000: episode: 729, duration: 0.110s, episode steps:  12, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000244, mae: 0.096958, mean_q: 0.132120\n",
      " 23090/50000: episode: 730, duration: 0.205s, episode steps:  24, steps per second: 117, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000195, mae: 0.097003, mean_q: 0.131777\n",
      " 23179/50000: episode: 731, duration: 0.558s, episode steps:  89, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.539 [0.000, 3.000],  loss: 0.000149, mae: 0.097441, mean_q: 0.132155\n",
      " 23188/50000: episode: 732, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000112, mae: 0.097599, mean_q: 0.131771\n",
      " 23215/50000: episode: 733, duration: 0.158s, episode steps:  27, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000190, mae: 0.096714, mean_q: 0.131484\n",
      " 23229/50000: episode: 734, duration: 0.084s, episode steps:  14, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.357 [1.000, 3.000],  loss: 0.000161, mae: 0.096300, mean_q: 0.130997\n",
      " 23258/50000: episode: 735, duration: 0.173s, episode steps:  29, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000205, mae: 0.096758, mean_q: 0.130820\n",
      " 23281/50000: episode: 736, duration: 0.140s, episode steps:  23, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000189, mae: 0.096289, mean_q: 0.131283\n",
      " 23291/50000: episode: 737, duration: 0.068s, episode steps:  10, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.000237, mae: 0.094038, mean_q: 0.129097\n",
      " 23323/50000: episode: 738, duration: 0.203s, episode steps:  32, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000164, mae: 0.096260, mean_q: 0.131132\n",
      " 23349/50000: episode: 739, duration: 0.157s, episode steps:  26, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.000202, mae: 0.095413, mean_q: 0.129950\n",
      " 23370/50000: episode: 740, duration: 0.128s, episode steps:  21, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000224, mae: 0.095901, mean_q: 0.129673\n",
      " 23409/50000: episode: 741, duration: 0.226s, episode steps:  39, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000164, mae: 0.095328, mean_q: 0.129561\n",
      " 23414/50000: episode: 742, duration: 0.034s, episode steps:   5, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [1.000, 2.000],  loss: 0.000125, mae: 0.094873, mean_q: 0.129178\n",
      " 23452/50000: episode: 743, duration: 0.222s, episode steps:  38, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000220, mae: 0.095359, mean_q: 0.128943\n",
      " 23507/50000: episode: 744, duration: 0.319s, episode steps:  55, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.345 [0.000, 3.000],  loss: 0.000454, mae: 0.095367, mean_q: 0.129506\n",
      " 23551/50000: episode: 745, duration: 0.268s, episode steps:  44, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.386 [0.000, 3.000],  loss: 0.000233, mae: 0.094983, mean_q: 0.129913\n",
      " 23611/50000: episode: 746, duration: 0.413s, episode steps:  60, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000154, mae: 0.094671, mean_q: 0.128594\n",
      " 23620/50000: episode: 747, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000154, mae: 0.094095, mean_q: 0.127342\n",
      " 23633/50000: episode: 748, duration: 0.094s, episode steps:  13, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.000236, mae: 0.094729, mean_q: 0.128502\n",
      " 23657/50000: episode: 749, duration: 0.171s, episode steps:  24, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000233, mae: 0.092982, mean_q: 0.126948\n",
      " 23678/50000: episode: 750, duration: 0.133s, episode steps:  21, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000137, mae: 0.094353, mean_q: 0.128941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23696/50000: episode: 751, duration: 0.113s, episode steps:  18, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000194, mae: 0.093531, mean_q: 0.128295\n",
      " 23724/50000: episode: 752, duration: 0.180s, episode steps:  28, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000160, mae: 0.093723, mean_q: 0.127853\n",
      " 23772/50000: episode: 753, duration: 0.282s, episode steps:  48, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000181, mae: 0.093966, mean_q: 0.129276\n",
      " 23777/50000: episode: 754, duration: 0.036s, episode steps:   5, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.600 [2.000, 3.000],  loss: 0.000184, mae: 0.093756, mean_q: 0.128275\n",
      " 23796/50000: episode: 755, duration: 0.112s, episode steps:  19, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.000193, mae: 0.094213, mean_q: 0.127013\n",
      " 23808/50000: episode: 756, duration: 0.075s, episode steps:  12, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.000118, mae: 0.093000, mean_q: 0.126295\n",
      " 23815/50000: episode: 757, duration: 0.046s, episode steps:   7, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.000223, mae: 0.096109, mean_q: 0.128843\n",
      " 23844/50000: episode: 758, duration: 0.169s, episode steps:  29, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000181, mae: 0.093122, mean_q: 0.126398\n",
      " 23922/50000: episode: 759, duration: 0.447s, episode steps:  78, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000180, mae: 0.092615, mean_q: 0.125722\n",
      " 23942/50000: episode: 760, duration: 0.116s, episode steps:  20, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000184, mae: 0.093479, mean_q: 0.127704\n",
      " 23959/50000: episode: 761, duration: 0.103s, episode steps:  17, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000187, mae: 0.091347, mean_q: 0.124875\n",
      " 23979/50000: episode: 762, duration: 0.119s, episode steps:  20, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.850 [0.000, 3.000],  loss: 0.000230, mae: 0.091578, mean_q: 0.124648\n",
      " 23990/50000: episode: 763, duration: 0.067s, episode steps:  11, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000134, mae: 0.093574, mean_q: 0.128012\n",
      " 24033/50000: episode: 764, duration: 0.250s, episode steps:  43, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.442 [0.000, 3.000],  loss: 0.000215, mae: 0.091465, mean_q: 0.123925\n",
      " 24090/50000: episode: 765, duration: 0.328s, episode steps:  57, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.246 [0.000, 3.000],  loss: 0.000177, mae: 0.091152, mean_q: 0.124392\n",
      " 24129/50000: episode: 766, duration: 0.237s, episode steps:  39, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000163, mae: 0.091985, mean_q: 0.124699\n",
      " 24142/50000: episode: 767, duration: 0.078s, episode steps:  13, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000237, mae: 0.090957, mean_q: 0.122037\n",
      " 24176/50000: episode: 768, duration: 0.200s, episode steps:  34, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000149, mae: 0.090806, mean_q: 0.124214\n",
      " 24212/50000: episode: 769, duration: 0.207s, episode steps:  36, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.000203, mae: 0.090088, mean_q: 0.122566\n",
      " 24233/50000: episode: 770, duration: 0.127s, episode steps:  21, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.000171, mae: 0.090372, mean_q: 0.123496\n",
      " 24268/50000: episode: 771, duration: 0.220s, episode steps:  35, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000189, mae: 0.089916, mean_q: 0.121613\n",
      " 24326/50000: episode: 772, duration: 0.362s, episode steps:  58, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.000197, mae: 0.089937, mean_q: 0.122071\n",
      " 24347/50000: episode: 773, duration: 0.133s, episode steps:  21, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.000157, mae: 0.090088, mean_q: 0.122294\n",
      " 24425/50000: episode: 774, duration: 0.505s, episode steps:  78, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000177, mae: 0.088715, mean_q: 0.120404\n",
      " 24451/50000: episode: 775, duration: 0.179s, episode steps:  26, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000172, mae: 0.088895, mean_q: 0.121167\n",
      " 24460/50000: episode: 776, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000129, mae: 0.087542, mean_q: 0.119333\n",
      " 24465/50000: episode: 777, duration: 0.041s, episode steps:   5, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.000085, mae: 0.090211, mean_q: 0.123637\n",
      " 24491/50000: episode: 778, duration: 0.183s, episode steps:  26, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000147, mae: 0.088738, mean_q: 0.120680\n",
      " 24564/50000: episode: 779, duration: 0.502s, episode steps:  73, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.726 [0.000, 3.000],  loss: 0.000164, mae: 0.088497, mean_q: 0.121020\n",
      " 24588/50000: episode: 780, duration: 0.179s, episode steps:  24, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000155, mae: 0.088649, mean_q: 0.119971\n",
      " 24630/50000: episode: 781, duration: 0.307s, episode steps:  42, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.452 [0.000, 3.000],  loss: 0.000136, mae: 0.087981, mean_q: 0.119116\n",
      " 24683/50000: episode: 782, duration: 0.316s, episode steps:  53, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000143, mae: 0.087483, mean_q: 0.118910\n",
      " 24727/50000: episode: 783, duration: 0.258s, episode steps:  44, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.000149, mae: 0.087123, mean_q: 0.117840\n",
      " 24733/50000: episode: 784, duration: 0.040s, episode steps:   6, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000159, mae: 0.086691, mean_q: 0.117244\n",
      " 24756/50000: episode: 785, duration: 0.136s, episode steps:  23, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.174 [0.000, 3.000],  loss: 0.000133, mae: 0.087197, mean_q: 0.119100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24770/50000: episode: 786, duration: 0.087s, episode steps:  14, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.000155, mae: 0.086510, mean_q: 0.118503\n",
      " 24775/50000: episode: 787, duration: 0.034s, episode steps:   5, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000105, mae: 0.087582, mean_q: 0.118947\n",
      " 24790/50000: episode: 788, duration: 0.089s, episode steps:  15, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000117, mae: 0.086870, mean_q: 0.117816\n",
      " 24818/50000: episode: 789, duration: 0.179s, episode steps:  28, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.000204, mae: 0.085467, mean_q: 0.116448\n",
      " 24829/50000: episode: 790, duration: 0.076s, episode steps:  11, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000159, mae: 0.087030, mean_q: 0.117893\n",
      " 24916/50000: episode: 791, duration: 0.544s, episode steps:  87, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.000141, mae: 0.086579, mean_q: 0.118306\n",
      " 24991/50000: episode: 792, duration: 0.490s, episode steps:  75, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000147, mae: 0.086211, mean_q: 0.117186\n",
      " 25029/50000: episode: 793, duration: 0.255s, episode steps:  38, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000159, mae: 0.085602, mean_q: 0.115754\n",
      " 25060/50000: episode: 794, duration: 0.185s, episode steps:  31, steps per second: 167, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.000122, mae: 0.085995, mean_q: 0.117330\n",
      " 25075/50000: episode: 795, duration: 0.095s, episode steps:  15, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000119, mae: 0.084700, mean_q: 0.115465\n",
      " 25087/50000: episode: 796, duration: 0.075s, episode steps:  12, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.000143, mae: 0.086222, mean_q: 0.117082\n",
      " 25097/50000: episode: 797, duration: 0.067s, episode steps:  10, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000194, mae: 0.085713, mean_q: 0.116062\n",
      " 25119/50000: episode: 798, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000165, mae: 0.084565, mean_q: 0.114490\n",
      " 25146/50000: episode: 799, duration: 0.187s, episode steps:  27, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000164, mae: 0.083815, mean_q: 0.114276\n",
      " 25165/50000: episode: 800, duration: 0.149s, episode steps:  19, steps per second: 128, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000139, mae: 0.084388, mean_q: 0.114613\n",
      " 25231/50000: episode: 801, duration: 0.522s, episode steps:  66, steps per second: 126, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000131, mae: 0.084777, mean_q: 0.115849\n",
      " 25279/50000: episode: 802, duration: 0.351s, episode steps:  48, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000163, mae: 0.084317, mean_q: 0.114257\n",
      " 25307/50000: episode: 803, duration: 0.162s, episode steps:  28, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.000178, mae: 0.084036, mean_q: 0.113883\n",
      " 25323/50000: episode: 804, duration: 0.099s, episode steps:  16, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000149, mae: 0.084083, mean_q: 0.114148\n",
      " 25335/50000: episode: 805, duration: 0.074s, episode steps:  12, steps per second: 163, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000111, mae: 0.083169, mean_q: 0.112851\n",
      " 25368/50000: episode: 806, duration: 0.193s, episode steps:  33, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.697 [0.000, 3.000],  loss: 0.000567, mae: 0.083692, mean_q: 0.114753\n",
      " 25379/50000: episode: 807, duration: 0.072s, episode steps:  11, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.000205, mae: 0.085648, mean_q: 0.118635\n",
      " 25393/50000: episode: 808, duration: 0.092s, episode steps:  14, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.000194, mae: 0.084412, mean_q: 0.115601\n",
      " 25448/50000: episode: 809, duration: 0.380s, episode steps:  55, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.000149, mae: 0.083367, mean_q: 0.113697\n",
      " 25569/50000: episode: 810, duration: 0.855s, episode steps: 121, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000135, mae: 0.083122, mean_q: 0.112907\n",
      " 25596/50000: episode: 811, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000094, mae: 0.083258, mean_q: 0.112621\n",
      " 25612/50000: episode: 812, duration: 0.110s, episode steps:  16, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.000116, mae: 0.082389, mean_q: 0.111699\n",
      " 25625/50000: episode: 813, duration: 0.089s, episode steps:  13, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.000118, mae: 0.081763, mean_q: 0.111435\n",
      " 25636/50000: episode: 814, duration: 0.075s, episode steps:  11, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.000135, mae: 0.083690, mean_q: 0.113717\n",
      " 25733/50000: episode: 815, duration: 0.623s, episode steps:  97, steps per second: 156, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.691 [0.000, 3.000],  loss: 0.000144, mae: 0.081918, mean_q: 0.110793\n",
      " 25760/50000: episode: 816, duration: 0.175s, episode steps:  27, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000172, mae: 0.081091, mean_q: 0.109685\n",
      " 25766/50000: episode: 817, duration: 0.048s, episode steps:   6, steps per second: 126, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.000143, mae: 0.080910, mean_q: 0.108578\n",
      " 25822/50000: episode: 818, duration: 0.384s, episode steps:  56, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000125, mae: 0.080832, mean_q: 0.110108\n",
      " 25844/50000: episode: 819, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000154, mae: 0.080283, mean_q: 0.109211\n",
      " 25853/50000: episode: 820, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000106, mae: 0.080180, mean_q: 0.108475\n",
      " 25865/50000: episode: 821, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000123, mae: 0.080982, mean_q: 0.110171\n",
      " 25872/50000: episode: 822, duration: 0.056s, episode steps:   7, steps per second: 125, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000103, mae: 0.079981, mean_q: 0.108059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25880/50000: episode: 823, duration: 0.064s, episode steps:   8, steps per second: 125, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000197, mae: 0.081193, mean_q: 0.108392\n",
      " 25915/50000: episode: 824, duration: 0.250s, episode steps:  35, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.257 [0.000, 3.000],  loss: 0.000126, mae: 0.080322, mean_q: 0.108595\n",
      " 25972/50000: episode: 825, duration: 0.360s, episode steps:  57, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.000411, mae: 0.079580, mean_q: 0.108809\n",
      " 26012/50000: episode: 826, duration: 0.254s, episode steps:  40, steps per second: 158, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000123, mae: 0.079560, mean_q: 0.108844\n",
      " 26037/50000: episode: 827, duration: 0.158s, episode steps:  25, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.880 [0.000, 3.000],  loss: 0.000152, mae: 0.079166, mean_q: 0.108531\n",
      " 26122/50000: episode: 828, duration: 0.487s, episode steps:  85, steps per second: 174, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.624 [0.000, 3.000],  loss: 0.000151, mae: 0.078803, mean_q: 0.107228\n",
      " 26200/50000: episode: 829, duration: 0.488s, episode steps:  78, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.000134, mae: 0.078748, mean_q: 0.107432\n",
      " 26211/50000: episode: 830, duration: 0.073s, episode steps:  11, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 2.000],  loss: 0.000095, mae: 0.078053, mean_q: 0.106169\n",
      " 26223/50000: episode: 831, duration: 0.079s, episode steps:  12, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000078, mae: 0.079281, mean_q: 0.107402\n",
      " 26248/50000: episode: 832, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000086, mae: 0.078655, mean_q: 0.106359\n",
      " 26260/50000: episode: 833, duration: 0.087s, episode steps:  12, steps per second: 138, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000092, mae: 0.078360, mean_q: 0.106133\n",
      " 26335/50000: episode: 834, duration: 0.463s, episode steps:  75, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000117, mae: 0.077573, mean_q: 0.104848\n",
      " 26356/50000: episode: 835, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000159, mae: 0.076761, mean_q: 0.104210\n",
      " 26390/50000: episode: 836, duration: 0.220s, episode steps:  34, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.735 [0.000, 3.000],  loss: 0.000127, mae: 0.076837, mean_q: 0.104866\n",
      " 26416/50000: episode: 837, duration: 0.152s, episode steps:  26, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.000131, mae: 0.076716, mean_q: 0.104035\n",
      " 26429/50000: episode: 838, duration: 0.080s, episode steps:  13, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.000092, mae: 0.077173, mean_q: 0.104650\n",
      " 26447/50000: episode: 839, duration: 0.109s, episode steps:  18, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.000108, mae: 0.076890, mean_q: 0.103487\n",
      " 26470/50000: episode: 840, duration: 0.137s, episode steps:  23, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000117, mae: 0.076685, mean_q: 0.103035\n",
      " 26535/50000: episode: 841, duration: 0.385s, episode steps:  65, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000122, mae: 0.075953, mean_q: 0.102685\n",
      " 26572/50000: episode: 842, duration: 0.254s, episode steps:  37, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.189 [0.000, 3.000],  loss: 0.000094, mae: 0.076282, mean_q: 0.103670\n",
      " 26626/50000: episode: 843, duration: 0.344s, episode steps:  54, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000120, mae: 0.075633, mean_q: 0.102892\n",
      " 26655/50000: episode: 844, duration: 0.179s, episode steps:  29, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000102, mae: 0.075202, mean_q: 0.102435\n",
      " 26704/50000: episode: 845, duration: 0.305s, episode steps:  49, steps per second: 161, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000119, mae: 0.075555, mean_q: 0.101878\n",
      " 26727/50000: episode: 846, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000122, mae: 0.073910, mean_q: 0.100990\n",
      " 26771/50000: episode: 847, duration: 0.339s, episode steps:  44, steps per second: 130, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.386 [0.000, 3.000],  loss: 0.000112, mae: 0.074802, mean_q: 0.102365\n",
      " 26806/50000: episode: 848, duration: 0.274s, episode steps:  35, steps per second: 128, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000147, mae: 0.073200, mean_q: 0.099519\n",
      " 26816/50000: episode: 849, duration: 0.076s, episode steps:  10, steps per second: 131, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000082, mae: 0.074296, mean_q: 0.101380\n",
      " 26830/50000: episode: 850, duration: 0.094s, episode steps:  14, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.000089, mae: 0.074056, mean_q: 0.102055\n",
      " 26913/50000: episode: 851, duration: 0.550s, episode steps:  83, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000095, mae: 0.074038, mean_q: 0.100709\n",
      " 26939/50000: episode: 852, duration: 0.202s, episode steps:  26, steps per second: 128, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000107, mae: 0.073430, mean_q: 0.100229\n",
      " 26963/50000: episode: 853, duration: 0.202s, episode steps:  24, steps per second: 119, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000119, mae: 0.074211, mean_q: 0.101065\n",
      " 27023/50000: episode: 854, duration: 0.449s, episode steps:  60, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000107, mae: 0.072913, mean_q: 0.099008\n",
      " 27048/50000: episode: 855, duration: 0.173s, episode steps:  25, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.000120, mae: 0.073773, mean_q: 0.100113\n",
      " 27074/50000: episode: 856, duration: 0.186s, episode steps:  26, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.000121, mae: 0.072831, mean_q: 0.098636\n",
      " 27095/50000: episode: 857, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000088, mae: 0.072433, mean_q: 0.099873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27125/50000: episode: 858, duration: 0.214s, episode steps:  30, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.000103, mae: 0.072894, mean_q: 0.099371\n",
      " 27140/50000: episode: 859, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000122, mae: 0.073825, mean_q: 0.100054\n",
      " 27159/50000: episode: 860, duration: 0.138s, episode steps:  19, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000123, mae: 0.071667, mean_q: 0.097866\n",
      " 27184/50000: episode: 861, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000160, mae: 0.070966, mean_q: 0.096723\n",
      " 27206/50000: episode: 862, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.227 [0.000, 3.000],  loss: 0.000110, mae: 0.072991, mean_q: 0.099420\n",
      " 27235/50000: episode: 863, duration: 0.224s, episode steps:  29, steps per second: 130, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.000086, mae: 0.072070, mean_q: 0.098240\n",
      " 27247/50000: episode: 864, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000068, mae: 0.071941, mean_q: 0.098422\n",
      " 27260/50000: episode: 865, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.000132, mae: 0.070962, mean_q: 0.097683\n",
      " 27271/50000: episode: 866, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000148, mae: 0.071814, mean_q: 0.098019\n",
      " 27296/50000: episode: 867, duration: 0.180s, episode steps:  25, steps per second: 139, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000124, mae: 0.070983, mean_q: 0.096194\n",
      " 27334/50000: episode: 868, duration: 0.286s, episode steps:  38, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.553 [0.000, 3.000],  loss: 0.000105, mae: 0.070967, mean_q: 0.097057\n",
      " 27345/50000: episode: 869, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.000100, mae: 0.072479, mean_q: 0.097868\n",
      " 27458/50000: episode: 870, duration: 0.775s, episode steps: 113, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.000091, mae: 0.071033, mean_q: 0.096492\n",
      " 27499/50000: episode: 871, duration: 0.287s, episode steps:  41, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.000108, mae: 0.070089, mean_q: 0.095632\n",
      " 27540/50000: episode: 872, duration: 0.246s, episode steps:  41, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.268 [0.000, 3.000],  loss: 0.000094, mae: 0.070816, mean_q: 0.096974\n",
      " 27646/50000: episode: 873, duration: 0.607s, episode steps: 106, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.642 [0.000, 3.000],  loss: 0.000083, mae: 0.070356, mean_q: 0.095550\n",
      " 27695/50000: episode: 874, duration: 0.288s, episode steps:  49, steps per second: 170, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.347 [0.000, 3.000],  loss: 0.000091, mae: 0.069172, mean_q: 0.093717\n",
      " 27755/50000: episode: 875, duration: 0.349s, episode steps:  60, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.000087, mae: 0.069570, mean_q: 0.094487\n",
      " 27785/50000: episode: 876, duration: 0.174s, episode steps:  30, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000607, mae: 0.068607, mean_q: 0.092747\n",
      " 27836/50000: episode: 877, duration: 0.320s, episode steps:  51, steps per second: 159, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000098, mae: 0.069680, mean_q: 0.094977\n",
      " 27888/50000: episode: 878, duration: 0.302s, episode steps:  52, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 0.000096, mae: 0.068729, mean_q: 0.094266\n",
      " 27922/50000: episode: 879, duration: 0.197s, episode steps:  34, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.794 [0.000, 3.000],  loss: 0.000120, mae: 0.067650, mean_q: 0.091935\n",
      " 27975/50000: episode: 880, duration: 0.315s, episode steps:  53, steps per second: 168, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000114, mae: 0.067979, mean_q: 0.092571\n",
      " 28000/50000: episode: 881, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.000100, mae: 0.067508, mean_q: 0.092376\n",
      " 28013/50000: episode: 882, duration: 0.090s, episode steps:  13, steps per second: 144, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.000123, mae: 0.067976, mean_q: 0.092918\n",
      " 28040/50000: episode: 883, duration: 0.172s, episode steps:  27, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000115, mae: 0.066975, mean_q: 0.091394\n",
      " 28050/50000: episode: 884, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 3.000],  loss: 0.000096, mae: 0.069231, mean_q: 0.094177\n",
      " 28109/50000: episode: 885, duration: 0.356s, episode steps:  59, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.000094, mae: 0.068025, mean_q: 0.092804\n",
      " 28120/50000: episode: 886, duration: 0.071s, episode steps:  11, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.000106, mae: 0.067996, mean_q: 0.091762\n",
      " 28132/50000: episode: 887, duration: 0.079s, episode steps:  12, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.083 [0.000, 3.000],  loss: 0.000106, mae: 0.067364, mean_q: 0.091667\n",
      " 28144/50000: episode: 888, duration: 0.082s, episode steps:  12, steps per second: 146, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.917 [0.000, 3.000],  loss: 0.000072, mae: 0.066902, mean_q: 0.091561\n",
      " 28161/50000: episode: 889, duration: 0.113s, episode steps:  17, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.000071, mae: 0.067055, mean_q: 0.091810\n",
      " 28185/50000: episode: 890, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000064, mae: 0.067559, mean_q: 0.092347\n",
      " 28192/50000: episode: 891, duration: 0.049s, episode steps:   7, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [1.000, 3.000],  loss: 0.000073, mae: 0.068959, mean_q: 0.093523\n",
      " 28199/50000: episode: 892, duration: 0.049s, episode steps:   7, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000142, mae: 0.068017, mean_q: 0.091800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28259/50000: episode: 893, duration: 0.394s, episode steps:  60, steps per second: 152, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.383 [0.000, 3.000],  loss: 0.000095, mae: 0.066616, mean_q: 0.090581\n",
      " 28282/50000: episode: 894, duration: 0.161s, episode steps:  23, steps per second: 143, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.000069, mae: 0.066052, mean_q: 0.089319\n",
      " 28341/50000: episode: 895, duration: 0.349s, episode steps:  59, steps per second: 169, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.763 [0.000, 3.000],  loss: 0.000076, mae: 0.066176, mean_q: 0.089759\n",
      " 28372/50000: episode: 896, duration: 0.193s, episode steps:  31, steps per second: 160, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000092, mae: 0.065986, mean_q: 0.090396\n",
      " 28428/50000: episode: 897, duration: 0.342s, episode steps:  56, steps per second: 164, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.000083, mae: 0.066116, mean_q: 0.089739\n",
      " 28455/50000: episode: 898, duration: 0.157s, episode steps:  27, steps per second: 172, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000098, mae: 0.065384, mean_q: 0.088247\n",
      " 28518/50000: episode: 899, duration: 0.369s, episode steps:  63, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: 0.000080, mae: 0.065545, mean_q: 0.089462\n",
      " 28550/50000: episode: 900, duration: 0.194s, episode steps:  32, steps per second: 165, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.000080, mae: 0.065172, mean_q: 0.089109\n",
      " 28604/50000: episode: 901, duration: 0.353s, episode steps:  54, steps per second: 153, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000100, mae: 0.064817, mean_q: 0.087649\n",
      " 28657/50000: episode: 902, duration: 0.373s, episode steps:  53, steps per second: 142, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: 0.000080, mae: 0.064497, mean_q: 0.088092\n",
      " 28662/50000: episode: 903, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 2.000],  loss: 0.000141, mae: 0.063388, mean_q: 0.086210\n",
      " 28679/50000: episode: 904, duration: 0.184s, episode steps:  17, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000101, mae: 0.063736, mean_q: 0.086423\n",
      " 28696/50000: episode: 905, duration: 0.209s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000099, mae: 0.062598, mean_q: 0.085563\n",
      " 28708/50000: episode: 906, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000085, mae: 0.064966, mean_q: 0.088352\n",
      " 28785/50000: episode: 907, duration: 0.906s, episode steps:  77, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.000083, mae: 0.063610, mean_q: 0.086737\n",
      " 28801/50000: episode: 908, duration: 0.196s, episode steps:  16, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.000072, mae: 0.064260, mean_q: 0.086630\n",
      " 28841/50000: episode: 909, duration: 0.482s, episode steps:  40, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000082, mae: 0.063653, mean_q: 0.086432\n",
      " 28866/50000: episode: 910, duration: 0.308s, episode steps:  25, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000071, mae: 0.063870, mean_q: 0.087374\n",
      " 28884/50000: episode: 911, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000098, mae: 0.063176, mean_q: 0.085710\n",
      " 28906/50000: episode: 912, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000077, mae: 0.063487, mean_q: 0.085823\n",
      " 28929/50000: episode: 913, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.391 [0.000, 3.000],  loss: 0.000075, mae: 0.063467, mean_q: 0.086626\n",
      " 28961/50000: episode: 914, duration: 0.385s, episode steps:  32, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.000082, mae: 0.062397, mean_q: 0.084831\n",
      " 28982/50000: episode: 915, duration: 0.261s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.238 [0.000, 3.000],  loss: 0.000071, mae: 0.062944, mean_q: 0.084936\n",
      " 29015/50000: episode: 916, duration: 0.400s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.000074, mae: 0.062365, mean_q: 0.084802\n",
      " 29038/50000: episode: 917, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.870 [0.000, 3.000],  loss: 0.000065, mae: 0.063126, mean_q: 0.086303\n",
      " 29050/50000: episode: 918, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.000073, mae: 0.062896, mean_q: 0.085188\n",
      " 29093/50000: episode: 919, duration: 0.520s, episode steps:  43, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000090, mae: 0.061966, mean_q: 0.084610\n",
      " 29116/50000: episode: 920, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.957 [0.000, 3.000],  loss: 0.000078, mae: 0.062930, mean_q: 0.086241\n",
      " 29123/50000: episode: 921, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002126, mae: 0.063614, mean_q: 0.085939\n",
      " 29140/50000: episode: 922, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.000125, mae: 0.064208, mean_q: 0.091279\n",
      " 29147/50000: episode: 923, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000054, mae: 0.064820, mean_q: 0.089049\n",
      " 29239/50000: episode: 924, duration: 1.079s, episode steps:  92, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000087, mae: 0.062323, mean_q: 0.084776\n",
      " 29282/50000: episode: 925, duration: 0.516s, episode steps:  43, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.372 [0.000, 3.000],  loss: 0.000079, mae: 0.061766, mean_q: 0.083631\n",
      " 29307/50000: episode: 926, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000064, mae: 0.062153, mean_q: 0.084019\n",
      " 29333/50000: episode: 927, duration: 0.319s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000088, mae: 0.061333, mean_q: 0.083263\n",
      " 29349/50000: episode: 928, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000065, mae: 0.060383, mean_q: 0.082446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29358/50000: episode: 929, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000056, mae: 0.062061, mean_q: 0.084699\n",
      " 29363/50000: episode: 930, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.000031, mae: 0.062565, mean_q: 0.085044\n",
      " 29377/50000: episode: 931, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000109, mae: 0.060735, mean_q: 0.082643\n",
      " 29440/50000: episode: 932, duration: 0.744s, episode steps:  63, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000067, mae: 0.060759, mean_q: 0.082367\n",
      " 29491/50000: episode: 933, duration: 0.604s, episode steps:  51, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.569 [0.000, 3.000],  loss: 0.000069, mae: 0.060796, mean_q: 0.082239\n",
      " 29553/50000: episode: 934, duration: 0.732s, episode steps:  62, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000088, mae: 0.060497, mean_q: 0.082780\n",
      " 29572/50000: episode: 935, duration: 0.236s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.947 [0.000, 3.000],  loss: 0.000087, mae: 0.059095, mean_q: 0.079970\n",
      " 29580/50000: episode: 936, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000080, mae: 0.058928, mean_q: 0.080723\n",
      " 29601/50000: episode: 937, duration: 0.257s, episode steps:  21, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.952 [0.000, 3.000],  loss: 0.000070, mae: 0.059785, mean_q: 0.081074\n",
      " 29639/50000: episode: 938, duration: 0.454s, episode steps:  38, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000063, mae: 0.060142, mean_q: 0.081774\n",
      " 29661/50000: episode: 939, duration: 0.270s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.773 [0.000, 3.000],  loss: 0.000059, mae: 0.059971, mean_q: 0.081131\n",
      " 29686/50000: episode: 940, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.160 [0.000, 3.000],  loss: 0.000066, mae: 0.059932, mean_q: 0.080773\n",
      " 29719/50000: episode: 941, duration: 0.399s, episode steps:  33, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.758 [0.000, 3.000],  loss: 0.000074, mae: 0.059526, mean_q: 0.080835\n",
      " 29729/50000: episode: 942, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.000054, mae: 0.061023, mean_q: 0.082868\n",
      " 29795/50000: episode: 943, duration: 0.775s, episode steps:  66, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000068, mae: 0.059508, mean_q: 0.080765\n",
      " 29805/50000: episode: 944, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [1.000, 3.000],  loss: 0.000076, mae: 0.059972, mean_q: 0.081716\n",
      " 29834/50000: episode: 945, duration: 0.350s, episode steps:  29, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000078, mae: 0.059212, mean_q: 0.080593\n",
      " 29864/50000: episode: 946, duration: 0.362s, episode steps:  30, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.000060, mae: 0.059289, mean_q: 0.080342\n",
      " 29908/50000: episode: 947, duration: 0.522s, episode steps:  44, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000073, mae: 0.058650, mean_q: 0.080403\n",
      " 29939/50000: episode: 948, duration: 0.374s, episode steps:  31, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.710 [0.000, 3.000],  loss: 0.000081, mae: 0.058004, mean_q: 0.078966\n",
      " 29956/50000: episode: 949, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000067, mae: 0.058513, mean_q: 0.079754\n",
      " 29999/50000: episode: 950, duration: 0.520s, episode steps:  43, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.209 [0.000, 3.000],  loss: 0.000065, mae: 0.058793, mean_q: 0.079922\n",
      " 30007/50000: episode: 951, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.500 [0.000, 3.000],  loss: 0.000078, mae: 0.058366, mean_q: 0.079373\n",
      " 30018/50000: episode: 952, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.000095, mae: 0.057072, mean_q: 0.076757\n",
      " 30053/50000: episode: 953, duration: 0.425s, episode steps:  35, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.000057, mae: 0.058575, mean_q: 0.079425\n",
      " 30118/50000: episode: 954, duration: 0.767s, episode steps:  65, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.000064, mae: 0.057957, mean_q: 0.078876\n",
      " 30133/50000: episode: 955, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000054, mae: 0.056989, mean_q: 0.078194\n",
      " 30171/50000: episode: 956, duration: 0.455s, episode steps:  38, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.342 [0.000, 3.000],  loss: 0.000060, mae: 0.057852, mean_q: 0.078892\n",
      " 30214/50000: episode: 957, duration: 0.511s, episode steps:  43, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.767 [0.000, 3.000],  loss: 0.000773, mae: 0.060046, mean_q: 0.084965\n",
      " 30221/50000: episode: 958, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000083, mae: 0.059251, mean_q: 0.080179\n",
      " 30243/50000: episode: 959, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.955 [0.000, 3.000],  loss: 0.000082, mae: 0.058184, mean_q: 0.079622\n",
      " 30264/50000: episode: 960, duration: 0.273s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.048 [0.000, 3.000],  loss: 0.000099, mae: 0.058561, mean_q: 0.079259\n",
      " 30279/50000: episode: 961, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.000078, mae: 0.056985, mean_q: 0.077566\n",
      " 30343/50000: episode: 962, duration: 0.755s, episode steps:  64, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000078, mae: 0.057560, mean_q: 0.077311\n",
      " 30363/50000: episode: 963, duration: 0.250s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.000087, mae: 0.056527, mean_q: 0.076188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30464/50000: episode: 964, duration: 1.183s, episode steps: 101, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.000065, mae: 0.056076, mean_q: 0.075895\n",
      " 30516/50000: episode: 965, duration: 0.617s, episode steps:  52, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.827 [0.000, 3.000],  loss: 0.000068, mae: 0.055200, mean_q: 0.075091\n",
      " 30552/50000: episode: 966, duration: 0.440s, episode steps:  36, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000069, mae: 0.055434, mean_q: 0.075376\n",
      " 30575/50000: episode: 967, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000077, mae: 0.055476, mean_q: 0.075055\n",
      " 30600/50000: episode: 968, duration: 0.303s, episode steps:  25, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.000065, mae: 0.055318, mean_q: 0.074917\n",
      " 30646/50000: episode: 969, duration: 0.549s, episode steps:  46, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.000067, mae: 0.054790, mean_q: 0.074416\n",
      " 30721/50000: episode: 970, duration: 0.885s, episode steps:  75, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.000066, mae: 0.054590, mean_q: 0.073700\n",
      " 30754/50000: episode: 971, duration: 0.396s, episode steps:  33, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.000055, mae: 0.054018, mean_q: 0.073581\n",
      " 30789/50000: episode: 972, duration: 0.417s, episode steps:  35, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000061, mae: 0.053757, mean_q: 0.072580\n",
      " 30846/50000: episode: 973, duration: 0.674s, episode steps:  57, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.000050, mae: 0.053525, mean_q: 0.072930\n",
      " 30854/50000: episode: 974, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000025, mae: 0.054151, mean_q: 0.073188\n",
      " 30874/50000: episode: 975, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000049, mae: 0.053566, mean_q: 0.072876\n",
      " 30894/50000: episode: 976, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000067, mae: 0.053432, mean_q: 0.072695\n",
      " 30943/50000: episode: 977, duration: 0.582s, episode steps:  49, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.000063, mae: 0.053337, mean_q: 0.072509\n",
      " 30972/50000: episode: 978, duration: 0.357s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.241 [0.000, 3.000],  loss: 0.000060, mae: 0.053568, mean_q: 0.072603\n",
      " 31001/50000: episode: 979, duration: 0.350s, episode steps:  29, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.345 [0.000, 3.000],  loss: 0.000054, mae: 0.052869, mean_q: 0.071619\n",
      " 31071/50000: episode: 980, duration: 0.823s, episode steps:  70, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.000058, mae: 0.053416, mean_q: 0.072590\n",
      " 31088/50000: episode: 981, duration: 0.207s, episode steps:  17, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.118 [0.000, 3.000],  loss: 0.000033, mae: 0.053928, mean_q: 0.073191\n",
      " 31106/50000: episode: 982, duration: 0.227s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000045, mae: 0.052407, mean_q: 0.070630\n",
      " 31137/50000: episode: 983, duration: 0.382s, episode steps:  31, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000067, mae: 0.053240, mean_q: 0.072467\n",
      " 31182/50000: episode: 984, duration: 0.547s, episode steps:  45, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.000059, mae: 0.052791, mean_q: 0.072248\n",
      " 31194/50000: episode: 985, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000094, mae: 0.052513, mean_q: 0.070517\n",
      " 31242/50000: episode: 986, duration: 0.570s, episode steps:  48, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000065, mae: 0.052800, mean_q: 0.072216\n",
      " 31254/50000: episode: 987, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000061, mae: 0.052755, mean_q: 0.072059\n",
      " 31336/50000: episode: 988, duration: 0.965s, episode steps:  82, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.317 [0.000, 3.000],  loss: 0.000055, mae: 0.052398, mean_q: 0.070884\n",
      " 31396/50000: episode: 989, duration: 0.708s, episode steps:  60, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000053, mae: 0.052153, mean_q: 0.070280\n",
      " 31410/50000: episode: 990, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000052, mae: 0.051532, mean_q: 0.069673\n",
      " 31464/50000: episode: 991, duration: 0.642s, episode steps:  54, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000060, mae: 0.051608, mean_q: 0.070321\n",
      " 31516/50000: episode: 992, duration: 0.623s, episode steps:  52, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 0.000064, mae: 0.051220, mean_q: 0.069208\n",
      " 31524/50000: episode: 993, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000080, mae: 0.051461, mean_q: 0.070959\n",
      " 31590/50000: episode: 994, duration: 0.778s, episode steps:  66, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.000060, mae: 0.051284, mean_q: 0.069809\n",
      " 31605/50000: episode: 995, duration: 0.184s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000049, mae: 0.050855, mean_q: 0.069667\n",
      " 31615/50000: episode: 996, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000044, mae: 0.052551, mean_q: 0.071430\n",
      " 31663/50000: episode: 997, duration: 0.573s, episode steps:  48, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000050, mae: 0.051662, mean_q: 0.069465\n",
      " 31693/50000: episode: 998, duration: 0.364s, episode steps:  30, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000062, mae: 0.050232, mean_q: 0.067662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31726/50000: episode: 999, duration: 0.402s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.000051, mae: 0.050902, mean_q: 0.068738\n",
      " 31741/50000: episode: 1000, duration: 0.184s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000060, mae: 0.051035, mean_q: 0.069414\n",
      " 31771/50000: episode: 1001, duration: 0.362s, episode steps:  30, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.000066, mae: 0.050422, mean_q: 0.067741\n",
      " 31784/50000: episode: 1002, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000041, mae: 0.050770, mean_q: 0.068380\n",
      " 31816/50000: episode: 1003, duration: 0.453s, episode steps:  32, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.062 [0.000, 3.000],  loss: 0.000051, mae: 0.050315, mean_q: 0.068225\n",
      " 31837/50000: episode: 1004, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000036, mae: 0.050336, mean_q: 0.068414\n",
      " 31860/50000: episode: 1005, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000038, mae: 0.050016, mean_q: 0.067920\n",
      " 31889/50000: episode: 1006, duration: 0.355s, episode steps:  29, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.000066, mae: 0.049879, mean_q: 0.068286\n",
      " 31916/50000: episode: 1007, duration: 0.330s, episode steps:  27, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.000053, mae: 0.049958, mean_q: 0.068169\n",
      " 31936/50000: episode: 1008, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.850 [0.000, 3.000],  loss: 0.000058, mae: 0.049286, mean_q: 0.066555\n",
      " 32023/50000: episode: 1009, duration: 1.027s, episode steps:  87, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000056, mae: 0.049482, mean_q: 0.066751\n",
      " 32035/50000: episode: 1010, duration: 0.151s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001288, mae: 0.049344, mean_q: 0.067069\n",
      " 32127/50000: episode: 1011, duration: 1.085s, episode steps:  92, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.815 [0.000, 3.000],  loss: 0.000059, mae: 0.048883, mean_q: 0.066923\n",
      " 32156/50000: episode: 1012, duration: 0.357s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000072, mae: 0.048524, mean_q: 0.065551\n",
      " 32190/50000: episode: 1013, duration: 0.409s, episode steps:  34, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000056, mae: 0.048063, mean_q: 0.064738\n",
      " 32221/50000: episode: 1014, duration: 0.381s, episode steps:  31, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.000049, mae: 0.047916, mean_q: 0.064635\n",
      " 32240/50000: episode: 1015, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000062, mae: 0.046862, mean_q: 0.063548\n",
      " 32277/50000: episode: 1016, duration: 0.449s, episode steps:  37, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: 0.000050, mae: 0.047395, mean_q: 0.064503\n",
      " 32284/50000: episode: 1017, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000043, mae: 0.046572, mean_q: 0.063753\n",
      " 32296/50000: episode: 1018, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000045, mae: 0.046820, mean_q: 0.064534\n",
      " 32312/50000: episode: 1019, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000034, mae: 0.047290, mean_q: 0.064364\n",
      " 32344/50000: episode: 1020, duration: 0.386s, episode steps:  32, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000037, mae: 0.047643, mean_q: 0.064614\n",
      " 32369/50000: episode: 1021, duration: 0.307s, episode steps:  25, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000057, mae: 0.046126, mean_q: 0.061923\n",
      " 32424/50000: episode: 1022, duration: 0.658s, episode steps:  55, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.327 [0.000, 3.000],  loss: 0.000046, mae: 0.046620, mean_q: 0.062723\n",
      " 32467/50000: episode: 1023, duration: 0.515s, episode steps:  43, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000042, mae: 0.046485, mean_q: 0.063066\n",
      " 32536/50000: episode: 1024, duration: 0.822s, episode steps:  69, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000049, mae: 0.046134, mean_q: 0.062590\n",
      " 32619/50000: episode: 1025, duration: 0.986s, episode steps:  83, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.373 [0.000, 3.000],  loss: 0.000041, mae: 0.046567, mean_q: 0.062633\n",
      " 32647/50000: episode: 1026, duration: 0.341s, episode steps:  28, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.000040, mae: 0.046243, mean_q: 0.062915\n",
      " 32747/50000: episode: 1027, duration: 1.181s, episode steps: 100, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000047, mae: 0.045638, mean_q: 0.061967\n",
      " 32781/50000: episode: 1028, duration: 0.420s, episode steps:  34, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000044, mae: 0.045942, mean_q: 0.061868\n",
      " 32813/50000: episode: 1029, duration: 0.392s, episode steps:  32, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.000051, mae: 0.045222, mean_q: 0.061178\n",
      " 32828/50000: episode: 1030, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000039, mae: 0.044949, mean_q: 0.060573\n",
      " 32862/50000: episode: 1031, duration: 0.410s, episode steps:  34, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000036, mae: 0.045154, mean_q: 0.061026\n",
      " 32879/50000: episode: 1032, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.059 [0.000, 3.000],  loss: 0.000047, mae: 0.045309, mean_q: 0.061295\n",
      " 32940/50000: episode: 1033, duration: 0.760s, episode steps:  61, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.262 [0.000, 3.000],  loss: 0.000043, mae: 0.045114, mean_q: 0.061027\n",
      " 32955/50000: episode: 1034, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000048, mae: 0.044402, mean_q: 0.059491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32979/50000: episode: 1035, duration: 0.296s, episode steps:  24, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000038, mae: 0.045418, mean_q: 0.061479\n",
      " 33015/50000: episode: 1036, duration: 0.435s, episode steps:  36, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000035, mae: 0.044346, mean_q: 0.060090\n",
      " 33029/50000: episode: 1037, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000044, mae: 0.044650, mean_q: 0.060178\n",
      " 33101/50000: episode: 1038, duration: 0.854s, episode steps:  72, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.306 [0.000, 3.000],  loss: 0.000042, mae: 0.044305, mean_q: 0.060094\n",
      " 33135/50000: episode: 1039, duration: 0.417s, episode steps:  34, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000043, mae: 0.044209, mean_q: 0.060174\n",
      " 33162/50000: episode: 1040, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000041, mae: 0.043977, mean_q: 0.060147\n",
      " 33184/50000: episode: 1041, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.000045, mae: 0.044396, mean_q: 0.060439\n",
      " 33224/50000: episode: 1042, duration: 0.494s, episode steps:  40, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.000039, mae: 0.043724, mean_q: 0.059471\n",
      " 33251/50000: episode: 1043, duration: 0.331s, episode steps:  27, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.815 [0.000, 3.000],  loss: 0.000039, mae: 0.043971, mean_q: 0.059510\n",
      " 33293/50000: episode: 1044, duration: 0.505s, episode steps:  42, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.452 [0.000, 3.000],  loss: 0.000032, mae: 0.043863, mean_q: 0.059431\n",
      " 33339/50000: episode: 1045, duration: 0.553s, episode steps:  46, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000044, mae: 0.043621, mean_q: 0.059245\n",
      " 33345/50000: episode: 1046, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.000035, mae: 0.044133, mean_q: 0.060371\n",
      " 33357/50000: episode: 1047, duration: 0.157s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [1.000, 3.000],  loss: 0.000047, mae: 0.043823, mean_q: 0.059654\n",
      " 33380/50000: episode: 1048, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000069, mae: 0.043098, mean_q: 0.058282\n",
      " 33411/50000: episode: 1049, duration: 0.378s, episode steps:  31, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.129 [0.000, 3.000],  loss: 0.000040, mae: 0.043180, mean_q: 0.058788\n",
      " 33452/50000: episode: 1050, duration: 0.495s, episode steps:  41, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000046, mae: 0.042838, mean_q: 0.058157\n",
      " 33500/50000: episode: 1051, duration: 0.577s, episode steps:  48, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000033, mae: 0.043108, mean_q: 0.058102\n",
      " 33549/50000: episode: 1052, duration: 0.585s, episode steps:  49, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.673 [0.000, 3.000],  loss: 0.000030, mae: 0.042606, mean_q: 0.057793\n",
      " 33577/50000: episode: 1053, duration: 0.340s, episode steps:  28, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000588, mae: 0.043152, mean_q: 0.060241\n",
      " 33597/50000: episode: 1054, duration: 0.250s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000042, mae: 0.042929, mean_q: 0.059381\n",
      " 33612/50000: episode: 1055, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000054, mae: 0.042150, mean_q: 0.057493\n",
      " 33620/50000: episode: 1056, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.000043, mae: 0.042288, mean_q: 0.057172\n",
      " 33645/50000: episode: 1057, duration: 0.305s, episode steps:  25, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.120 [0.000, 3.000],  loss: 0.000617, mae: 0.042321, mean_q: 0.057255\n",
      " 33697/50000: episode: 1058, duration: 0.622s, episode steps:  52, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.788 [0.000, 3.000],  loss: 0.000059, mae: 0.042460, mean_q: 0.058859\n",
      " 33749/50000: episode: 1059, duration: 0.623s, episode steps:  52, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000039, mae: 0.042362, mean_q: 0.057660\n",
      " 33800/50000: episode: 1060, duration: 0.614s, episode steps:  51, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000037, mae: 0.041973, mean_q: 0.057362\n",
      " 33847/50000: episode: 1061, duration: 0.565s, episode steps:  47, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000038, mae: 0.042000, mean_q: 0.056811\n",
      " 33855/50000: episode: 1062, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000048, mae: 0.041784, mean_q: 0.056870\n",
      " 33886/50000: episode: 1063, duration: 0.381s, episode steps:  31, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000040, mae: 0.041897, mean_q: 0.057026\n",
      " 33894/50000: episode: 1064, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000028, mae: 0.042284, mean_q: 0.057831\n",
      " 33945/50000: episode: 1065, duration: 0.617s, episode steps:  51, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.275 [0.000, 3.000],  loss: 0.000041, mae: 0.041658, mean_q: 0.056812\n",
      " 33962/50000: episode: 1066, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000042, mae: 0.041930, mean_q: 0.057042\n",
      " 34009/50000: episode: 1067, duration: 0.569s, episode steps:  47, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000049, mae: 0.041289, mean_q: 0.055643\n",
      " 34027/50000: episode: 1068, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.000041, mae: 0.041952, mean_q: 0.056532\n",
      " 34088/50000: episode: 1069, duration: 0.729s, episode steps:  61, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.672 [0.000, 3.000],  loss: 0.000032, mae: 0.041440, mean_q: 0.055844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34171/50000: episode: 1070, duration: 0.979s, episode steps:  83, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000037, mae: 0.041071, mean_q: 0.055703\n",
      " 34195/50000: episode: 1071, duration: 0.302s, episode steps:  24, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000035, mae: 0.040870, mean_q: 0.055402\n",
      " 34236/50000: episode: 1072, duration: 0.497s, episode steps:  41, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000034, mae: 0.040916, mean_q: 0.055289\n",
      " 34311/50000: episode: 1073, duration: 0.893s, episode steps:  75, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.747 [0.000, 3.000],  loss: 0.000036, mae: 0.040523, mean_q: 0.055113\n",
      " 34352/50000: episode: 1074, duration: 0.497s, episode steps:  41, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.756 [0.000, 3.000],  loss: 0.000386, mae: 0.040677, mean_q: 0.055511\n",
      " 34363/50000: episode: 1075, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000135, mae: 0.041576, mean_q: 0.060560\n",
      " 34410/50000: episode: 1076, duration: 0.567s, episode steps:  47, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.553 [0.000, 3.000],  loss: 0.000044, mae: 0.039940, mean_q: 0.054779\n",
      " 34434/50000: episode: 1077, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000033, mae: 0.039384, mean_q: 0.053547\n",
      " 34464/50000: episode: 1078, duration: 0.369s, episode steps:  30, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000043, mae: 0.039204, mean_q: 0.052773\n",
      " 34486/50000: episode: 1079, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000029, mae: 0.039548, mean_q: 0.053472\n",
      " 34505/50000: episode: 1080, duration: 0.239s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.158 [0.000, 3.000],  loss: 0.000037, mae: 0.039658, mean_q: 0.053923\n",
      " 34527/50000: episode: 1081, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.955 [0.000, 3.000],  loss: 0.000038, mae: 0.039069, mean_q: 0.052641\n",
      " 34542/50000: episode: 1082, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000037, mae: 0.038046, mean_q: 0.052467\n",
      " 34610/50000: episode: 1083, duration: 0.809s, episode steps:  68, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000036, mae: 0.039046, mean_q: 0.053264\n",
      " 34655/50000: episode: 1084, duration: 0.544s, episode steps:  45, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.844 [0.000, 3.000],  loss: 0.000044, mae: 0.038644, mean_q: 0.052899\n",
      " 34669/50000: episode: 1085, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000034, mae: 0.038833, mean_q: 0.053308\n",
      " 34711/50000: episode: 1086, duration: 0.507s, episode steps:  42, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000380, mae: 0.038896, mean_q: 0.054673\n",
      " 34731/50000: episode: 1087, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000048, mae: 0.038135, mean_q: 0.052604\n",
      " 34748/50000: episode: 1088, duration: 0.217s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.000042, mae: 0.037933, mean_q: 0.052598\n",
      " 34784/50000: episode: 1089, duration: 0.443s, episode steps:  36, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000053, mae: 0.038102, mean_q: 0.052319\n",
      " 34800/50000: episode: 1090, duration: 0.198s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.000045, mae: 0.037677, mean_q: 0.051398\n",
      " 34836/50000: episode: 1091, duration: 0.435s, episode steps:  36, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.000036, mae: 0.037826, mean_q: 0.051365\n",
      " 34847/50000: episode: 1092, duration: 0.140s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.636 [0.000, 2.000],  loss: 0.000029, mae: 0.038257, mean_q: 0.051283\n",
      " 34866/50000: episode: 1093, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000029, mae: 0.037751, mean_q: 0.050970\n",
      " 34887/50000: episode: 1094, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000041, mae: 0.037596, mean_q: 0.050714\n",
      " 34917/50000: episode: 1095, duration: 0.371s, episode steps:  30, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000033, mae: 0.036838, mean_q: 0.050111\n",
      " 34924/50000: episode: 1096, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.000031, mae: 0.037631, mean_q: 0.051629\n",
      " 34950/50000: episode: 1097, duration: 0.319s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000033, mae: 0.037132, mean_q: 0.050431\n",
      " 34972/50000: episode: 1098, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000028, mae: 0.037299, mean_q: 0.050196\n",
      " 35021/50000: episode: 1099, duration: 0.590s, episode steps:  49, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000030, mae: 0.037058, mean_q: 0.049773\n",
      " 35029/50000: episode: 1100, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000030, mae: 0.037262, mean_q: 0.050031\n",
      " 35075/50000: episode: 1101, duration: 0.555s, episode steps:  46, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 0.000028, mae: 0.036650, mean_q: 0.049324\n",
      " 35087/50000: episode: 1102, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000031, mae: 0.036713, mean_q: 0.049661\n",
      " 35114/50000: episode: 1103, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000026, mae: 0.036615, mean_q: 0.050112\n",
      " 35136/50000: episode: 1104, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000028, mae: 0.036774, mean_q: 0.049764\n",
      " 35142/50000: episode: 1105, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.000034, mae: 0.037507, mean_q: 0.051068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35161/50000: episode: 1106, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000037, mae: 0.036567, mean_q: 0.049401\n",
      " 35183/50000: episode: 1107, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.136 [0.000, 3.000],  loss: 0.000031, mae: 0.036624, mean_q: 0.050001\n",
      " 35215/50000: episode: 1108, duration: 0.389s, episode steps:  32, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.156 [0.000, 3.000],  loss: 0.000026, mae: 0.036722, mean_q: 0.050251\n",
      " 35224/50000: episode: 1109, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000037, mae: 0.036643, mean_q: 0.049906\n",
      " 35302/50000: episode: 1110, duration: 0.922s, episode steps:  78, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000027, mae: 0.036615, mean_q: 0.049716\n",
      " 35372/50000: episode: 1111, duration: 0.835s, episode steps:  70, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.000027, mae: 0.035903, mean_q: 0.048601\n",
      " 35388/50000: episode: 1112, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000023, mae: 0.036347, mean_q: 0.048894\n",
      " 35456/50000: episode: 1113, duration: 0.811s, episode steps:  68, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.691 [0.000, 3.000],  loss: 0.000026, mae: 0.035991, mean_q: 0.048755\n",
      " 35521/50000: episode: 1114, duration: 0.776s, episode steps:  65, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000028, mae: 0.035674, mean_q: 0.048275\n",
      " 35536/50000: episode: 1115, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000029, mae: 0.034956, mean_q: 0.047873\n",
      " 35579/50000: episode: 1116, duration: 0.522s, episode steps:  43, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000023, mae: 0.035569, mean_q: 0.048026\n",
      " 35608/50000: episode: 1117, duration: 0.357s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000024, mae: 0.035912, mean_q: 0.048391\n",
      " 35626/50000: episode: 1118, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.000028, mae: 0.035388, mean_q: 0.048108\n",
      " 35652/50000: episode: 1119, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000020, mae: 0.035712, mean_q: 0.048355\n",
      " 35664/50000: episode: 1120, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000031, mae: 0.035170, mean_q: 0.048189\n",
      " 35693/50000: episode: 1121, duration: 0.355s, episode steps:  29, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.103 [0.000, 3.000],  loss: 0.000033, mae: 0.035205, mean_q: 0.048179\n",
      " 35701/50000: episode: 1122, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000036, mae: 0.034415, mean_q: 0.046329\n",
      " 35774/50000: episode: 1123, duration: 0.867s, episode steps:  73, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.425 [0.000, 3.000],  loss: 0.000027, mae: 0.035045, mean_q: 0.047494\n",
      " 35796/50000: episode: 1124, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000021, mae: 0.034788, mean_q: 0.047148\n",
      " 35838/50000: episode: 1125, duration: 0.512s, episode steps:  42, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.000028, mae: 0.034736, mean_q: 0.047065\n",
      " 35846/50000: episode: 1126, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 2.000],  loss: 0.000047, mae: 0.033650, mean_q: 0.044834\n",
      " 35861/50000: episode: 1127, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000025, mae: 0.034718, mean_q: 0.047649\n",
      " 35897/50000: episode: 1128, duration: 0.436s, episode steps:  36, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000025, mae: 0.034447, mean_q: 0.046810\n",
      " 35993/50000: episode: 1129, duration: 1.136s, episode steps:  96, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.771 [0.000, 3.000],  loss: 0.000027, mae: 0.034204, mean_q: 0.046532\n",
      " 36003/50000: episode: 1130, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 2.000],  loss: 0.000029, mae: 0.033741, mean_q: 0.045920\n",
      " 36025/50000: episode: 1131, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.773 [0.000, 3.000],  loss: 0.000023, mae: 0.034172, mean_q: 0.046173\n",
      " 36044/50000: episode: 1132, duration: 0.237s, episode steps:  19, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.000026, mae: 0.033978, mean_q: 0.046530\n",
      " 36051/50000: episode: 1133, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000026, mae: 0.034702, mean_q: 0.047129\n",
      " 36085/50000: episode: 1134, duration: 0.417s, episode steps:  34, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.000028, mae: 0.033769, mean_q: 0.046178\n",
      " 36107/50000: episode: 1135, duration: 0.270s, episode steps:  22, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000025, mae: 0.033587, mean_q: 0.045773\n",
      " 36124/50000: episode: 1136, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.000027, mae: 0.034209, mean_q: 0.046649\n",
      " 36223/50000: episode: 1137, duration: 1.180s, episode steps:  99, steps per second:  84, episode reward:  1.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.000024, mae: 0.033939, mean_q: 0.046341\n",
      " 36279/50000: episode: 1138, duration: 0.674s, episode steps:  56, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000022, mae: 0.033831, mean_q: 0.045736\n",
      " 36308/50000: episode: 1139, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000533, mae: 0.034774, mean_q: 0.048449\n",
      " 36325/50000: episode: 1140, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000032, mae: 0.034008, mean_q: 0.046664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36370/50000: episode: 1141, duration: 0.550s, episode steps:  45, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000023, mae: 0.033486, mean_q: 0.046069\n",
      " 36406/50000: episode: 1142, duration: 0.446s, episode steps:  36, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.167 [0.000, 3.000],  loss: 0.000029, mae: 0.033029, mean_q: 0.045073\n",
      " 36429/50000: episode: 1143, duration: 0.284s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000030, mae: 0.033282, mean_q: 0.045297\n",
      " 36458/50000: episode: 1144, duration: 0.356s, episode steps:  29, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.000035, mae: 0.033491, mean_q: 0.045550\n",
      " 36497/50000: episode: 1145, duration: 0.470s, episode steps:  39, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000018, mae: 0.033220, mean_q: 0.045466\n",
      " 36508/50000: episode: 1146, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.000025, mae: 0.033322, mean_q: 0.045699\n",
      " 36544/50000: episode: 1147, duration: 0.435s, episode steps:  36, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: 0.000022, mae: 0.033273, mean_q: 0.045154\n",
      " 36556/50000: episode: 1148, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000018, mae: 0.033464, mean_q: 0.046198\n",
      " 36592/50000: episode: 1149, duration: 0.436s, episode steps:  36, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000024, mae: 0.033174, mean_q: 0.044976\n",
      " 36605/50000: episode: 1150, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000023, mae: 0.033449, mean_q: 0.045518\n",
      " 36623/50000: episode: 1151, duration: 0.228s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000023, mae: 0.033003, mean_q: 0.045329\n",
      " 36644/50000: episode: 1152, duration: 0.263s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000029, mae: 0.033217, mean_q: 0.044873\n",
      " 36701/50000: episode: 1153, duration: 0.685s, episode steps:  57, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.491 [0.000, 3.000],  loss: 0.000027, mae: 0.032477, mean_q: 0.044228\n",
      " 36724/50000: episode: 1154, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.783 [0.000, 3.000],  loss: 0.000024, mae: 0.032768, mean_q: 0.044502\n",
      " 36732/50000: episode: 1155, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000029, mae: 0.032515, mean_q: 0.044396\n",
      " 36746/50000: episode: 1156, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000025, mae: 0.032513, mean_q: 0.043928\n",
      " 36777/50000: episode: 1157, duration: 0.379s, episode steps:  31, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.258 [0.000, 3.000],  loss: 0.000023, mae: 0.032446, mean_q: 0.044295\n",
      " 36790/50000: episode: 1158, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000019, mae: 0.032132, mean_q: 0.042989\n",
      " 36810/50000: episode: 1159, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000020, mae: 0.032240, mean_q: 0.043432\n",
      " 36836/50000: episode: 1160, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.808 [0.000, 3.000],  loss: 0.000026, mae: 0.031775, mean_q: 0.043052\n",
      " 36841/50000: episode: 1161, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [1.000, 2.000],  loss: 0.000021, mae: 0.031987, mean_q: 0.043000\n",
      " 36863/50000: episode: 1162, duration: 0.274s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000026, mae: 0.031797, mean_q: 0.043015\n",
      " 36896/50000: episode: 1163, duration: 0.404s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.697 [0.000, 3.000],  loss: 0.000494, mae: 0.032872, mean_q: 0.046484\n",
      " 36911/50000: episode: 1164, duration: 0.186s, episode steps:  15, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.267 [0.000, 3.000],  loss: 0.000031, mae: 0.031228, mean_q: 0.043397\n",
      " 36948/50000: episode: 1165, duration: 0.450s, episode steps:  37, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 0.000032, mae: 0.031128, mean_q: 0.042791\n",
      " 36959/50000: episode: 1166, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000032, mae: 0.030605, mean_q: 0.042034\n",
      " 36975/50000: episode: 1167, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [0.000, 3.000],  loss: 0.000032, mae: 0.030766, mean_q: 0.042362\n",
      " 36988/50000: episode: 1168, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.923 [0.000, 3.000],  loss: 0.000031, mae: 0.031009, mean_q: 0.042193\n",
      " 37034/50000: episode: 1169, duration: 0.557s, episode steps:  46, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 0.000030, mae: 0.031436, mean_q: 0.042843\n",
      " 37063/50000: episode: 1170, duration: 0.357s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.172 [0.000, 3.000],  loss: 0.000028, mae: 0.030862, mean_q: 0.042590\n",
      " 37123/50000: episode: 1171, duration: 0.718s, episode steps:  60, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000026, mae: 0.030896, mean_q: 0.042196\n",
      " 37148/50000: episode: 1172, duration: 0.307s, episode steps:  25, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000021, mae: 0.030861, mean_q: 0.041672\n",
      " 37181/50000: episode: 1173, duration: 0.404s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000023, mae: 0.030554, mean_q: 0.041412\n",
      " 37197/50000: episode: 1174, duration: 0.201s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000021, mae: 0.030252, mean_q: 0.041453\n",
      " 37208/50000: episode: 1175, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000028, mae: 0.030619, mean_q: 0.041305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37216/50000: episode: 1176, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000027, mae: 0.029727, mean_q: 0.040429\n",
      " 37229/50000: episode: 1177, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.077 [0.000, 3.000],  loss: 0.000019, mae: 0.031167, mean_q: 0.042356\n",
      " 37250/50000: episode: 1178, duration: 0.264s, episode steps:  21, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000026, mae: 0.030616, mean_q: 0.041998\n",
      " 37257/50000: episode: 1179, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000018, mae: 0.029862, mean_q: 0.040450\n",
      " 37297/50000: episode: 1180, duration: 0.485s, episode steps:  40, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.700 [0.000, 3.000],  loss: 0.000024, mae: 0.030109, mean_q: 0.041364\n",
      " 37335/50000: episode: 1181, duration: 0.458s, episode steps:  38, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000019, mae: 0.030163, mean_q: 0.041123\n",
      " 37348/50000: episode: 1182, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000018, mae: 0.030056, mean_q: 0.041058\n",
      " 37380/50000: episode: 1183, duration: 0.394s, episode steps:  32, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000015, mae: 0.030657, mean_q: 0.041791\n",
      " 37400/50000: episode: 1184, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000009, mae: 0.030267, mean_q: 0.040999\n",
      " 37439/50000: episode: 1185, duration: 0.476s, episode steps:  39, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000019, mae: 0.030115, mean_q: 0.040666\n",
      " 37481/50000: episode: 1186, duration: 0.513s, episode steps:  42, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.000021, mae: 0.029918, mean_q: 0.040687\n",
      " 37508/50000: episode: 1187, duration: 0.332s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.296 [0.000, 3.000],  loss: 0.000023, mae: 0.029804, mean_q: 0.040191\n",
      " 37518/50000: episode: 1188, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.000022, mae: 0.029419, mean_q: 0.040125\n",
      " 37555/50000: episode: 1189, duration: 0.453s, episode steps:  37, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000017, mae: 0.029840, mean_q: 0.040692\n",
      " 37560/50000: episode: 1190, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000017, mae: 0.029931, mean_q: 0.040409\n",
      " 37650/50000: episode: 1191, duration: 1.070s, episode steps:  90, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000020, mae: 0.029593, mean_q: 0.040354\n",
      " 37700/50000: episode: 1192, duration: 0.604s, episode steps:  50, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.000020, mae: 0.029340, mean_q: 0.039896\n",
      " 37748/50000: episode: 1193, duration: 0.577s, episode steps:  48, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000017, mae: 0.029345, mean_q: 0.039951\n",
      " 37791/50000: episode: 1194, duration: 0.522s, episode steps:  43, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.372 [0.000, 3.000],  loss: 0.000018, mae: 0.029399, mean_q: 0.040162\n",
      " 37803/50000: episode: 1195, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000009, mae: 0.029433, mean_q: 0.039759\n",
      " 37830/50000: episode: 1196, duration: 0.336s, episode steps:  27, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000573, mae: 0.029676, mean_q: 0.041352\n",
      " 37846/50000: episode: 1197, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000040, mae: 0.030299, mean_q: 0.043043\n",
      " 37854/50000: episode: 1198, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.250 [1.000, 3.000],  loss: 0.000025, mae: 0.029690, mean_q: 0.040113\n",
      " 37872/50000: episode: 1199, duration: 0.266s, episode steps:  18, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000022, mae: 0.029450, mean_q: 0.040202\n",
      " 37934/50000: episode: 1200, duration: 0.817s, episode steps:  62, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000018, mae: 0.029277, mean_q: 0.039812\n",
      " 37948/50000: episode: 1201, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.000011, mae: 0.028992, mean_q: 0.039289\n",
      " 37956/50000: episode: 1202, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.125 [0.000, 3.000],  loss: 0.000025, mae: 0.029389, mean_q: 0.039973\n",
      " 38007/50000: episode: 1203, duration: 0.633s, episode steps:  51, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.373 [0.000, 3.000],  loss: 0.000016, mae: 0.029033, mean_q: 0.039408\n",
      " 38012/50000: episode: 1204, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000011, mae: 0.028574, mean_q: 0.038681\n",
      " 38110/50000: episode: 1205, duration: 1.193s, episode steps:  98, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000020, mae: 0.028724, mean_q: 0.038970\n",
      " 38138/50000: episode: 1206, duration: 0.357s, episode steps:  28, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.000018, mae: 0.028521, mean_q: 0.038834\n",
      " 38151/50000: episode: 1207, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [1.000, 3.000],  loss: 0.000019, mae: 0.028383, mean_q: 0.038702\n",
      " 38188/50000: episode: 1208, duration: 0.452s, episode steps:  37, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.595 [0.000, 3.000],  loss: 0.000017, mae: 0.028610, mean_q: 0.038566\n",
      " 38264/50000: episode: 1209, duration: 0.909s, episode steps:  76, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.539 [0.000, 3.000],  loss: 0.000016, mae: 0.028471, mean_q: 0.038731\n",
      " 38296/50000: episode: 1210, duration: 0.389s, episode steps:  32, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000019, mae: 0.028198, mean_q: 0.038535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38313/50000: episode: 1211, duration: 0.213s, episode steps:  17, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000021, mae: 0.028351, mean_q: 0.038426\n",
      " 38323/50000: episode: 1212, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.000020, mae: 0.027706, mean_q: 0.037493\n",
      " 38328/50000: episode: 1213, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.000013, mae: 0.029051, mean_q: 0.039249\n",
      " 38360/50000: episode: 1214, duration: 0.399s, episode steps:  32, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000015, mae: 0.028107, mean_q: 0.037961\n",
      " 38374/50000: episode: 1215, duration: 0.176s, episode steps:  14, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000021, mae: 0.027990, mean_q: 0.037963\n",
      " 38432/50000: episode: 1216, duration: 0.709s, episode steps:  58, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000020, mae: 0.027915, mean_q: 0.037969\n",
      " 38448/50000: episode: 1217, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000021, mae: 0.027931, mean_q: 0.037925\n",
      " 38482/50000: episode: 1218, duration: 0.420s, episode steps:  34, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.324 [0.000, 3.000],  loss: 0.000015, mae: 0.027757, mean_q: 0.037706\n",
      " 38508/50000: episode: 1219, duration: 0.320s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000026, mae: 0.027604, mean_q: 0.037696\n",
      " 38542/50000: episode: 1220, duration: 0.412s, episode steps:  34, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000014, mae: 0.027963, mean_q: 0.037859\n",
      " 38574/50000: episode: 1221, duration: 0.391s, episode steps:  32, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000015, mae: 0.027614, mean_q: 0.037363\n",
      " 38601/50000: episode: 1222, duration: 0.333s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000014, mae: 0.027607, mean_q: 0.037633\n",
      " 38623/50000: episode: 1223, duration: 0.271s, episode steps:  22, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000719, mae: 0.029410, mean_q: 0.042903\n",
      " 38697/50000: episode: 1224, duration: 0.878s, episode steps:  74, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 0.000029, mae: 0.026708, mean_q: 0.037381\n",
      " 38720/50000: episode: 1225, duration: 0.285s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.000020, mae: 0.026413, mean_q: 0.036041\n",
      " 38784/50000: episode: 1226, duration: 0.771s, episode steps:  64, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.000019, mae: 0.026100, mean_q: 0.035638\n",
      " 38810/50000: episode: 1227, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.000603, mae: 0.027691, mean_q: 0.040107\n",
      " 38857/50000: episode: 1228, duration: 0.567s, episode steps:  47, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000019, mae: 0.026932, mean_q: 0.037055\n",
      " 38903/50000: episode: 1229, duration: 0.554s, episode steps:  46, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.739 [0.000, 3.000],  loss: 0.000021, mae: 0.026624, mean_q: 0.036155\n",
      " 38936/50000: episode: 1230, duration: 0.402s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.000020, mae: 0.026266, mean_q: 0.035869\n",
      " 38978/50000: episode: 1231, duration: 0.516s, episode steps:  42, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000020, mae: 0.026480, mean_q: 0.036321\n",
      " 38994/50000: episode: 1232, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.000017, mae: 0.026021, mean_q: 0.035078\n",
      " 39007/50000: episode: 1233, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.000016, mae: 0.025934, mean_q: 0.035389\n",
      " 39070/50000: episode: 1234, duration: 0.755s, episode steps:  63, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.000485, mae: 0.027871, mean_q: 0.039422\n",
      " 39084/50000: episode: 1235, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.000023, mae: 0.026519, mean_q: 0.035944\n",
      " 39137/50000: episode: 1236, duration: 0.661s, episode steps:  53, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.264 [0.000, 3.000],  loss: 0.000020, mae: 0.026484, mean_q: 0.036189\n",
      " 39159/50000: episode: 1237, duration: 0.273s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000022, mae: 0.026194, mean_q: 0.035753\n",
      " 39208/50000: episode: 1238, duration: 0.593s, episode steps:  49, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.367 [0.000, 3.000],  loss: 0.000021, mae: 0.026530, mean_q: 0.036074\n",
      " 39214/50000: episode: 1239, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000026, mae: 0.025949, mean_q: 0.035102\n",
      " 39222/50000: episode: 1240, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000023, mae: 0.026399, mean_q: 0.035991\n",
      " 39258/50000: episode: 1241, duration: 0.442s, episode steps:  36, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.361 [0.000, 3.000],  loss: 0.000419, mae: 0.027717, mean_q: 0.039925\n",
      " 39272/50000: episode: 1242, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.000019, mae: 0.026829, mean_q: 0.036970\n",
      " 39295/50000: episode: 1243, duration: 0.285s, episode steps:  23, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000018, mae: 0.026318, mean_q: 0.035856\n",
      " 39407/50000: episode: 1244, duration: 1.326s, episode steps: 112, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000022, mae: 0.026435, mean_q: 0.036312\n",
      " 39425/50000: episode: 1245, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: 0.000026, mae: 0.026515, mean_q: 0.036343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39475/50000: episode: 1246, duration: 0.603s, episode steps:  50, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.000024, mae: 0.026200, mean_q: 0.035845\n",
      " 39486/50000: episode: 1247, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [0.000, 3.000],  loss: 0.000018, mae: 0.026384, mean_q: 0.035443\n",
      " 39501/50000: episode: 1248, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000017, mae: 0.026030, mean_q: 0.035510\n",
      " 39555/50000: episode: 1249, duration: 0.651s, episode steps:  54, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000017, mae: 0.026188, mean_q: 0.035533\n",
      " 39563/50000: episode: 1250, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000021, mae: 0.026476, mean_q: 0.035918\n",
      " 39600/50000: episode: 1251, duration: 0.451s, episode steps:  37, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.703 [0.000, 3.000],  loss: 0.000028, mae: 0.026022, mean_q: 0.035264\n",
      " 39657/50000: episode: 1252, duration: 0.689s, episode steps:  57, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.000016, mae: 0.025979, mean_q: 0.035210\n",
      " 39683/50000: episode: 1253, duration: 0.324s, episode steps:  26, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.000019, mae: 0.025998, mean_q: 0.035220\n",
      " 39700/50000: episode: 1254, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.824 [0.000, 3.000],  loss: 0.000016, mae: 0.026074, mean_q: 0.036308\n",
      " 39745/50000: episode: 1255, duration: 0.547s, episode steps:  45, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000017, mae: 0.025932, mean_q: 0.034976\n",
      " 39773/50000: episode: 1256, duration: 0.342s, episode steps:  28, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.000015, mae: 0.026093, mean_q: 0.035255\n",
      " 39785/50000: episode: 1257, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000014, mae: 0.026083, mean_q: 0.035590\n",
      " 39829/50000: episode: 1258, duration: 0.537s, episode steps:  44, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000015, mae: 0.025949, mean_q: 0.035329\n",
      " 39865/50000: episode: 1259, duration: 0.441s, episode steps:  36, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.000016, mae: 0.025359, mean_q: 0.034191\n",
      " 39892/50000: episode: 1260, duration: 0.336s, episode steps:  27, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000018, mae: 0.025804, mean_q: 0.034758\n",
      " 39907/50000: episode: 1261, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.133 [0.000, 3.000],  loss: 0.000019, mae: 0.025380, mean_q: 0.034352\n",
      " 39929/50000: episode: 1262, duration: 0.276s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000017, mae: 0.025251, mean_q: 0.034112\n",
      " 39970/50000: episode: 1263, duration: 0.498s, episode steps:  41, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.780 [0.000, 3.000],  loss: 0.000015, mae: 0.025549, mean_q: 0.034664\n",
      " 39996/50000: episode: 1264, duration: 0.322s, episode steps:  26, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.192 [0.000, 3.000],  loss: 0.000021, mae: 0.025129, mean_q: 0.033766\n",
      " 40056/50000: episode: 1265, duration: 0.720s, episode steps:  60, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000013, mae: 0.025318, mean_q: 0.034551\n",
      " 40089/50000: episode: 1266, duration: 0.403s, episode steps:  33, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.000016, mae: 0.025165, mean_q: 0.034062\n",
      " 40110/50000: episode: 1267, duration: 0.265s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000017, mae: 0.025016, mean_q: 0.033986\n",
      " 40122/50000: episode: 1268, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.000022, mae: 0.024374, mean_q: 0.033121\n",
      " 40149/50000: episode: 1269, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000014, mae: 0.024950, mean_q: 0.033832\n",
      " 40161/50000: episode: 1270, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000021, mae: 0.024801, mean_q: 0.033333\n",
      " 40168/50000: episode: 1271, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.857 [0.000, 3.000],  loss: 0.000019, mae: 0.024564, mean_q: 0.032728\n",
      " 40188/50000: episode: 1272, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000015, mae: 0.024837, mean_q: 0.033768\n",
      " 40207/50000: episode: 1273, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.000018, mae: 0.024562, mean_q: 0.033267\n",
      " 40224/50000: episode: 1274, duration: 0.215s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.000012, mae: 0.025096, mean_q: 0.034247\n",
      " 40260/50000: episode: 1275, duration: 0.442s, episode steps:  36, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.000017, mae: 0.024492, mean_q: 0.033324\n",
      " 40299/50000: episode: 1276, duration: 0.478s, episode steps:  39, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000015, mae: 0.024582, mean_q: 0.033308\n",
      " 40382/50000: episode: 1277, duration: 0.991s, episode steps:  83, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000014, mae: 0.024484, mean_q: 0.033107\n",
      " 40416/50000: episode: 1278, duration: 0.415s, episode steps:  34, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000453, mae: 0.025383, mean_q: 0.036126\n",
      " 40439/50000: episode: 1279, duration: 0.295s, episode steps:  23, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.652 [0.000, 3.000],  loss: 0.000021, mae: 0.024355, mean_q: 0.033854\n",
      " 40451/50000: episode: 1280, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.750 [0.000, 3.000],  loss: 0.000017, mae: 0.024296, mean_q: 0.032538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40465/50000: episode: 1281, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000015, mae: 0.024440, mean_q: 0.033410\n",
      " 40477/50000: episode: 1282, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000017, mae: 0.024277, mean_q: 0.032702\n",
      " 40541/50000: episode: 1283, duration: 0.772s, episode steps:  64, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000013, mae: 0.024067, mean_q: 0.032842\n",
      " 40668/50000: episode: 1284, duration: 1.504s, episode steps: 127, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000013, mae: 0.024023, mean_q: 0.032651\n",
      " 40676/50000: episode: 1285, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.875 [1.000, 3.000],  loss: 0.000014, mae: 0.023402, mean_q: 0.031543\n",
      " 40755/50000: episode: 1286, duration: 0.942s, episode steps:  79, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.443 [0.000, 3.000],  loss: 0.000014, mae: 0.023393, mean_q: 0.031574\n",
      " 40801/50000: episode: 1287, duration: 0.561s, episode steps:  46, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000010, mae: 0.023590, mean_q: 0.032060\n",
      " 40864/50000: episode: 1288, duration: 0.755s, episode steps:  63, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000011, mae: 0.023360, mean_q: 0.031665\n",
      " 40926/50000: episode: 1289, duration: 0.749s, episode steps:  62, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.000013, mae: 0.023284, mean_q: 0.031739\n",
      " 40939/50000: episode: 1290, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.000011, mae: 0.023260, mean_q: 0.031755\n",
      " 40982/50000: episode: 1291, duration: 0.518s, episode steps:  43, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000011, mae: 0.023251, mean_q: 0.031931\n",
      " 41001/50000: episode: 1292, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.842 [0.000, 3.000],  loss: 0.000014, mae: 0.023165, mean_q: 0.031302\n",
      " 41081/50000: episode: 1293, duration: 0.960s, episode steps:  80, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.762 [0.000, 3.000],  loss: 0.000013, mae: 0.022943, mean_q: 0.031059\n",
      " 41146/50000: episode: 1294, duration: 0.780s, episode steps:  65, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.323 [0.000, 3.000],  loss: 0.000012, mae: 0.022795, mean_q: 0.030928\n",
      " 41159/50000: episode: 1295, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.000017, mae: 0.022629, mean_q: 0.030900\n",
      " 41171/50000: episode: 1296, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000012, mae: 0.022989, mean_q: 0.031099\n",
      " 41187/50000: episode: 1297, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000013, mae: 0.022530, mean_q: 0.030673\n",
      " 41200/50000: episode: 1298, duration: 0.172s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.000014, mae: 0.022524, mean_q: 0.030597\n",
      " 41279/50000: episode: 1299, duration: 0.951s, episode steps:  79, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.000196, mae: 0.022786, mean_q: 0.031089\n",
      " 41332/50000: episode: 1300, duration: 0.641s, episode steps:  53, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.660 [0.000, 3.000],  loss: 0.000306, mae: 0.023408, mean_q: 0.032927\n",
      " 41339/50000: episode: 1301, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [1.000, 3.000],  loss: 0.000047, mae: 0.024368, mean_q: 0.037809\n",
      " 41407/50000: episode: 1302, duration: 0.814s, episode steps:  68, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.676 [0.000, 3.000],  loss: 0.000018, mae: 0.022857, mean_q: 0.031574\n",
      " 41423/50000: episode: 1303, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.000017, mae: 0.022745, mean_q: 0.030678\n",
      " 41460/50000: episode: 1304, duration: 0.458s, episode steps:  37, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.216 [0.000, 3.000],  loss: 0.000014, mae: 0.022935, mean_q: 0.031158\n",
      " 41484/50000: episode: 1305, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000013, mae: 0.022990, mean_q: 0.031285\n",
      " 41525/50000: episode: 1306, duration: 0.498s, episode steps:  41, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000014, mae: 0.022909, mean_q: 0.031208\n",
      " 41609/50000: episode: 1307, duration: 1.008s, episode steps:  84, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.000190, mae: 0.023230, mean_q: 0.032021\n",
      " 41631/50000: episode: 1308, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [0.000, 3.000],  loss: 0.000012, mae: 0.022951, mean_q: 0.031607\n",
      " 41668/50000: episode: 1309, duration: 0.457s, episode steps:  37, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.649 [0.000, 3.000],  loss: 0.000013, mae: 0.022928, mean_q: 0.031360\n",
      " 41690/50000: episode: 1310, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000015, mae: 0.022772, mean_q: 0.030969\n",
      " 41755/50000: episode: 1311, duration: 0.781s, episode steps:  65, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.569 [0.000, 3.000],  loss: 0.000240, mae: 0.023652, mean_q: 0.033436\n",
      " 41772/50000: episode: 1312, duration: 0.215s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.941 [0.000, 3.000],  loss: 0.000014, mae: 0.023210, mean_q: 0.031613\n",
      " 41784/50000: episode: 1313, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.000014, mae: 0.023354, mean_q: 0.032018\n",
      " 41792/50000: episode: 1314, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [1.000, 3.000],  loss: 0.001779, mae: 0.024532, mean_q: 0.034070\n",
      " 41802/50000: episode: 1315, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.600 [0.000, 3.000],  loss: 0.000034, mae: 0.025681, mean_q: 0.040362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41841/50000: episode: 1316, duration: 0.478s, episode steps:  39, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.000030, mae: 0.023591, mean_q: 0.033622\n",
      " 41882/50000: episode: 1317, duration: 0.501s, episode steps:  41, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000021, mae: 0.022991, mean_q: 0.031012\n",
      " 41913/50000: episode: 1318, duration: 0.386s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000019, mae: 0.022737, mean_q: 0.031065\n",
      " 41941/50000: episode: 1319, duration: 0.351s, episode steps:  28, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000016, mae: 0.022602, mean_q: 0.030589\n",
      " 41964/50000: episode: 1320, duration: 0.290s, episode steps:  23, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000633, mae: 0.023427, mean_q: 0.032283\n",
      " 41986/50000: episode: 1321, duration: 0.276s, episode steps:  22, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000027, mae: 0.024259, mean_q: 0.035961\n",
      " 42004/50000: episode: 1322, duration: 0.229s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000020, mae: 0.023054, mean_q: 0.031587\n",
      " 42035/50000: episode: 1323, duration: 0.386s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.742 [0.000, 3.000],  loss: 0.000023, mae: 0.023034, mean_q: 0.031209\n",
      " 42062/50000: episode: 1324, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.000017, mae: 0.022982, mean_q: 0.031224\n",
      " 42081/50000: episode: 1325, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.842 [0.000, 3.000],  loss: 0.000018, mae: 0.022682, mean_q: 0.030867\n",
      " 42106/50000: episode: 1326, duration: 0.317s, episode steps:  25, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.000012, mae: 0.022929, mean_q: 0.031033\n",
      " 42145/50000: episode: 1327, duration: 0.481s, episode steps:  39, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000013, mae: 0.022576, mean_q: 0.030716\n",
      " 42194/50000: episode: 1328, duration: 0.598s, episode steps:  49, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.265 [0.000, 3.000],  loss: 0.000014, mae: 0.022649, mean_q: 0.030736\n",
      " 42252/50000: episode: 1329, duration: 0.703s, episode steps:  58, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.000012, mae: 0.022393, mean_q: 0.030431\n",
      " 42272/50000: episode: 1330, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000017, mae: 0.022478, mean_q: 0.030410\n",
      " 42286/50000: episode: 1331, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.000011, mae: 0.022264, mean_q: 0.029874\n",
      " 42299/50000: episode: 1332, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000011, mae: 0.022213, mean_q: 0.030102\n",
      " 42351/50000: episode: 1333, duration: 0.631s, episode steps:  52, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000292, mae: 0.023194, mean_q: 0.032801\n",
      " 42390/50000: episode: 1334, duration: 0.478s, episode steps:  39, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000015, mae: 0.022308, mean_q: 0.030379\n",
      " 42405/50000: episode: 1335, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000009, mae: 0.022326, mean_q: 0.030510\n",
      " 42426/50000: episode: 1336, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000013, mae: 0.022401, mean_q: 0.030447\n",
      " 42458/50000: episode: 1337, duration: 0.393s, episode steps:  32, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.000012, mae: 0.022197, mean_q: 0.030049\n",
      " 42474/50000: episode: 1338, duration: 0.201s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000019, mae: 0.021668, mean_q: 0.029699\n",
      " 42485/50000: episode: 1339, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000012, mae: 0.022095, mean_q: 0.029672\n",
      " 42516/50000: episode: 1340, duration: 0.384s, episode steps:  31, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.194 [0.000, 3.000],  loss: 0.000012, mae: 0.021721, mean_q: 0.029502\n",
      " 42526/50000: episode: 1341, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.000011, mae: 0.022082, mean_q: 0.029686\n",
      " 42567/50000: episode: 1342, duration: 0.503s, episode steps:  41, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.732 [0.000, 3.000],  loss: 0.000013, mae: 0.021736, mean_q: 0.029365\n",
      " 42608/50000: episode: 1343, duration: 0.503s, episode steps:  41, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.000011, mae: 0.021508, mean_q: 0.029070\n",
      " 42637/50000: episode: 1344, duration: 0.364s, episode steps:  29, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.000014, mae: 0.021768, mean_q: 0.029415\n",
      " 42667/50000: episode: 1345, duration: 0.373s, episode steps:  30, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.000016, mae: 0.021480, mean_q: 0.029218\n",
      " 42685/50000: episode: 1346, duration: 0.226s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000008, mae: 0.021325, mean_q: 0.028827\n",
      " 42716/50000: episode: 1347, duration: 0.385s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000015, mae: 0.021223, mean_q: 0.028701\n",
      " 42802/50000: episode: 1348, duration: 1.035s, episode steps:  86, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.000012, mae: 0.020987, mean_q: 0.028501\n",
      " 42836/50000: episode: 1349, duration: 0.423s, episode steps:  34, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.000011, mae: 0.021175, mean_q: 0.029036\n",
      " 42872/50000: episode: 1350, duration: 0.444s, episode steps:  36, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000010, mae: 0.020778, mean_q: 0.028222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42896/50000: episode: 1351, duration: 0.302s, episode steps:  24, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000011, mae: 0.020806, mean_q: 0.028385\n",
      " 42952/50000: episode: 1352, duration: 0.682s, episode steps:  56, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.000012, mae: 0.020820, mean_q: 0.028240\n",
      " 42968/50000: episode: 1353, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000010, mae: 0.020495, mean_q: 0.028005\n",
      " 42978/50000: episode: 1354, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 2.000],  loss: 0.000010, mae: 0.020438, mean_q: 0.027703\n",
      " 43023/50000: episode: 1355, duration: 0.567s, episode steps:  45, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000010, mae: 0.020772, mean_q: 0.028218\n",
      " 43061/50000: episode: 1356, duration: 0.469s, episode steps:  38, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.000010, mae: 0.020583, mean_q: 0.027914\n",
      " 43105/50000: episode: 1357, duration: 0.535s, episode steps:  44, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.000010, mae: 0.020441, mean_q: 0.027620\n",
      " 43122/50000: episode: 1358, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000013, mae: 0.020424, mean_q: 0.027899\n",
      " 43170/50000: episode: 1359, duration: 0.587s, episode steps:  48, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000010, mae: 0.020206, mean_q: 0.027426\n",
      " 43216/50000: episode: 1360, duration: 0.558s, episode steps:  46, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 0.000012, mae: 0.020237, mean_q: 0.027563\n",
      " 43246/50000: episode: 1361, duration: 0.373s, episode steps:  30, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000009, mae: 0.020429, mean_q: 0.027994\n",
      " 43260/50000: episode: 1362, duration: 0.177s, episode steps:  14, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.000011, mae: 0.020405, mean_q: 0.027654\n",
      " 43314/50000: episode: 1363, duration: 0.658s, episode steps:  54, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.352 [0.000, 3.000],  loss: 0.000011, mae: 0.020315, mean_q: 0.027758\n",
      " 43351/50000: episode: 1364, duration: 0.455s, episode steps:  37, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000010, mae: 0.020207, mean_q: 0.027767\n",
      " 43441/50000: episode: 1365, duration: 1.080s, episode steps:  90, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.678 [0.000, 3.000],  loss: 0.000010, mae: 0.020372, mean_q: 0.027739\n",
      " 43450/50000: episode: 1366, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.000010, mae: 0.020036, mean_q: 0.027529\n",
      " 43488/50000: episode: 1367, duration: 0.467s, episode steps:  38, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.184 [0.000, 3.000],  loss: 0.000013, mae: 0.020128, mean_q: 0.027175\n",
      " 43520/50000: episode: 1368, duration: 0.396s, episode steps:  32, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000011, mae: 0.019981, mean_q: 0.027246\n",
      " 43571/50000: episode: 1369, duration: 0.617s, episode steps:  51, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000011, mae: 0.020015, mean_q: 0.027157\n",
      " 43584/50000: episode: 1370, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000012, mae: 0.020134, mean_q: 0.027390\n",
      " 43597/50000: episode: 1371, duration: 0.170s, episode steps:  13, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000012, mae: 0.019777, mean_q: 0.027236\n",
      " 43611/50000: episode: 1372, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000008, mae: 0.019878, mean_q: 0.027065\n",
      " 43658/50000: episode: 1373, duration: 0.574s, episode steps:  47, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000009, mae: 0.019956, mean_q: 0.027096\n",
      " 43677/50000: episode: 1374, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.211 [0.000, 3.000],  loss: 0.000009, mae: 0.020054, mean_q: 0.027637\n",
      " 43684/50000: episode: 1375, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.000004, mae: 0.019744, mean_q: 0.026729\n",
      " 43713/50000: episode: 1376, duration: 0.359s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000010, mae: 0.019933, mean_q: 0.026877\n",
      " 43723/50000: episode: 1377, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000011, mae: 0.019559, mean_q: 0.026911\n",
      " 43764/50000: episode: 1378, duration: 0.503s, episode steps:  41, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.951 [0.000, 3.000],  loss: 0.000378, mae: 0.020798, mean_q: 0.029897\n",
      " 43802/50000: episode: 1379, duration: 0.467s, episode steps:  38, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000010, mae: 0.019865, mean_q: 0.027283\n",
      " 43849/50000: episode: 1380, duration: 0.576s, episode steps:  47, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: 0.000010, mae: 0.019661, mean_q: 0.027008\n",
      " 43886/50000: episode: 1381, duration: 0.455s, episode steps:  37, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000011, mae: 0.019660, mean_q: 0.026779\n",
      " 43895/50000: episode: 1382, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000013, mae: 0.019747, mean_q: 0.027405\n",
      " 43913/50000: episode: 1383, duration: 0.231s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000011, mae: 0.019868, mean_q: 0.027196\n",
      " 43944/50000: episode: 1384, duration: 0.383s, episode steps:  31, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000011, mae: 0.019568, mean_q: 0.026571\n",
      " 43978/50000: episode: 1385, duration: 0.419s, episode steps:  34, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.088 [0.000, 3.000],  loss: 0.000010, mae: 0.019606, mean_q: 0.026703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44147/50000: episode: 1386, duration: 2.025s, episode steps: 169, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000009, mae: 0.019595, mean_q: 0.026553\n",
      " 44160/50000: episode: 1387, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000009, mae: 0.019284, mean_q: 0.026186\n",
      " 44246/50000: episode: 1388, duration: 1.033s, episode steps:  86, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000010, mae: 0.019258, mean_q: 0.026113\n",
      " 44360/50000: episode: 1389, duration: 1.386s, episode steps: 114, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.000008, mae: 0.019169, mean_q: 0.025992\n",
      " 44379/50000: episode: 1390, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.000010, mae: 0.019013, mean_q: 0.025907\n",
      " 44410/50000: episode: 1391, duration: 0.384s, episode steps:  31, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.806 [0.000, 3.000],  loss: 0.000009, mae: 0.018920, mean_q: 0.025693\n",
      " 44436/50000: episode: 1392, duration: 0.325s, episode steps:  26, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.000011, mae: 0.018959, mean_q: 0.025994\n",
      " 44441/50000: episode: 1393, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000015, mae: 0.018494, mean_q: 0.024811\n",
      " 44458/50000: episode: 1394, duration: 0.219s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.647 [0.000, 3.000],  loss: 0.000013, mae: 0.018595, mean_q: 0.025008\n",
      " 44486/50000: episode: 1395, duration: 0.350s, episode steps:  28, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000547, mae: 0.020290, mean_q: 0.030025\n",
      " 44497/50000: episode: 1396, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000014, mae: 0.019311, mean_q: 0.027018\n",
      " 44540/50000: episode: 1397, duration: 0.527s, episode steps:  43, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000353, mae: 0.020094, mean_q: 0.028902\n",
      " 44587/50000: episode: 1398, duration: 0.571s, episode steps:  47, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.298 [0.000, 3.000],  loss: 0.000012, mae: 0.019294, mean_q: 0.026695\n",
      " 44631/50000: episode: 1399, duration: 0.538s, episode steps:  44, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000012, mae: 0.019217, mean_q: 0.026016\n",
      " 44648/50000: episode: 1400, duration: 0.214s, episode steps:  17, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000012, mae: 0.019287, mean_q: 0.026022\n",
      " 44671/50000: episode: 1401, duration: 0.364s, episode steps:  23, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000010, mae: 0.019159, mean_q: 0.025982\n",
      " 44722/50000: episode: 1402, duration: 0.621s, episode steps:  51, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.745 [0.000, 3.000],  loss: 0.000010, mae: 0.019089, mean_q: 0.025828\n",
      " 44786/50000: episode: 1403, duration: 0.776s, episode steps:  64, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.000012, mae: 0.018984, mean_q: 0.025737\n",
      " 44858/50000: episode: 1404, duration: 0.878s, episode steps:  72, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000010, mae: 0.018970, mean_q: 0.025736\n",
      " 44866/50000: episode: 1405, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000007, mae: 0.018761, mean_q: 0.025464\n",
      " 44883/50000: episode: 1406, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.059 [0.000, 3.000],  loss: 0.000009, mae: 0.019006, mean_q: 0.025938\n",
      " 44907/50000: episode: 1407, duration: 0.305s, episode steps:  24, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000009, mae: 0.018944, mean_q: 0.025741\n",
      " 44926/50000: episode: 1408, duration: 0.242s, episode steps:  19, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000009, mae: 0.018887, mean_q: 0.025599\n",
      " 44956/50000: episode: 1409, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000008, mae: 0.018751, mean_q: 0.025675\n",
      " 44975/50000: episode: 1410, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.000006, mae: 0.018844, mean_q: 0.025786\n",
      " 45034/50000: episode: 1411, duration: 0.713s, episode steps:  59, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.000010, mae: 0.018722, mean_q: 0.025516\n",
      " 45044/50000: episode: 1412, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.001456, mae: 0.021107, mean_q: 0.031729\n",
      " 45065/50000: episode: 1413, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000038, mae: 0.019686, mean_q: 0.028502\n",
      " 45109/50000: episode: 1414, duration: 0.539s, episode steps:  44, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000012, mae: 0.018836, mean_q: 0.025816\n",
      " 45149/50000: episode: 1415, duration: 0.490s, episode steps:  40, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000667, mae: 0.020210, mean_q: 0.029023\n",
      " 45213/50000: episode: 1416, duration: 0.779s, episode steps:  64, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000048, mae: 0.019742, mean_q: 0.029542\n",
      " 45236/50000: episode: 1417, duration: 0.289s, episode steps:  23, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.957 [0.000, 3.000],  loss: 0.000022, mae: 0.019348, mean_q: 0.026247\n",
      " 45270/50000: episode: 1418, duration: 0.421s, episode steps:  34, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.676 [0.000, 3.000],  loss: 0.000022, mae: 0.019163, mean_q: 0.026017\n",
      " 45297/50000: episode: 1419, duration: 0.339s, episode steps:  27, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.704 [0.000, 3.000],  loss: 0.000017, mae: 0.019122, mean_q: 0.026130\n",
      " 45315/50000: episode: 1420, duration: 0.232s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.000016, mae: 0.019130, mean_q: 0.026075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45376/50000: episode: 1421, duration: 0.740s, episode steps:  61, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.623 [0.000, 3.000],  loss: 0.000016, mae: 0.019400, mean_q: 0.026679\n",
      " 45398/50000: episode: 1422, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000020, mae: 0.019192, mean_q: 0.026229\n",
      " 45417/50000: episode: 1423, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.789 [0.000, 3.000],  loss: 0.000025, mae: 0.019290, mean_q: 0.026269\n",
      " 45423/50000: episode: 1424, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000021, mae: 0.019512, mean_q: 0.026832\n",
      " 45453/50000: episode: 1425, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000017, mae: 0.019328, mean_q: 0.026478\n",
      " 45498/50000: episode: 1426, duration: 0.550s, episode steps:  45, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000016, mae: 0.019088, mean_q: 0.026144\n",
      " 45515/50000: episode: 1427, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.000010, mae: 0.018833, mean_q: 0.025658\n",
      " 45560/50000: episode: 1428, duration: 0.554s, episode steps:  45, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000016, mae: 0.019018, mean_q: 0.025854\n",
      " 45587/50000: episode: 1429, duration: 0.338s, episode steps:  27, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000011, mae: 0.018806, mean_q: 0.025554\n",
      " 45598/50000: episode: 1430, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000016, mae: 0.018957, mean_q: 0.025793\n",
      " 45637/50000: episode: 1431, duration: 0.481s, episode steps:  39, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000007, mae: 0.018945, mean_q: 0.025706\n",
      " 45703/50000: episode: 1432, duration: 0.799s, episode steps:  66, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.182 [0.000, 3.000],  loss: 0.000014, mae: 0.018761, mean_q: 0.025345\n",
      " 45725/50000: episode: 1433, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000009, mae: 0.018725, mean_q: 0.025363\n",
      " 45752/50000: episode: 1434, duration: 0.339s, episode steps:  27, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.000009, mae: 0.018676, mean_q: 0.025531\n",
      " 45771/50000: episode: 1435, duration: 0.246s, episode steps:  19, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.737 [0.000, 3.000],  loss: 0.000007, mae: 0.018424, mean_q: 0.025012\n",
      " 45784/50000: episode: 1436, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000010, mae: 0.018966, mean_q: 0.025586\n",
      " 45793/50000: episode: 1437, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [1.000, 2.000],  loss: 0.000013, mae: 0.018598, mean_q: 0.025457\n",
      " 45939/50000: episode: 1438, duration: 1.752s, episode steps: 146, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.671 [0.000, 3.000],  loss: 0.000011, mae: 0.018652, mean_q: 0.025358\n",
      " 45960/50000: episode: 1439, duration: 0.271s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000011, mae: 0.018180, mean_q: 0.024573\n",
      " 46016/50000: episode: 1440, duration: 0.679s, episode steps:  56, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.000269, mae: 0.019132, mean_q: 0.026875\n",
      " 46035/50000: episode: 1441, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000017, mae: 0.018383, mean_q: 0.026105\n",
      " 46094/50000: episode: 1442, duration: 0.720s, episode steps:  59, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000012, mae: 0.018440, mean_q: 0.025347\n",
      " 46215/50000: episode: 1443, duration: 1.462s, episode steps: 121, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000012, mae: 0.018516, mean_q: 0.025242\n",
      " 46259/50000: episode: 1444, duration: 0.539s, episode steps:  44, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.000336, mae: 0.019950, mean_q: 0.029622\n",
      " 46276/50000: episode: 1445, duration: 0.225s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.059 [0.000, 3.000],  loss: 0.000027, mae: 0.019081, mean_q: 0.026422\n",
      " 46289/50000: episode: 1446, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.000030, mae: 0.018835, mean_q: 0.025891\n",
      " 46361/50000: episode: 1447, duration: 0.874s, episode steps:  72, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000021, mae: 0.018662, mean_q: 0.025628\n",
      " 46426/50000: episode: 1448, duration: 0.796s, episode steps:  65, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.000226, mae: 0.019222, mean_q: 0.027481\n",
      " 46447/50000: episode: 1449, duration: 0.265s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.190 [0.000, 3.000],  loss: 0.000047, mae: 0.018902, mean_q: 0.026462\n",
      " 46471/50000: episode: 1450, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000014, mae: 0.018537, mean_q: 0.025075\n",
      " 46506/50000: episode: 1451, duration: 0.431s, episode steps:  35, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000020, mae: 0.018820, mean_q: 0.025966\n",
      " 46537/50000: episode: 1452, duration: 0.387s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.290 [0.000, 3.000],  loss: 0.000025, mae: 0.018860, mean_q: 0.025830\n",
      " 46561/50000: episode: 1453, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.000026, mae: 0.018796, mean_q: 0.025734\n",
      " 46577/50000: episode: 1454, duration: 0.201s, episode steps:  16, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000015, mae: 0.019194, mean_q: 0.026550\n",
      " 46587/50000: episode: 1455, duration: 0.138s, episode steps:  10, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.100 [0.000, 3.000],  loss: 0.000014, mae: 0.018848, mean_q: 0.025936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46614/50000: episode: 1456, duration: 0.338s, episode steps:  27, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.000020, mae: 0.018772, mean_q: 0.025599\n",
      " 46625/50000: episode: 1457, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.909 [1.000, 3.000],  loss: 0.000028, mae: 0.019083, mean_q: 0.026222\n",
      " 46647/50000: episode: 1458, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000018, mae: 0.018752, mean_q: 0.025791\n",
      " 46662/50000: episode: 1459, duration: 0.191s, episode steps:  15, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000030, mae: 0.019082, mean_q: 0.025742\n",
      " 46696/50000: episode: 1460, duration: 0.423s, episode steps:  34, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000426, mae: 0.019848, mean_q: 0.028524\n",
      " 46718/50000: episode: 1461, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000023, mae: 0.019798, mean_q: 0.027941\n",
      " 46762/50000: episode: 1462, duration: 0.543s, episode steps:  44, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.000026, mae: 0.019248, mean_q: 0.026355\n",
      " 46789/50000: episode: 1463, duration: 0.341s, episode steps:  27, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000015, mae: 0.018985, mean_q: 0.025604\n",
      " 46808/50000: episode: 1464, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.000016, mae: 0.019072, mean_q: 0.025985\n",
      " 46827/50000: episode: 1465, duration: 0.244s, episode steps:  19, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.000014, mae: 0.018909, mean_q: 0.025741\n",
      " 46847/50000: episode: 1466, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.050 [0.000, 3.000],  loss: 0.000019, mae: 0.019022, mean_q: 0.025920\n",
      " 46879/50000: episode: 1467, duration: 0.399s, episode steps:  32, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.000018, mae: 0.019242, mean_q: 0.025884\n",
      " 46949/50000: episode: 1468, duration: 0.851s, episode steps:  70, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000013, mae: 0.018562, mean_q: 0.024928\n",
      " 46958/50000: episode: 1469, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.000008, mae: 0.018307, mean_q: 0.024771\n",
      " 46976/50000: episode: 1470, duration: 0.230s, episode steps:  18, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: 0.000010, mae: 0.018612, mean_q: 0.025273\n",
      " 46995/50000: episode: 1471, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.000014, mae: 0.018285, mean_q: 0.024719\n",
      " 47026/50000: episode: 1472, duration: 0.387s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.710 [0.000, 3.000],  loss: 0.000018, mae: 0.018392, mean_q: 0.024784\n",
      " 47035/50000: episode: 1473, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000007, mae: 0.018357, mean_q: 0.025025\n",
      " 47053/50000: episode: 1474, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000016, mae: 0.018323, mean_q: 0.024813\n",
      " 47070/50000: episode: 1475, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000019, mae: 0.018329, mean_q: 0.024950\n",
      " 47118/50000: episode: 1476, duration: 0.593s, episode steps:  48, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000015, mae: 0.017837, mean_q: 0.024103\n",
      " 47179/50000: episode: 1477, duration: 0.740s, episode steps:  61, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.787 [0.000, 3.000],  loss: 0.000233, mae: 0.018746, mean_q: 0.026272\n",
      " 47224/50000: episode: 1478, duration: 0.554s, episode steps:  45, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.000020, mae: 0.018379, mean_q: 0.025194\n",
      " 47236/50000: episode: 1479, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.917 [0.000, 3.000],  loss: 0.000012, mae: 0.017723, mean_q: 0.024169\n",
      " 47249/50000: episode: 1480, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.000013, mae: 0.018275, mean_q: 0.024630\n",
      " 47262/50000: episode: 1481, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000008, mae: 0.017875, mean_q: 0.024062\n",
      " 47280/50000: episode: 1482, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000007, mae: 0.017886, mean_q: 0.024067\n",
      " 47310/50000: episode: 1483, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000009, mae: 0.017949, mean_q: 0.024396\n",
      " 47323/50000: episode: 1484, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.000011, mae: 0.017920, mean_q: 0.023950\n",
      " 47348/50000: episode: 1485, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.000011, mae: 0.017768, mean_q: 0.023608\n",
      " 47363/50000: episode: 1486, duration: 0.191s, episode steps:  15, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000008, mae: 0.017514, mean_q: 0.023538\n",
      " 47406/50000: episode: 1487, duration: 0.524s, episode steps:  43, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 0.000010, mae: 0.017696, mean_q: 0.023790\n",
      " 47444/50000: episode: 1488, duration: 0.469s, episode steps:  38, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.000007, mae: 0.017567, mean_q: 0.023782\n",
      " 47467/50000: episode: 1489, duration: 0.290s, episode steps:  23, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.174 [0.000, 3.000],  loss: 0.000011, mae: 0.017258, mean_q: 0.023357\n",
      " 47519/50000: episode: 1490, duration: 0.637s, episode steps:  52, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000009, mae: 0.017405, mean_q: 0.023528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47569/50000: episode: 1491, duration: 0.663s, episode steps:  50, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000007, mae: 0.017227, mean_q: 0.023289\n",
      " 47581/50000: episode: 1492, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000007, mae: 0.017083, mean_q: 0.023352\n",
      " 47593/50000: episode: 1493, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.000008, mae: 0.017443, mean_q: 0.023366\n",
      " 47613/50000: episode: 1494, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.050 [0.000, 3.000],  loss: 0.000009, mae: 0.016889, mean_q: 0.022510\n",
      " 47631/50000: episode: 1495, duration: 0.285s, episode steps:  18, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.000008, mae: 0.017152, mean_q: 0.023374\n",
      " 47670/50000: episode: 1496, duration: 0.564s, episode steps:  39, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000006, mae: 0.017027, mean_q: 0.023124\n",
      " 47702/50000: episode: 1497, duration: 0.401s, episode steps:  32, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.969 [0.000, 3.000],  loss: 0.000008, mae: 0.017101, mean_q: 0.023071\n",
      " 47755/50000: episode: 1498, duration: 0.645s, episode steps:  53, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: 0.000007, mae: 0.016912, mean_q: 0.022905\n",
      " 47808/50000: episode: 1499, duration: 0.677s, episode steps:  53, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000007, mae: 0.016941, mean_q: 0.022983\n",
      " 47876/50000: episode: 1500, duration: 0.830s, episode steps:  68, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.603 [0.000, 3.000],  loss: 0.000007, mae: 0.016824, mean_q: 0.022842\n",
      " 47938/50000: episode: 1501, duration: 0.757s, episode steps:  62, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.742 [0.000, 3.000],  loss: 0.000008, mae: 0.016630, mean_q: 0.022688\n",
      " 47976/50000: episode: 1502, duration: 0.476s, episode steps:  38, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000006, mae: 0.016554, mean_q: 0.022429\n",
      " 48001/50000: episode: 1503, duration: 0.322s, episode steps:  25, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.000007, mae: 0.016479, mean_q: 0.022255\n",
      " 48010/50000: episode: 1504, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.000006, mae: 0.016725, mean_q: 0.023068\n",
      " 48019/50000: episode: 1505, duration: 0.122s, episode steps:   9, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.222 [0.000, 3.000],  loss: 0.000006, mae: 0.016612, mean_q: 0.022381\n",
      " 48056/50000: episode: 1506, duration: 0.464s, episode steps:  37, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.784 [0.000, 3.000],  loss: 0.000006, mae: 0.016464, mean_q: 0.022346\n",
      " 48096/50000: episode: 1507, duration: 0.495s, episode steps:  40, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.350 [0.000, 3.000],  loss: 0.000006, mae: 0.016398, mean_q: 0.022325\n",
      " 48130/50000: episode: 1508, duration: 0.424s, episode steps:  34, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000005, mae: 0.016327, mean_q: 0.022169\n",
      " 48202/50000: episode: 1509, duration: 0.882s, episode steps:  72, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000426, mae: 0.017930, mean_q: 0.027449\n",
      " 48238/50000: episode: 1510, duration: 0.451s, episode steps:  36, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000009, mae: 0.016797, mean_q: 0.023396\n",
      " 48253/50000: episode: 1511, duration: 0.196s, episode steps:  15, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000008, mae: 0.016608, mean_q: 0.022877\n",
      " 48314/50000: episode: 1512, duration: 0.745s, episode steps:  61, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 0.000011, mae: 0.016705, mean_q: 0.022920\n",
      " 48335/50000: episode: 1513, duration: 0.272s, episode steps:  21, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000007, mae: 0.016778, mean_q: 0.022882\n",
      " 48351/50000: episode: 1514, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.000010, mae: 0.017060, mean_q: 0.023018\n",
      " 48368/50000: episode: 1515, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000008, mae: 0.016815, mean_q: 0.023398\n",
      " 48394/50000: episode: 1516, duration: 0.332s, episode steps:  26, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000008, mae: 0.016717, mean_q: 0.022624\n",
      " 48446/50000: episode: 1517, duration: 0.647s, episode steps:  52, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.000007, mae: 0.016709, mean_q: 0.022618\n",
      " 48477/50000: episode: 1518, duration: 0.391s, episode steps:  31, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.645 [0.000, 3.000],  loss: 0.000010, mae: 0.016640, mean_q: 0.022679\n",
      " 48494/50000: episode: 1519, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.235 [0.000, 3.000],  loss: 0.000014, mae: 0.016782, mean_q: 0.022839\n",
      " 48510/50000: episode: 1520, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000008, mae: 0.016355, mean_q: 0.022210\n",
      " 48532/50000: episode: 1521, duration: 0.285s, episode steps:  22, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000009, mae: 0.016606, mean_q: 0.022450\n",
      " 48579/50000: episode: 1522, duration: 0.584s, episode steps:  47, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000322, mae: 0.017437, mean_q: 0.024814\n",
      " 48592/50000: episode: 1523, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.000009, mae: 0.016896, mean_q: 0.024121\n",
      " 48626/50000: episode: 1524, duration: 0.422s, episode steps:  34, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.088 [0.000, 3.000],  loss: 0.000009, mae: 0.017012, mean_q: 0.023241\n",
      " 48682/50000: episode: 1525, duration: 0.689s, episode steps:  56, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.000010, mae: 0.016962, mean_q: 0.023125\n",
      " 48694/50000: episode: 1526, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.000015, mae: 0.016918, mean_q: 0.022804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48704/50000: episode: 1527, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000008, mae: 0.016759, mean_q: 0.022548\n",
      " 48753/50000: episode: 1528, duration: 0.613s, episode steps:  49, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000008, mae: 0.016902, mean_q: 0.022954\n",
      " 48763/50000: episode: 1529, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000008, mae: 0.016754, mean_q: 0.022963\n",
      " 48774/50000: episode: 1530, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000008, mae: 0.016917, mean_q: 0.023000\n",
      " 48802/50000: episode: 1531, duration: 0.353s, episode steps:  28, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.000010, mae: 0.016667, mean_q: 0.022633\n",
      " 48830/50000: episode: 1532, duration: 0.351s, episode steps:  28, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.000009, mae: 0.016569, mean_q: 0.022458\n",
      " 48851/50000: episode: 1533, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000005, mae: 0.016689, mean_q: 0.022697\n",
      " 48870/50000: episode: 1534, duration: 0.243s, episode steps:  19, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000008, mae: 0.016583, mean_q: 0.022484\n",
      " 48909/50000: episode: 1535, duration: 0.484s, episode steps:  39, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000007, mae: 0.016543, mean_q: 0.022378\n",
      " 48944/50000: episode: 1536, duration: 0.436s, episode steps:  35, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.171 [0.000, 3.000],  loss: 0.000007, mae: 0.016696, mean_q: 0.022655\n",
      " 48978/50000: episode: 1537, duration: 0.424s, episode steps:  34, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.971 [0.000, 3.000],  loss: 0.000007, mae: 0.016396, mean_q: 0.022261\n",
      " 49010/50000: episode: 1538, duration: 0.400s, episode steps:  32, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.969 [0.000, 3.000],  loss: 0.000008, mae: 0.016331, mean_q: 0.022275\n",
      " 49023/50000: episode: 1539, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.000011, mae: 0.016438, mean_q: 0.022249\n",
      " 49066/50000: episode: 1540, duration: 0.537s, episode steps:  43, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.070 [0.000, 3.000],  loss: 0.000007, mae: 0.016166, mean_q: 0.022127\n",
      " 49079/50000: episode: 1541, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000008, mae: 0.016382, mean_q: 0.022267\n",
      " 49089/50000: episode: 1542, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.000008, mae: 0.016310, mean_q: 0.022136\n",
      " 49130/50000: episode: 1543, duration: 0.509s, episode steps:  41, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000008, mae: 0.016235, mean_q: 0.022067\n",
      " 49159/50000: episode: 1544, duration: 0.363s, episode steps:  29, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.000006, mae: 0.016212, mean_q: 0.022045\n",
      " 49208/50000: episode: 1545, duration: 0.600s, episode steps:  49, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.327 [0.000, 3.000],  loss: 0.000008, mae: 0.016196, mean_q: 0.022008\n",
      " 49250/50000: episode: 1546, duration: 0.517s, episode steps:  42, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.190 [0.000, 3.000],  loss: 0.000349, mae: 0.016822, mean_q: 0.023859\n",
      " 49280/50000: episode: 1547, duration: 0.377s, episode steps:  30, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000028, mae: 0.016985, mean_q: 0.024955\n",
      " 49329/50000: episode: 1548, duration: 0.605s, episode steps:  49, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.000013, mae: 0.016151, mean_q: 0.021905\n",
      " 49346/50000: episode: 1549, duration: 0.226s, episode steps:  17, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000011, mae: 0.015890, mean_q: 0.021521\n",
      " 49353/50000: episode: 1550, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.143 [0.000, 3.000],  loss: 0.000005, mae: 0.016154, mean_q: 0.022436\n",
      " 49394/50000: episode: 1551, duration: 0.508s, episode steps:  41, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.146 [0.000, 3.000],  loss: 0.000011, mae: 0.016018, mean_q: 0.021848\n",
      " 49405/50000: episode: 1552, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.091 [0.000, 3.000],  loss: 0.000010, mae: 0.015935, mean_q: 0.021800\n",
      " 49436/50000: episode: 1553, duration: 0.389s, episode steps:  31, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.194 [0.000, 3.000],  loss: 0.000011, mae: 0.015839, mean_q: 0.021560\n",
      " 49469/50000: episode: 1554, duration: 0.409s, episode steps:  33, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000010, mae: 0.015901, mean_q: 0.021509\n",
      " 49494/50000: episode: 1555, duration: 0.314s, episode steps:  25, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.160 [0.000, 3.000],  loss: 0.000009, mae: 0.015937, mean_q: 0.021613\n",
      " 49553/50000: episode: 1556, duration: 0.723s, episode steps:  59, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000009, mae: 0.015718, mean_q: 0.021348\n",
      " 49572/50000: episode: 1557, duration: 0.241s, episode steps:  19, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000009, mae: 0.015763, mean_q: 0.021479\n",
      " 49583/50000: episode: 1558, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.818 [1.000, 3.000],  loss: 0.000005, mae: 0.015606, mean_q: 0.021231\n",
      " 49599/50000: episode: 1559, duration: 0.212s, episode steps:  16, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000006, mae: 0.015833, mean_q: 0.021462\n",
      " 49621/50000: episode: 1560, duration: 0.279s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.000009, mae: 0.015489, mean_q: 0.021024\n",
      " 49690/50000: episode: 1561, duration: 0.841s, episode steps:  69, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.275 [0.000, 3.000],  loss: 0.000222, mae: 0.016543, mean_q: 0.023392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49744/50000: episode: 1562, duration: 0.660s, episode steps:  54, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.000008, mae: 0.016100, mean_q: 0.021910\n",
      " 49757/50000: episode: 1563, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000007, mae: 0.016241, mean_q: 0.022127\n",
      " 49776/50000: episode: 1564, duration: 0.244s, episode steps:  19, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.263 [0.000, 3.000],  loss: 0.000008, mae: 0.016091, mean_q: 0.022164\n",
      " 49806/50000: episode: 1565, duration: 0.373s, episode steps:  30, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000009, mae: 0.016103, mean_q: 0.021858\n",
      " 49817/50000: episode: 1566, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000007, mae: 0.015916, mean_q: 0.021521\n",
      " 49842/50000: episode: 1567, duration: 0.316s, episode steps:  25, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.040 [0.000, 3.000],  loss: 0.000008, mae: 0.015743, mean_q: 0.021314\n",
      " 49858/50000: episode: 1568, duration: 0.201s, episode steps:  16, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.000009, mae: 0.015720, mean_q: 0.021230\n",
      " 49880/50000: episode: 1569, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000006, mae: 0.015996, mean_q: 0.022123\n",
      " 49902/50000: episode: 1570, duration: 0.280s, episode steps:  22, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000007, mae: 0.016025, mean_q: 0.021737\n",
      " 49943/50000: episode: 1571, duration: 0.508s, episode steps:  41, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.000008, mae: 0.015929, mean_q: 0.021695\n",
      " 49966/50000: episode: 1572, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.000008, mae: 0.015968, mean_q: 0.021570\n",
      " 49977/50000: episode: 1573, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.091 [0.000, 3.000],  loss: 0.000006, mae: 0.016161, mean_q: 0.021916\n",
      "done, took 445.352 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69a526d30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7afbdd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mH\u001b[0mHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 1: reward: 0.000, steps: 26\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFF\u001b[41mH\u001b[0mFFFG\n",
      "Episode 2: reward: 0.000, steps: 48\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "F\u001b[41mH\u001b[0mFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 3: reward: 0.000, steps: 15\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFF\u001b[41mH\u001b[0mFFFG\n",
      "Episode 4: reward: 0.000, steps: 86\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mH\u001b[0mHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 5: reward: 0.000, steps: 48\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mH\u001b[0mHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 6: reward: 0.000, steps: 20\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "F\u001b[41mH\u001b[0mFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 7: reward: 0.000, steps: 81\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "F\u001b[41mH\u001b[0mFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 8: reward: 0.000, steps: 13\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mH\u001b[0mHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Episode 9: reward: 0.000, steps: 9\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "\u001b[41mF\u001b[0mHFFHFHF\n",
      "FFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "\u001b[41mF\u001b[0mFFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "F\u001b[41mF\u001b[0mFHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FF\u001b[41mF\u001b[0mHFFFG\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFF\u001b[41mH\u001b[0mFFFG\n",
      "Episode 10: reward: 0.000, steps: 147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69a526c10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2edfc",
   "metadata": {},
   "source": [
    "## Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a285331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88cc9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   200/50000: episode: 1, duration: 2.847s, episode steps: 200, steps per second:  70, episode reward: -2000.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000],  loss: 2.090592, mae: 58.836896, mean_q: 150.402672\n",
      "   400/50000: episode: 2, duration: 1.154s, episode steps: 200, steps per second: 173, episode reward: -2000.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000],  loss: 38.393116, mae: 57.845596, mean_q: 139.599899\n",
      "   600/50000: episode: 3, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -2000.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000],  loss: 32.164146, mae: 47.821381, mean_q: 105.263924\n",
      "   800/50000: episode: 4, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -1982.000, mean reward: -9.910 [-10.000, -1.000], mean action: 4.950 [0.000, 5.000],  loss: 26.047260, mae: 31.729910, mean_q: 65.319054\n",
      "  1000/50000: episode: 5, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -1595.000, mean reward: -7.975 [-10.000, -1.000], mean action: 3.875 [0.000, 5.000],  loss: 22.750294, mae: 30.736906, mean_q: 49.805145\n",
      "  1200/50000: episode: 6, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 15.855307, mae: 33.306763, mean_q: 47.299881\n",
      "  1400/50000: episode: 7, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 11.726250, mae: 34.545330, mean_q: 46.358662\n",
      "  1600/50000: episode: 8, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.015 [0.000, 3.000],  loss: 11.718725, mae: 34.578304, mean_q: 45.268051\n",
      "  1800/50000: episode: 9, duration: 1.178s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 8.227891, mae: 34.252773, mean_q: 44.400631\n",
      "  2000/50000: episode: 10, duration: 1.137s, episode steps: 200, steps per second: 176, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 8.623734, mae: 34.007122, mean_q: 42.039738\n",
      "  2200/50000: episode: 11, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 5.823331, mae: 34.565109, mean_q: 40.510952\n",
      "  2400/50000: episode: 12, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 4.072859, mae: 33.709000, mean_q: 38.928429\n",
      "  2600/50000: episode: 13, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.796053, mae: 33.306522, mean_q: 37.046764\n",
      "  2800/50000: episode: 14, duration: 1.142s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.462436, mae: 31.764145, mean_q: 34.604153\n",
      "  3000/50000: episode: 15, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 5.149950, mae: 31.383572, mean_q: 31.854683\n",
      "  3200/50000: episode: 16, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.694318, mae: 30.532909, mean_q: 29.109741\n",
      "  3400/50000: episode: 17, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.005 [0.000, 1.000],  loss: 2.841753, mae: 29.376049, mean_q: 26.684134\n",
      "  3600/50000: episode: 18, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.010 [0.000, 1.000],  loss: 1.757810, mae: 27.639044, mean_q: 24.526796\n",
      "  3800/50000: episode: 19, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.615534, mae: 26.389753, mean_q: 21.899456\n",
      "  4000/50000: episode: 20, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.050 [0.000, 3.000],  loss: 1.105527, mae: 25.099808, mean_q: 19.270414\n",
      "  4200/50000: episode: 21, duration: 1.521s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.035 [0.000, 1.000],  loss: 1.062146, mae: 24.502266, mean_q: 16.965521\n",
      "  4400/50000: episode: 22, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.060 [0.000, 1.000],  loss: 0.785437, mae: 23.565962, mean_q: 14.357627\n",
      "  4600/50000: episode: 23, duration: 1.394s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 1.000],  loss: 0.468143, mae: 22.391569, mean_q: 11.769732\n",
      "  4800/50000: episode: 24, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 1.000],  loss: 0.492479, mae: 21.596186, mean_q: 9.416655\n",
      "  5000/50000: episode: 25, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -209.000, mean reward: -1.045 [-10.000, -1.000], mean action: 0.195 [0.000, 5.000],  loss: 0.161326, mae: 21.228437, mean_q: 7.178393\n",
      "  5200/50000: episode: 26, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.230 [0.000, 1.000],  loss: 0.127335, mae: 20.671227, mean_q: 4.934838\n",
      "  5400/50000: episode: 27, duration: 1.137s, episode steps: 200, steps per second: 176, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.310 [0.000, 1.000],  loss: 0.053277, mae: 20.927238, mean_q: 2.903076\n",
      "  5600/50000: episode: 28, duration: 1.194s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.615 [0.000, 3.000],  loss: 0.037275, mae: 20.290012, mean_q: 0.963659\n",
      "  5800/50000: episode: 29, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.033935, mae: 18.857672, mean_q: -0.559779\n",
      "  6000/50000: episode: 30, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 3.000],  loss: 0.044214, mae: 18.973499, mean_q: -1.275184\n",
      "  6200/50000: episode: 31, duration: 1.476s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.052038, mae: 18.731958, mean_q: -1.645759\n",
      "  6400/50000: episode: 32, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.057192, mae: 17.165249, mean_q: -1.970591\n",
      "  6600/50000: episode: 33, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.065948, mae: 16.785339, mean_q: -2.328120\n",
      "  6800/50000: episode: 34, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.068766, mae: 16.133211, mean_q: -2.816587\n",
      "  7000/50000: episode: 35, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.093615, mae: 16.465389, mean_q: -3.427871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7200/50000: episode: 36, duration: 1.166s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 4.000],  loss: 0.134354, mae: 17.374413, mean_q: -3.940053\n",
      "  7400/50000: episode: 37, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -209.000, mean reward: -1.045 [-10.000, -1.000], mean action: 1.245 [0.000, 4.000],  loss: 0.177593, mae: 19.336262, mean_q: -4.802045\n",
      "  7600/50000: episode: 38, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 4.000],  loss: 0.150491, mae: 20.740026, mean_q: -5.517731\n",
      "  7800/50000: episode: 39, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 3.000],  loss: 0.245424, mae: 21.905876, mean_q: -6.209590\n",
      "  8000/50000: episode: 40, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.245104, mae: 22.645718, mean_q: -6.861734\n",
      "  8200/50000: episode: 41, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.257057, mae: 23.921354, mean_q: -7.809174\n",
      "  8400/50000: episode: 42, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.360113, mae: 25.173029, mean_q: -8.659241\n",
      "  8600/50000: episode: 43, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 3.000],  loss: 0.523122, mae: 26.145737, mean_q: -9.587956\n",
      "  8800/50000: episode: 44, duration: 1.188s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.521702, mae: 26.529665, mean_q: -9.946791\n",
      "  9000/50000: episode: 45, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 3.000],  loss: 0.518019, mae: 27.026800, mean_q: -10.771243\n",
      "  9200/50000: episode: 46, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.570659, mae: 26.750811, mean_q: -10.971862\n",
      "  9400/50000: episode: 47, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 3.000],  loss: 0.469042, mae: 26.967999, mean_q: -11.235981\n",
      "  9600/50000: episode: 48, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.562962, mae: 28.103760, mean_q: -11.627517\n",
      "  9800/50000: episode: 49, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 3.000],  loss: 0.508274, mae: 28.355318, mean_q: -12.426610\n",
      " 10000/50000: episode: 50, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.780353, mae: 29.569904, mean_q: -13.147774\n",
      " 10200/50000: episode: 51, duration: 1.238s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.686186, mae: 29.831158, mean_q: -14.166985\n",
      " 10400/50000: episode: 52, duration: 1.161s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.832211, mae: 30.692669, mean_q: -15.160159\n",
      " 10600/50000: episode: 53, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 3.000],  loss: 1.061642, mae: 31.273335, mean_q: -15.941049\n",
      " 10800/50000: episode: 54, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.765 [0.000, 4.000],  loss: 0.885461, mae: 31.272921, mean_q: -16.354269\n",
      " 11000/50000: episode: 55, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.868187, mae: 31.088474, mean_q: -16.876656\n",
      " 11200/50000: episode: 56, duration: 1.192s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.685 [0.000, 3.000],  loss: 1.234807, mae: 31.120981, mean_q: -17.188599\n",
      " 11400/50000: episode: 57, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.590 [0.000, 4.000],  loss: 1.057212, mae: 31.478165, mean_q: -17.596001\n",
      " 11600/50000: episode: 58, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.715 [0.000, 3.000],  loss: 1.140873, mae: 32.058502, mean_q: -18.189669\n",
      " 11800/50000: episode: 59, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 3.000],  loss: 1.045545, mae: 32.272564, mean_q: -18.188736\n",
      " 12000/50000: episode: 60, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.595 [0.000, 3.000],  loss: 1.559744, mae: 32.593651, mean_q: -18.731165\n",
      " 12200/50000: episode: 61, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.949551, mae: 32.767841, mean_q: -19.084700\n",
      " 12400/50000: episode: 62, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 3.000],  loss: 1.477119, mae: 33.053032, mean_q: -19.376167\n",
      " 12600/50000: episode: 63, duration: 1.159s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 3.000],  loss: 1.499739, mae: 33.489845, mean_q: -19.889198\n",
      " 12800/50000: episode: 64, duration: 1.132s, episode steps: 200, steps per second: 177, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.760 [0.000, 3.000],  loss: 1.303556, mae: 33.634460, mean_q: -20.250067\n",
      " 13000/50000: episode: 65, duration: 1.141s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 3.000],  loss: 1.442381, mae: 34.375080, mean_q: -20.894804\n",
      " 13200/50000: episode: 66, duration: 1.163s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 3.000],  loss: 1.643552, mae: 35.104893, mean_q: -21.426401\n",
      " 13400/50000: episode: 67, duration: 1.143s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 3.000],  loss: 1.791551, mae: 35.778168, mean_q: -21.898247\n",
      " 13600/50000: episode: 68, duration: 1.197s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 3.000],  loss: 1.465706, mae: 36.024818, mean_q: -22.413780\n",
      " 13800/50000: episode: 69, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.665 [0.000, 3.000],  loss: 1.128724, mae: 36.275803, mean_q: -23.126099\n",
      " 14000/50000: episode: 70, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.740 [0.000, 3.000],  loss: 2.009563, mae: 36.985085, mean_q: -23.602880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14200/50000: episode: 71, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.605 [0.000, 3.000],  loss: 1.494521, mae: 37.538380, mean_q: -24.195137\n",
      " 14400/50000: episode: 72, duration: 1.193s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.550 [0.000, 3.000],  loss: 1.868576, mae: 37.934402, mean_q: -24.564581\n",
      " 14600/50000: episode: 73, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.715 [0.000, 3.000],  loss: 2.007037, mae: 37.778702, mean_q: -25.014921\n",
      " 14800/50000: episode: 74, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.565 [0.000, 3.000],  loss: 2.185457, mae: 38.331329, mean_q: -25.346403\n",
      " 15000/50000: episode: 75, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.625 [0.000, 3.000],  loss: 1.986482, mae: 38.548344, mean_q: -25.780649\n",
      " 15200/50000: episode: 76, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -209.000, mean reward: -1.045 [-10.000, -1.000], mean action: 1.505 [0.000, 4.000],  loss: 1.874126, mae: 38.653759, mean_q: -26.374540\n",
      " 15400/50000: episode: 77, duration: 1.162s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 3.000],  loss: 2.516104, mae: 39.173138, mean_q: -26.830276\n",
      " 15600/50000: episode: 78, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.785 [0.000, 3.000],  loss: 2.096861, mae: 39.378918, mean_q: -27.307798\n",
      " 15800/50000: episode: 79, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 3.000],  loss: 2.082671, mae: 39.598454, mean_q: -27.636534\n",
      " 16000/50000: episode: 80, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.575 [0.000, 3.000],  loss: 3.111607, mae: 39.800880, mean_q: -27.955776\n",
      " 16200/50000: episode: 81, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.575 [0.000, 3.000],  loss: 2.575042, mae: 39.862991, mean_q: -28.443413\n",
      " 16400/50000: episode: 82, duration: 1.167s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.680 [0.000, 3.000],  loss: 2.064872, mae: 39.978455, mean_q: -28.972893\n",
      " 16600/50000: episode: 83, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 3.000],  loss: 2.313392, mae: 40.153774, mean_q: -29.450689\n",
      " 16800/50000: episode: 84, duration: 1.157s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 3.000],  loss: 2.832191, mae: 40.301468, mean_q: -29.707256\n",
      " 17000/50000: episode: 85, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.680 [0.000, 3.000],  loss: 3.178149, mae: 40.276318, mean_q: -30.165220\n",
      " 17200/50000: episode: 86, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 3.000],  loss: 2.483938, mae: 40.824478, mean_q: -30.815485\n",
      " 17400/50000: episode: 87, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 3.000],  loss: 2.534457, mae: 40.931332, mean_q: -31.223278\n",
      " 17600/50000: episode: 88, duration: 1.544s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 3.000],  loss: 3.053412, mae: 41.300926, mean_q: -31.521530\n",
      " 17800/50000: episode: 89, duration: 2.410s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.605 [0.000, 3.000],  loss: 2.591969, mae: 41.421085, mean_q: -31.792847\n",
      " 18000/50000: episode: 90, duration: 2.362s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 3.000],  loss: 2.839834, mae: 41.796074, mean_q: -32.240719\n",
      " 18200/50000: episode: 91, duration: 2.306s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 3.000],  loss: 2.707398, mae: 41.799408, mean_q: -32.744274\n",
      " 18400/50000: episode: 92, duration: 2.317s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.705 [0.000, 3.000],  loss: 3.259320, mae: 42.233810, mean_q: -33.037888\n",
      " 18600/50000: episode: 93, duration: 2.310s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 3.000],  loss: 2.558855, mae: 41.959583, mean_q: -33.169247\n",
      " 18800/50000: episode: 94, duration: 2.320s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 3.000],  loss: 2.103853, mae: 42.117638, mean_q: -33.662792\n",
      " 19000/50000: episode: 95, duration: 2.315s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.825 [0.000, 3.000],  loss: 3.628976, mae: 42.465633, mean_q: -33.774960\n",
      " 19200/50000: episode: 96, duration: 2.331s, episode steps: 200, steps per second:  86, episode reward: -209.000, mean reward: -1.045 [-10.000, -1.000], mean action: 1.535 [0.000, 4.000],  loss: 2.692487, mae: 42.877510, mean_q: -34.144794\n",
      " 19400/50000: episode: 97, duration: 2.321s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.550 [0.000, 3.000],  loss: 2.057047, mae: 42.883575, mean_q: -34.736664\n",
      " 19600/50000: episode: 98, duration: 2.332s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.800 [0.000, 3.000],  loss: 3.185372, mae: 42.955379, mean_q: -34.867142\n",
      " 19800/50000: episode: 99, duration: 2.315s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.620 [0.000, 3.000],  loss: 3.908089, mae: 42.820511, mean_q: -34.843521\n",
      " 20000/50000: episode: 100, duration: 2.315s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.625 [0.000, 3.000],  loss: 4.556859, mae: 42.438683, mean_q: -34.929989\n",
      " 20200/50000: episode: 101, duration: 2.318s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 3.000],  loss: 3.781483, mae: 42.166172, mean_q: -35.020809\n",
      " 20400/50000: episode: 102, duration: 2.322s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.625 [0.000, 3.000],  loss: 3.288117, mae: 42.374481, mean_q: -35.288498\n",
      " 20600/50000: episode: 103, duration: 2.308s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 3.000],  loss: 3.840671, mae: 42.648438, mean_q: -35.572475\n",
      " 20800/50000: episode: 104, duration: 2.323s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 3.000],  loss: 4.203413, mae: 42.828495, mean_q: -35.785748\n",
      " 21000/50000: episode: 105, duration: 2.360s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.565 [0.000, 3.000],  loss: 3.925495, mae: 43.394875, mean_q: -36.399117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21200/50000: episode: 106, duration: 2.436s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 3.000],  loss: 2.722883, mae: 43.565842, mean_q: -36.955078\n",
      " 21400/50000: episode: 107, duration: 2.427s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 3.000],  loss: 4.169387, mae: 44.063580, mean_q: -36.989063\n",
      " 21600/50000: episode: 108, duration: 2.486s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.485 [0.000, 3.000],  loss: 3.540305, mae: 44.754868, mean_q: -37.579819\n",
      " 21800/50000: episode: 109, duration: 2.366s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 3.000],  loss: 3.645775, mae: 44.845181, mean_q: -37.979019\n",
      " 22000/50000: episode: 110, duration: 2.339s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 3.000],  loss: 3.984644, mae: 44.648739, mean_q: -38.208473\n",
      " 22200/50000: episode: 111, duration: 2.431s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 3.000],  loss: 3.621942, mae: 44.756485, mean_q: -38.525555\n",
      " 22400/50000: episode: 112, duration: 2.356s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 4.000],  loss: 4.515306, mae: 44.638538, mean_q: -38.919628\n",
      " 22600/50000: episode: 113, duration: 2.359s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 3.000],  loss: 4.632945, mae: 44.541973, mean_q: -39.314404\n",
      " 22800/50000: episode: 114, duration: 2.409s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.515 [0.000, 3.000],  loss: 4.514296, mae: 44.822655, mean_q: -39.751392\n",
      " 23000/50000: episode: 115, duration: 2.428s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 3.000],  loss: 5.497055, mae: 44.669624, mean_q: -39.867081\n",
      " 23200/50000: episode: 116, duration: 2.410s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 3.000],  loss: 5.056760, mae: 44.677597, mean_q: -40.135929\n",
      " 23400/50000: episode: 117, duration: 2.438s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 3.000],  loss: 3.756397, mae: 45.337391, mean_q: -40.798439\n",
      " 23600/50000: episode: 118, duration: 2.424s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 3.000],  loss: 3.989656, mae: 45.462120, mean_q: -41.198349\n",
      " 23800/50000: episode: 119, duration: 2.386s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 3.000],  loss: 4.113647, mae: 45.888260, mean_q: -41.693916\n",
      " 24000/50000: episode: 120, duration: 2.397s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 3.000],  loss: 4.259127, mae: 45.738525, mean_q: -42.015213\n",
      " 24200/50000: episode: 121, duration: 2.418s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 3.000],  loss: 5.029426, mae: 45.592651, mean_q: -42.408379\n",
      " 24400/50000: episode: 122, duration: 2.575s, episode steps: 200, steps per second:  78, episode reward: -209.000, mean reward: -1.045 [-10.000, -1.000], mean action: 1.585 [0.000, 4.000],  loss: 5.658788, mae: 45.854713, mean_q: -42.532627\n",
      " 24600/50000: episode: 123, duration: 2.363s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 3.000],  loss: 5.526298, mae: 44.990746, mean_q: -42.287495\n",
      " 24800/50000: episode: 124, duration: 2.381s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 3.000],  loss: 5.166180, mae: 44.828236, mean_q: -42.356579\n",
      " 25000/50000: episode: 125, duration: 2.424s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 3.000],  loss: 7.015751, mae: 44.730682, mean_q: -42.498165\n",
      " 25200/50000: episode: 126, duration: 2.593s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.585 [0.000, 3.000],  loss: 5.358115, mae: 44.736904, mean_q: -42.905224\n",
      " 25400/50000: episode: 127, duration: 2.423s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 3.000],  loss: 3.478317, mae: 45.004845, mean_q: -43.348133\n",
      " 25600/50000: episode: 128, duration: 2.431s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 3.000],  loss: 4.980648, mae: 45.052990, mean_q: -43.566788\n",
      " 25800/50000: episode: 129, duration: 2.332s, episode steps: 200, steps per second:  86, episode reward: -209.000, mean reward: -1.045 [-10.000, -1.000], mean action: 1.405 [0.000, 4.000],  loss: 5.999531, mae: 45.103775, mean_q: -43.712215\n",
      " 26000/50000: episode: 130, duration: 2.356s, episode steps: 200, steps per second:  85, episode reward: -218.000, mean reward: -1.090 [-10.000, -1.000], mean action: 1.495 [0.000, 4.000],  loss: 4.916174, mae: 45.039024, mean_q: -43.865028\n",
      " 26200/50000: episode: 131, duration: 2.341s, episode steps: 200, steps per second:  85, episode reward: -263.000, mean reward: -1.315 [-10.000, -1.000], mean action: 1.650 [0.000, 4.000],  loss: 3.606477, mae: 45.178635, mean_q: -44.323467\n",
      " 26400/50000: episode: 132, duration: 2.340s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 3.000],  loss: 5.169191, mae: 45.295547, mean_q: -44.435650\n",
      " 26600/50000: episode: 133, duration: 2.335s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 3.000],  loss: 5.080184, mae: 45.571476, mean_q: -44.483917\n",
      " 26800/50000: episode: 134, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.585 [0.000, 4.000],  loss: 4.540360, mae: 45.846485, mean_q: -44.956280\n",
      " 27000/50000: episode: 135, duration: 2.349s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.510 [0.000, 3.000],  loss: 5.212873, mae: 45.932949, mean_q: -45.083046\n",
      " 27200/50000: episode: 136, duration: 2.337s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 3.000],  loss: 5.597277, mae: 45.995602, mean_q: -44.982319\n",
      " 27400/50000: episode: 137, duration: 2.360s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 3.000],  loss: 4.534602, mae: 46.030888, mean_q: -45.277695\n",
      " 27600/50000: episode: 138, duration: 2.352s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 3.000],  loss: 5.636594, mae: 46.443668, mean_q: -45.549454\n",
      " 27800/50000: episode: 139, duration: 2.351s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 3.000],  loss: 4.471482, mae: 46.429153, mean_q: -45.902687\n",
      " 28000/50000: episode: 140, duration: 2.352s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 3.000],  loss: 4.823188, mae: 46.250187, mean_q: -46.236031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28200/50000: episode: 141, duration: 2.360s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 3.000],  loss: 6.492307, mae: 46.252743, mean_q: -46.367767\n",
      " 28400/50000: episode: 142, duration: 2.335s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 3.000],  loss: 6.118598, mae: 46.457973, mean_q: -46.587170\n",
      " 28600/50000: episode: 143, duration: 2.341s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 3.000],  loss: 5.495004, mae: 46.746170, mean_q: -46.682999\n",
      " 28800/50000: episode: 144, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 3.000],  loss: 5.071979, mae: 46.646606, mean_q: -47.071026\n",
      " 29000/50000: episode: 145, duration: 2.336s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 3.000],  loss: 6.760350, mae: 46.391441, mean_q: -47.081043\n",
      " 29200/50000: episode: 146, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 3.000],  loss: 7.174131, mae: 46.466732, mean_q: -47.185234\n",
      " 29400/50000: episode: 147, duration: 2.434s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 3.000],  loss: 6.871830, mae: 46.696407, mean_q: -47.377548\n",
      " 29600/50000: episode: 148, duration: 2.344s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 3.000],  loss: 4.524269, mae: 46.614567, mean_q: -47.261383\n",
      " 29800/50000: episode: 149, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.565 [0.000, 3.000],  loss: 6.544231, mae: 46.576641, mean_q: -47.201080\n",
      " 30000/50000: episode: 150, duration: 2.357s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.625 [0.000, 3.000],  loss: 7.788453, mae: 46.307480, mean_q: -47.033535\n",
      " 30200/50000: episode: 151, duration: 2.349s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 3.000],  loss: 6.052033, mae: 46.367882, mean_q: -47.047997\n",
      " 30400/50000: episode: 152, duration: 2.354s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 3.000],  loss: 4.846065, mae: 46.474136, mean_q: -47.110634\n",
      " 30600/50000: episode: 153, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 3.000],  loss: 5.539486, mae: 46.424297, mean_q: -47.136196\n",
      " 30800/50000: episode: 154, duration: 2.365s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 3.000],  loss: 5.598939, mae: 45.885292, mean_q: -47.098156\n",
      " 31000/50000: episode: 155, duration: 2.344s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 3.000],  loss: 6.066171, mae: 45.760197, mean_q: -47.329571\n",
      " 31200/50000: episode: 156, duration: 2.365s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 3.000],  loss: 6.469536, mae: 45.741138, mean_q: -47.319046\n",
      " 31400/50000: episode: 157, duration: 2.354s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.635 [0.000, 3.000],  loss: 6.281748, mae: 45.953514, mean_q: -47.734966\n",
      " 31600/50000: episode: 158, duration: 2.354s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.550 [0.000, 3.000],  loss: 5.039224, mae: 46.181862, mean_q: -48.080612\n",
      " 31800/50000: episode: 159, duration: 2.359s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 3.000],  loss: 6.425939, mae: 46.431953, mean_q: -48.207138\n",
      " 32000/50000: episode: 160, duration: 2.367s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 3.000],  loss: 8.034829, mae: 46.561398, mean_q: -48.172157\n",
      " 32200/50000: episode: 161, duration: 2.388s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 3.000],  loss: 6.447693, mae: 46.730392, mean_q: -48.328922\n",
      " 32400/50000: episode: 162, duration: 2.357s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 3.000],  loss: 7.354629, mae: 46.660824, mean_q: -48.493362\n",
      " 32600/50000: episode: 163, duration: 2.369s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.485 [0.000, 3.000],  loss: 8.900524, mae: 46.659508, mean_q: -48.501320\n",
      " 32800/50000: episode: 164, duration: 2.376s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 3.000],  loss: 7.062984, mae: 46.833683, mean_q: -48.281406\n",
      " 33000/50000: episode: 165, duration: 2.379s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.590 [0.000, 3.000],  loss: 5.337380, mae: 46.893139, mean_q: -48.334991\n",
      " 33200/50000: episode: 166, duration: 2.367s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 3.000],  loss: 4.943285, mae: 46.784500, mean_q: -48.429386\n",
      " 33400/50000: episode: 167, duration: 2.364s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 3.000],  loss: 7.977098, mae: 46.727238, mean_q: -48.068851\n",
      " 33600/50000: episode: 168, duration: 2.371s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 3.000],  loss: 6.846843, mae: 46.228233, mean_q: -47.791550\n",
      " 33800/50000: episode: 169, duration: 2.370s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 3.000],  loss: 6.004432, mae: 46.153488, mean_q: -47.833038\n",
      " 34000/50000: episode: 170, duration: 2.381s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 3.000],  loss: 7.864011, mae: 46.038605, mean_q: -47.586777\n",
      " 34200/50000: episode: 171, duration: 2.365s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 3.000],  loss: 4.979467, mae: 46.237755, mean_q: -47.870956\n",
      " 34400/50000: episode: 172, duration: 2.463s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 3.000],  loss: 4.456071, mae: 46.290794, mean_q: -48.260502\n",
      " 34600/50000: episode: 173, duration: 2.497s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 3.000],  loss: 5.974466, mae: 46.357109, mean_q: -48.285889\n",
      " 34800/50000: episode: 174, duration: 2.384s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 3.000],  loss: 5.623330, mae: 46.732635, mean_q: -48.575890\n",
      " 35000/50000: episode: 175, duration: 2.447s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 3.000],  loss: 6.100913, mae: 46.828468, mean_q: -48.568951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35200/50000: episode: 176, duration: 2.376s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 3.000],  loss: 6.171040, mae: 46.851852, mean_q: -48.742855\n",
      " 35400/50000: episode: 177, duration: 2.387s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.625 [0.000, 3.000],  loss: 7.781847, mae: 46.755905, mean_q: -48.574364\n",
      " 35600/50000: episode: 178, duration: 2.379s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 3.000],  loss: 4.743392, mae: 47.013935, mean_q: -49.109344\n",
      " 35800/50000: episode: 179, duration: 2.368s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 3.000],  loss: 8.719033, mae: 46.764530, mean_q: -48.809200\n",
      " 36000/50000: episode: 180, duration: 2.372s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.555 [0.000, 3.000],  loss: 6.506629, mae: 46.668941, mean_q: -48.993240\n",
      " 36200/50000: episode: 181, duration: 2.375s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 3.000],  loss: 6.798018, mae: 46.449921, mean_q: -49.087002\n",
      " 36400/50000: episode: 182, duration: 2.389s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.690 [0.000, 3.000],  loss: 5.142757, mae: 46.426128, mean_q: -49.285332\n",
      " 36600/50000: episode: 183, duration: 2.376s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 3.000],  loss: 8.857290, mae: 46.511295, mean_q: -49.262520\n",
      " 36800/50000: episode: 184, duration: 2.371s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 3.000],  loss: 4.925446, mae: 46.608261, mean_q: -49.552837\n",
      " 37000/50000: episode: 185, duration: 2.383s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 3.000],  loss: 7.413600, mae: 46.583519, mean_q: -49.691017\n",
      " 37200/50000: episode: 186, duration: 2.389s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 3.000],  loss: 4.708914, mae: 46.782207, mean_q: -50.025269\n",
      " 37400/50000: episode: 187, duration: 2.398s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 3.000],  loss: 5.691254, mae: 47.043285, mean_q: -50.159855\n",
      " 37600/50000: episode: 188, duration: 2.393s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 3.000],  loss: 6.276261, mae: 47.090870, mean_q: -50.295753\n",
      " 37800/50000: episode: 189, duration: 2.388s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.590 [0.000, 3.000],  loss: 7.358158, mae: 47.205429, mean_q: -50.340858\n",
      " 38000/50000: episode: 190, duration: 2.399s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 3.000],  loss: 6.436033, mae: 47.193817, mean_q: -50.295742\n",
      " 38200/50000: episode: 191, duration: 2.455s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 3.000],  loss: 6.548927, mae: 47.420155, mean_q: -50.532383\n",
      " 38400/50000: episode: 192, duration: 2.381s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 3.000],  loss: 7.603340, mae: 47.646572, mean_q: -50.491074\n",
      " 38600/50000: episode: 193, duration: 2.388s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 3.000],  loss: 7.552489, mae: 47.737431, mean_q: -50.291016\n",
      " 38800/50000: episode: 194, duration: 2.393s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 3.000],  loss: 7.565004, mae: 47.459885, mean_q: -50.236332\n",
      " 39000/50000: episode: 195, duration: 2.410s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 3.000],  loss: 7.245313, mae: 47.576237, mean_q: -50.363537\n",
      " 39200/50000: episode: 196, duration: 2.417s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 3.000],  loss: 5.891680, mae: 47.451492, mean_q: -50.305275\n",
      " 39400/50000: episode: 197, duration: 2.397s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.595 [0.000, 3.000],  loss: 6.866041, mae: 47.392788, mean_q: -50.285660\n",
      " 39600/50000: episode: 198, duration: 2.402s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.560 [0.000, 3.000],  loss: 8.439603, mae: 47.721981, mean_q: -50.229874\n",
      " 39800/50000: episode: 199, duration: 2.408s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.600 [0.000, 3.000],  loss: 8.222496, mae: 47.491264, mean_q: -50.162109\n",
      " 40000/50000: episode: 200, duration: 2.405s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 3.000],  loss: 6.656462, mae: 47.273418, mean_q: -50.036621\n",
      " 40200/50000: episode: 201, duration: 2.401s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 3.000],  loss: 5.350165, mae: 47.281708, mean_q: -50.113056\n",
      " 40400/50000: episode: 202, duration: 2.417s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 3.000],  loss: 6.634733, mae: 47.285927, mean_q: -50.138866\n",
      " 40600/50000: episode: 203, duration: 2.411s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 3.000],  loss: 6.280344, mae: 47.201794, mean_q: -50.312767\n",
      " 40800/50000: episode: 204, duration: 2.396s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 3.000],  loss: 6.631282, mae: 47.363270, mean_q: -50.354477\n",
      " 41000/50000: episode: 205, duration: 2.404s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 3.000],  loss: 5.521561, mae: 47.296776, mean_q: -50.403179\n",
      " 41200/50000: episode: 206, duration: 2.410s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 3.000],  loss: 5.695190, mae: 47.237350, mean_q: -50.520496\n",
      " 41400/50000: episode: 207, duration: 2.399s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 3.000],  loss: 6.894239, mae: 47.399044, mean_q: -50.563194\n",
      " 41600/50000: episode: 208, duration: 2.407s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.495 [0.000, 3.000],  loss: 7.270103, mae: 47.216293, mean_q: -50.644005\n",
      " 41800/50000: episode: 209, duration: 2.405s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 3.000],  loss: 5.147336, mae: 47.336285, mean_q: -50.859779\n",
      " 42000/50000: episode: 210, duration: 2.405s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 3.000],  loss: 5.138771, mae: 47.564610, mean_q: -51.053421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42200/50000: episode: 211, duration: 2.409s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.555 [0.000, 3.000],  loss: 9.398576, mae: 47.514175, mean_q: -51.030186\n",
      " 42400/50000: episode: 212, duration: 2.426s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 3.000],  loss: 8.617304, mae: 47.705467, mean_q: -51.030548\n",
      " 42600/50000: episode: 213, duration: 2.407s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 3.000],  loss: 5.846580, mae: 47.818451, mean_q: -51.234497\n",
      " 42800/50000: episode: 214, duration: 2.412s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 3.000],  loss: 7.953347, mae: 47.832558, mean_q: -50.992355\n",
      " 43000/50000: episode: 215, duration: 2.473s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 3.000],  loss: 5.662087, mae: 47.712791, mean_q: -51.025440\n",
      " 43200/50000: episode: 216, duration: 2.445s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 3.000],  loss: 5.323236, mae: 47.844883, mean_q: -51.224506\n",
      " 43400/50000: episode: 217, duration: 2.417s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 3.000],  loss: 10.341323, mae: 47.838886, mean_q: -51.000160\n",
      " 43600/50000: episode: 218, duration: 2.423s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 3.000],  loss: 6.766722, mae: 47.919945, mean_q: -51.129307\n",
      " 43800/50000: episode: 219, duration: 2.418s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 3.000],  loss: 8.115319, mae: 47.882145, mean_q: -51.164051\n",
      " 44000/50000: episode: 220, duration: 2.416s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 3.000],  loss: 7.377036, mae: 47.805630, mean_q: -51.040997\n",
      " 44200/50000: episode: 221, duration: 2.424s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 3.000],  loss: 7.224788, mae: 47.612217, mean_q: -50.997593\n",
      " 44400/50000: episode: 222, duration: 2.547s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 3.000],  loss: 6.123156, mae: 47.326504, mean_q: -51.091816\n",
      " 44600/50000: episode: 223, duration: 2.472s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 3.000],  loss: 4.821740, mae: 47.394806, mean_q: -51.204189\n",
      " 44800/50000: episode: 224, duration: 2.423s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 3.000],  loss: 4.918190, mae: 47.871620, mean_q: -51.527977\n",
      " 45000/50000: episode: 225, duration: 2.424s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 3.000],  loss: 5.335198, mae: 47.956875, mean_q: -51.666874\n",
      " 45200/50000: episode: 226, duration: 2.424s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 3.000],  loss: 7.867053, mae: 47.948029, mean_q: -51.648937\n",
      " 45400/50000: episode: 227, duration: 2.426s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.640 [0.000, 3.000],  loss: 8.749226, mae: 48.171932, mean_q: -51.765392\n",
      " 45600/50000: episode: 228, duration: 2.437s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.645 [0.000, 3.000],  loss: 6.112959, mae: 47.969852, mean_q: -52.004326\n",
      " 45800/50000: episode: 229, duration: 2.423s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 3.000],  loss: 7.931075, mae: 48.136520, mean_q: -52.130508\n",
      " 46000/50000: episode: 230, duration: 2.422s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 3.000],  loss: 7.852858, mae: 48.048050, mean_q: -52.003342\n",
      " 46200/50000: episode: 231, duration: 2.432s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 3.000],  loss: 7.350077, mae: 48.161015, mean_q: -51.937923\n",
      " 46400/50000: episode: 232, duration: 2.430s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 3.000],  loss: 6.344336, mae: 48.305870, mean_q: -52.227539\n",
      " 46600/50000: episode: 233, duration: 2.427s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 3.000],  loss: 7.736279, mae: 48.400478, mean_q: -52.154835\n",
      " 46800/50000: episode: 234, duration: 2.431s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 3.000],  loss: 7.346929, mae: 48.345829, mean_q: -52.209423\n",
      " 47000/50000: episode: 235, duration: 2.431s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 3.000],  loss: 6.621163, mae: 48.422630, mean_q: -52.189434\n",
      " 47200/50000: episode: 236, duration: 2.432s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.655 [0.000, 3.000],  loss: 6.809886, mae: 48.355324, mean_q: -52.380245\n",
      " 47400/50000: episode: 237, duration: 2.446s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 3.000],  loss: 8.390689, mae: 48.539536, mean_q: -52.379074\n",
      " 47600/50000: episode: 238, duration: 2.441s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 3.000],  loss: 8.736441, mae: 48.335155, mean_q: -52.335674\n",
      " 47800/50000: episode: 239, duration: 2.431s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 3.000],  loss: 6.516072, mae: 48.337059, mean_q: -52.283329\n",
      " 48000/50000: episode: 240, duration: 2.437s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 3.000],  loss: 9.725930, mae: 48.294064, mean_q: -52.035843\n",
      " 48200/50000: episode: 241, duration: 2.427s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 3.000],  loss: 5.709441, mae: 48.141094, mean_q: -52.035324\n",
      " 48400/50000: episode: 242, duration: 2.442s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 3.000],  loss: 6.327811, mae: 48.226681, mean_q: -52.059185\n",
      " 48600/50000: episode: 243, duration: 2.439s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 3.000],  loss: 8.168074, mae: 48.183029, mean_q: -51.852695\n",
      " 48800/50000: episode: 244, duration: 2.434s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.495 [0.000, 3.000],  loss: 7.554484, mae: 48.318565, mean_q: -52.063175\n",
      " 49000/50000: episode: 245, duration: 2.428s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 3.000],  loss: 7.570289, mae: 48.278458, mean_q: -52.136532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49200/50000: episode: 246, duration: 2.431s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.690 [0.000, 3.000],  loss: 9.083593, mae: 48.180008, mean_q: -52.119617\n",
      " 49400/50000: episode: 247, duration: 2.432s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 3.000],  loss: 7.180509, mae: 47.985428, mean_q: -52.112637\n",
      " 49600/50000: episode: 248, duration: 2.457s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 3.000],  loss: 7.680777, mae: 48.157574, mean_q: -52.225986\n",
      " 49800/50000: episode: 249, duration: 2.441s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 3.000],  loss: 5.981421, mae: 48.365150, mean_q: -52.494064\n",
      " 50000/50000: episode: 250, duration: 2.444s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 3.000],  loss: 7.728397, mae: 48.672203, mean_q: -52.458229\n",
      "done, took 501.072 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69a3baa00>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c927dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "Episode 1: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "Episode 2: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "Episode 3: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "Episode 5: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "Episode 6: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "Episode 7: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "Episode 8: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "Episode 9: reward: -200.000, steps: 200\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "Episode 10: reward: -200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69a3ba790>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537ed49",
   "metadata": {},
   "source": [
    "## Roulette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0855074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(\"Roulette-v0\")\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dd7f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7/50000: episode: 1, duration: 0.521s, episode steps:   7, steps per second:  13, episode reward: -2.000, mean reward: -0.286 [-1.000,  1.000], mean action: 20.571 [8.000, 37.000],  loss: --, mae: --, mean_q: --\n",
      "    47/50000: episode: 2, duration: 0.030s, episode steps:  40, steps per second: 1340, episode reward:  7.000, mean reward:  0.175 [-1.000,  1.000], mean action: 17.950 [2.000, 37.000],  loss: --, mae: --, mean_q: --\n",
      "    90/50000: episode: 3, duration: 0.032s, episode steps:  43, steps per second: 1336, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.070 [0.000, 37.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   117/50000: episode: 4, duration: 1.897s, episode steps:  27, steps per second:  14, episode reward:  2.000, mean reward:  0.074 [-1.000,  1.000], mean action: 17.741 [0.000, 37.000],  loss: 0.485328, mae: 0.028672, mean_q: 0.006086\n",
      "   122/50000: episode: 5, duration: 0.034s, episode steps:   5, steps per second: 145, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 21.600 [7.000, 37.000],  loss: 0.485574, mae: 0.033507, mean_q: 0.015416\n",
      "   130/50000: episode: 6, duration: 0.051s, episode steps:   8, steps per second: 156, episode reward: -1.000, mean reward: -0.125 [-1.000,  1.000], mean action: 23.750 [6.000, 37.000],  loss: 0.479059, mae: 0.036241, mean_q: 0.021380\n",
      "   198/50000: episode: 7, duration: 0.391s, episode steps:  68, steps per second: 174, episode reward: -9.000, mean reward: -0.132 [-1.000,  1.000], mean action: 18.441 [0.000, 37.000],  loss: 0.472372, mae: 0.048472, mean_q: 0.047506\n",
      "   215/50000: episode: 8, duration: 0.099s, episode steps:  17, steps per second: 171, episode reward: -6.000, mean reward: -0.353 [-1.000,  1.000], mean action: 18.471 [1.000, 37.000],  loss: 0.465260, mae: 0.059671, mean_q: 0.073438\n",
      "   230/50000: episode: 9, duration: 0.091s, episode steps:  15, steps per second: 165, episode reward:  2.000, mean reward:  0.133 [-1.000,  1.000], mean action: 16.867 [1.000, 37.000],  loss: 0.461548, mae: 0.063932, mean_q: 0.082561\n",
      "   234/50000: episode: 10, duration: 0.037s, episode steps:   4, steps per second: 108, episode reward: -1.000, mean reward: -0.250 [-1.000,  1.000], mean action: 19.500 [0.000, 37.000],  loss: 0.451972, mae: 0.066145, mean_q: 0.089521\n",
      "   253/50000: episode: 11, duration: 0.116s, episode steps:  19, steps per second: 164, episode reward:  6.000, mean reward:  0.316 [-1.000,  1.000], mean action: 19.158 [1.000, 37.000],  loss: 0.457548, mae: 0.070034, mean_q: 0.096153\n",
      "   353/50000: episode: 12, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.890 [0.000, 36.000],  loss: 0.458006, mae: 0.084583, mean_q: 0.130924\n",
      "   415/50000: episode: 13, duration: 0.397s, episode steps:  62, steps per second: 156, episode reward: 13.000, mean reward:  0.210 [-1.000,  1.000], mean action: 19.629 [0.000, 37.000],  loss: 0.467645, mae: 0.100268, mean_q: 0.159901\n",
      "   462/50000: episode: 14, duration: 0.291s, episode steps:  47, steps per second: 162, episode reward: 39.000, mean reward:  0.830 [-1.000, 36.000], mean action: 17.255 [0.000, 37.000],  loss: 0.463670, mae: 0.108564, mean_q: 0.186975\n",
      "   478/50000: episode: 15, duration: 0.101s, episode steps:  16, steps per second: 158, episode reward: -3.000, mean reward: -0.188 [-1.000,  1.000], mean action: 17.312 [1.000, 37.000],  loss: 1.778952, mae: 0.116435, mean_q: 0.210035\n",
      "   481/50000: episode: 16, duration: 0.024s, episode steps:   3, steps per second: 125, episode reward: -2.000, mean reward: -0.667 [-1.000,  0.000], mean action: 15.000 [0.000, 37.000],  loss: 0.448311, mae: 0.115088, mean_q: 0.218346\n",
      "   507/50000: episode: 17, duration: 0.163s, episode steps:  26, steps per second: 159, episode reward: -5.000, mean reward: -0.192 [-1.000,  1.000], mean action: 22.115 [5.000, 37.000],  loss: 0.463707, mae: 0.117692, mean_q: 0.228937\n",
      "   515/50000: episode: 18, duration: 0.051s, episode steps:   8, steps per second: 156, episode reward:  5.000, mean reward:  0.625 [-1.000,  1.000], mean action: 15.500 [1.000, 37.000],  loss: 0.468583, mae: 0.120656, mean_q: 0.241809\n",
      "   603/50000: episode: 19, duration: 0.535s, episode steps:  88, steps per second: 165, episode reward: -3.000, mean reward: -0.034 [-1.000,  1.000], mean action: 19.170 [1.000, 37.000],  loss: 1.183381, mae: 0.128938, mean_q: 0.279764\n",
      "   619/50000: episode: 20, duration: 0.105s, episode steps:  16, steps per second: 153, episode reward: -5.000, mean reward: -0.312 [-1.000,  1.000], mean action: 21.438 [7.000, 37.000],  loss: 1.768588, mae: 0.136783, mean_q: 0.313960\n",
      "   642/50000: episode: 21, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: -10.000, mean reward: -0.435 [-1.000,  1.000], mean action: 18.783 [0.000, 37.000],  loss: 1.378406, mae: 0.138748, mean_q: 0.326234\n",
      "   676/50000: episode: 22, duration: 0.214s, episode steps:  34, steps per second: 159, episode reward:  9.000, mean reward:  0.265 [-1.000,  1.000], mean action: 20.176 [0.000, 37.000],  loss: 1.685000, mae: 0.141859, mean_q: 0.341264\n",
      "   686/50000: episode: 23, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: -7.000, mean reward: -0.700 [-1.000,  1.000], mean action: 22.700 [0.000, 37.000],  loss: 0.491231, mae: 0.143377, mean_q: 0.348481\n",
      "   741/50000: episode: 24, duration: 0.322s, episode steps:  55, steps per second: 171, episode reward:  8.000, mean reward:  0.145 [-1.000,  1.000], mean action: 20.309 [1.000, 37.000],  loss: 1.249759, mae: 0.148424, mean_q: 0.366001\n",
      "   755/50000: episode: 25, duration: 0.084s, episode steps:  14, steps per second: 166, episode reward: -1.000, mean reward: -0.071 [-1.000,  1.000], mean action: 19.571 [5.000, 37.000],  loss: 1.968134, mae: 0.153511, mean_q: 0.385219\n",
      "   783/50000: episode: 26, duration: 0.173s, episode steps:  28, steps per second: 162, episode reward:  5.000, mean reward:  0.179 [-1.000,  1.000], mean action: 17.893 [0.000, 37.000],  loss: 1.969164, mae: 0.156766, mean_q: 0.394699\n",
      "   798/50000: episode: 27, duration: 0.090s, episode steps:  15, steps per second: 166, episode reward: -4.000, mean reward: -0.267 [-1.000,  1.000], mean action: 18.400 [2.000, 37.000],  loss: 0.489770, mae: 0.157738, mean_q: 0.402982\n",
      "   818/50000: episode: 28, duration: 0.119s, episode steps:  20, steps per second: 168, episode reward: -5.000, mean reward: -0.250 [-1.000,  1.000], mean action: 20.800 [3.000, 37.000],  loss: 1.544324, mae: 0.162191, mean_q: 0.408952\n",
      "   901/50000: episode: 29, duration: 0.522s, episode steps:  83, steps per second: 159, episode reward: -4.000, mean reward: -0.048 [-1.000,  1.000], mean action: 18.614 [0.000, 37.000],  loss: 1.989414, mae: 0.171422, mean_q: 0.424641\n",
      "   926/50000: episode: 30, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 16.440 [0.000, 37.000],  loss: 1.327289, mae: 0.178875, mean_q: 0.434019\n",
      "  1000/50000: episode: 31, duration: 0.537s, episode steps:  74, steps per second: 138, episode reward: -5.000, mean reward: -0.068 [-1.000,  1.000], mean action: 15.838 [0.000, 37.000],  loss: 1.073840, mae: 0.185009, mean_q: 0.439792\n",
      "  1036/50000: episode: 32, duration: 0.215s, episode steps:  36, steps per second: 168, episode reward:  1.000, mean reward:  0.028 [-1.000,  1.000], mean action: 16.139 [0.000, 37.000],  loss: 0.508289, mae: 0.190239, mean_q: 0.453885\n",
      "  1044/50000: episode: 33, duration: 0.055s, episode steps:   8, steps per second: 145, episode reward: -5.000, mean reward: -0.625 [-1.000,  1.000], mean action: 14.750 [0.000, 37.000],  loss: 0.486666, mae: 0.191653, mean_q: 0.460142\n",
      "  1046/50000: episode: 34, duration: 0.017s, episode steps:   2, steps per second: 117, episode reward: -1.000, mean reward: -0.500 [-1.000,  0.000], mean action: 23.500 [10.000, 37.000],  loss: 0.490407, mae: 0.191979, mean_q: 0.461932\n",
      "  1109/50000: episode: 35, duration: 0.399s, episode steps:  63, steps per second: 158, episode reward: -6.000, mean reward: -0.095 [-1.000,  1.000], mean action: 16.952 [1.000, 37.000],  loss: 1.821731, mae: 0.199128, mean_q: 0.466606\n",
      "  1164/50000: episode: 36, duration: 0.380s, episode steps:  55, steps per second: 145, episode reward: -8.000, mean reward: -0.145 [-1.000,  1.000], mean action: 16.255 [0.000, 37.000],  loss: 0.488547, mae: 0.205050, mean_q: 0.469164\n",
      "  1240/50000: episode: 37, duration: 0.502s, episode steps:  76, steps per second: 151, episode reward:  3.000, mean reward:  0.039 [-1.000,  1.000], mean action: 20.197 [0.000, 37.000],  loss: 1.308330, mae: 0.215481, mean_q: 0.469813\n",
      "  1243/50000: episode: 38, duration: 0.023s, episode steps:   3, steps per second: 132, episode reward: -2.000, mean reward: -0.667 [-1.000,  0.000], mean action: 33.000 [30.000, 37.000],  loss: 0.473938, mae: 0.218994, mean_q: 0.480554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1333/50000: episode: 39, duration: 0.526s, episode steps:  90, steps per second: 171, episode reward: 11.000, mean reward:  0.122 [-1.000,  1.000], mean action: 18.567 [0.000, 37.000],  loss: 0.733445, mae: 0.230873, mean_q: 0.497998\n",
      "  1433/50000: episode: 40, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 20.220 [0.000, 36.000],  loss: 1.119097, mae: 0.248843, mean_q: 0.523875\n",
      "  1438/50000: episode: 41, duration: 0.041s, episode steps:   5, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.600 [5.000, 37.000],  loss: 4.663131, mae: 0.264188, mean_q: 0.541516\n",
      "  1517/50000: episode: 42, duration: 0.500s, episode steps:  79, steps per second: 158, episode reward: -12.000, mean reward: -0.152 [-1.000,  1.000], mean action: 17.810 [1.000, 37.000],  loss: 0.757116, mae: 0.265277, mean_q: 0.554143\n",
      "  1549/50000: episode: 43, duration: 0.183s, episode steps:  32, steps per second: 175, episode reward: -9.000, mean reward: -0.281 [-1.000,  1.000], mean action: 19.344 [3.000, 37.000],  loss: 1.160535, mae: 0.275398, mean_q: 0.570590\n",
      "  1627/50000: episode: 44, duration: 0.463s, episode steps:  78, steps per second: 169, episode reward:  3.000, mean reward:  0.038 [-1.000,  1.000], mean action: 19.744 [0.000, 37.000],  loss: 1.307312, mae: 0.285497, mean_q: 0.581054\n",
      "  1657/50000: episode: 45, duration: 0.181s, episode steps:  30, steps per second: 166, episode reward:  1.000, mean reward:  0.033 [-1.000,  1.000], mean action: 17.567 [0.000, 37.000],  loss: 1.181645, mae: 0.295769, mean_q: 0.579873\n",
      "  1757/50000: episode: 46, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.450 [0.000, 36.000],  loss: 1.542378, mae: 0.307732, mean_q: 0.598137\n",
      "  1759/50000: episode: 47, duration: 0.017s, episode steps:   2, steps per second: 120, episode reward:  1.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 32.500 [28.000, 37.000],  loss: 0.508419, mae: 0.314951, mean_q: 0.611573\n",
      "  1786/50000: episode: 48, duration: 0.157s, episode steps:  27, steps per second: 172, episode reward:  4.000, mean reward:  0.148 [-1.000,  1.000], mean action: 18.667 [3.000, 37.000],  loss: 0.512729, mae: 0.318754, mean_q: 0.615897\n",
      "  1879/50000: episode: 49, duration: 0.530s, episode steps:  93, steps per second: 176, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 20.129 [0.000, 37.000],  loss: 0.730769, mae: 0.332500, mean_q: 0.636767\n",
      "  1883/50000: episode: 50, duration: 0.029s, episode steps:   4, steps per second: 140, episode reward: -1.000, mean reward: -0.250 [-1.000,  1.000], mean action: 25.500 [17.000, 37.000],  loss: 0.483720, mae: 0.341765, mean_q: 0.645069\n",
      "  1983/50000: episode: 51, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.220 [1.000, 35.000],  loss: 0.503051, mae: 0.354057, mean_q: 0.658258\n",
      "  2076/50000: episode: 52, duration: 0.566s, episode steps:  93, steps per second: 164, episode reward: -2.000, mean reward: -0.022 [-1.000,  1.000], mean action: 17.860 [0.000, 37.000],  loss: 0.725837, mae: 0.375593, mean_q: 0.676956\n",
      "  2110/50000: episode: 53, duration: 0.196s, episode steps:  34, steps per second: 174, episode reward: -5.000, mean reward: -0.147 [-1.000,  1.000], mean action: 19.941 [2.000, 37.000],  loss: 0.495603, mae: 0.386445, mean_q: 0.684733\n",
      "  2123/50000: episode: 54, duration: 0.085s, episode steps:  13, steps per second: 153, episode reward: -6.000, mean reward: -0.462 [-1.000,  1.000], mean action: 18.308 [1.000, 37.000],  loss: 0.499783, mae: 0.390595, mean_q: 0.688695\n",
      "  2223/50000: episode: 55, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 16.000, mean reward:  0.160 [-1.000,  1.000], mean action: 17.100 [0.000, 36.000],  loss: 1.130341, mae: 0.401646, mean_q: 0.695740\n",
      "  2225/50000: episode: 56, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: -1.000, mean reward: -0.500 [-1.000,  0.000], mean action: 25.000 [13.000, 37.000],  loss: 0.522417, mae: 0.410165, mean_q: 0.705790\n",
      "  2257/50000: episode: 57, duration: 0.191s, episode steps:  32, steps per second: 168, episode reward: -3.000, mean reward: -0.094 [-1.000,  1.000], mean action: 19.312 [0.000, 37.000],  loss: 0.504215, mae: 0.413779, mean_q: 0.709882\n",
      "  2357/50000: episode: 58, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 17.360 [0.000, 36.000],  loss: 1.139846, mae: 0.429691, mean_q: 0.721737\n",
      "  2414/50000: episode: 59, duration: 0.338s, episode steps:  57, steps per second: 169, episode reward: -8.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.439 [0.000, 37.000],  loss: 0.869398, mae: 0.444557, mean_q: 0.724153\n",
      "  2429/50000: episode: 60, duration: 0.090s, episode steps:  15, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.933 [0.000, 37.000],  loss: 0.499911, mae: 0.451044, mean_q: 0.720963\n",
      "  2492/50000: episode: 61, duration: 0.365s, episode steps:  63, steps per second: 172, episode reward: 10.000, mean reward:  0.159 [-1.000,  1.000], mean action: 19.619 [1.000, 37.000],  loss: 0.507532, mae: 0.459568, mean_q: 0.724399\n",
      "  2592/50000: episode: 62, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 17.870 [0.000, 36.000],  loss: 0.927413, mae: 0.476468, mean_q: 0.750622\n",
      "  2692/50000: episode: 63, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 19.040 [0.000, 36.000],  loss: 1.115648, mae: 0.493903, mean_q: 0.762192\n",
      "  2792/50000: episode: 64, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 19.150 [1.000, 36.000],  loss: 0.710294, mae: 0.509708, mean_q: 0.779569\n",
      "  2803/50000: episode: 65, duration: 0.073s, episode steps:  11, steps per second: 151, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 16.909 [4.000, 37.000],  loss: 0.497677, mae: 0.519795, mean_q: 0.791252\n",
      "  2903/50000: episode: 66, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.980 [0.000, 36.000],  loss: 1.332700, mae: 0.530764, mean_q: 0.799084\n",
      "  2962/50000: episode: 67, duration: 0.371s, episode steps:  59, steps per second: 159, episode reward: -6.000, mean reward: -0.102 [-1.000,  1.000], mean action: 19.203 [0.000, 37.000],  loss: 0.488277, mae: 0.543480, mean_q: 0.813624\n",
      "  2983/50000: episode: 68, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: -6.000, mean reward: -0.286 [-1.000,  1.000], mean action: 16.476 [0.000, 37.000],  loss: 1.500775, mae: 0.551330, mean_q: 0.823331\n",
      "  3031/50000: episode: 69, duration: 0.369s, episode steps:  48, steps per second: 130, episode reward: -5.000, mean reward: -0.104 [-1.000,  1.000], mean action: 21.000 [0.000, 37.000],  loss: 0.946969, mae: 0.557045, mean_q: 0.825370\n",
      "  3131/50000: episode: 70, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: -20.000, mean reward: -0.200 [-1.000,  1.000], mean action: 17.950 [0.000, 36.000],  loss: 0.707860, mae: 0.570713, mean_q: 0.838284\n",
      "  3171/50000: episode: 71, duration: 0.270s, episode steps:  40, steps per second: 148, episode reward: -3.000, mean reward: -0.075 [-1.000,  1.000], mean action: 17.175 [0.000, 37.000],  loss: 1.016105, mae: 0.582957, mean_q: 0.848893\n",
      "  3254/50000: episode: 72, duration: 0.521s, episode steps:  83, steps per second: 159, episode reward: -14.000, mean reward: -0.169 [-1.000,  1.000], mean action: 20.783 [0.000, 37.000],  loss: 0.493112, mae: 0.591867, mean_q: 0.865202\n",
      "  3267/50000: episode: 73, duration: 0.079s, episode steps:  13, steps per second: 165, episode reward: -4.000, mean reward: -0.308 [-1.000,  1.000], mean action: 18.077 [3.000, 37.000],  loss: 0.480469, mae: 0.599494, mean_q: 0.879792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3329/50000: episode: 74, duration: 0.358s, episode steps:  62, steps per second: 173, episode reward: -15.000, mean reward: -0.242 [-1.000,  1.000], mean action: 20.371 [2.000, 37.000],  loss: 0.844432, mae: 0.607254, mean_q: 0.883779\n",
      "  3354/50000: episode: 75, duration: 0.147s, episode steps:  25, steps per second: 170, episode reward:  4.000, mean reward:  0.160 [-1.000,  1.000], mean action: 16.760 [0.000, 37.000],  loss: 0.489134, mae: 0.615614, mean_q: 0.888515\n",
      "  3392/50000: episode: 76, duration: 0.223s, episode steps:  38, steps per second: 170, episode reward:  7.000, mean reward:  0.184 [-1.000,  1.000], mean action: 18.816 [1.000, 37.000],  loss: 0.486714, mae: 0.620472, mean_q: 0.888068\n",
      "  3492/50000: episode: 77, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.640 [0.000, 36.000],  loss: 0.698074, mae: 0.631401, mean_q: 0.903439\n",
      "  3592/50000: episode: 78, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.460 [0.000, 36.000],  loss: 0.713059, mae: 0.648293, mean_q: 0.919865\n",
      "  3692/50000: episode: 79, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 19.000, mean reward:  0.190 [-1.000, 36.000], mean action: 17.370 [0.000, 36.000],  loss: 0.712552, mae: 0.667031, mean_q: 0.931337\n",
      "  3782/50000: episode: 80, duration: 0.533s, episode steps:  90, steps per second: 169, episode reward: -3.000, mean reward: -0.033 [-1.000,  1.000], mean action: 19.967 [0.000, 37.000],  loss: 0.728751, mae: 0.683700, mean_q: 0.941964\n",
      "  3802/50000: episode: 81, duration: 0.118s, episode steps:  20, steps per second: 169, episode reward: -9.000, mean reward: -0.450 [-1.000,  1.000], mean action: 19.150 [2.000, 37.000],  loss: 0.477399, mae: 0.690963, mean_q: 0.949659\n",
      "  3902/50000: episode: 82, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 15.620 [0.000, 36.000],  loss: 1.544812, mae: 0.700830, mean_q: 0.948956\n",
      "  3973/50000: episode: 83, duration: 0.446s, episode steps:  71, steps per second: 159, episode reward: -2.000, mean reward: -0.028 [-1.000,  1.000], mean action: 18.141 [0.000, 37.000],  loss: 1.379302, mae: 0.711692, mean_q: 0.951172\n",
      "  4040/50000: episode: 84, duration: 0.425s, episode steps:  67, steps per second: 157, episode reward: 14.000, mean reward:  0.209 [-1.000,  1.000], mean action: 17.985 [1.000, 37.000],  loss: 1.119203, mae: 0.720718, mean_q: 0.954032\n",
      "  4053/50000: episode: 85, duration: 0.080s, episode steps:  13, steps per second: 162, episode reward: -6.000, mean reward: -0.462 [-1.000,  1.000], mean action: 24.923 [5.000, 37.000],  loss: 2.118697, mae: 0.728306, mean_q: 0.961819\n",
      "  4098/50000: episode: 86, duration: 0.262s, episode steps:  45, steps per second: 172, episode reward:  4.000, mean reward:  0.089 [-1.000,  1.000], mean action: 20.378 [1.000, 37.000],  loss: 0.976165, mae: 0.731833, mean_q: 0.965854\n",
      "  4104/50000: episode: 87, duration: 0.040s, episode steps:   6, steps per second: 151, episode reward:  3.000, mean reward:  0.500 [-1.000,  1.000], mean action: 22.333 [9.000, 37.000],  loss: 4.028444, mae: 0.740418, mean_q: 0.967784\n",
      "  4204/50000: episode: 88, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.380 [0.000, 36.000],  loss: 1.755592, mae: 0.745137, mean_q: 0.972127\n",
      "  4246/50000: episode: 89, duration: 0.269s, episode steps:  42, steps per second: 156, episode reward: -3.000, mean reward: -0.071 [-1.000,  1.000], mean action: 18.310 [0.000, 37.000],  loss: 0.992502, mae: 0.753977, mean_q: 0.978548\n",
      "  4346/50000: episode: 90, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 16.890 [0.000, 36.000],  loss: 1.124593, mae: 0.763998, mean_q: 0.980341\n",
      "  4446/50000: episode: 91, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: -36.000, mean reward: -0.360 [-1.000,  1.000], mean action: 16.910 [0.000, 36.000],  loss: 1.331471, mae: 0.777504, mean_q: 1.002946\n",
      "  4520/50000: episode: 92, duration: 0.481s, episode steps:  74, steps per second: 154, episode reward: -11.000, mean reward: -0.149 [-1.000,  1.000], mean action: 18.797 [1.000, 37.000],  loss: 0.488358, mae: 0.786812, mean_q: 1.021982\n",
      "  4524/50000: episode: 93, duration: 0.028s, episode steps:   4, steps per second: 143, episode reward:  1.000, mean reward:  0.250 [-1.000,  1.000], mean action: 19.000 [8.000, 37.000],  loss: 0.501847, mae: 0.791167, mean_q: 1.024862\n",
      "  4624/50000: episode: 94, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 19.240 [0.000, 36.000],  loss: 0.694478, mae: 0.796645, mean_q: 1.031258\n",
      "  4724/50000: episode: 95, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 17.500 [0.000, 36.000],  loss: 1.333846, mae: 0.808967, mean_q: 1.027963\n",
      "  4793/50000: episode: 96, duration: 0.434s, episode steps:  69, steps per second: 159, episode reward: -22.000, mean reward: -0.319 [-1.000,  1.000], mean action: 17.261 [1.000, 37.000],  loss: 0.485050, mae: 0.818834, mean_q: 1.026290\n",
      "  4893/50000: episode: 97, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.760 [1.000, 36.000],  loss: 0.697078, mae: 0.827227, mean_q: 1.039088\n",
      "  4993/50000: episode: 98, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 18.580 [0.000, 36.000],  loss: 0.490135, mae: 0.837030, mean_q: 1.047500\n",
      "  5011/50000: episode: 99, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward:  3.000, mean reward:  0.167 [-1.000,  1.000], mean action: 16.333 [1.000, 37.000],  loss: 1.648654, mae: 0.845579, mean_q: 1.060177\n",
      "  5020/50000: episode: 100, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  2.000, mean reward:  0.222 [-1.000,  1.000], mean action: 17.556 [2.000, 37.000],  loss: 0.466257, mae: 0.844226, mean_q: 1.064720\n",
      "  5120/50000: episode: 101, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.490 [1.000, 34.000],  loss: 0.699362, mae: 0.849769, mean_q: 1.063413\n",
      "  5138/50000: episode: 102, duration: 0.107s, episode steps:  18, steps per second: 168, episode reward:  3.000, mean reward:  0.167 [-1.000,  1.000], mean action: 19.111 [3.000, 37.000],  loss: 0.500431, mae: 0.857019, mean_q: 1.058835\n",
      "  5186/50000: episode: 103, duration: 0.280s, episode steps:  48, steps per second: 171, episode reward:  1.000, mean reward:  0.021 [-1.000,  1.000], mean action: 17.979 [1.000, 37.000],  loss: 0.478918, mae: 0.860218, mean_q: 1.058506\n",
      "  5240/50000: episode: 104, duration: 0.321s, episode steps:  54, steps per second: 168, episode reward: -11.000, mean reward: -0.204 [-1.000,  1.000], mean action: 17.926 [0.000, 37.000],  loss: 0.486187, mae: 0.865043, mean_q: 1.054309\n",
      "  5340/50000: episode: 105, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 19.460 [0.000, 36.000],  loss: 0.909525, mae: 0.872412, mean_q: 1.061682\n",
      "  5440/50000: episode: 106, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 13.000, mean reward:  0.130 [-1.000, 36.000], mean action: 16.970 [0.000, 36.000],  loss: 0.697509, mae: 0.882014, mean_q: 1.082451\n",
      "  5540/50000: episode: 107, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.290 [0.000, 36.000],  loss: 0.701804, mae: 0.890851, mean_q: 1.085200\n",
      "  5640/50000: episode: 108, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 19.290 [0.000, 36.000],  loss: 0.485111, mae: 0.900214, mean_q: 1.101786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5740/50000: episode: 109, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.460 [0.000, 36.000],  loss: 0.695969, mae: 0.909205, mean_q: 1.107297\n",
      "  5836/50000: episode: 110, duration: 0.571s, episode steps:  96, steps per second: 168, episode reward: -7.000, mean reward: -0.073 [-1.000,  1.000], mean action: 17.510 [0.000, 37.000],  loss: 0.708194, mae: 0.918010, mean_q: 1.109317\n",
      "  5886/50000: episode: 111, duration: 0.318s, episode steps:  50, steps per second: 157, episode reward: -7.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.160 [0.000, 37.000],  loss: 0.494519, mae: 0.923960, mean_q: 1.119688\n",
      "  5986/50000: episode: 112, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 17.980 [0.000, 36.000],  loss: 0.696756, mae: 0.928687, mean_q: 1.110535\n",
      "  6086/50000: episode: 113, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 21.540 [1.000, 36.000],  loss: 0.703618, mae: 0.936376, mean_q: 1.118050\n",
      "  6089/50000: episode: 114, duration: 0.022s, episode steps:   3, steps per second: 134, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 16.667 [2.000, 37.000],  loss: 0.480579, mae: 0.940689, mean_q: 1.125997\n",
      "  6189/50000: episode: 115, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 17.210 [0.000, 36.000],  loss: 0.696396, mae: 0.945692, mean_q: 1.130504\n",
      "  6233/50000: episode: 116, duration: 0.270s, episode steps:  44, steps per second: 163, episode reward: -3.000, mean reward: -0.068 [-1.000,  1.000], mean action: 21.068 [2.000, 37.000],  loss: 0.486463, mae: 0.949179, mean_q: 1.140635\n",
      "  6316/50000: episode: 117, duration: 0.481s, episode steps:  83, steps per second: 172, episode reward:  8.000, mean reward:  0.096 [-1.000,  1.000], mean action: 18.217 [1.000, 37.000],  loss: 0.493159, mae: 0.953502, mean_q: 1.139944\n",
      "  6411/50000: episode: 118, duration: 0.544s, episode steps:  95, steps per second: 175, episode reward: -4.000, mean reward: -0.042 [-1.000,  1.000], mean action: 17.853 [1.000, 37.000],  loss: 0.932894, mae: 0.959138, mean_q: 1.140037\n",
      "  6511/50000: episode: 119, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.450 [0.000, 36.000],  loss: 0.485423, mae: 0.963915, mean_q: 1.154783\n",
      "  6569/50000: episode: 120, duration: 0.366s, episode steps:  58, steps per second: 158, episode reward: -7.000, mean reward: -0.121 [-1.000,  1.000], mean action: 16.241 [0.000, 37.000],  loss: 0.485682, mae: 0.967739, mean_q: 1.171542\n",
      "  6635/50000: episode: 121, duration: 0.408s, episode steps:  66, steps per second: 162, episode reward: -3.000, mean reward: -0.045 [-1.000,  1.000], mean action: 18.985 [1.000, 37.000],  loss: 1.135259, mae: 0.973920, mean_q: 1.179562\n",
      "  6735/50000: episode: 122, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 19.140 [1.000, 36.000],  loss: 0.703165, mae: 0.982600, mean_q: 1.184701\n",
      "  6835/50000: episode: 123, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.910 [1.000, 36.000],  loss: 0.701037, mae: 0.992028, mean_q: 1.191278\n",
      "  6839/50000: episode: 124, duration: 0.028s, episode steps:   4, steps per second: 143, episode reward:  1.000, mean reward:  0.250 [-1.000,  1.000], mean action: 28.000 [22.000, 37.000],  loss: 5.752208, mae: 1.002849, mean_q: 1.196990\n",
      "  6939/50000: episode: 125, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 19.900 [0.000, 36.000],  loss: 1.546200, mae: 1.002777, mean_q: 1.201488\n",
      "  7039/50000: episode: 126, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.780 [0.000, 36.000],  loss: 0.491369, mae: 1.011075, mean_q: 1.211597\n",
      "  7124/50000: episode: 127, duration: 0.490s, episode steps:  85, steps per second: 173, episode reward: -6.000, mean reward: -0.071 [-1.000,  1.000], mean action: 16.871 [0.000, 37.000],  loss: 1.230710, mae: 1.019974, mean_q: 1.214665\n",
      "  7216/50000: episode: 128, duration: 0.529s, episode steps:  92, steps per second: 174, episode reward:  7.000, mean reward:  0.076 [-1.000,  1.000], mean action: 17.098 [0.000, 37.000],  loss: 0.496226, mae: 1.027059, mean_q: 1.226581\n",
      "  7219/50000: episode: 129, duration: 0.023s, episode steps:   3, steps per second: 133, episode reward: -2.000, mean reward: -0.667 [-1.000,  0.000], mean action: 22.000 [5.000, 37.000],  loss: 0.478689, mae: 1.032251, mean_q: 1.231295\n",
      "  7232/50000: episode: 130, duration: 0.079s, episode steps:  13, steps per second: 164, episode reward: -2.000, mean reward: -0.154 [-1.000,  1.000], mean action: 18.154 [1.000, 37.000],  loss: 0.495119, mae: 1.032964, mean_q: 1.231649\n",
      "  7332/50000: episode: 131, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.370 [1.000, 36.000],  loss: 1.126682, mae: 1.040916, mean_q: 1.241726\n",
      "  7432/50000: episode: 132, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.510 [1.000, 36.000],  loss: 0.701254, mae: 1.050654, mean_q: 1.254960\n",
      "  7532/50000: episode: 133, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 17.830 [0.000, 36.000],  loss: 0.707677, mae: 1.060456, mean_q: 1.266488\n",
      "  7632/50000: episode: 134, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 19.470 [1.000, 36.000],  loss: 0.910962, mae: 1.070623, mean_q: 1.277230\n",
      "  7684/50000: episode: 135, duration: 0.361s, episode steps:  52, steps per second: 144, episode reward: -3.000, mean reward: -0.058 [-1.000,  1.000], mean action: 16.923 [1.000, 37.000],  loss: 0.487928, mae: 1.075539, mean_q: 1.281547\n",
      "  7729/50000: episode: 136, duration: 0.317s, episode steps:  45, steps per second: 142, episode reward:  4.000, mean reward:  0.089 [-1.000,  1.000], mean action: 14.467 [0.000, 37.000],  loss: 0.493839, mae: 1.079705, mean_q: 1.287660\n",
      "  7829/50000: episode: 137, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 19.240 [0.000, 36.000],  loss: 0.488759, mae: 1.084729, mean_q: 1.290686\n",
      "  7929/50000: episode: 138, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.050 [0.000, 36.000],  loss: 0.708992, mae: 1.091930, mean_q: 1.291693\n",
      "  7948/50000: episode: 139, duration: 0.115s, episode steps:  19, steps per second: 165, episode reward: -2.000, mean reward: -0.105 [-1.000,  1.000], mean action: 21.421 [4.000, 37.000],  loss: 0.503475, mae: 1.098370, mean_q: 1.287300\n",
      "  8048/50000: episode: 140, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 18.150 [0.000, 36.000],  loss: 0.701817, mae: 1.102106, mean_q: 1.293118\n",
      "  8148/50000: episode: 141, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 20.390 [0.000, 36.000],  loss: 0.700843, mae: 1.109277, mean_q: 1.299704\n",
      "  8248/50000: episode: 142, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.010 [0.000, 36.000],  loss: 0.699017, mae: 1.114663, mean_q: 1.306370\n",
      "  8348/50000: episode: 143, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 17.370 [0.000, 36.000],  loss: 0.910196, mae: 1.121198, mean_q: 1.319277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8448/50000: episode: 144, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 16.740 [0.000, 36.000],  loss: 0.708754, mae: 1.127749, mean_q: 1.326183\n",
      "  8512/50000: episode: 145, duration: 0.372s, episode steps:  64, steps per second: 172, episode reward: -5.000, mean reward: -0.078 [-1.000,  1.000], mean action: 17.969 [1.000, 37.000],  loss: 1.481320, mae: 1.133777, mean_q: 1.342596\n",
      "  8612/50000: episode: 146, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.110 [1.000, 36.000],  loss: 1.126159, mae: 1.138198, mean_q: 1.349930\n",
      "  8644/50000: episode: 147, duration: 0.195s, episode steps:  32, steps per second: 164, episode reward: -7.000, mean reward: -0.219 [-1.000,  1.000], mean action: 18.875 [0.000, 37.000],  loss: 0.495174, mae: 1.142061, mean_q: 1.353413\n",
      "  8724/50000: episode: 148, duration: 0.476s, episode steps:  80, steps per second: 168, episode reward:  1.000, mean reward:  0.013 [-1.000,  1.000], mean action: 18.250 [1.000, 37.000],  loss: 1.023680, mae: 1.147148, mean_q: 1.344308\n",
      "  8824/50000: episode: 149, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 17.320 [0.000, 36.000],  loss: 0.494582, mae: 1.154569, mean_q: 1.360631\n",
      "  8826/50000: episode: 150, duration: 0.016s, episode steps:   2, steps per second: 122, episode reward: -1.000, mean reward: -0.500 [-1.000,  0.000], mean action: 36.500 [36.000, 37.000],  loss: 0.521016, mae: 1.158911, mean_q: 1.370809\n",
      "  8926/50000: episode: 151, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 16.830 [0.000, 36.000],  loss: 0.702223, mae: 1.161998, mean_q: 1.378924\n",
      "  9026/50000: episode: 152, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.480 [1.000, 36.000],  loss: 0.706670, mae: 1.170522, mean_q: 1.394346\n",
      "  9126/50000: episode: 153, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 19.450 [1.000, 36.000],  loss: 0.915253, mae: 1.179951, mean_q: 1.412621\n",
      "  9180/50000: episode: 154, duration: 0.376s, episode steps:  54, steps per second: 144, episode reward: -13.000, mean reward: -0.241 [-1.000,  1.000], mean action: 19.852 [0.000, 37.000],  loss: 0.495647, mae: 1.187005, mean_q: 1.436115\n",
      "  9204/50000: episode: 155, duration: 0.145s, episode steps:  24, steps per second: 165, episode reward: -7.000, mean reward: -0.292 [-1.000,  1.000], mean action: 20.625 [2.000, 37.000],  loss: 0.481921, mae: 1.190431, mean_q: 1.451535\n",
      "  9304/50000: episode: 156, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 16.640 [0.000, 35.000],  loss: 0.704604, mae: 1.195953, mean_q: 1.469050\n",
      "  9404/50000: episode: 157, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 18.000, mean reward:  0.180 [-1.000,  1.000], mean action: 18.690 [0.000, 36.000],  loss: 0.501277, mae: 1.205721, mean_q: 1.467552\n",
      "  9405/50000: episode: 158, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 37.000 [37.000, 37.000],  loss: 0.528692, mae: 1.211840, mean_q: 1.467595\n",
      "  9423/50000: episode: 159, duration: 0.109s, episode steps:  18, steps per second: 166, episode reward: -1.000, mean reward: -0.056 [-1.000,  1.000], mean action: 21.278 [2.000, 37.000],  loss: 1.673778, mae: 1.214324, mean_q: 1.469359\n",
      "  9523/50000: episode: 160, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.970 [0.000, 36.000],  loss: 0.708541, mae: 1.219400, mean_q: 1.482433\n",
      "  9533/50000: episode: 161, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: -3.000, mean reward: -0.300 [-1.000,  1.000], mean action: 25.000 [11.000, 37.000],  loss: 0.493416, mae: 1.224062, mean_q: 1.481469\n",
      "  9633/50000: episode: 162, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.090 [0.000, 36.000],  loss: 0.925818, mae: 1.230943, mean_q: 1.469645\n",
      "  9721/50000: episode: 163, duration: 0.508s, episode steps:  88, steps per second: 173, episode reward: -7.000, mean reward: -0.080 [-1.000,  1.000], mean action: 16.682 [0.000, 37.000],  loss: 0.499438, mae: 1.240533, mean_q: 1.464369\n",
      "  9814/50000: episode: 164, duration: 0.539s, episode steps:  93, steps per second: 173, episode reward: -4.000, mean reward: -0.043 [-1.000,  1.000], mean action: 17.968 [1.000, 37.000],  loss: 0.498025, mae: 1.248749, mean_q: 1.472657\n",
      "  9914/50000: episode: 165, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 18.920 [0.000, 36.000],  loss: 0.704704, mae: 1.258891, mean_q: 1.489369\n",
      " 10014/50000: episode: 166, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 20.310 [1.000, 36.000],  loss: 0.921645, mae: 1.268392, mean_q: 1.493513\n",
      " 10114/50000: episode: 167, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.120 [1.000, 36.000],  loss: 0.707428, mae: 1.276516, mean_q: 1.500558\n",
      " 10140/50000: episode: 168, duration: 0.178s, episode steps:  26, steps per second: 146, episode reward: -11.000, mean reward: -0.423 [-1.000,  1.000], mean action: 16.923 [2.000, 37.000],  loss: 0.490658, mae: 1.281851, mean_q: 1.512864\n",
      " 10240/50000: episode: 169, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 17.960 [0.000, 36.000],  loss: 0.490414, mae: 1.286647, mean_q: 1.517118\n",
      " 10340/50000: episode: 170, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 16.490 [0.000, 35.000],  loss: 0.703589, mae: 1.296951, mean_q: 1.529157\n",
      " 10353/50000: episode: 171, duration: 0.087s, episode steps:  13, steps per second: 150, episode reward: -2.000, mean reward: -0.154 [-1.000,  1.000], mean action: 17.077 [1.000, 37.000],  loss: 0.499467, mae: 1.301396, mean_q: 1.539239\n",
      " 10395/50000: episode: 172, duration: 0.271s, episode steps:  42, steps per second: 155, episode reward: -15.000, mean reward: -0.357 [-1.000,  1.000], mean action: 19.333 [0.000, 37.000],  loss: 0.486451, mae: 1.303853, mean_q: 1.541702\n",
      " 10495/50000: episode: 173, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 16.990 [0.000, 36.000],  loss: 0.492261, mae: 1.308812, mean_q: 1.541687\n",
      " 10595/50000: episode: 174, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 19.230 [0.000, 36.000],  loss: 0.709869, mae: 1.317539, mean_q: 1.549808\n",
      " 10695/50000: episode: 175, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 12.000, mean reward:  0.120 [-1.000,  1.000], mean action: 17.700 [0.000, 36.000],  loss: 0.496926, mae: 1.326516, mean_q: 1.555679\n",
      " 10795/50000: episode: 176, duration: 0.660s, episode steps: 100, steps per second: 151, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 18.340 [0.000, 36.000],  loss: 0.490511, mae: 1.333387, mean_q: 1.573878\n",
      " 10895/50000: episode: 177, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.750 [0.000, 35.000],  loss: 0.505598, mae: 1.341076, mean_q: 1.577048\n",
      " 10961/50000: episode: 178, duration: 0.387s, episode steps:  66, steps per second: 170, episode reward: -1.000, mean reward: -0.015 [-1.000,  1.000], mean action: 18.667 [0.000, 37.000],  loss: 0.824080, mae: 1.350073, mean_q: 1.586277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11061/50000: episode: 179, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 12.000, mean reward:  0.120 [-1.000,  1.000], mean action: 19.720 [1.000, 36.000],  loss: 0.495003, mae: 1.357636, mean_q: 1.588710\n",
      " 11161/50000: episode: 180, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 17.770 [0.000, 36.000],  loss: 0.501512, mae: 1.365465, mean_q: 1.603818\n",
      " 11227/50000: episode: 181, duration: 0.476s, episode steps:  66, steps per second: 139, episode reward: -17.000, mean reward: -0.258 [-1.000,  1.000], mean action: 19.818 [0.000, 37.000],  loss: 0.819046, mae: 1.373986, mean_q: 1.606990\n",
      " 11327/50000: episode: 182, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.560 [0.000, 36.000],  loss: 0.496797, mae: 1.379809, mean_q: 1.607665\n",
      " 11409/50000: episode: 183, duration: 0.486s, episode steps:  82, steps per second: 169, episode reward:  7.000, mean reward:  0.085 [-1.000,  1.000], mean action: 18.585 [1.000, 37.000],  loss: 0.755185, mae: 1.386731, mean_q: 1.605166\n",
      " 11420/50000: episode: 184, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 21.727 [2.000, 37.000],  loss: 0.483613, mae: 1.388443, mean_q: 1.605736\n",
      " 11451/50000: episode: 185, duration: 0.202s, episode steps:  31, steps per second: 153, episode reward: -8.000, mean reward: -0.258 [-1.000,  1.000], mean action: 19.226 [1.000, 37.000],  loss: 1.184596, mae: 1.390868, mean_q: 1.607247\n",
      " 11551/50000: episode: 186, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -20.000, mean reward: -0.200 [-1.000,  1.000], mean action: 17.280 [0.000, 36.000],  loss: 0.497276, mae: 1.395307, mean_q: 1.617172\n",
      " 11651/50000: episode: 187, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.630 [0.000, 36.000],  loss: 0.500305, mae: 1.401277, mean_q: 1.622397\n",
      " 11710/50000: episode: 188, duration: 0.368s, episode steps:  59, steps per second: 160, episode reward: 10.000, mean reward:  0.169 [-1.000,  1.000], mean action: 18.746 [0.000, 37.000],  loss: 0.850009, mae: 1.408058, mean_q: 1.631638\n",
      " 11810/50000: episode: 189, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 17.880 [1.000, 36.000],  loss: 0.710680, mae: 1.413450, mean_q: 1.651273\n",
      " 11885/50000: episode: 190, duration: 0.438s, episode steps:  75, steps per second: 171, episode reward: -2.000, mean reward: -0.027 [-1.000,  1.000], mean action: 18.173 [0.000, 37.000],  loss: 0.782873, mae: 1.420819, mean_q: 1.662207\n",
      " 11985/50000: episode: 191, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 16.910 [1.000, 36.000],  loss: 0.717414, mae: 1.426430, mean_q: 1.672626\n",
      " 12085/50000: episode: 192, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 18.030 [0.000, 36.000],  loss: 0.716720, mae: 1.435712, mean_q: 1.679131\n",
      " 12185/50000: episode: 193, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.910 [0.000, 35.000],  loss: 0.499448, mae: 1.444481, mean_q: 1.690121\n",
      " 12285/50000: episode: 194, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.250 [0.000, 36.000],  loss: 0.504563, mae: 1.454850, mean_q: 1.696336\n",
      " 12385/50000: episode: 195, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 17.960 [1.000, 36.000],  loss: 0.714321, mae: 1.464256, mean_q: 1.699135\n",
      " 12485/50000: episode: 196, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 18.060 [1.000, 36.000],  loss: 0.497377, mae: 1.472049, mean_q: 1.710125\n",
      " 12562/50000: episode: 197, duration: 0.456s, episode steps:  77, steps per second: 169, episode reward: -10.000, mean reward: -0.130 [-1.000,  1.000], mean action: 18.753 [1.000, 37.000],  loss: 0.503346, mae: 1.481819, mean_q: 1.708123\n",
      " 12662/50000: episode: 198, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 20.010 [0.000, 36.000],  loss: 0.714832, mae: 1.489783, mean_q: 1.705878\n",
      " 12715/50000: episode: 199, duration: 0.313s, episode steps:  53, steps per second: 169, episode reward: -20.000, mean reward: -0.377 [-1.000,  1.000], mean action: 18.245 [0.000, 37.000],  loss: 0.500000, mae: 1.495080, mean_q: 1.714863\n",
      " 12726/50000: episode: 200, duration: 0.068s, episode steps:  11, steps per second: 161, episode reward: -4.000, mean reward: -0.364 [-1.000,  1.000], mean action: 14.909 [3.000, 37.000],  loss: 0.520962, mae: 1.497516, mean_q: 1.722861\n",
      " 12826/50000: episode: 201, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 17.580 [1.000, 36.000],  loss: 0.498923, mae: 1.500891, mean_q: 1.723697\n",
      " 12926/50000: episode: 202, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 17.560 [0.000, 36.000],  loss: 0.924919, mae: 1.508608, mean_q: 1.736643\n",
      " 12965/50000: episode: 203, duration: 0.279s, episode steps:  39, steps per second: 140, episode reward: -10.000, mean reward: -0.256 [-1.000,  1.000], mean action: 18.949 [1.000, 37.000],  loss: 1.053123, mae: 1.513827, mean_q: 1.743915\n",
      " 13065/50000: episode: 204, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 18.270 [0.000, 36.000],  loss: 0.714715, mae: 1.518100, mean_q: 1.756361\n",
      " 13165/50000: episode: 205, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.200 [0.000, 36.000],  loss: 0.711854, mae: 1.528176, mean_q: 1.753129\n",
      " 13265/50000: episode: 206, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.970 [1.000, 36.000],  loss: 0.499290, mae: 1.535471, mean_q: 1.751793\n",
      " 13365/50000: episode: 207, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 19.610 [0.000, 36.000],  loss: 0.923214, mae: 1.541037, mean_q: 1.742403\n",
      " 13418/50000: episode: 208, duration: 0.313s, episode steps:  53, steps per second: 169, episode reward: -2.000, mean reward: -0.038 [-1.000,  1.000], mean action: 19.075 [0.000, 37.000],  loss: 0.904338, mae: 1.544687, mean_q: 1.740357\n",
      " 13492/50000: episode: 209, duration: 0.431s, episode steps:  74, steps per second: 172, episode reward: -15.000, mean reward: -0.203 [-1.000,  1.000], mean action: 18.986 [0.000, 37.000],  loss: 0.500910, mae: 1.547642, mean_q: 1.741927\n",
      " 13539/50000: episode: 210, duration: 0.299s, episode steps:  47, steps per second: 157, episode reward: -12.000, mean reward: -0.255 [-1.000,  1.000], mean action: 19.234 [0.000, 37.000],  loss: 0.499310, mae: 1.551629, mean_q: 1.741891\n",
      " 13572/50000: episode: 211, duration: 0.195s, episode steps:  33, steps per second: 169, episode reward:  6.000, mean reward:  0.182 [-1.000,  1.000], mean action: 20.848 [0.000, 37.000],  loss: 0.491956, mae: 1.552771, mean_q: 1.743896\n",
      " 13598/50000: episode: 212, duration: 0.160s, episode steps:  26, steps per second: 162, episode reward:  7.000, mean reward:  0.269 [-1.000,  1.000], mean action: 16.769 [2.000, 37.000],  loss: 1.320598, mae: 1.555296, mean_q: 1.745573\n",
      " 13675/50000: episode: 213, duration: 0.450s, episode steps:  77, steps per second: 171, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.182 [1.000, 37.000],  loss: 1.048758, mae: 1.558180, mean_q: 1.756653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13775/50000: episode: 214, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 20.820 [1.000, 36.000],  loss: 0.496770, mae: 1.560336, mean_q: 1.760649\n",
      " 13875/50000: episode: 215, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 18.710 [1.000, 36.000],  loss: 0.704427, mae: 1.562714, mean_q: 1.760864\n",
      " 13975/50000: episode: 216, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 17.050 [1.000, 36.000],  loss: 0.714172, mae: 1.567259, mean_q: 1.770627\n",
      " 14075/50000: episode: 217, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 19.920 [1.000, 36.000],  loss: 1.135584, mae: 1.572985, mean_q: 1.788957\n",
      " 14175/50000: episode: 218, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.540 [0.000, 36.000],  loss: 0.715656, mae: 1.578994, mean_q: 1.806634\n",
      " 14191/50000: episode: 219, duration: 0.098s, episode steps:  16, steps per second: 164, episode reward: -1.000, mean reward: -0.062 [-1.000,  1.000], mean action: 23.125 [9.000, 37.000],  loss: 0.499049, mae: 1.582728, mean_q: 1.817285\n",
      " 14291/50000: episode: 220, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.020 [0.000, 36.000],  loss: 0.501429, mae: 1.586820, mean_q: 1.817267\n",
      " 14294/50000: episode: 221, duration: 0.023s, episode steps:   3, steps per second: 130, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 26.000 [19.000, 37.000],  loss: 0.484470, mae: 1.591345, mean_q: 1.808855\n",
      " 14394/50000: episode: 222, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 20.450 [0.000, 36.000],  loss: 0.501197, mae: 1.595429, mean_q: 1.818945\n",
      " 14494/50000: episode: 223, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -23.000, mean reward: -0.230 [-1.000,  1.000], mean action: 19.110 [0.000, 37.000],  loss: 0.498783, mae: 1.603593, mean_q: 1.825488\n",
      " 14540/50000: episode: 224, duration: 0.298s, episode steps:  46, steps per second: 154, episode reward: -9.000, mean reward: -0.196 [-1.000,  1.000], mean action: 20.826 [1.000, 37.000],  loss: 0.506458, mae: 1.609698, mean_q: 1.836698\n",
      " 14640/50000: episode: 225, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.290 [0.000, 36.000],  loss: 0.710748, mae: 1.614893, mean_q: 1.836812\n",
      " 14740/50000: episode: 226, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 19.720 [1.000, 36.000],  loss: 0.717018, mae: 1.621360, mean_q: 1.844864\n",
      " 14840/50000: episode: 227, duration: 1.117s, episode steps: 100, steps per second:  90, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 19.000 [0.000, 36.000],  loss: 0.712063, mae: 1.626186, mean_q: 1.841831\n",
      " 14886/50000: episode: 228, duration: 0.583s, episode steps:  46, steps per second:  79, episode reward: 22.000, mean reward:  0.478 [-1.000, 36.000], mean action: 18.435 [0.000, 37.000],  loss: 0.498916, mae: 1.627464, mean_q: 1.843771\n",
      " 14982/50000: episode: 229, duration: 1.139s, episode steps:  96, steps per second:  84, episode reward: -9.000, mean reward: -0.094 [-1.000,  1.000], mean action: 18.792 [0.000, 37.000],  loss: 0.944471, mae: 1.630499, mean_q: 1.846242\n",
      " 15082/50000: episode: 230, duration: 1.198s, episode steps: 100, steps per second:  83, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 18.250 [0.000, 36.000],  loss: 0.501579, mae: 1.635625, mean_q: 1.864450\n",
      " 15117/50000: episode: 231, duration: 0.429s, episode steps:  35, steps per second:  82, episode reward:  6.000, mean reward:  0.171 [-1.000,  1.000], mean action: 16.171 [0.000, 37.000],  loss: 0.506881, mae: 1.639254, mean_q: 1.867754\n",
      " 15217/50000: episode: 232, duration: 1.194s, episode steps: 100, steps per second:  84, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 19.660 [0.000, 36.000],  loss: 0.501143, mae: 1.645655, mean_q: 1.873442\n",
      " 15317/50000: episode: 233, duration: 1.188s, episode steps: 100, steps per second:  84, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 19.130 [1.000, 36.000],  loss: 0.501030, mae: 1.649564, mean_q: 1.887349\n",
      " 15417/50000: episode: 234, duration: 1.199s, episode steps: 100, steps per second:  83, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 20.280 [1.000, 35.000],  loss: 1.144138, mae: 1.656829, mean_q: 1.904659\n",
      " 15495/50000: episode: 235, duration: 0.933s, episode steps:  78, steps per second:  84, episode reward: -17.000, mean reward: -0.218 [-1.000,  1.000], mean action: 18.795 [0.000, 37.000],  loss: 0.511273, mae: 1.662941, mean_q: 1.915513\n",
      " 15595/50000: episode: 236, duration: 1.192s, episode steps: 100, steps per second:  84, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 19.970 [0.000, 36.000],  loss: 0.504116, mae: 1.668134, mean_q: 1.915926\n",
      " 15695/50000: episode: 237, duration: 1.376s, episode steps: 100, steps per second:  73, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.180 [1.000, 36.000],  loss: 0.504512, mae: 1.674896, mean_q: 1.911560\n",
      " 15731/50000: episode: 238, duration: 0.445s, episode steps:  36, steps per second:  81, episode reward: -5.000, mean reward: -0.139 [-1.000,  1.000], mean action: 18.556 [0.000, 37.000],  loss: 1.097940, mae: 1.680587, mean_q: 1.892075\n",
      " 15779/50000: episode: 239, duration: 0.584s, episode steps:  48, steps per second:  82, episode reward: -1.000, mean reward: -0.021 [-1.000,  1.000], mean action: 17.667 [0.000, 37.000],  loss: 0.505188, mae: 1.683303, mean_q: 1.894338\n",
      " 15859/50000: episode: 240, duration: 0.955s, episode steps:  80, steps per second:  84, episode reward:  7.000, mean reward:  0.087 [-1.000,  1.000], mean action: 19.750 [0.000, 37.000],  loss: 0.501451, mae: 1.687544, mean_q: 1.902273\n",
      " 15959/50000: episode: 241, duration: 1.192s, episode steps: 100, steps per second:  84, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.330 [0.000, 36.000],  loss: 0.922754, mae: 1.693274, mean_q: 1.914933\n",
      " 16059/50000: episode: 242, duration: 1.193s, episode steps: 100, steps per second:  84, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 17.720 [0.000, 36.000],  loss: 0.504332, mae: 1.697786, mean_q: 1.906754\n",
      " 16159/50000: episode: 243, duration: 1.211s, episode steps: 100, steps per second:  83, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 19.410 [0.000, 36.000],  loss: 0.515002, mae: 1.701401, mean_q: 1.904141\n",
      " 16259/50000: episode: 244, duration: 1.194s, episode steps: 100, steps per second:  84, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 19.220 [1.000, 36.000],  loss: 0.715584, mae: 1.706293, mean_q: 1.905952\n",
      " 16359/50000: episode: 245, duration: 1.189s, episode steps: 100, steps per second:  84, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 17.560 [0.000, 36.000],  loss: 0.499374, mae: 1.711018, mean_q: 1.902511\n",
      " 16459/50000: episode: 246, duration: 1.202s, episode steps: 100, steps per second:  83, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.550 [0.000, 36.000],  loss: 0.508330, mae: 1.715763, mean_q: 1.894768\n",
      " 16468/50000: episode: 247, duration: 0.118s, episode steps:   9, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.444 [6.000, 37.000],  loss: 0.484008, mae: 1.717205, mean_q: 1.885665\n",
      " 16497/50000: episode: 248, duration: 0.358s, episode steps:  29, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.310 [2.000, 37.000],  loss: 0.486632, mae: 1.717133, mean_q: 1.887148\n",
      " 16509/50000: episode: 249, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward: -7.000, mean reward: -0.583 [-1.000,  1.000], mean action: 16.500 [4.000, 37.000],  loss: 0.495529, mae: 1.717562, mean_q: 1.891786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16516/50000: episode: 250, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward:  2.000, mean reward:  0.286 [-1.000,  1.000], mean action: 24.000 [14.000, 37.000],  loss: 0.497095, mae: 1.717655, mean_q: 1.893322\n",
      " 16616/50000: episode: 251, duration: 1.190s, episode steps: 100, steps per second:  84, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 20.320 [0.000, 36.000],  loss: 0.499561, mae: 1.717838, mean_q: 1.895601\n",
      " 16716/50000: episode: 252, duration: 1.196s, episode steps: 100, steps per second:  84, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 19.050 [0.000, 36.000],  loss: 0.503277, mae: 1.719669, mean_q: 1.893147\n",
      " 16816/50000: episode: 253, duration: 1.191s, episode steps: 100, steps per second:  84, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 20.060 [0.000, 36.000],  loss: 0.717643, mae: 1.720493, mean_q: 1.892837\n",
      " 16868/50000: episode: 254, duration: 0.631s, episode steps:  52, steps per second:  82, episode reward: 40.000, mean reward:  0.769 [-1.000, 36.000], mean action: 18.904 [0.000, 37.000],  loss: 0.903885, mae: 1.721701, mean_q: 1.905154\n",
      " 16938/50000: episode: 255, duration: 0.837s, episode steps:  70, steps per second:  84, episode reward: -15.000, mean reward: -0.214 [-1.000,  1.000], mean action: 17.086 [0.000, 37.000],  loss: 0.804490, mae: 1.721497, mean_q: 1.903616\n",
      " 16974/50000: episode: 256, duration: 0.439s, episode steps:  36, steps per second:  82, episode reward: -1.000, mean reward: -0.028 [-1.000,  1.000], mean action: 17.833 [1.000, 37.000],  loss: 1.112661, mae: 1.721585, mean_q: 1.893463\n",
      " 17074/50000: episode: 257, duration: 1.187s, episode steps: 100, steps per second:  84, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 19.030 [0.000, 36.000],  loss: 0.925296, mae: 1.721156, mean_q: 1.882800\n",
      " 17093/50000: episode: 258, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward: -6.000, mean reward: -0.316 [-1.000,  1.000], mean action: 21.526 [3.000, 37.000],  loss: 0.506605, mae: 1.721137, mean_q: 1.881018\n",
      " 17193/50000: episode: 259, duration: 1.195s, episode steps: 100, steps per second:  84, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.590 [1.000, 36.000],  loss: 1.129462, mae: 1.723094, mean_q: 1.884894\n",
      " 17293/50000: episode: 260, duration: 1.199s, episode steps: 100, steps per second:  83, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.220 [1.000, 36.000],  loss: 1.135210, mae: 1.724756, mean_q: 1.890471\n",
      " 17393/50000: episode: 261, duration: 1.190s, episode steps: 100, steps per second:  84, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 16.220 [0.000, 36.000],  loss: 0.505916, mae: 1.725571, mean_q: 1.899945\n",
      " 17412/50000: episode: 262, duration: 0.239s, episode steps:  19, steps per second:  79, episode reward: -8.000, mean reward: -0.421 [-1.000,  1.000], mean action: 18.684 [0.000, 37.000],  loss: 0.503372, mae: 1.725838, mean_q: 1.897166\n",
      " 17512/50000: episode: 263, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: 43.000, mean reward:  0.430 [-1.000, 36.000], mean action: 18.350 [0.000, 36.000],  loss: 0.921010, mae: 1.728156, mean_q: 1.895699\n",
      " 17612/50000: episode: 264, duration: 1.386s, episode steps: 100, steps per second:  72, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 18.470 [0.000, 36.000],  loss: 0.492072, mae: 1.728423, mean_q: 1.881930\n",
      " 17712/50000: episode: 265, duration: 1.315s, episode steps: 100, steps per second:  76, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.860 [0.000, 36.000],  loss: 0.715046, mae: 1.728983, mean_q: 1.884193\n",
      " 17812/50000: episode: 266, duration: 1.287s, episode steps: 100, steps per second:  78, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 16.510 [0.000, 36.000],  loss: 0.920181, mae: 1.728535, mean_q: 1.874593\n",
      " 17912/50000: episode: 267, duration: 1.231s, episode steps: 100, steps per second:  81, episode reward: -28.000, mean reward: -0.280 [-1.000,  1.000], mean action: 16.890 [0.000, 36.000],  loss: 0.503601, mae: 1.727708, mean_q: 1.876828\n",
      " 18012/50000: episode: 268, duration: 1.283s, episode steps: 100, steps per second:  78, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.470 [1.000, 36.000],  loss: 0.496614, mae: 1.726085, mean_q: 1.868476\n",
      " 18096/50000: episode: 269, duration: 1.040s, episode steps:  84, steps per second:  81, episode reward: 11.000, mean reward:  0.131 [-1.000,  1.000], mean action: 17.476 [0.000, 37.000],  loss: 0.498955, mae: 1.722948, mean_q: 1.865957\n",
      " 18196/50000: episode: 270, duration: 1.375s, episode steps: 100, steps per second:  73, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 21.030 [1.000, 36.000],  loss: 0.915618, mae: 1.723636, mean_q: 1.868529\n",
      " 18296/50000: episode: 271, duration: 1.272s, episode steps: 100, steps per second:  79, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 17.340 [0.000, 35.000],  loss: 0.705492, mae: 1.723018, mean_q: 1.873783\n",
      " 18396/50000: episode: 272, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.360 [1.000, 36.000],  loss: 0.707095, mae: 1.723382, mean_q: 1.873245\n",
      " 18489/50000: episode: 273, duration: 1.182s, episode steps:  93, steps per second:  79, episode reward: -2.000, mean reward: -0.022 [-1.000,  1.000], mean action: 18.290 [1.000, 37.000],  loss: 0.503452, mae: 1.723862, mean_q: 1.878039\n",
      " 18556/50000: episode: 274, duration: 0.816s, episode steps:  67, steps per second:  82, episode reward: -2.000, mean reward: -0.030 [-1.000,  1.000], mean action: 16.478 [0.000, 37.000],  loss: 0.820389, mae: 1.724456, mean_q: 1.877898\n",
      " 18656/50000: episode: 275, duration: 1.189s, episode steps: 100, steps per second:  84, episode reward: 12.000, mean reward:  0.120 [-1.000,  1.000], mean action: 17.550 [0.000, 36.000],  loss: 0.707254, mae: 1.724032, mean_q: 1.869207\n",
      " 18756/50000: episode: 276, duration: 1.198s, episode steps: 100, steps per second:  83, episode reward: 20.000, mean reward:  0.200 [-1.000,  1.000], mean action: 20.700 [0.000, 36.000],  loss: 0.914443, mae: 1.724584, mean_q: 1.875109\n",
      " 18847/50000: episode: 277, duration: 1.092s, episode steps:  91, steps per second:  83, episode reward: -8.000, mean reward: -0.088 [-1.000,  1.000], mean action: 16.615 [1.000, 37.000],  loss: 0.499371, mae: 1.724439, mean_q: 1.874110\n",
      " 18947/50000: episode: 278, duration: 1.200s, episode steps: 100, steps per second:  83, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.460 [0.000, 36.000],  loss: 0.709044, mae: 1.724038, mean_q: 1.877026\n",
      " 19047/50000: episode: 279, duration: 1.199s, episode steps: 100, steps per second:  83, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.360 [0.000, 36.000],  loss: 0.923656, mae: 1.723527, mean_q: 1.872451\n",
      " 19147/50000: episode: 280, duration: 1.196s, episode steps: 100, steps per second:  84, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.450 [0.000, 36.000],  loss: 0.494826, mae: 1.724876, mean_q: 1.870165\n",
      " 19247/50000: episode: 281, duration: 1.201s, episode steps: 100, steps per second:  83, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.200 [0.000, 36.000],  loss: 0.501078, mae: 1.725687, mean_q: 1.874910\n",
      " 19347/50000: episode: 282, duration: 1.200s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 21.110 [1.000, 36.000],  loss: 0.926300, mae: 1.724468, mean_q: 1.869448\n",
      " 19408/50000: episode: 283, duration: 0.743s, episode steps:  61, steps per second:  82, episode reward:  6.000, mean reward:  0.098 [-1.000,  1.000], mean action: 17.574 [0.000, 37.000],  loss: 0.494825, mae: 1.722185, mean_q: 1.874421\n",
      " 19496/50000: episode: 284, duration: 1.055s, episode steps:  88, steps per second:  83, episode reward: 19.000, mean reward:  0.216 [-1.000,  1.000], mean action: 19.420 [0.000, 37.000],  loss: 0.734287, mae: 1.722003, mean_q: 1.869036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19535/50000: episode: 285, duration: 0.485s, episode steps:  39, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.179 [2.000, 37.000],  loss: 1.048471, mae: 1.723696, mean_q: 1.864921\n",
      " 19635/50000: episode: 286, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.050 [0.000, 36.000],  loss: 0.921980, mae: 1.724815, mean_q: 1.870423\n",
      " 19735/50000: episode: 287, duration: 1.196s, episode steps: 100, steps per second:  84, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.980 [1.000, 35.000],  loss: 0.507679, mae: 1.723826, mean_q: 1.877504\n",
      " 19835/50000: episode: 288, duration: 1.194s, episode steps: 100, steps per second:  84, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 20.250 [0.000, 36.000],  loss: 0.509084, mae: 1.724358, mean_q: 1.883993\n",
      " 19935/50000: episode: 289, duration: 1.197s, episode steps: 100, steps per second:  84, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 19.370 [0.000, 36.000],  loss: 0.715467, mae: 1.725320, mean_q: 1.891340\n",
      " 19957/50000: episode: 290, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward: 11.000, mean reward:  0.500 [-1.000,  1.000], mean action: 23.045 [2.000, 37.000],  loss: 0.497862, mae: 1.726234, mean_q: 1.887466\n",
      " 20057/50000: episode: 291, duration: 1.194s, episode steps: 100, steps per second:  84, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 16.730 [1.000, 36.000],  loss: 0.713433, mae: 1.727789, mean_q: 1.893202\n",
      " 20157/50000: episode: 292, duration: 1.202s, episode steps: 100, steps per second:  83, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 19.260 [0.000, 36.000],  loss: 0.710666, mae: 1.729941, mean_q: 1.914250\n",
      " 20257/50000: episode: 293, duration: 1.196s, episode steps: 100, steps per second:  84, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.760 [1.000, 36.000],  loss: 0.704286, mae: 1.732473, mean_q: 1.919995\n",
      " 20357/50000: episode: 294, duration: 1.196s, episode steps: 100, steps per second:  84, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.570 [0.000, 36.000],  loss: 0.499896, mae: 1.735858, mean_q: 1.924867\n",
      " 20457/50000: episode: 295, duration: 1.198s, episode steps: 100, steps per second:  84, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.000 [0.000, 36.000],  loss: 0.929680, mae: 1.741158, mean_q: 1.920124\n",
      " 20482/50000: episode: 296, duration: 0.314s, episode steps:  25, steps per second:  80, episode reward: -12.000, mean reward: -0.480 [-1.000,  1.000], mean action: 15.960 [1.000, 37.000],  loss: 0.511412, mae: 1.741290, mean_q: 1.922693\n",
      " 20582/50000: episode: 297, duration: 1.203s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 17.730 [0.000, 36.000],  loss: 0.510539, mae: 1.742577, mean_q: 1.914832\n",
      " 20682/50000: episode: 298, duration: 1.201s, episode steps: 100, steps per second:  83, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 19.040 [0.000, 36.000],  loss: 0.921659, mae: 1.744877, mean_q: 1.909566\n",
      " 20742/50000: episode: 299, duration: 0.728s, episode steps:  60, steps per second:  82, episode reward:  7.000, mean reward:  0.117 [-1.000,  1.000], mean action: 19.550 [0.000, 37.000],  loss: 0.508466, mae: 1.747030, mean_q: 1.897474\n",
      " 20783/50000: episode: 300, duration: 0.505s, episode steps:  41, steps per second:  81, episode reward:  2.000, mean reward:  0.049 [-1.000,  1.000], mean action: 18.902 [2.000, 37.000],  loss: 0.495031, mae: 1.748321, mean_q: 1.893749\n",
      " 20864/50000: episode: 301, duration: 0.978s, episode steps:  81, steps per second:  83, episode reward: 12.000, mean reward:  0.148 [-1.000,  1.000], mean action: 19.111 [0.000, 37.000],  loss: 1.275603, mae: 1.751068, mean_q: 1.898184\n",
      " 20935/50000: episode: 302, duration: 0.859s, episode steps:  71, steps per second:  83, episode reward:  8.000, mean reward:  0.113 [-1.000,  1.000], mean action: 17.817 [0.000, 37.000],  loss: 0.498337, mae: 1.750939, mean_q: 1.905369\n",
      " 20999/50000: episode: 303, duration: 0.780s, episode steps:  64, steps per second:  82, episode reward: -7.000, mean reward: -0.109 [-1.000,  1.000], mean action: 18.828 [0.000, 37.000],  loss: 0.498217, mae: 1.751443, mean_q: 1.919976\n",
      " 21087/50000: episode: 304, duration: 1.057s, episode steps:  88, steps per second:  83, episode reward:  1.000, mean reward:  0.011 [-1.000,  1.000], mean action: 18.511 [0.000, 37.000],  loss: 0.496742, mae: 1.753285, mean_q: 1.928363\n",
      " 21187/50000: episode: 305, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.470 [0.000, 36.000],  loss: 0.496832, mae: 1.755099, mean_q: 1.942421\n",
      " 21287/50000: episode: 306, duration: 1.280s, episode steps: 100, steps per second:  78, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 20.160 [0.000, 36.000],  loss: 0.715518, mae: 1.757396, mean_q: 1.944590\n",
      " 21387/50000: episode: 307, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 18.850 [1.000, 36.000],  loss: 0.709828, mae: 1.760625, mean_q: 1.956954\n",
      " 21487/50000: episode: 308, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 19.420 [0.000, 36.000],  loss: 0.716972, mae: 1.765491, mean_q: 1.969017\n",
      " 21587/50000: episode: 309, duration: 1.288s, episode steps: 100, steps per second:  78, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.560 [0.000, 36.000],  loss: 0.928854, mae: 1.772896, mean_q: 1.987958\n",
      " 21687/50000: episode: 310, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.420 [0.000, 36.000],  loss: 0.710634, mae: 1.777535, mean_q: 1.978146\n",
      " 21787/50000: episode: 311, duration: 1.257s, episode steps: 100, steps per second:  80, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.030 [0.000, 36.000],  loss: 0.714130, mae: 1.782260, mean_q: 1.983220\n",
      " 21825/50000: episode: 312, duration: 0.490s, episode steps:  38, steps per second:  78, episode reward: -5.000, mean reward: -0.132 [-1.000,  1.000], mean action: 17.263 [0.000, 37.000],  loss: 0.500471, mae: 1.784437, mean_q: 1.978307\n",
      " 21925/50000: episode: 313, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 16.430 [0.000, 35.000],  loss: 0.916791, mae: 1.786782, mean_q: 1.983742\n",
      " 22025/50000: episode: 314, duration: 1.204s, episode steps: 100, steps per second:  83, episode reward: 16.000, mean reward:  0.160 [-1.000,  1.000], mean action: 18.010 [1.000, 36.000],  loss: 0.717523, mae: 1.790745, mean_q: 1.994459\n",
      " 22051/50000: episode: 315, duration: 0.334s, episode steps:  26, steps per second:  78, episode reward: -1.000, mean reward: -0.038 [-1.000,  1.000], mean action: 18.308 [1.000, 37.000],  loss: 2.111880, mae: 1.796157, mean_q: 2.000443\n",
      " 22151/50000: episode: 316, duration: 1.214s, episode steps: 100, steps per second:  82, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.200 [0.000, 36.000],  loss: 0.717181, mae: 1.797837, mean_q: 1.999385\n",
      " 22251/50000: episode: 317, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.280 [0.000, 36.000],  loss: 0.718576, mae: 1.803158, mean_q: 2.016232\n",
      " 22351/50000: episode: 318, duration: 1.202s, episode steps: 100, steps per second:  83, episode reward:  9.000, mean reward:  0.090 [-1.000,  1.000], mean action: 19.770 [0.000, 37.000],  loss: 1.138088, mae: 1.809014, mean_q: 2.029104\n",
      " 22451/50000: episode: 319, duration: 1.201s, episode steps: 100, steps per second:  83, episode reward: 27.000, mean reward:  0.270 [-1.000, 36.000], mean action: 18.680 [0.000, 36.000],  loss: 0.505052, mae: 1.814679, mean_q: 2.047495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22551/50000: episode: 320, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.490 [1.000, 36.000],  loss: 0.513384, mae: 1.821989, mean_q: 2.049634\n",
      " 22625/50000: episode: 321, duration: 0.897s, episode steps:  74, steps per second:  82, episode reward: -11.000, mean reward: -0.149 [-1.000,  1.000], mean action: 19.932 [0.000, 37.000],  loss: 0.503113, mae: 1.826504, mean_q: 2.041167\n",
      " 22725/50000: episode: 322, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.340 [1.000, 36.000],  loss: 0.931173, mae: 1.832850, mean_q: 2.043111\n",
      " 22748/50000: episode: 323, duration: 0.296s, episode steps:  23, steps per second:  78, episode reward: -2.000, mean reward: -0.087 [-1.000,  1.000], mean action: 22.043 [2.000, 37.000],  loss: 0.501355, mae: 1.834953, mean_q: 2.043576\n",
      " 22779/50000: episode: 324, duration: 0.385s, episode steps:  31, steps per second:  81, episode reward: -8.000, mean reward: -0.258 [-1.000,  1.000], mean action: 17.774 [3.000, 37.000],  loss: 1.846896, mae: 1.837883, mean_q: 2.045371\n",
      " 22879/50000: episode: 325, duration: 1.199s, episode steps: 100, steps per second:  83, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 17.320 [1.000, 36.000],  loss: 0.721952, mae: 1.841090, mean_q: 2.056094\n",
      " 22967/50000: episode: 326, duration: 1.059s, episode steps:  88, steps per second:  83, episode reward: 15.000, mean reward:  0.170 [-1.000,  1.000], mean action: 17.682 [0.000, 37.000],  loss: 0.503062, mae: 1.844629, mean_q: 2.061002\n",
      " 23067/50000: episode: 327, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 17.680 [0.000, 36.000],  loss: 0.717721, mae: 1.848557, mean_q: 2.036685\n",
      " 23167/50000: episode: 328, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: 19.000, mean reward:  0.190 [-1.000,  1.000], mean action: 18.520 [1.000, 37.000],  loss: 0.513161, mae: 1.851537, mean_q: 2.028839\n",
      " 23252/50000: episode: 329, duration: 1.031s, episode steps:  85, steps per second:  82, episode reward: -16.000, mean reward: -0.188 [-1.000,  1.000], mean action: 18.776 [0.000, 37.000],  loss: 0.507417, mae: 1.854035, mean_q: 2.045412\n",
      " 23352/50000: episode: 330, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.560 [0.000, 36.000],  loss: 0.510382, mae: 1.856306, mean_q: 2.060009\n",
      " 23452/50000: episode: 331, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.680 [0.000, 36.000],  loss: 0.501469, mae: 1.858577, mean_q: 2.053122\n",
      " 23552/50000: episode: 332, duration: 1.212s, episode steps: 100, steps per second:  82, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.470 [0.000, 36.000],  loss: 0.723280, mae: 1.862287, mean_q: 2.053251\n",
      " 23623/50000: episode: 333, duration: 0.861s, episode steps:  71, steps per second:  82, episode reward: -14.000, mean reward: -0.197 [-1.000,  1.000], mean action: 20.028 [0.000, 37.000],  loss: 0.510061, mae: 1.864192, mean_q: 2.057767\n",
      " 23723/50000: episode: 334, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.850 [0.000, 36.000],  loss: 0.710720, mae: 1.866412, mean_q: 2.057497\n",
      " 23744/50000: episode: 335, duration: 0.266s, episode steps:  21, steps per second:  79, episode reward:  4.000, mean reward:  0.190 [-1.000,  1.000], mean action: 18.714 [1.000, 37.000],  loss: 0.491700, mae: 1.868626, mean_q: 2.065068\n",
      " 23844/50000: episode: 336, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 17.130 [0.000, 36.000],  loss: 0.710160, mae: 1.870718, mean_q: 2.063743\n",
      " 23944/50000: episode: 337, duration: 1.204s, episode steps: 100, steps per second:  83, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.240 [2.000, 36.000],  loss: 0.714721, mae: 1.874237, mean_q: 2.057265\n",
      " 23966/50000: episode: 338, duration: 0.283s, episode steps:  22, steps per second:  78, episode reward: -9.000, mean reward: -0.409 [-1.000,  1.000], mean action: 18.955 [0.000, 37.000],  loss: 0.511168, mae: 1.875132, mean_q: 2.045854\n",
      " 24066/50000: episode: 339, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 19.990 [0.000, 36.000],  loss: 0.706879, mae: 1.875917, mean_q: 2.050431\n",
      " 24166/50000: episode: 340, duration: 1.204s, episode steps: 100, steps per second:  83, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.430 [1.000, 36.000],  loss: 0.924689, mae: 1.877705, mean_q: 2.064510\n",
      " 24266/50000: episode: 341, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 19.420 [1.000, 36.000],  loss: 0.512852, mae: 1.877301, mean_q: 2.057649\n",
      " 24291/50000: episode: 342, duration: 0.317s, episode steps:  25, steps per second:  79, episode reward: 33.000, mean reward:  1.320 [-1.000, 36.000], mean action: 18.840 [0.000, 37.000],  loss: 2.198510, mae: 1.881872, mean_q: 2.065812\n",
      " 24391/50000: episode: 343, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 16.640 [0.000, 36.000],  loss: 0.715859, mae: 1.880827, mean_q: 2.063284\n",
      " 24490/50000: episode: 344, duration: 1.233s, episode steps:  99, steps per second:  80, episode reward: -8.000, mean reward: -0.081 [-1.000,  1.000], mean action: 15.606 [0.000, 37.000],  loss: 0.517954, mae: 1.882640, mean_q: 2.067758\n",
      " 24590/50000: episode: 345, duration: 1.340s, episode steps: 100, steps per second:  75, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 18.720 [0.000, 36.000],  loss: 0.508432, mae: 1.883797, mean_q: 2.069861\n",
      " 24690/50000: episode: 346, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 16.140 [0.000, 36.000],  loss: 0.507601, mae: 1.887901, mean_q: 2.079967\n",
      " 24759/50000: episode: 347, duration: 0.871s, episode steps:  69, steps per second:  79, episode reward: -12.000, mean reward: -0.174 [-1.000,  1.000], mean action: 16.812 [1.000, 37.000],  loss: 0.501570, mae: 1.891307, mean_q: 2.094719\n",
      " 24859/50000: episode: 348, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.510 [0.000, 36.000],  loss: 0.924481, mae: 1.893929, mean_q: 2.108090\n",
      " 24959/50000: episode: 349, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 19.900 [1.000, 36.000],  loss: 0.719991, mae: 1.897843, mean_q: 2.111163\n",
      " 25059/50000: episode: 350, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.390 [0.000, 36.000],  loss: 0.927098, mae: 1.901523, mean_q: 2.116299\n",
      " 25159/50000: episode: 351, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.080 [1.000, 36.000],  loss: 0.497439, mae: 1.904168, mean_q: 2.123348\n",
      " 25259/50000: episode: 352, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.210 [0.000, 36.000],  loss: 0.506037, mae: 1.909738, mean_q: 2.132432\n",
      " 25359/50000: episode: 353, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 17.850 [0.000, 35.000],  loss: 0.720876, mae: 1.917328, mean_q: 2.152314\n",
      " 25459/50000: episode: 354, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.920 [0.000, 36.000],  loss: 0.931716, mae: 1.925483, mean_q: 2.150941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25559/50000: episode: 355, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 17.370 [0.000, 36.000],  loss: 0.513654, mae: 1.931684, mean_q: 2.145947\n",
      " 25659/50000: episode: 356, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.040 [0.000, 36.000],  loss: 0.714177, mae: 1.937458, mean_q: 2.147753\n",
      " 25693/50000: episode: 357, duration: 0.425s, episode steps:  34, steps per second:  80, episode reward: -1.000, mean reward: -0.029 [-1.000,  1.000], mean action: 19.294 [1.000, 37.000],  loss: 0.518326, mae: 1.939619, mean_q: 2.149633\n",
      " 25704/50000: episode: 358, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward:  4.000, mean reward:  0.364 [-1.000,  1.000], mean action: 27.364 [8.000, 37.000],  loss: 0.491442, mae: 1.940172, mean_q: 2.153050\n",
      " 25804/50000: episode: 359, duration: 1.204s, episode steps: 100, steps per second:  83, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.600 [0.000, 36.000],  loss: 0.514708, mae: 1.943707, mean_q: 2.153638\n",
      " 25904/50000: episode: 360, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.450 [0.000, 36.000],  loss: 0.502384, mae: 1.949108, mean_q: 2.154019\n",
      " 26004/50000: episode: 361, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: 12.000, mean reward:  0.120 [-1.000,  1.000], mean action: 19.150 [1.000, 36.000],  loss: 0.504005, mae: 1.951394, mean_q: 2.156324\n",
      " 26104/50000: episode: 362, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 16.550 [0.000, 35.000],  loss: 0.712391, mae: 1.954500, mean_q: 2.162180\n",
      " 26204/50000: episode: 363, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 20.630 [0.000, 36.000],  loss: 0.716071, mae: 1.957409, mean_q: 2.168719\n",
      " 26304/50000: episode: 364, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.440 [0.000, 36.000],  loss: 0.501758, mae: 1.960199, mean_q: 2.167814\n",
      " 26404/50000: episode: 365, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.130 [0.000, 36.000],  loss: 0.506245, mae: 1.963660, mean_q: 2.151425\n",
      " 26450/50000: episode: 366, duration: 0.568s, episode steps:  46, steps per second:  81, episode reward: -7.000, mean reward: -0.152 [-1.000,  1.000], mean action: 19.565 [1.000, 37.000],  loss: 0.502762, mae: 1.965024, mean_q: 2.156955\n",
      " 26550/50000: episode: 367, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 19.550 [0.000, 36.000],  loss: 0.927427, mae: 1.968801, mean_q: 2.162210\n",
      " 26650/50000: episode: 368, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.590 [0.000, 36.000],  loss: 0.500536, mae: 1.972174, mean_q: 2.171685\n",
      " 26750/50000: episode: 369, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 19.300 [0.000, 36.000],  loss: 0.517145, mae: 1.976153, mean_q: 2.184503\n",
      " 26850/50000: episode: 370, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.450 [1.000, 36.000],  loss: 0.927485, mae: 1.981860, mean_q: 2.195598\n",
      " 26950/50000: episode: 371, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 18.680 [0.000, 35.000],  loss: 0.720131, mae: 1.986624, mean_q: 2.206723\n",
      " 27050/50000: episode: 372, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 18.090 [0.000, 36.000],  loss: 0.509289, mae: 1.989833, mean_q: 2.209366\n",
      " 27150/50000: episode: 373, duration: 1.219s, episode steps: 100, steps per second:  82, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 17.990 [0.000, 36.000],  loss: 0.515357, mae: 1.997056, mean_q: 2.219430\n",
      " 27250/50000: episode: 374, duration: 1.213s, episode steps: 100, steps per second:  82, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.970 [0.000, 36.000],  loss: 0.514167, mae: 2.002187, mean_q: 2.209145\n",
      " 27350/50000: episode: 375, duration: 1.217s, episode steps: 100, steps per second:  82, episode reward: 18.000, mean reward:  0.180 [-1.000,  1.000], mean action: 18.140 [0.000, 36.000],  loss: 0.931070, mae: 2.007342, mean_q: 2.193273\n",
      " 27450/50000: episode: 376, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 18.930 [0.000, 36.000],  loss: 0.922256, mae: 2.010062, mean_q: 2.194976\n",
      " 27550/50000: episode: 377, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.400 [0.000, 36.000],  loss: 0.923456, mae: 2.011681, mean_q: 2.190907\n",
      " 27650/50000: episode: 378, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 15.990 [0.000, 35.000],  loss: 0.497933, mae: 2.012707, mean_q: 2.197761\n",
      " 27750/50000: episode: 379, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.940 [0.000, 36.000],  loss: 0.501695, mae: 2.013970, mean_q: 2.210896\n",
      " 27841/50000: episode: 380, duration: 1.108s, episode steps:  91, steps per second:  82, episode reward: 10.000, mean reward:  0.110 [-1.000,  1.000], mean action: 18.066 [0.000, 37.000],  loss: 0.736114, mae: 2.016464, mean_q: 2.225064\n",
      " 27941/50000: episode: 381, duration: 1.214s, episode steps: 100, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.580 [1.000, 36.000],  loss: 1.138337, mae: 2.023027, mean_q: 2.237707\n",
      " 28041/50000: episode: 382, duration: 1.213s, episode steps: 100, steps per second:  82, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.360 [1.000, 36.000],  loss: 0.927131, mae: 2.028064, mean_q: 2.237962\n",
      " 28141/50000: episode: 383, duration: 1.224s, episode steps: 100, steps per second:  82, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 19.410 [0.000, 36.000],  loss: 0.928793, mae: 2.031838, mean_q: 2.227559\n",
      " 28221/50000: episode: 384, duration: 0.981s, episode steps:  80, steps per second:  82, episode reward: -11.000, mean reward: -0.138 [-1.000,  1.000], mean action: 18.938 [0.000, 37.000],  loss: 0.765469, mae: 2.032718, mean_q: 2.220352\n",
      " 28265/50000: episode: 385, duration: 0.544s, episode steps:  44, steps per second:  81, episode reward:  3.000, mean reward:  0.068 [-1.000,  1.000], mean action: 20.591 [0.000, 37.000],  loss: 1.463690, mae: 2.036385, mean_q: 2.219901\n",
      " 28365/50000: episode: 386, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 19.170 [1.000, 35.000],  loss: 0.517059, mae: 2.036730, mean_q: 2.221041\n",
      " 28395/50000: episode: 387, duration: 0.381s, episode steps:  30, steps per second:  79, episode reward: -11.000, mean reward: -0.367 [-1.000,  1.000], mean action: 19.267 [0.000, 37.000],  loss: 1.201733, mae: 2.038114, mean_q: 2.233232\n",
      " 28495/50000: episode: 388, duration: 1.305s, episode steps: 100, steps per second:  77, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.870 [0.000, 36.000],  loss: 0.507846, mae: 2.037814, mean_q: 2.235412\n",
      " 28595/50000: episode: 389, duration: 1.314s, episode steps: 100, steps per second:  76, episode reward: 27.000, mean reward:  0.270 [-1.000, 36.000], mean action: 17.810 [0.000, 36.000],  loss: 0.721654, mae: 2.041806, mean_q: 2.236845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28695/50000: episode: 390, duration: 1.310s, episode steps: 100, steps per second:  76, episode reward: 18.000, mean reward:  0.180 [-1.000,  1.000], mean action: 16.050 [0.000, 35.000],  loss: 0.505023, mae: 2.044721, mean_q: 2.231536\n",
      " 28795/50000: episode: 391, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.670 [0.000, 35.000],  loss: 0.714604, mae: 2.046653, mean_q: 2.234587\n",
      " 28863/50000: episode: 392, duration: 0.833s, episode steps:  68, steps per second:  82, episode reward:  1.000, mean reward:  0.015 [-1.000,  1.000], mean action: 20.544 [0.000, 37.000],  loss: 1.433593, mae: 2.049597, mean_q: 2.242629\n",
      " 28963/50000: episode: 393, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: -20.000, mean reward: -0.200 [-1.000,  1.000], mean action: 18.360 [0.000, 36.000],  loss: 0.719710, mae: 2.049534, mean_q: 2.243866\n",
      " 29063/50000: episode: 394, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.780 [1.000, 36.000],  loss: 0.718199, mae: 2.053234, mean_q: 2.251960\n",
      " 29163/50000: episode: 395, duration: 1.216s, episode steps: 100, steps per second:  82, episode reward: -32.000, mean reward: -0.320 [-1.000,  1.000], mean action: 18.170 [0.000, 36.000],  loss: 0.924945, mae: 2.057435, mean_q: 2.261317\n",
      " 29263/50000: episode: 396, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 18.990 [0.000, 36.000],  loss: 0.511519, mae: 2.059492, mean_q: 2.262035\n",
      " 29363/50000: episode: 397, duration: 1.211s, episode steps: 100, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.330 [0.000, 36.000],  loss: 0.715787, mae: 2.061523, mean_q: 2.271161\n",
      " 29463/50000: episode: 398, duration: 1.217s, episode steps: 100, steps per second:  82, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.330 [2.000, 36.000],  loss: 0.715573, mae: 2.062830, mean_q: 2.269221\n",
      " 29563/50000: episode: 399, duration: 1.218s, episode steps: 100, steps per second:  82, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.920 [0.000, 36.000],  loss: 0.508458, mae: 2.065402, mean_q: 2.260943\n",
      " 29644/50000: episode: 400, duration: 0.988s, episode steps:  81, steps per second:  82, episode reward:  4.000, mean reward:  0.049 [-1.000,  1.000], mean action: 19.753 [2.000, 37.000],  loss: 0.770424, mae: 2.065927, mean_q: 2.262831\n",
      " 29740/50000: episode: 401, duration: 1.165s, episode steps:  96, steps per second:  82, episode reward:  5.000, mean reward:  0.052 [-1.000,  1.000], mean action: 19.865 [0.000, 37.000],  loss: 0.503082, mae: 2.068240, mean_q: 2.265154\n",
      " 29840/50000: episode: 402, duration: 1.223s, episode steps: 100, steps per second:  82, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 18.230 [0.000, 36.000],  loss: 0.727054, mae: 2.069438, mean_q: 2.257933\n",
      " 29940/50000: episode: 403, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.770 [0.000, 36.000],  loss: 0.726151, mae: 2.069020, mean_q: 2.259634\n",
      " 30040/50000: episode: 404, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.630 [0.000, 36.000],  loss: 0.719370, mae: 2.070258, mean_q: 2.260248\n",
      " 30044/50000: episode: 405, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward: -3.000, mean reward: -0.750 [-1.000,  0.000], mean action: 21.500 [9.000, 37.000],  loss: 0.538671, mae: 2.071161, mean_q: 2.256517\n",
      " 30144/50000: episode: 406, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 20.800 [0.000, 36.000],  loss: 0.509166, mae: 2.074119, mean_q: 2.265578\n",
      " 30167/50000: episode: 407, duration: 0.299s, episode steps:  23, steps per second:  77, episode reward: -2.000, mean reward: -0.087 [-1.000,  1.000], mean action: 17.913 [1.000, 37.000],  loss: 2.328296, mae: 2.078964, mean_q: 2.285708\n",
      " 30267/50000: episode: 408, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 18.900 [1.000, 36.000],  loss: 0.719950, mae: 2.077200, mean_q: 2.291571\n",
      " 30367/50000: episode: 409, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 17.980 [0.000, 36.000],  loss: 0.720828, mae: 2.081276, mean_q: 2.288556\n",
      " 30467/50000: episode: 410, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.610 [2.000, 36.000],  loss: 0.711659, mae: 2.084073, mean_q: 2.282614\n",
      " 30567/50000: episode: 411, duration: 1.218s, episode steps: 100, steps per second:  82, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.740 [0.000, 36.000],  loss: 0.722940, mae: 2.086086, mean_q: 2.281908\n",
      " 30667/50000: episode: 412, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward: 41.000, mean reward:  0.410 [-1.000, 36.000], mean action: 17.650 [0.000, 36.000],  loss: 0.927214, mae: 2.088071, mean_q: 2.276581\n",
      " 30716/50000: episode: 413, duration: 0.608s, episode steps:  49, steps per second:  81, episode reward: -6.000, mean reward: -0.122 [-1.000,  1.000], mean action: 16.939 [1.000, 37.000],  loss: 0.942887, mae: 2.089929, mean_q: 2.268192\n",
      " 30734/50000: episode: 414, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward:  3.000, mean reward:  0.167 [-1.000,  1.000], mean action: 18.500 [3.000, 37.000],  loss: 0.505750, mae: 2.089261, mean_q: 2.265886\n",
      " 30823/50000: episode: 415, duration: 1.167s, episode steps:  89, steps per second:  76, episode reward: -12.000, mean reward: -0.135 [-1.000,  1.000], mean action: 19.685 [2.000, 37.000],  loss: 0.746447, mae: 2.090389, mean_q: 2.272274\n",
      " 30831/50000: episode: 416, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: -1.000, mean reward: -0.125 [-1.000,  1.000], mean action: 19.750 [0.000, 37.000],  loss: 0.520652, mae: 2.091028, mean_q: 2.282957\n",
      " 30931/50000: episode: 417, duration: 1.303s, episode steps: 100, steps per second:  77, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 16.620 [0.000, 36.000],  loss: 0.725874, mae: 2.092278, mean_q: 2.286258\n",
      " 31031/50000: episode: 418, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.570 [0.000, 36.000],  loss: 0.723544, mae: 2.093066, mean_q: 2.286548\n",
      " 31131/50000: episode: 419, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 19.310 [1.000, 36.000],  loss: 0.720080, mae: 2.094970, mean_q: 2.272370\n",
      " 31231/50000: episode: 420, duration: 1.228s, episode steps: 100, steps per second:  81, episode reward: 20.000, mean reward:  0.200 [-1.000,  1.000], mean action: 19.700 [0.000, 36.000],  loss: 0.516062, mae: 2.094393, mean_q: 2.267656\n",
      " 31300/50000: episode: 421, duration: 0.849s, episode steps:  69, steps per second:  81, episode reward: 12.000, mean reward:  0.174 [-1.000,  1.000], mean action: 18.681 [0.000, 37.000],  loss: 0.522877, mae: 2.092866, mean_q: 2.261630\n",
      " 31302/50000: episode: 422, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward:  1.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 29.000 [21.000, 37.000],  loss: 0.496655, mae: 2.091758, mean_q: 2.255136\n",
      " 31398/50000: episode: 423, duration: 1.231s, episode steps:  96, steps per second:  78, episode reward: -1.000, mean reward: -0.010 [-1.000,  1.000], mean action: 18.146 [0.000, 37.000],  loss: 0.719670, mae: 2.093055, mean_q: 2.252639\n",
      " 31498/50000: episode: 424, duration: 1.287s, episode steps: 100, steps per second:  78, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 18.290 [0.000, 36.000],  loss: 0.920555, mae: 2.092419, mean_q: 2.251619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31598/50000: episode: 425, duration: 1.254s, episode steps: 100, steps per second:  80, episode reward: 21.000, mean reward:  0.210 [-1.000, 36.000], mean action: 16.650 [0.000, 36.000],  loss: 1.133982, mae: 2.090260, mean_q: 2.252120\n",
      " 31618/50000: episode: 426, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward: 32.000, mean reward:  1.600 [-1.000, 36.000], mean action: 18.700 [0.000, 37.000],  loss: 0.526306, mae: 2.088499, mean_q: 2.248194\n",
      " 31718/50000: episode: 427, duration: 1.219s, episode steps: 100, steps per second:  82, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.020 [1.000, 36.000],  loss: 0.724563, mae: 2.085856, mean_q: 2.249848\n",
      " 31761/50000: episode: 428, duration: 0.537s, episode steps:  43, steps per second:  80, episode reward:  2.000, mean reward:  0.047 [-1.000,  1.000], mean action: 20.535 [1.000, 37.000],  loss: 0.524877, mae: 2.084145, mean_q: 2.256690\n",
      " 31842/50000: episode: 429, duration: 1.030s, episode steps:  81, steps per second:  79, episode reward: -24.000, mean reward: -0.296 [-1.000,  1.000], mean action: 18.481 [1.000, 37.000],  loss: 0.765297, mae: 2.086353, mean_q: 2.275404\n",
      " 31910/50000: episode: 430, duration: 0.866s, episode steps:  68, steps per second:  79, episode reward: 15.000, mean reward:  0.221 [-1.000,  1.000], mean action: 17.824 [0.000, 37.000],  loss: 1.120528, mae: 2.088083, mean_q: 2.285785\n",
      " 32010/50000: episode: 431, duration: 1.306s, episode steps: 100, steps per second:  77, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 17.610 [1.000, 36.000],  loss: 0.721312, mae: 2.090141, mean_q: 2.280961\n",
      " 32110/50000: episode: 432, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 16.000, mean reward:  0.160 [-1.000,  1.000], mean action: 17.650 [0.000, 36.000],  loss: 0.720794, mae: 2.093241, mean_q: 2.280735\n",
      " 32210/50000: episode: 433, duration: 1.358s, episode steps: 100, steps per second:  74, episode reward: -20.000, mean reward: -0.200 [-1.000,  1.000], mean action: 17.840 [0.000, 36.000],  loss: 0.720308, mae: 2.096204, mean_q: 2.288095\n",
      " 32310/50000: episode: 434, duration: 1.302s, episode steps: 100, steps per second:  77, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 19.460 [0.000, 36.000],  loss: 0.718607, mae: 2.097722, mean_q: 2.299554\n",
      " 32410/50000: episode: 435, duration: 1.268s, episode steps: 100, steps per second:  79, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 15.900 [0.000, 36.000],  loss: 0.513104, mae: 2.099493, mean_q: 2.306379\n",
      " 32510/50000: episode: 436, duration: 1.291s, episode steps: 100, steps per second:  77, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.570 [0.000, 36.000],  loss: 0.719387, mae: 2.101075, mean_q: 2.321029\n",
      " 32556/50000: episode: 437, duration: 0.623s, episode steps:  46, steps per second:  74, episode reward:  9.000, mean reward:  0.196 [-1.000,  1.000], mean action: 16.478 [0.000, 37.000],  loss: 0.960047, mae: 2.104868, mean_q: 2.314967\n",
      " 32630/50000: episode: 438, duration: 0.945s, episode steps:  74, steps per second:  78, episode reward: -9.000, mean reward: -0.122 [-1.000,  1.000], mean action: 20.622 [0.000, 37.000],  loss: 0.798975, mae: 2.106807, mean_q: 2.307651\n",
      " 32730/50000: episode: 439, duration: 1.259s, episode steps: 100, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.900 [0.000, 36.000],  loss: 0.729734, mae: 2.109125, mean_q: 2.309809\n",
      " 32809/50000: episode: 440, duration: 1.001s, episode steps:  79, steps per second:  79, episode reward: 12.000, mean reward:  0.152 [-1.000,  1.000], mean action: 18.772 [1.000, 37.000],  loss: 0.497677, mae: 2.111053, mean_q: 2.300924\n",
      " 32909/50000: episode: 441, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.870 [0.000, 36.000],  loss: 0.934871, mae: 2.115641, mean_q: 2.302380\n",
      " 33009/50000: episode: 442, duration: 1.296s, episode steps: 100, steps per second:  77, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.030 [1.000, 36.000],  loss: 0.507488, mae: 2.116656, mean_q: 2.307663\n",
      " 33109/50000: episode: 443, duration: 1.384s, episode steps: 100, steps per second:  72, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 17.510 [0.000, 36.000],  loss: 0.519533, mae: 2.122173, mean_q: 2.313603\n",
      " 33209/50000: episode: 444, duration: 1.281s, episode steps: 100, steps per second:  78, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.620 [0.000, 36.000],  loss: 0.932074, mae: 2.126311, mean_q: 2.315949\n",
      " 33309/50000: episode: 445, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.410 [0.000, 36.000],  loss: 0.717329, mae: 2.127580, mean_q: 2.317370\n",
      " 33409/50000: episode: 446, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 20.070 [0.000, 36.000],  loss: 1.335477, mae: 2.131506, mean_q: 2.315479\n",
      " 33416/50000: episode: 447, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 14.714 [1.000, 37.000],  loss: 0.537679, mae: 2.131210, mean_q: 2.313126\n",
      " 33516/50000: episode: 448, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: -34.000, mean reward: -0.340 [-1.000,  1.000], mean action: 15.980 [0.000, 36.000],  loss: 0.723979, mae: 2.130084, mean_q: 2.307174\n",
      " 33548/50000: episode: 449, duration: 0.410s, episode steps:  32, steps per second:  78, episode reward: -3.000, mean reward: -0.094 [-1.000,  1.000], mean action: 18.062 [1.000, 37.000],  loss: 0.515870, mae: 2.128900, mean_q: 2.307871\n",
      " 33585/50000: episode: 450, duration: 0.471s, episode steps:  37, steps per second:  79, episode reward: -4.000, mean reward: -0.108 [-1.000,  1.000], mean action: 20.568 [3.000, 37.000],  loss: 0.513589, mae: 2.127789, mean_q: 2.307831\n",
      " 33641/50000: episode: 451, duration: 0.710s, episode steps:  56, steps per second:  79, episode reward: 38.000, mean reward:  0.679 [-1.000, 36.000], mean action: 17.607 [0.000, 37.000],  loss: 0.881726, mae: 2.127640, mean_q: 2.309431\n",
      " 33741/50000: episode: 452, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.560 [0.000, 36.000],  loss: 0.510683, mae: 2.128129, mean_q: 2.307881\n",
      " 33841/50000: episode: 453, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 18.410 [0.000, 36.000],  loss: 0.713293, mae: 2.128975, mean_q: 2.305126\n",
      " 33941/50000: episode: 454, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 17.590 [0.000, 36.000],  loss: 1.116871, mae: 2.129030, mean_q: 2.315389\n",
      " 34041/50000: episode: 455, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.270 [0.000, 36.000],  loss: 0.514483, mae: 2.129936, mean_q: 2.306426\n",
      " 34103/50000: episode: 456, duration: 0.771s, episode steps:  62, steps per second:  80, episode reward:  3.000, mean reward:  0.048 [-1.000,  1.000], mean action: 18.048 [0.000, 37.000],  loss: 0.848870, mae: 2.131075, mean_q: 2.310684\n",
      " 34197/50000: episode: 457, duration: 1.209s, episode steps:  94, steps per second:  78, episode reward:  3.000, mean reward:  0.032 [-1.000,  1.000], mean action: 18.043 [0.000, 37.000],  loss: 0.521989, mae: 2.132410, mean_q: 2.315156\n",
      " 34297/50000: episode: 458, duration: 1.349s, episode steps: 100, steps per second:  74, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 18.060 [1.000, 36.000],  loss: 0.523333, mae: 2.131250, mean_q: 2.314018\n",
      " 34323/50000: episode: 459, duration: 0.335s, episode steps:  26, steps per second:  78, episode reward: -1.000, mean reward: -0.038 [-1.000,  1.000], mean action: 21.000 [2.000, 37.000],  loss: 1.329406, mae: 2.130329, mean_q: 2.314579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34423/50000: episode: 460, duration: 1.255s, episode steps: 100, steps per second:  80, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.990 [0.000, 36.000],  loss: 0.921960, mae: 2.129062, mean_q: 2.312829\n",
      " 34523/50000: episode: 461, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 18.000 [0.000, 36.000],  loss: 0.718562, mae: 2.130076, mean_q: 2.318004\n",
      " 34623/50000: episode: 462, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 17.040 [0.000, 35.000],  loss: 0.730141, mae: 2.134229, mean_q: 2.328695\n",
      " 34723/50000: episode: 463, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.690 [0.000, 36.000],  loss: 1.346847, mae: 2.138036, mean_q: 2.330814\n",
      " 34823/50000: episode: 464, duration: 1.227s, episode steps: 100, steps per second:  82, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 19.470 [0.000, 36.000],  loss: 0.932625, mae: 2.138882, mean_q: 2.329318\n",
      " 34861/50000: episode: 465, duration: 0.480s, episode steps:  38, steps per second:  79, episode reward:  5.000, mean reward:  0.132 [-1.000,  1.000], mean action: 18.895 [1.000, 37.000],  loss: 0.514460, mae: 2.138818, mean_q: 2.327727\n",
      " 34885/50000: episode: 466, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward: -1.000, mean reward: -0.042 [-1.000,  1.000], mean action: 21.250 [1.000, 37.000],  loss: 0.510771, mae: 2.140222, mean_q: 2.330290\n",
      " 34985/50000: episode: 467, duration: 1.229s, episode steps: 100, steps per second:  81, episode reward: 18.000, mean reward:  0.180 [-1.000,  1.000], mean action: 20.560 [0.000, 36.000],  loss: 0.926124, mae: 2.144681, mean_q: 2.337108\n",
      " 35085/50000: episode: 468, duration: 1.227s, episode steps: 100, steps per second:  82, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 18.520 [0.000, 36.000],  loss: 0.720280, mae: 2.148768, mean_q: 2.347857\n",
      " 35185/50000: episode: 469, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.580 [0.000, 36.000],  loss: 0.716943, mae: 2.152131, mean_q: 2.350778\n",
      " 35225/50000: episode: 470, duration: 0.511s, episode steps:  40, steps per second:  78, episode reward: -3.000, mean reward: -0.075 [-1.000,  1.000], mean action: 19.125 [0.000, 37.000],  loss: 1.554235, mae: 2.154058, mean_q: 2.354556\n",
      " 35325/50000: episode: 471, duration: 1.251s, episode steps: 100, steps per second:  80, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 18.660 [0.000, 36.000],  loss: 0.922991, mae: 2.153739, mean_q: 2.351960\n",
      " 35425/50000: episode: 472, duration: 1.231s, episode steps: 100, steps per second:  81, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.880 [0.000, 36.000],  loss: 0.729105, mae: 2.157367, mean_q: 2.358459\n",
      " 35525/50000: episode: 473, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 16.800 [0.000, 36.000],  loss: 0.514346, mae: 2.158648, mean_q: 2.344376\n",
      " 35625/50000: episode: 474, duration: 1.226s, episode steps: 100, steps per second:  82, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.150 [0.000, 36.000],  loss: 0.718876, mae: 2.160098, mean_q: 2.338270\n",
      " 35725/50000: episode: 475, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 19.710 [0.000, 36.000],  loss: 0.513923, mae: 2.160267, mean_q: 2.346089\n",
      " 35753/50000: episode: 476, duration: 0.358s, episode steps:  28, steps per second:  78, episode reward:  5.000, mean reward:  0.179 [-1.000,  1.000], mean action: 19.714 [2.000, 37.000],  loss: 0.519954, mae: 2.161170, mean_q: 2.346897\n",
      " 35846/50000: episode: 477, duration: 1.138s, episode steps:  93, steps per second:  82, episode reward: -10.000, mean reward: -0.108 [-1.000,  1.000], mean action: 16.398 [0.000, 37.000],  loss: 0.737491, mae: 2.163368, mean_q: 2.356371\n",
      " 35946/50000: episode: 478, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 18.220 [0.000, 36.000],  loss: 1.331095, mae: 2.165461, mean_q: 2.364607\n",
      " 35994/50000: episode: 479, duration: 0.604s, episode steps:  48, steps per second:  79, episode reward: -3.000, mean reward: -0.062 [-1.000,  1.000], mean action: 20.667 [0.000, 37.000],  loss: 0.503567, mae: 2.165938, mean_q: 2.366522\n",
      " 36094/50000: episode: 480, duration: 1.224s, episode steps: 100, steps per second:  82, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 20.280 [0.000, 36.000],  loss: 0.928814, mae: 2.168251, mean_q: 2.371024\n",
      " 36194/50000: episode: 481, duration: 1.228s, episode steps: 100, steps per second:  81, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 19.230 [0.000, 36.000],  loss: 0.940909, mae: 2.172259, mean_q: 2.367957\n",
      " 36294/50000: episode: 482, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 18.770 [0.000, 36.000],  loss: 0.513655, mae: 2.175939, mean_q: 2.369138\n",
      " 36376/50000: episode: 483, duration: 1.012s, episode steps:  82, steps per second:  81, episode reward: -3.000, mean reward: -0.037 [-1.000,  1.000], mean action: 18.402 [0.000, 37.000],  loss: 0.522036, mae: 2.179364, mean_q: 2.363830\n",
      " 36476/50000: episode: 484, duration: 1.235s, episode steps: 100, steps per second:  81, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.300 [0.000, 36.000],  loss: 0.518126, mae: 2.180257, mean_q: 2.374070\n",
      " 36576/50000: episode: 485, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 18.660 [0.000, 36.000],  loss: 0.912392, mae: 2.183020, mean_q: 2.382811\n",
      " 36636/50000: episode: 486, duration: 0.749s, episode steps:  60, steps per second:  80, episode reward: -1.000, mean reward: -0.017 [-1.000,  1.000], mean action: 20.417 [0.000, 37.000],  loss: 0.507374, mae: 2.183741, mean_q: 2.375627\n",
      " 36736/50000: episode: 487, duration: 1.258s, episode steps: 100, steps per second:  80, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 18.370 [2.000, 36.000],  loss: 0.516182, mae: 2.185375, mean_q: 2.367259\n",
      " 36836/50000: episode: 488, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.900 [0.000, 36.000],  loss: 0.510805, mae: 2.186694, mean_q: 2.363583\n",
      " 36878/50000: episode: 489, duration: 0.530s, episode steps:  42, steps per second:  79, episode reward: -5.000, mean reward: -0.119 [-1.000,  1.000], mean action: 18.452 [0.000, 37.000],  loss: 1.024084, mae: 2.187656, mean_q: 2.364033\n",
      " 36978/50000: episode: 490, duration: 1.232s, episode steps: 100, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.110 [1.000, 36.000],  loss: 0.514908, mae: 2.185077, mean_q: 2.363166\n",
      " 37078/50000: episode: 491, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.420 [0.000, 36.000],  loss: 0.708002, mae: 2.183932, mean_q: 2.358877\n",
      " 37178/50000: episode: 492, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.770 [1.000, 36.000],  loss: 0.732561, mae: 2.183373, mean_q: 2.360789\n",
      " 37182/50000: episode: 493, duration: 0.060s, episode steps:   4, steps per second:  66, episode reward: -1.000, mean reward: -0.250 [-1.000,  1.000], mean action: 24.000 [5.000, 37.000],  loss: 0.499244, mae: 2.183707, mean_q: 2.361094\n",
      " 37234/50000: episode: 494, duration: 0.652s, episode steps:  52, steps per second:  80, episode reward: -17.000, mean reward: -0.327 [-1.000,  1.000], mean action: 18.115 [0.000, 37.000],  loss: 0.519471, mae: 2.184517, mean_q: 2.363601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37298/50000: episode: 495, duration: 0.799s, episode steps:  64, steps per second:  80, episode reward: -11.000, mean reward: -0.172 [-1.000,  1.000], mean action: 18.875 [0.000, 37.000],  loss: 0.821763, mae: 2.184775, mean_q: 2.372266\n",
      " 37398/50000: episode: 496, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 19.030 [0.000, 36.000],  loss: 0.923124, mae: 2.185327, mean_q: 2.371370\n",
      " 37498/50000: episode: 497, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: 19.000, mean reward:  0.190 [-1.000, 36.000], mean action: 19.070 [0.000, 36.000],  loss: 0.513420, mae: 2.186928, mean_q: 2.376266\n",
      " 37598/50000: episode: 498, duration: 1.240s, episode steps: 100, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.470 [1.000, 36.000],  loss: 0.716688, mae: 2.188260, mean_q: 2.380855\n",
      " 37642/50000: episode: 499, duration: 0.556s, episode steps:  44, steps per second:  79, episode reward:  3.000, mean reward:  0.068 [-1.000,  1.000], mean action: 20.136 [0.000, 37.000],  loss: 0.972886, mae: 2.190618, mean_q: 2.375187\n",
      " 37742/50000: episode: 500, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 16.770 [0.000, 36.000],  loss: 0.933749, mae: 2.191858, mean_q: 2.373040\n",
      " 37745/50000: episode: 501, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward: -2.000, mean reward: -0.667 [-1.000,  0.000], mean action: 24.000 [6.000, 37.000],  loss: 0.496289, mae: 2.190127, mean_q: 2.371175\n",
      " 37845/50000: episode: 502, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.740 [0.000, 36.000],  loss: 0.711453, mae: 2.190701, mean_q: 2.370080\n",
      " 37945/50000: episode: 503, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.190 [0.000, 36.000],  loss: 0.524802, mae: 2.190373, mean_q: 2.379005\n",
      " 38045/50000: episode: 504, duration: 1.375s, episode steps: 100, steps per second:  73, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 18.260 [0.000, 36.000],  loss: 0.513099, mae: 2.191197, mean_q: 2.400843\n",
      " 38112/50000: episode: 505, duration: 0.921s, episode steps:  67, steps per second:  73, episode reward: -10.000, mean reward: -0.149 [-1.000,  1.000], mean action: 18.239 [1.000, 37.000],  loss: 0.812999, mae: 2.192257, mean_q: 2.413345\n",
      " 38212/50000: episode: 506, duration: 1.309s, episode steps: 100, steps per second:  76, episode reward: -20.000, mean reward: -0.200 [-1.000,  1.000], mean action: 16.400 [0.000, 36.000],  loss: 0.509798, mae: 2.192256, mean_q: 2.410719\n",
      " 38312/50000: episode: 507, duration: 1.311s, episode steps: 100, steps per second:  76, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 17.410 [1.000, 35.000],  loss: 0.721372, mae: 2.195448, mean_q: 2.414433\n",
      " 38321/50000: episode: 508, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  2.000, mean reward:  0.222 [-1.000,  1.000], mean action: 17.556 [4.000, 37.000],  loss: 0.518218, mae: 2.197974, mean_q: 2.417956\n",
      " 38421/50000: episode: 509, duration: 1.327s, episode steps: 100, steps per second:  75, episode reward: 33.000, mean reward:  0.330 [-1.000, 36.000], mean action: 19.050 [0.000, 36.000],  loss: 0.724427, mae: 2.200087, mean_q: 2.412316\n",
      " 38521/50000: episode: 510, duration: 1.262s, episode steps: 100, steps per second:  79, episode reward: 12.000, mean reward:  0.120 [-1.000,  1.000], mean action: 16.690 [0.000, 35.000],  loss: 0.924622, mae: 2.201248, mean_q: 2.402176\n",
      " 38621/50000: episode: 511, duration: 1.325s, episode steps: 100, steps per second:  75, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 18.620 [0.000, 36.000],  loss: 0.933638, mae: 2.202673, mean_q: 2.396751\n",
      " 38721/50000: episode: 512, duration: 1.319s, episode steps: 100, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.210 [0.000, 36.000],  loss: 0.712069, mae: 2.204053, mean_q: 2.392663\n",
      " 38821/50000: episode: 513, duration: 1.278s, episode steps: 100, steps per second:  78, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.840 [0.000, 36.000],  loss: 0.522485, mae: 2.207048, mean_q: 2.401671\n",
      " 38880/50000: episode: 514, duration: 0.783s, episode steps:  59, steps per second:  75, episode reward: 18.000, mean reward:  0.305 [-1.000,  1.000], mean action: 19.220 [1.000, 37.000],  loss: 0.507062, mae: 2.206872, mean_q: 2.407856\n",
      " 38936/50000: episode: 515, duration: 0.731s, episode steps:  56, steps per second:  77, episode reward:  7.000, mean reward:  0.125 [-1.000,  1.000], mean action: 18.768 [0.000, 37.000],  loss: 0.512654, mae: 2.207061, mean_q: 2.408473\n",
      " 38955/50000: episode: 516, duration: 0.254s, episode steps:  19, steps per second:  75, episode reward: -2.000, mean reward: -0.105 [-1.000,  1.000], mean action: 15.105 [0.000, 37.000],  loss: 0.496214, mae: 2.207472, mean_q: 2.412989\n",
      " 39055/50000: episode: 517, duration: 1.246s, episode steps: 100, steps per second:  80, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 17.800 [0.000, 36.000],  loss: 0.515948, mae: 2.208377, mean_q: 2.404504\n",
      " 39155/50000: episode: 518, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.630 [1.000, 36.000],  loss: 1.127842, mae: 2.210854, mean_q: 2.389100\n",
      " 39255/50000: episode: 519, duration: 1.240s, episode steps: 100, steps per second:  81, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.600 [0.000, 36.000],  loss: 1.140717, mae: 2.214335, mean_q: 2.382723\n",
      " 39329/50000: episode: 520, duration: 0.921s, episode steps:  74, steps per second:  80, episode reward: -9.000, mean reward: -0.122 [-1.000,  1.000], mean action: 19.486 [2.000, 37.000],  loss: 1.076098, mae: 2.214110, mean_q: 2.388285\n",
      " 39429/50000: episode: 521, duration: 1.246s, episode steps: 100, steps per second:  80, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.410 [0.000, 36.000],  loss: 1.136525, mae: 2.214549, mean_q: 2.393060\n",
      " 39529/50000: episode: 522, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 18.630 [1.000, 36.000],  loss: 0.923184, mae: 2.215496, mean_q: 2.395060\n",
      " 39629/50000: episode: 523, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 19.850 [1.000, 36.000],  loss: 0.719748, mae: 2.216803, mean_q: 2.398280\n",
      " 39689/50000: episode: 524, duration: 0.754s, episode steps:  60, steps per second:  80, episode reward: -13.000, mean reward: -0.217 [-1.000,  1.000], mean action: 17.467 [0.000, 37.000],  loss: 0.853815, mae: 2.216858, mean_q: 2.393637\n",
      " 39789/50000: episode: 525, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 19.340 [0.000, 36.000],  loss: 1.135043, mae: 2.218823, mean_q: 2.391923\n",
      " 39889/50000: episode: 526, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: -20.000, mean reward: -0.200 [-1.000,  1.000], mean action: 17.360 [0.000, 36.000],  loss: 0.716960, mae: 2.218285, mean_q: 2.381588\n",
      " 39989/50000: episode: 527, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.820 [0.000, 36.000],  loss: 0.923763, mae: 2.219468, mean_q: 2.383293\n",
      " 40043/50000: episode: 528, duration: 0.688s, episode steps:  54, steps per second:  79, episode reward:  1.000, mean reward:  0.019 [-1.000,  1.000], mean action: 18.741 [1.000, 37.000],  loss: 0.510735, mae: 2.218651, mean_q: 2.394491\n",
      " 40143/50000: episode: 529, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.440 [0.000, 36.000],  loss: 1.129027, mae: 2.217479, mean_q: 2.381850\n",
      " 40157/50000: episode: 530, duration: 0.183s, episode steps:  14, steps per second:  77, episode reward:  5.000, mean reward:  0.357 [-1.000,  1.000], mean action: 21.714 [2.000, 37.000],  loss: 0.509911, mae: 2.216290, mean_q: 2.379615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40158/50000: episode: 531, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 37.000 [37.000, 37.000],  loss: 0.488312, mae: 2.215705, mean_q: 2.378824\n",
      " 40258/50000: episode: 532, duration: 1.258s, episode steps: 100, steps per second:  79, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 18.400 [0.000, 36.000],  loss: 0.922294, mae: 2.216534, mean_q: 2.388005\n",
      " 40358/50000: episode: 533, duration: 1.246s, episode steps: 100, steps per second:  80, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 18.580 [0.000, 36.000],  loss: 0.519095, mae: 2.212683, mean_q: 2.380848\n",
      " 40458/50000: episode: 534, duration: 1.243s, episode steps: 100, steps per second:  80, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 16.710 [0.000, 36.000],  loss: 0.723376, mae: 2.212133, mean_q: 2.381513\n",
      " 40492/50000: episode: 535, duration: 0.436s, episode steps:  34, steps per second:  78, episode reward:  1.000, mean reward:  0.029 [-1.000,  1.000], mean action: 21.059 [0.000, 37.000],  loss: 0.501226, mae: 2.210678, mean_q: 2.380500\n",
      " 40592/50000: episode: 536, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.220 [0.000, 36.000],  loss: 0.731047, mae: 2.210601, mean_q: 2.378187\n",
      " 40692/50000: episode: 537, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 18.000, mean reward:  0.180 [-1.000,  1.000], mean action: 17.870 [0.000, 36.000],  loss: 0.524938, mae: 2.208880, mean_q: 2.367276\n",
      " 40792/50000: episode: 538, duration: 1.242s, episode steps: 100, steps per second:  81, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.880 [0.000, 36.000],  loss: 0.514363, mae: 2.208523, mean_q: 2.368020\n",
      " 40892/50000: episode: 539, duration: 1.342s, episode steps: 100, steps per second:  75, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 17.980 [0.000, 36.000],  loss: 0.924257, mae: 2.207515, mean_q: 2.364297\n",
      " 40992/50000: episode: 540, duration: 1.242s, episode steps: 100, steps per second:  81, episode reward: 12.000, mean reward:  0.120 [-1.000,  1.000], mean action: 17.420 [1.000, 36.000],  loss: 0.715913, mae: 2.206139, mean_q: 2.373177\n",
      " 41092/50000: episode: 541, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.110 [0.000, 36.000],  loss: 0.514766, mae: 2.205531, mean_q: 2.377545\n",
      " 41192/50000: episode: 542, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 17.910 [0.000, 36.000],  loss: 0.519377, mae: 2.205018, mean_q: 2.377272\n",
      " 41254/50000: episode: 543, duration: 0.782s, episode steps:  62, steps per second:  79, episode reward:  1.000, mean reward:  0.016 [-1.000,  1.000], mean action: 19.532 [0.000, 37.000],  loss: 0.849296, mae: 2.205788, mean_q: 2.386734\n",
      " 41354/50000: episode: 544, duration: 1.243s, episode steps: 100, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.450 [0.000, 36.000],  loss: 0.515141, mae: 2.206244, mean_q: 2.396527\n",
      " 41454/50000: episode: 545, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 16.230 [0.000, 36.000],  loss: 1.145533, mae: 2.209636, mean_q: 2.390664\n",
      " 41554/50000: episode: 546, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 16.340 [0.000, 36.000],  loss: 0.730472, mae: 2.211282, mean_q: 2.379481\n",
      " 41654/50000: episode: 547, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward: 37.000, mean reward:  0.370 [-1.000, 36.000], mean action: 17.840 [0.000, 36.000],  loss: 0.735233, mae: 2.210057, mean_q: 2.385805\n",
      " 41754/50000: episode: 548, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 19.440 [1.000, 36.000],  loss: 0.517931, mae: 2.207626, mean_q: 2.396158\n",
      " 41816/50000: episode: 549, duration: 0.778s, episode steps:  62, steps per second:  80, episode reward: -15.000, mean reward: -0.242 [-1.000,  1.000], mean action: 21.919 [1.000, 37.000],  loss: 0.517391, mae: 2.206981, mean_q: 2.392219\n",
      " 41916/50000: episode: 550, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.360 [1.000, 36.000],  loss: 1.131381, mae: 2.209399, mean_q: 2.393427\n",
      " 42016/50000: episode: 551, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 19.020 [0.000, 36.000],  loss: 0.928193, mae: 2.210038, mean_q: 2.397364\n",
      " 42116/50000: episode: 552, duration: 1.253s, episode steps: 100, steps per second:  80, episode reward: 14.000, mean reward:  0.140 [-1.000,  1.000], mean action: 20.230 [0.000, 36.000],  loss: 0.714771, mae: 2.209500, mean_q: 2.388275\n",
      " 42216/50000: episode: 553, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 17.510 [0.000, 36.000],  loss: 0.717092, mae: 2.209234, mean_q: 2.378078\n",
      " 42316/50000: episode: 554, duration: 1.253s, episode steps: 100, steps per second:  80, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.360 [0.000, 36.000],  loss: 0.512479, mae: 2.206567, mean_q: 2.379235\n",
      " 42416/50000: episode: 555, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 21.180 [1.000, 36.000],  loss: 0.923570, mae: 2.207391, mean_q: 2.380718\n",
      " 42437/50000: episode: 556, duration: 0.277s, episode steps:  21, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.762 [1.000, 37.000],  loss: 0.509744, mae: 2.207305, mean_q: 2.370892\n",
      " 42458/50000: episode: 557, duration: 0.277s, episode steps:  21, steps per second:  76, episode reward: -2.000, mean reward: -0.095 [-1.000,  1.000], mean action: 17.762 [0.000, 37.000],  loss: 0.505025, mae: 2.207232, mean_q: 2.368860\n",
      " 42558/50000: episode: 558, duration: 1.254s, episode steps: 100, steps per second:  80, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 18.560 [0.000, 36.000],  loss: 0.710770, mae: 2.208099, mean_q: 2.372131\n",
      " 42658/50000: episode: 559, duration: 1.250s, episode steps: 100, steps per second:  80, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.380 [0.000, 36.000],  loss: 1.128032, mae: 2.208987, mean_q: 2.365767\n",
      " 42758/50000: episode: 560, duration: 1.273s, episode steps: 100, steps per second:  79, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 18.470 [0.000, 36.000],  loss: 1.333154, mae: 2.209646, mean_q: 2.374608\n",
      " 42858/50000: episode: 561, duration: 1.246s, episode steps: 100, steps per second:  80, episode reward: 41.000, mean reward:  0.410 [-1.000, 36.000], mean action: 18.890 [0.000, 36.000],  loss: 0.918073, mae: 2.209193, mean_q: 2.390308\n",
      " 42958/50000: episode: 562, duration: 1.252s, episode steps: 100, steps per second:  80, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 16.200 [0.000, 36.000],  loss: 0.920633, mae: 2.209498, mean_q: 2.386254\n",
      " 43058/50000: episode: 563, duration: 1.246s, episode steps: 100, steps per second:  80, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.160 [0.000, 36.000],  loss: 0.916555, mae: 2.210997, mean_q: 2.374743\n",
      " 43143/50000: episode: 564, duration: 1.063s, episode steps:  85, steps per second:  80, episode reward: -2.000, mean reward: -0.024 [-1.000,  1.000], mean action: 19.718 [0.000, 37.000],  loss: 0.990003, mae: 2.210854, mean_q: 2.376919\n",
      " 43243/50000: episode: 565, duration: 1.252s, episode steps: 100, steps per second:  80, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.140 [0.000, 36.000],  loss: 0.728807, mae: 2.211047, mean_q: 2.381557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43343/50000: episode: 566, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.000 [0.000, 36.000],  loss: 0.518296, mae: 2.210508, mean_q: 2.375360\n",
      " 43443/50000: episode: 567, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.040 [0.000, 36.000],  loss: 0.513692, mae: 2.210317, mean_q: 2.368148\n",
      " 43543/50000: episode: 568, duration: 1.251s, episode steps: 100, steps per second:  80, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 17.990 [0.000, 36.000],  loss: 0.722480, mae: 2.210497, mean_q: 2.372429\n",
      " 43643/50000: episode: 569, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 19.400 [0.000, 36.000],  loss: 0.919754, mae: 2.210685, mean_q: 2.375262\n",
      " 43743/50000: episode: 570, duration: 1.299s, episode steps: 100, steps per second:  77, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 18.120 [0.000, 36.000],  loss: 0.721893, mae: 2.212259, mean_q: 2.384071\n",
      " 43843/50000: episode: 571, duration: 1.382s, episode steps: 100, steps per second:  72, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.470 [0.000, 36.000],  loss: 0.927572, mae: 2.212375, mean_q: 2.391980\n",
      " 43859/50000: episode: 572, duration: 0.214s, episode steps:  16, steps per second:  75, episode reward: -1.000, mean reward: -0.062 [-1.000,  1.000], mean action: 20.125 [1.000, 37.000],  loss: 0.513888, mae: 2.212319, mean_q: 2.393417\n",
      " 43959/50000: episode: 573, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.730 [0.000, 36.000],  loss: 1.540042, mae: 2.214833, mean_q: 2.393481\n",
      " 44059/50000: episode: 574, duration: 1.270s, episode steps: 100, steps per second:  79, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 19.600 [0.000, 36.000],  loss: 0.510849, mae: 2.215105, mean_q: 2.392007\n",
      " 44110/50000: episode: 575, duration: 0.646s, episode steps:  51, steps per second:  79, episode reward: -12.000, mean reward: -0.235 [-1.000,  1.000], mean action: 16.863 [0.000, 37.000],  loss: 0.503875, mae: 2.215601, mean_q: 2.386478\n",
      " 44210/50000: episode: 576, duration: 1.250s, episode steps: 100, steps per second:  80, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 20.430 [0.000, 36.000],  loss: 0.718343, mae: 2.215946, mean_q: 2.378861\n",
      " 44310/50000: episode: 577, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 16.530 [0.000, 36.000],  loss: 0.711975, mae: 2.213516, mean_q: 2.369195\n",
      " 44410/50000: episode: 578, duration: 1.252s, episode steps: 100, steps per second:  80, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 19.130 [0.000, 36.000],  loss: 0.515083, mae: 2.211415, mean_q: 2.361960\n",
      " 44510/50000: episode: 579, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 16.950 [0.000, 36.000],  loss: 0.514114, mae: 2.208686, mean_q: 2.360749\n",
      " 44545/50000: episode: 580, duration: 0.448s, episode steps:  35, steps per second:  78, episode reward: 10.000, mean reward:  0.286 [-1.000,  1.000], mean action: 24.314 [2.000, 37.000],  loss: 0.505443, mae: 2.208272, mean_q: 2.362767\n",
      " 44645/50000: episode: 581, duration: 1.253s, episode steps: 100, steps per second:  80, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 17.330 [0.000, 36.000],  loss: 0.722255, mae: 2.207519, mean_q: 2.358479\n",
      " 44745/50000: episode: 582, duration: 1.249s, episode steps: 100, steps per second:  80, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 18.230 [1.000, 36.000],  loss: 0.714932, mae: 2.206893, mean_q: 2.362901\n",
      " 44845/50000: episode: 583, duration: 1.253s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.170 [1.000, 36.000],  loss: 0.509260, mae: 2.205319, mean_q: 2.360888\n",
      " 44870/50000: episode: 584, duration: 0.330s, episode steps:  25, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 19.840 [0.000, 37.000],  loss: 2.151114, mae: 2.206654, mean_q: 2.358178\n",
      " 44970/50000: episode: 585, duration: 1.254s, episode steps: 100, steps per second:  80, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.840 [0.000, 36.000],  loss: 0.721480, mae: 2.203277, mean_q: 2.353765\n",
      " 45070/50000: episode: 586, duration: 1.254s, episode steps: 100, steps per second:  80, episode reward: 18.000, mean reward:  0.180 [-1.000,  1.000], mean action: 19.580 [0.000, 36.000],  loss: 0.725488, mae: 2.199858, mean_q: 2.348888\n",
      " 45170/50000: episode: 587, duration: 1.252s, episode steps: 100, steps per second:  80, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.900 [1.000, 36.000],  loss: 0.921192, mae: 2.198665, mean_q: 2.352767\n",
      " 45270/50000: episode: 588, duration: 1.254s, episode steps: 100, steps per second:  80, episode reward: 22.000, mean reward:  0.220 [-1.000,  1.000], mean action: 19.100 [0.000, 36.000],  loss: 0.516920, mae: 2.195510, mean_q: 2.354459\n",
      " 45370/50000: episode: 589, duration: 1.255s, episode steps: 100, steps per second:  80, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.480 [0.000, 36.000],  loss: 0.521511, mae: 2.191921, mean_q: 2.355347\n",
      " 45470/50000: episode: 590, duration: 1.253s, episode steps: 100, steps per second:  80, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 17.970 [0.000, 36.000],  loss: 0.714394, mae: 2.189987, mean_q: 2.338389\n",
      " 45570/50000: episode: 591, duration: 1.257s, episode steps: 100, steps per second:  80, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 18.640 [0.000, 36.000],  loss: 0.719510, mae: 2.188010, mean_q: 2.337141\n",
      " 45670/50000: episode: 592, duration: 1.265s, episode steps: 100, steps per second:  79, episode reward: -32.000, mean reward: -0.320 [-1.000,  1.000], mean action: 17.230 [0.000, 36.000],  loss: 0.506935, mae: 2.186055, mean_q: 2.340670\n",
      " 45770/50000: episode: 593, duration: 1.250s, episode steps: 100, steps per second:  80, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 18.120 [0.000, 36.000],  loss: 0.717431, mae: 2.186371, mean_q: 2.336961\n",
      " 45870/50000: episode: 594, duration: 1.256s, episode steps: 100, steps per second:  80, episode reward: 41.000, mean reward:  0.410 [-1.000, 36.000], mean action: 18.010 [0.000, 35.000],  loss: 0.720073, mae: 2.186903, mean_q: 2.343586\n",
      " 45891/50000: episode: 595, duration: 0.282s, episode steps:  21, steps per second:  74, episode reward: -2.000, mean reward: -0.095 [-1.000,  1.000], mean action: 16.524 [0.000, 37.000],  loss: 1.507028, mae: 2.187799, mean_q: 2.343673\n",
      " 45991/50000: episode: 596, duration: 1.257s, episode steps: 100, steps per second:  80, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 17.650 [0.000, 36.000],  loss: 0.714942, mae: 2.186424, mean_q: 2.346239\n",
      " 46091/50000: episode: 597, duration: 1.250s, episode steps: 100, steps per second:  80, episode reward: 10.000, mean reward:  0.100 [-1.000,  1.000], mean action: 18.120 [0.000, 36.000],  loss: 0.712730, mae: 2.186708, mean_q: 2.343005\n",
      " 46191/50000: episode: 598, duration: 1.260s, episode steps: 100, steps per second:  79, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 19.360 [0.000, 36.000],  loss: 1.127507, mae: 2.186044, mean_q: 2.339028\n",
      " 46291/50000: episode: 599, duration: 1.265s, episode steps: 100, steps per second:  79, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 19.390 [0.000, 36.000],  loss: 0.721592, mae: 2.184350, mean_q: 2.341956\n",
      " 46391/50000: episode: 600, duration: 1.258s, episode steps: 100, steps per second:  79, episode reward: 45.000, mean reward:  0.450 [-1.000, 36.000], mean action: 16.920 [0.000, 36.000],  loss: 1.126682, mae: 2.182673, mean_q: 2.341637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46491/50000: episode: 601, duration: 1.259s, episode steps: 100, steps per second:  79, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 17.670 [0.000, 36.000],  loss: 0.517170, mae: 2.181828, mean_q: 2.334739\n",
      " 46591/50000: episode: 602, duration: 1.254s, episode steps: 100, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 17.310 [0.000, 36.000],  loss: 0.922595, mae: 2.184266, mean_q: 2.334244\n",
      " 46691/50000: episode: 603, duration: 1.262s, episode steps: 100, steps per second:  79, episode reward: -26.000, mean reward: -0.260 [-1.000,  1.000], mean action: 20.050 [0.000, 36.000],  loss: 0.920961, mae: 2.184155, mean_q: 2.332712\n",
      " 46791/50000: episode: 604, duration: 1.260s, episode steps: 100, steps per second:  79, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 18.990 [0.000, 36.000],  loss: 0.724636, mae: 2.185110, mean_q: 2.337565\n",
      " 46891/50000: episode: 605, duration: 1.256s, episode steps: 100, steps per second:  80, episode reward: -18.000, mean reward: -0.180 [-1.000,  1.000], mean action: 17.790 [0.000, 36.000],  loss: 0.509903, mae: 2.184183, mean_q: 2.337180\n",
      " 46991/50000: episode: 606, duration: 1.253s, episode steps: 100, steps per second:  80, episode reward:  0.000, mean reward:  0.000 [-1.000,  1.000], mean action: 18.350 [0.000, 36.000],  loss: 1.130272, mae: 2.185095, mean_q: 2.335333\n",
      " 47091/50000: episode: 607, duration: 1.257s, episode steps: 100, steps per second:  80, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.610 [0.000, 35.000],  loss: 0.710209, mae: 2.184608, mean_q: 2.335062\n",
      " 47101/50000: episode: 608, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: -7.000, mean reward: -0.700 [-1.000,  1.000], mean action: 19.500 [2.000, 37.000],  loss: 0.506221, mae: 2.184970, mean_q: 2.336866\n",
      " 47201/50000: episode: 609, duration: 1.258s, episode steps: 100, steps per second:  79, episode reward:  4.000, mean reward:  0.040 [-1.000,  1.000], mean action: 17.940 [0.000, 36.000],  loss: 0.517408, mae: 2.184142, mean_q: 2.340773\n",
      " 47301/50000: episode: 610, duration: 1.257s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 17.720 [0.000, 36.000],  loss: 0.920547, mae: 2.184674, mean_q: 2.345240\n",
      " 47401/50000: episode: 611, duration: 1.260s, episode steps: 100, steps per second:  79, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.720 [0.000, 36.000],  loss: 0.931430, mae: 2.184693, mean_q: 2.345957\n",
      " 47501/50000: episode: 612, duration: 1.266s, episode steps: 100, steps per second:  79, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.960 [0.000, 36.000],  loss: 0.935838, mae: 2.184013, mean_q: 2.347937\n",
      " 47601/50000: episode: 613, duration: 1.262s, episode steps: 100, steps per second:  79, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 17.330 [0.000, 36.000],  loss: 0.930416, mae: 2.184447, mean_q: 2.351689\n",
      " 47701/50000: episode: 614, duration: 1.270s, episode steps: 100, steps per second:  79, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 17.840 [0.000, 36.000],  loss: 0.516720, mae: 2.183728, mean_q: 2.347348\n",
      " 47801/50000: episode: 615, duration: 1.262s, episode steps: 100, steps per second:  79, episode reward: -6.000, mean reward: -0.060 [-1.000,  1.000], mean action: 18.240 [0.000, 36.000],  loss: 0.917124, mae: 2.183557, mean_q: 2.337152\n",
      " 47901/50000: episode: 616, duration: 1.261s, episode steps: 100, steps per second:  79, episode reward: -16.000, mean reward: -0.160 [-1.000,  1.000], mean action: 18.650 [0.000, 36.000],  loss: 0.525797, mae: 2.181549, mean_q: 2.330793\n",
      " 48001/50000: episode: 617, duration: 1.256s, episode steps: 100, steps per second:  80, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 20.420 [2.000, 36.000],  loss: 0.516869, mae: 2.178366, mean_q: 2.340216\n",
      " 48021/50000: episode: 618, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: -3.000, mean reward: -0.150 [-1.000,  1.000], mean action: 21.050 [0.000, 37.000],  loss: 0.534534, mae: 2.178353, mean_q: 2.342321\n",
      " 48032/50000: episode: 619, duration: 0.150s, episode steps:  11, steps per second:  74, episode reward: -2.000, mean reward: -0.182 [-1.000,  1.000], mean action: 20.364 [1.000, 37.000],  loss: 0.495731, mae: 2.177306, mean_q: 2.342069\n",
      " 48077/50000: episode: 620, duration: 0.577s, episode steps:  45, steps per second:  78, episode reward: -8.000, mean reward: -0.178 [-1.000,  1.000], mean action: 19.800 [0.000, 37.000],  loss: 0.527186, mae: 2.177739, mean_q: 2.346549\n",
      " 48177/50000: episode: 621, duration: 1.321s, episode steps: 100, steps per second:  76, episode reward: -24.000, mean reward: -0.240 [-1.000,  1.000], mean action: 17.770 [0.000, 35.000],  loss: 1.127908, mae: 2.177906, mean_q: 2.351573\n",
      " 48205/50000: episode: 622, duration: 0.399s, episode steps:  28, steps per second:  70, episode reward:  5.000, mean reward:  0.179 [-1.000,  1.000], mean action: 17.321 [0.000, 37.000],  loss: 0.513502, mae: 2.177613, mean_q: 2.350267\n",
      " 48305/50000: episode: 623, duration: 1.335s, episode steps: 100, steps per second:  75, episode reward: -28.000, mean reward: -0.280 [-1.000,  1.000], mean action: 18.080 [0.000, 36.000],  loss: 0.719836, mae: 2.178017, mean_q: 2.352239\n",
      " 48405/50000: episode: 624, duration: 1.281s, episode steps: 100, steps per second:  78, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 18.270 [0.000, 36.000],  loss: 0.510021, mae: 2.175946, mean_q: 2.338956\n",
      " 48505/50000: episode: 625, duration: 1.380s, episode steps: 100, steps per second:  72, episode reward:  6.000, mean reward:  0.060 [-1.000,  1.000], mean action: 17.940 [0.000, 36.000],  loss: 0.529830, mae: 2.175670, mean_q: 2.329143\n",
      " 48605/50000: episode: 626, duration: 1.330s, episode steps: 100, steps per second:  75, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 16.280 [0.000, 36.000],  loss: 0.721386, mae: 2.173336, mean_q: 2.316646\n",
      " 48705/50000: episode: 627, duration: 1.293s, episode steps: 100, steps per second:  77, episode reward:  8.000, mean reward:  0.080 [-1.000,  1.000], mean action: 19.610 [0.000, 36.000],  loss: 0.718416, mae: 2.172972, mean_q: 2.320126\n",
      " 48805/50000: episode: 628, duration: 1.268s, episode steps: 100, steps per second:  79, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 19.510 [0.000, 36.000],  loss: 0.720912, mae: 2.171060, mean_q: 2.320854\n",
      " 48905/50000: episode: 629, duration: 1.264s, episode steps: 100, steps per second:  79, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 19.980 [0.000, 36.000],  loss: 0.934008, mae: 2.169825, mean_q: 2.311339\n",
      " 48917/50000: episode: 630, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward:  1.000, mean reward:  0.083 [-1.000,  1.000], mean action: 19.333 [1.000, 37.000],  loss: 0.491948, mae: 2.168921, mean_q: 2.305874\n",
      " 49017/50000: episode: 631, duration: 1.271s, episode steps: 100, steps per second:  79, episode reward: -4.000, mean reward: -0.040 [-1.000,  1.000], mean action: 18.510 [0.000, 36.000],  loss: 0.732003, mae: 2.169209, mean_q: 2.306136\n",
      " 49117/50000: episode: 632, duration: 1.264s, episode steps: 100, steps per second:  79, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 18.310 [0.000, 36.000],  loss: 0.930954, mae: 2.170057, mean_q: 2.303558\n",
      " 49217/50000: episode: 633, duration: 1.265s, episode steps: 100, steps per second:  79, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 18.800 [0.000, 36.000],  loss: 0.514776, mae: 2.166441, mean_q: 2.314924\n",
      " 49317/50000: episode: 634, duration: 1.263s, episode steps: 100, steps per second:  79, episode reward: -2.000, mean reward: -0.020 [-1.000,  1.000], mean action: 18.560 [0.000, 36.000],  loss: 0.521117, mae: 2.164404, mean_q: 2.325901\n",
      " 49417/50000: episode: 635, duration: 1.272s, episode steps: 100, steps per second:  79, episode reward: -10.000, mean reward: -0.100 [-1.000,  1.000], mean action: 16.920 [0.000, 36.000],  loss: 0.510918, mae: 2.163691, mean_q: 2.341661\n",
      " 49425/50000: episode: 636, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: -3.000, mean reward: -0.375 [-1.000,  1.000], mean action: 17.500 [3.000, 37.000],  loss: 0.521210, mae: 2.162884, mean_q: 2.355396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49525/50000: episode: 637, duration: 1.270s, episode steps: 100, steps per second:  79, episode reward:  2.000, mean reward:  0.020 [-1.000,  1.000], mean action: 16.660 [0.000, 36.000],  loss: 0.514085, mae: 2.163467, mean_q: 2.359917\n",
      " 49602/50000: episode: 638, duration: 0.975s, episode steps:  77, steps per second:  79, episode reward:  2.000, mean reward:  0.026 [-1.000,  1.000], mean action: 18.234 [0.000, 37.000],  loss: 0.774597, mae: 2.165223, mean_q: 2.361362\n",
      " 49702/50000: episode: 639, duration: 1.274s, episode steps: 100, steps per second:  78, episode reward: -8.000, mean reward: -0.080 [-1.000,  1.000], mean action: 17.570 [0.000, 36.000],  loss: 0.925892, mae: 2.166768, mean_q: 2.366888\n",
      " 49802/50000: episode: 640, duration: 1.275s, episode steps: 100, steps per second:  78, episode reward: -22.000, mean reward: -0.220 [-1.000,  1.000], mean action: 19.370 [0.000, 36.000],  loss: 0.921249, mae: 2.169911, mean_q: 2.357515\n",
      " 49902/50000: episode: 641, duration: 1.266s, episode steps: 100, steps per second:  79, episode reward: -14.000, mean reward: -0.140 [-1.000,  1.000], mean action: 16.680 [0.000, 36.000],  loss: 0.727199, mae: 2.172124, mean_q: 2.342054\n",
      " 49999/50000: episode: 642, duration: 1.239s, episode steps:  97, steps per second:  78, episode reward:  4.000, mean reward:  0.041 [-1.000,  1.000], mean action: 17.629 [0.000, 37.000],  loss: 0.728927, mae: 2.173065, mean_q: 2.338868\n",
      "done, took 530.843 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe699fbfca0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a821403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6863/2386494182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on_action_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just raise an exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
