{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1013c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Print the list of available OpenAI Gym environments\n",
    "print(\"List of available OpenAI Gym environments.\\n\")\n",
    "for i in gym.envs.registry.all():\n",
    "\tprint(str(i)[8:-1])\n",
    "\n",
    "# Get the environment name and number of episodes to run\n",
    "string = input(\"\\nEnter the name of the environment: \")\n",
    "episodes = int(input(\"Enter the number of episodes to run: \"))\n",
    "\n",
    "# Create the environment and reset it to the initial state\n",
    "env = gym.make(string)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "if type(env.action_space)==gym.spaces.box.Box:\n",
    "\tprint(\"Cannot train RL agent for environments with Box() space.\")\n",
    "\texit(0)\n",
    "\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c4f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Complex Neural Network for DQN, SARSA\n",
    "CD_model = Sequential()\n",
    "CD_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(16))\n",
    "CD_model.add(Activation('relu'))\n",
    "CD_model.add(Dense(nb_actions))\n",
    "CD_model.add(Activation('linear'))\n",
    "\n",
    "# Boltzmann Q Policy\n",
    "BQ_policy = BoltzmannQPolicy()\n",
    "\n",
    "# Sequential Memory\n",
    "S_memory = SequentialMemory(limit=100000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09a3fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "    57/50000: episode: 1, duration: 0.133s, episode steps:  57, steps per second: 427, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
      "    68/50000: episode: 2, duration: 0.008s, episode steps:  11, steps per second: 1411, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n",
      "    85/50000: episode: 3, duration: 0.011s, episode steps:  17, steps per second: 1576, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   112/50000: episode: 4, duration: 0.998s, episode steps:  27, steps per second:  27, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.499148, mae: 0.520363, mean_q: 0.057240\n",
      "   124/50000: episode: 5, duration: 0.053s, episode steps:  12, steps per second: 227, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.420611, mae: 0.511147, mean_q: 0.147164\n",
      "   179/50000: episode: 6, duration: 0.233s, episode steps:  55, steps per second: 236, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.212256, mae: 0.552020, mean_q: 0.560593\n",
      "   194/50000: episode: 7, duration: 0.066s, episode steps:  15, steps per second: 228, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.047068, mae: 0.659355, mean_q: 1.118700\n",
      "   208/50000: episode: 8, duration: 0.061s, episode steps:  14, steps per second: 229, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.033870, mae: 0.724988, mean_q: 1.286213\n",
      "   225/50000: episode: 9, duration: 0.074s, episode steps:  17, steps per second: 228, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.025410, mae: 0.770934, mean_q: 1.411446\n",
      "   251/50000: episode: 10, duration: 0.121s, episode steps:  26, steps per second: 214, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.021875, mae: 0.835094, mean_q: 1.548760\n",
      "   263/50000: episode: 11, duration: 0.055s, episode steps:  12, steps per second: 218, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.014983, mae: 0.866469, mean_q: 1.646748\n",
      "   284/50000: episode: 12, duration: 0.127s, episode steps:  21, steps per second: 166, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.012842, mae: 0.923616, mean_q: 1.799353\n",
      "   306/50000: episode: 13, duration: 0.130s, episode steps:  22, steps per second: 169, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.023362, mae: 0.999442, mean_q: 1.937365\n",
      "   330/50000: episode: 14, duration: 0.135s, episode steps:  24, steps per second: 177, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.032724, mae: 1.090506, mean_q: 2.107543\n",
      "   344/50000: episode: 15, duration: 0.084s, episode steps:  14, steps per second: 166, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.027048, mae: 1.153270, mean_q: 2.249196\n",
      "   372/50000: episode: 16, duration: 0.164s, episode steps:  28, steps per second: 171, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.031196, mae: 1.232590, mean_q: 2.392109\n",
      "   387/50000: episode: 17, duration: 0.089s, episode steps:  15, steps per second: 169, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.056921, mae: 1.342344, mean_q: 2.587912\n",
      "   407/50000: episode: 18, duration: 0.124s, episode steps:  20, steps per second: 162, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.050791, mae: 1.405364, mean_q: 2.719192\n",
      "   426/50000: episode: 19, duration: 0.109s, episode steps:  19, steps per second: 175, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.061088, mae: 1.491754, mean_q: 2.886817\n",
      "   459/50000: episode: 20, duration: 0.189s, episode steps:  33, steps per second: 174, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.080818, mae: 1.606813, mean_q: 3.081262\n",
      "   476/50000: episode: 21, duration: 0.109s, episode steps:  17, steps per second: 157, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.099663, mae: 1.707897, mean_q: 3.271519\n",
      "   509/50000: episode: 22, duration: 0.146s, episode steps:  33, steps per second: 226, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.102815, mae: 1.802690, mean_q: 3.459315\n",
      "   527/50000: episode: 23, duration: 0.085s, episode steps:  18, steps per second: 212, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.110462, mae: 1.930643, mean_q: 3.693413\n",
      "   549/50000: episode: 24, duration: 0.112s, episode steps:  22, steps per second: 196, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.145371, mae: 1.989225, mean_q: 3.781041\n",
      "   559/50000: episode: 25, duration: 0.048s, episode steps:  10, steps per second: 209, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.123666, mae: 2.039121, mean_q: 3.880592\n",
      "   582/50000: episode: 26, duration: 0.106s, episode steps:  23, steps per second: 216, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.157850, mae: 2.149338, mean_q: 4.088155\n",
      "   599/50000: episode: 27, duration: 0.074s, episode steps:  17, steps per second: 229, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.132013, mae: 2.227667, mean_q: 4.260202\n",
      "   619/50000: episode: 28, duration: 0.087s, episode steps:  20, steps per second: 231, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.148570, mae: 2.291220, mean_q: 4.369255\n",
      "   637/50000: episode: 29, duration: 0.082s, episode steps:  18, steps per second: 220, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.153287, mae: 2.357925, mean_q: 4.537370\n",
      "   651/50000: episode: 30, duration: 0.063s, episode steps:  14, steps per second: 223, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.212787, mae: 2.434177, mean_q: 4.605353\n",
      "   664/50000: episode: 31, duration: 0.059s, episode steps:  13, steps per second: 220, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.207171, mae: 2.489440, mean_q: 4.719368\n",
      "   679/50000: episode: 32, duration: 0.065s, episode steps:  15, steps per second: 229, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.235512, mae: 2.541510, mean_q: 4.788955\n",
      "   694/50000: episode: 33, duration: 0.071s, episode steps:  15, steps per second: 211, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.145570, mae: 2.633010, mean_q: 5.072073\n",
      "   715/50000: episode: 34, duration: 0.091s, episode steps:  21, steps per second: 232, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.173242, mae: 2.659182, mean_q: 5.063450\n",
      "   749/50000: episode: 35, duration: 0.169s, episode steps:  34, steps per second: 201, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.237289, mae: 2.775543, mean_q: 5.268679\n",
      "   769/50000: episode: 36, duration: 0.123s, episode steps:  20, steps per second: 163, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.141306, mae: 2.888248, mean_q: 5.557349\n",
      "   804/50000: episode: 37, duration: 0.203s, episode steps:  35, steps per second: 173, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 0.199802, mae: 2.983244, mean_q: 5.700805\n",
      "   831/50000: episode: 38, duration: 0.137s, episode steps:  27, steps per second: 197, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 0.213495, mae: 3.109656, mean_q: 5.978713\n",
      "   868/50000: episode: 39, duration: 0.230s, episode steps:  37, steps per second: 161, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 0.241448, mae: 3.222308, mean_q: 6.173504\n",
      "   881/50000: episode: 40, duration: 0.081s, episode steps:  13, steps per second: 161, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.201616, mae: 3.293728, mean_q: 6.380579\n",
      "   893/50000: episode: 41, duration: 0.089s, episode steps:  12, steps per second: 134, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.191815, mae: 3.320698, mean_q: 6.412539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   919/50000: episode: 42, duration: 0.149s, episode steps:  26, steps per second: 175, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.321444, mae: 3.388262, mean_q: 6.468228\n",
      "   947/50000: episode: 43, duration: 0.147s, episode steps:  28, steps per second: 190, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.161534, mae: 3.510058, mean_q: 6.839166\n",
      "   963/50000: episode: 44, duration: 0.076s, episode steps:  16, steps per second: 209, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.204501, mae: 3.562030, mean_q: 6.946006\n",
      "   997/50000: episode: 45, duration: 0.153s, episode steps:  34, steps per second: 222, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.173304, mae: 3.672399, mean_q: 7.203074\n",
      "  1047/50000: episode: 46, duration: 0.224s, episode steps:  50, steps per second: 223, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.243502, mae: 3.839982, mean_q: 7.504389\n",
      "  1066/50000: episode: 47, duration: 0.097s, episode steps:  19, steps per second: 196, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.165691, mae: 3.957451, mean_q: 7.845828\n",
      "  1102/50000: episode: 48, duration: 0.193s, episode steps:  36, steps per second: 186, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.226615, mae: 4.104570, mean_q: 8.144935\n",
      "  1130/50000: episode: 49, duration: 0.152s, episode steps:  28, steps per second: 184, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.200117, mae: 4.228742, mean_q: 8.367531\n",
      "  1141/50000: episode: 50, duration: 0.067s, episode steps:  11, steps per second: 163, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.299803, mae: 4.318218, mean_q: 8.503477\n",
      "  1171/50000: episode: 51, duration: 0.144s, episode steps:  30, steps per second: 208, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.414255, mae: 4.320007, mean_q: 8.458225\n",
      "  1187/50000: episode: 52, duration: 0.071s, episode steps:  16, steps per second: 226, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.346778, mae: 4.549673, mean_q: 8.909706\n",
      "  1210/50000: episode: 53, duration: 0.103s, episode steps:  23, steps per second: 223, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.291954, mae: 4.628362, mean_q: 9.151997\n",
      "  1281/50000: episode: 54, duration: 0.348s, episode steps:  71, steps per second: 204, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.288833, mae: 4.735557, mean_q: 9.445501\n",
      "  1346/50000: episode: 55, duration: 0.313s, episode steps:  65, steps per second: 208, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.316230, mae: 5.032713, mean_q: 10.061161\n",
      "  1476/50000: episode: 56, duration: 0.611s, episode steps: 130, steps per second: 213, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.405145, mae: 5.458882, mean_q: 10.935772\n",
      "  1558/50000: episode: 57, duration: 0.426s, episode steps:  82, steps per second: 192, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.457738, mae: 5.879632, mean_q: 11.871859\n",
      "  1750/50000: episode: 58, duration: 0.997s, episode steps: 192, steps per second: 193, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 0.456965, mae: 6.548290, mean_q: 13.311729\n",
      "  2014/50000: episode: 59, duration: 1.359s, episode steps: 264, steps per second: 194, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.587145, mae: 7.610445, mean_q: 15.550130\n",
      "  2204/50000: episode: 60, duration: 1.073s, episode steps: 190, steps per second: 177, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.605299, mae: 8.790365, mean_q: 17.950294\n",
      "  2354/50000: episode: 61, duration: 0.805s, episode steps: 150, steps per second: 186, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.698178, mae: 9.598560, mean_q: 19.585508\n",
      "done, took 13.009 seconds\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "name = f'dqn_{string}_params.h5f'\n",
    "dqn = DQNAgent(model=CD_model, nb_actions=nb_actions, memory=S_memory, nb_steps_warmup=100, target_model_update=1e-2, policy=BQ_policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "dqn.save_weights(name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f69ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 212.000, steps: 212\n",
      "Episode 2: reward: 201.000, steps: 201\n",
      "Episode 3: reward: 172.000, steps: 172\n",
      "Episode 4: reward: 491.000, steps: 491\n",
      "Episode 5: reward: 186.000, steps: 186\n",
      "Episode 6: reward: 261.000, steps: 261\n",
      "Episode 7: reward: 412.000, steps: 412\n",
      "Episode 8: reward: 300.000, steps: 300\n",
      "Episode 9: reward: 169.000, steps: 169\n",
      "Episode 10: reward: 220.000, steps: 220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8bc6429a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=episodes, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
